# Be a QualTRIXter {#qualTRIX}

[Screencasted Lecture Link](https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=422daef0-a158-48b3-a69d-ad9a00202939) 
 
```{r  include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA) #keeps out the hashtags in the knits
options(scipen=999)#eliminates scientific notation
```

The focus of this lecture is on the technical and mechanical tools available in Qualtrics (and likely other survey platforms) to increase the effectiveness of your survey.

## Navigating this Lesson

This lecture is just under one hour.  Plan for another 30 minutes for *intRavenous qualtRics* practice.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail.  All original materials are provided at the [Github site](https://github.com/lhbikos/ReC_Psychometrics) that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's [introduction](#ReCintro)

### Learning Objectives

Focusing on this week's materials, make sure you can: 

* Utilize basic Qualtrics tools (e.g,. question type, use of headers) so that surveys are present materials clearly to the respondent. 
* Incorporate more advanced tools (e.g., display logic, randomization) that may increase the respondent's ability to complete the survey and provide accurate responses.
* Provide a rationale for survey options that protect (or possibly reveal) an individual's identity.

### Planning for Practice

This is the second of a two-part lesson on questionnaire construction. At the end of this lesson is a detailed suggestion for practice that lists criteria for creating and piloting a survey of your own. There are four essential criteria for your survey:

* Adhere to the evidence-based practices identified in the lesson on [questionnaire construction](#QuestCon).
* Utilize four techniques (in the context of Qualtrics, I term these *qualTRIXter skills*) that increase the flow, effectiveness, and appearance of your survey.
* Pilot and consider feedback provided by those who took the survey.
* Import the data into the R environment.

### Readings & Resources

In preparing this chapter, I drew heavily from the tutorials available at the [Qualtrics support site](https://www.qualtrics.com/support/). I have tried to link them throughout the presentation. It is likely they could change at any time and/or they might not work on your particular browser (as I write this, half of them will not work on FireFox, but they do on Chrome and Edge).

### Packages

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.
```{r }
#will install the package if not already installed
#if(!require(qualtRics)){install.packages("qualtRics")}
```

## Research Vignette

I will demonstrate the qual"TRIX" by using a Qualtrics account at my institution, Seattle Pacific University. The only surveys in this account are for the *Recentering Psych Stats* chapters and lessons. All surveys are designed to not capture personally identifying information and not collecting IP addresses nor longitude/latitude. I use this survey in several lessons in this OER. If you haven't taken the survey yet, [I invite you to do so, now](https://spupsych.az1.qualtrics.com/jfe/form/SV_b2cClqAlLGQ6nLU). 

As a teaching activity for the ReCentering Psych Stats OER, the topic of the survey was selected to be consistent with the overall theme of OER. Specifically, the purpose of this study is to understand the campus climate for students whose identities make them vulnerable to bias and discrimination. These include  students who are Black, non-Black students of color, LGBTQ+ students, international students, and students with disabilities. 

After consulting with a diverse group of stakeholders and subject matter experts (and revising the response options numerous times) I have attempted to center anti-Black racism in the U.S. [@mosley_critical_2021; @mosley_radical_2020; @singh_building_2020].  In fact, the display logic does not present the race items when the course is offered outside the U.S. There are only five options for race: *biracial/multiracial*, *Black*, *non-Black person(s) of color*, *White*, and *I did not notice* (intended to capture a color-blind response). One unintended negative consequence of this design is that the response options could contribute to *colorism* [@adames_fallacy_2021; @capielo_rosario_acculturation_2019]. Another possibility is that the limited options may erase, or make invisible, other identities. At the time that I wrote up the first description of this survey, the murder of six Asian American women in Atlanta had just occurred. The Center for the Study of Hate and Extremeism has documented that while overall hate drimes dropped by 7% in 2020, anti-Asian hate crimes reported to the police in America's largest cities increased by 149% [@noauthor_fact_nodate]. These incidents have occurred not only in cities, but in our neighborhoods and on our campusus [@kim_guest_2021; @kim_yes_2021; @noauthor_stop_nodate]. While this survey is intended to assess campus climate as a function of race, it unfortunately does not distinguish between many identities that experience marginalization. 

Although the dataset should provide the opportunity to test a number of statistical models, one working hypothesis that framed the study is that the there will be a greater sense of belonging and less bias and discrimination when there is similar representation (of identities that are often marginalized) in the instructional faculty and student body. Termed,  "structural diversity" [@lewis_black_2019] this is likely an oversimplification.  In fact, an increase in diverse representation without attention to interacting factors can increase hostility on campus [@hurtado_linking_2007]. Thus, the task of rating of a single course relates to the larger campus along the dimensions of belonging and bias/discrimination. For example, if a single class has higher ratings on issues of inclusivity, diversity, and respect, we would expect that sentiment to be echoed in the broader institution.

The survey design has notable limitations  You will likely notice that we ask about demographic characteristics of the instructional staff and classmates in the course rated, but we do not ask about the demographic characteristics of the respondent. In making this decision, we likely lose important information. For example, Iacovino and James [-@iacovino_retaining_2016] have noted that White students perceive campus more favorably than Black student counterparts. 

The decision to not collect demographic details about the respondent was about protecting their (your) identity. As you will see, you have the opportunity to download and analyze the data. If a faculty member asked an entire class to take the survey, the datestamp and a handful of demographic identifiers could very likely identify a student. In certain circumstances, this might be risky in that private information (i.e., gender nonconformity, disclosure of a disability) along with course evaluation data and a datestamp could be related back to the student.  

Further, the items that ask respondents to *guess* the identities of the instructional staff and classmates are limited, and contrary to best practices in survey construction that recommend providing the option of a "write-in" a response.  

In parallel, the items asking respondents to identity characteristics of the instructional staff along dimensions of gender, international status, and disability are "large buckets" and do not include "write-in" options. Similarly, there was no intent to cause harm by erasing or making invisible individuals whose identities are better defined by different descriptors.  Further, no write-in items were allowed.  This was also intentional to prevent potential harm caused by people who could leave inappropriate, racist, or otherwise harmful comments. 

As I review Qualtrics essentials and trix, I will their use (if used) in the ReCentering Psych Stats survey.

## Qualtrics Essentials

Qualtrics is a powerful program and I find that many of the surveys we distribute don’t capitalize on the features Qualtrics has to offer.  Qualtrics has detailed tutorials and instructions that are well worth the investment of a weekend to review them. 

In this lecture I will point you to the elements that I think are critical to constructing online surveys. Because Qualtrics tutorials are (a) clear and thorough and (b) frequently updated, I will (a)  point you to the tutorials that are available at the time of this lecture prep, (b) tell you why I think they are appropriate, and (c) show you how we have used them in some of our own surveys.  

Even if you think you know what you are doing, start here (and then always take the time to "look around" at all the options on each window):  

**Survey Basic Overview**: The link below will give you an overview.  From there, you can follow all kinds of leads, looking for things you want to do with your survey – and getting ideas for what will improve it. 
https://www.qualtrics.com/support/survey-platform/survey-module/survey-module-overview/ 

[**Blocks**](https://www.qualtrics.com/support/survey-platform/survey-module/block-options/block-options-overview/) are the basic organizational tool in Qualtrics surveys.  Blocks have two purposes: (a) grouping items shown on "one page," and (b) using the block for easy ordering and/or random selection/presentation.  

[**Question types**](https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/question-types-guide/question-types-overview/):  Take a time to look at ALL the options.  You might be surprised to learn that there is a better choice than you might have imagined.  

Let's take a look at super basic/helpful question types:

* [**Text/graphic**](https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/question-types-guide/static-content/descriptive-text-and-graphic/): These are the types you should use for providing information (e.g., informed consent) to the participants.  
* [**Matrix table**](https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/question-types-guide/standard-content/matrix-table/) :  A more efficient way to use the Likert-style items (than multiple choice).  There is some controversy about whether not to use matrix tables vs. multiple choice dropdowns...    
  + Make sure to select a reasonable amount of header repetition.  This allows the respondent the maximum opportunity to see the column descriptors while they are responding.
* [**Slider**](https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/question-types-guide/standard-content/slider/) : This gets you continuous data on that 1 to 100 scale.  If the scale you are using is already published, and has not been psychometrically evaluated for slider use, you should probably stick with the format recommended in the publication.  But if YOU are designing a survey, think about this option. 
* [**Text Entry Questions**](https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/question-types-guide/standard-content/text-entry/)  have multiple options for length of answer.  Don't miss the options that include forms and content validation   
* [**Validation**](https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/validation/): Allows the user to allow certain types of information and specify their formats (e.g., numbers, e-mail addresses, dates). There is a balancing between being overly restricting and ensuring that the data is entered in the most clear and consistent way possible. A validation option I frequently use is one that asks individuals if they intended to leave something blank. This is tool that helps prevent missingness without forcing an individual to respond to an item that (a) might not be clear to them, (b) might not be appropriate or them, and/or (c) might result in an answer that is untrue for their unique circumstance.

## Qual-TRIX

[**Collaborating**](https://www.qualtrics.com/support/survey-platform/my-projects/sharing-a-project/) with other Qualtrics users in your institution is easy!
Scroll down to “Collaborating Inside Your Organization” and follow the instructions for adding individuals to your survey (you must “own” the survey…your collaborators will not be able to add others).
 
The ability to **schedule survey distributions** is like having your very own GA! If you have a roster(contact list) you can schedule distributions, reminders, and thank yous. Qualtrics will keep track of who responds and send reminders to the non-responders.  Here are resources for

* [E-mail overview](https://www.qualtrics.com/support/survey-platform/distributions-module/email-distribution/emails/emails-overview/)  
* [E-mail distribution management](https://www.qualtrics.com/support/survey-platform/distributions-module/email-distribution/emails/email-distribution-management/)  
* [Contact lists](https://www.qualtrics.com/support/survey-platform/contacts/creating-a-contact-list/)  
 
**Personalizing** invitations and surveys.  [Piped text](https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/piped-text/piped-text-overview/) is a way to personalize invitations and/or “carry forward” prior responses into new questions.

[**Randomization** of blocks](https://www.qualtrics.com/support/survey-platform/survey-module/survey-flow/standard-elements/randomizer/) (or a subset of blocks) can be use for several purposes such as: (a) using random selection to display one or more blocks to respondents -- as in a random clinical trial, (b) to randomly display a percentage of blocks or items to shorten the survey in a planned missing design, and (c) randomly display some or all of the blocks of the survey to all respondents so that when respondents experience test fatigue, when they quit responding, "the last items/surveys" aren't always the same ones -- thus distributing the missingness across surveys.   

[**Randomization** of items](https://www.qualtrics.com/support/survey-platform/survey-module/block-options/question-randomization/) within a block can be used for similar purposes.  You can also use this to display only some of the items (e.g., planned missingness). 

[**File upload** from respondents](https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/question-types-guide/advanced/file-upload/) is an additional package that requires the institution to pay a higher fee.  If available, this allows respondents to upload some sort of file (photo, powerpoint, .pdf).  We use it for poster contests at professional contests (where students upload their poster for online judging in advance of the conference).  A colleague of mine uses this function to collect application elements (i.e., resumes, cover letters, reference letters) to a fellowship program.  

* As researchers, we can also upload files (e.g., hardcopy of informed consent, documents to be reviewed) for use by the respondent. 

[**Display, Skip, and/or Branch Logic**](https://www.qualtrics.com/support/survey-platform/survey-module/question-options/display-logic/) can be used to help display to respondents *only* the items that pertain to them.  There are multiple approaches to doing this.  Using a display logic approach may feel a bit *backward* where the logic is applied *from* the landing spot.  We did this extensively in as study that involved two language versions and three age options.  

Two other approaches for these issues are [skip logic](https://www.qualtrics.com/support/survey-platform/survey-module/question-options/skip-logic/) and [branch logic](https://www.qualtrics.com/support/survey-platform/survey-module/survey-flow/standard-elements/branch-logic/) 


## Even moRe, particularly relevant to iRb

We can use Qualtrics tools for purposes beyond collecting and downloading data. These tools are especially useful when I think about IRB applications and ethics related to data collection.

[**Exporting to Word**](https://www.qualtrics.com/support/survey-platform/survey-module/survey-tools/import-and-export-surveys/) Helpful for your IRB application (and perhaps in a cloud so that a team can use track changes to edit), it is super simple to export the survey to Microsoft Word.  PLUS!  You have options for including question numbers, recode values, logic, etc., so that it is essentially a codebook.

[**Anonymizing responses**](https://www.qualtrics.com/support/survey-platform/distributions-module/web-distribution/anonymous-link/)  Another step toward an anonymous response is to withhold the IP address.  This is accomplished in the  Survey Options menu.

[**Prevent ballot box stuffing**](https://www.qualtrics.com/support/survey-platform/survey-module/survey-options/survey-protection/#PreventingRespondentsFromTakingYourSurveyMoreThanOnce) Want to make sure that respondents only answer once?  In the same Survey Options window, you can prevent ballot box stuffing.  


Other security options include

* Password protection
* HTTP Referer verification

Look also at:

* **Progress bar** to provide particpants hope (or despair) for "how much longer."
* **Survey termination** to connect cutom endings and thank-you notes.
* [**Partial completion**](https://www.qualtrics.com/support/survey-platform/survey-module/survey-options/partial-completion/) to specify how long the respondent has to complete the survey (after opening it) and whether it is recorded or deleted if it is not completed. 
  +  Related to this, back on the Data & Analysis tab, you can see both #s of [recorded responses and responses in progress](https://www.qualtrics.com/support/survey-platform/data-and-analysis-module/data/responses-in-progress/).  You also have options to manually determine how you want to include/exclude the response in progress. 
  +  ARGHGHGHGHGH!!!!   That grubby little "-->" submit and progress symbol is often the reason that surveys that are > 90% complete aren't counted as "complete."  What to do? Options: (a) don't say "Thanks and goodbye" on a page that has any items, and (b) provide instructions to look for the "-->" symbol to continue.

Finally, **PREVIEW PREVIEW PREVIEW**! There is no better way check your work than with previews.  


## intRavenous Qualtrics

Access credentials for the institutional account, individual user's account, and survey are essential for getting the survey items and/or results to export into R. The Qualtrics website provides a tutorial for [generating an API token](https://www.qualtrics.com/support/integrations/api-integration/overview/#GeneratingAnAPIToken).     

We need two pieces of information:  the **root_url** and an **API token**.  

* Log into your respective qualtrics.com account.
* Select Account Settings
* Choose "Qualtrics IDs" from the user name dropdown


We need the  **root_url**.  This is the first part of the web address for the Qualtrics account.  For our institution it is: spupsych.az1.qualtrics.com 

The API token is in the box labeled, "API." If it is empty, select, "Generate Token." If you do not have this option, locate the *brand administrator* for your Qualtrics account. They will need to set up your account so that you have API privileges.

*BE CAREFUL WITH THE API TOKEN*  This is the key to your Qualtrics accounts.  If you leave it in an .rmd file that you forward to someone else, this key and the base URL gives access to every survey in your account. If you share it, you could be releasing survey data to others that would violate confidentiality promises in an IRB application.

If you mistakenly give out your API token you can generate a new one within your Qualtrics account and re-protect all its contents.

You do need to change the API key/token if you want to download data from a different Qualtrics account.  If your list of surveys generates the wrong set of surveys, restart R, make sure you have the correct API token and try again.

```{r API token, eval=FALSE}
#only have to run this ONCE to draw from the same Qualtrics account...but will need to get different token if you are changing between accounts 
library(qualtRics)
#qualtRics::qualtrics_api_credentials(api_key = "mUgPMySYkiWpMFkwHale1QE5HNmh5LRUaA8d9PDg",
              #base_url = "spupsych.az1.qualtrics.com", overwrite = TRUE, install = TRUE)
```

*all_surveys()* generates a dataframe containing information about all the surveys stored on your Qualtrics account.

```{r Show all surveys, eval=FALSE}
surveys <- all_surveys() 
#View this as an object (found in the right: Environment).  
#Get survey id # for the next command
#If this is showing you the WRONG list of surveys, you are pulling from the wrong Qualtrics account (i.e., maybe this one instead of your own). Go back and change your API token (it saves your old one). Changing the API likely requires a restart of R.
surveys
```

To retrieve the survey, use the *fetch_survey()* function.  

```{r }
#obtained with the survey ID 
#"surveyID" should be the ID from above
#"verbose" prints messages to the R console
#"label", when TRUE, imports data as text responses; if FALSE prints the data as numerical responses
#"convert", when TRUE, attempts to convert certain question types to the "proper" data type in R; because I don't like guessing, I want to set up my own factors.
#"force_request", when TRUE, always downloads the survey from the API instead of from a temporary directory (i.e., it always goes to the primary source)
# "import_id", when TRUE includes the unique Qualtrics-assigned ID; since I have provided labels, I want false

#Out of the blue, I started getting an error, that R couldn't find function "fetch_survey."  After trying a million things, adding qualtRics:: to the front of it solved the problem
QTRX_df <-qualtRics::fetch_survey(surveyID = "SV_b2cClqAlLGQ6nLU", time_zone = NULL, verbose = FALSE, label=FALSE, convert=FALSE, force_request = TRUE, import_id = FALSE)

#useLocalTime = TRUE,
```

The optional script below will let you save the simulated data to your computing environment as either a .csv file (think "Excel lite") or .rds object (preserves any formatting you might do).
```{r}
#write the simulated data  as a .csv
#write.table(QTRX_df, file="QTRX_df.csv", sep=",", col.names=TRUE, row.names=FALSE)
#bring back the simulated dat from a .csv file
#QTRX_df <- read.csv ("QTRX_df.csv", header = TRUE)
```

```{r}
#to save the df as an .rds (think "R object") file on your computer; it should save in the same file as the .rmd file you are working with
#saveRDS(QTRX_df, "QTRX_df.rds")
#bring back the simulated dat from an .rds file
#QTRX_df <- readRDS("QTRX_df.rds")
```


### The Codebook

In order to prepare data from a survey, it is critical to know about its content, scoring directions for scales/subscales, and its design. As I demonstrated above, we can export a [codebook](./Rate-a-Course_Codebook.pdf), that is, a Word (or PDF) version of the survey with all the coding.  In Qualtrics the protocol is: Survey/Tools/ImportExport/Export Survey to Word.  Then select all the options you want (especially "Show Coded Values"). A tutorial provided by Qualtrics can be found [here](https://www.qualtrics.com/support/survey-platform/survey-module/survey-tools/import-and-export-surveys/). This same process can be used to print the PDF example I used above.

I recommend providing custom variable names and recode values directly in Qualtrics before exporting them into R.  A Qualtrics tutorial for this is provided [here](https://www.qualtrics.com/support/survey-platform/survey-module/question-options/recode-values/). In general, consider these qualities when creating variable names:

*	Brevity: historically, SPSS variable names could be a maximum of 8 characters.
*	Intuitive: although variables can be renamed in R (e.g., for use in charts and tables), it is helpful when the name imported from  Qualtrics provides some indication of what the variable is.
*	Systematic: start items in a scale with the same stem, followed by the item number -- ITEM1, ITEM2, ITEM3.

More complete information about data preparation is covered in chapters in the [ReCentering Psych Stats: Multivariate Modeling](https://lhbikos.github.io/ReC_MultivariateModeling/) text.

### Using data from an exported Qualtrics .csv file

The lecture focused on the "intRavenous" import.  It is is also possible to download the Qualtrics data in a variety of formats (e.g., CSV, Excel, SPSS).  Since I got started using files with the CSV extension (think "Excel" lite), that is my preference.

In Qualtrics, these are the steps to download the data:  Projects/YOURsurvey/Data & Analysis/Export & Import/Export data/CSV/Use numeric values

I think that it is critical that to save this file in the same folder as the .rmd file that you will use with the data. 

R is sensitive to characters used filenames  As downloaded, my Qualtrics .csv file had a long name with spaces and symbols that are not allowed.  Therore, I gave it a simple, sensible, filename, "ReC_Download210319.csv".  An idiosyncracy of mine is to datestamp filenames. I use two-digit representations of the year, month, and date so that if the letters preceding the date are the same, the files would alphabetize automatically.

```{r }
#library(qualtRics)
#QTRX_csv <- read_survey("ReC_Download210319.csv", strip_html = TRUE, import_id = FALSE, time_zone=NULL, legacy = FALSE)
```

Although minor tweaking may be required, the same script above should be applicable to this version of the data.  


### Tweaking Data Format
Two general approaches:

1. Inside Qualtrics:  Use the recode values option (found in the item's gearbox, to the left of the block) to specify variable names and recode values.  These should be preserved on the download.   https://www.qualtrics.com/support/survey-platform/survey-module/question-options/recode-values/

2. In the R script: In another lecture I demonstrate how to change the formats of data (character, string), selecting only the variables in which we are interested (e.g., excluding the meta-data), and renaming variables sensibly.

Both work!  Just a preference -- and probably have an explicit process approach within your research team/collaborators.


## Practice Problems
   
The suggestion for practice is to develop a questionnaire, format it, pilot it, and download it. Essentially you will be

* Formatting a survey on Qualtrics using all the best practices identified in the lecture
  + these include having an introductory statement (to include statement of confidentiality), directions for each sub-survey (if more than one), and closing statement.
  + selecting the most appropriate question type for the items.  For example, matrix instead of multiple choice. 
  + within question type, using the appropriate options for proper formatting (e.g., the anchors in a matrix should be topically consistent and equal-interval)
* The survey should include minimum of 3 of the qualTRIXter skills (identified in lecture); choose from
  + establishing collaboration
  + scheduling e-mail distribution and follow-up
  + personalizing the survey in some way
  + randomization of blocks or items
  + integrating display, skip, or branch logic (e.g., having males and females take a different route)
  + exporting the survey to Word
  + recoding variables in the item controls
  + anonymize the responses
  + prevent ballot box stuffing
  + include a progress bar
  + create a custom ending, e-mail, or thank-you note
  + something else that YOU discovered that isn't in the lecture
* Piloting it, getting their feedback, and identifying what problems are (and how you might fix them)
  + with 3 folks from your RVT, cohort, or this class
  + with 3 additional folks who aren't quite as "research savvy"
  + collect their feedback (ideally in a text-item directly on the survey itself) and write a brief summary (3 paragraphs max) of their impressions and how you might improve the survey
* Import the Qualtrics data directly R 
  + preferably, directly from Qualtrics with the API token, base URL, and survey ID
  + alternatively (for the same # of points) from the exported CSV file *via the qualtRics package* (required)

|Assignment Component                    | Points Possible  | Points Earned|
|:-------------------------------------- |:----------------:|:------------:|
|1. Qualtrics survey best practices      |      5           |              |           
|2. QualTRIXter skills (at least 3)      |      5           |              |
|3. Minimum of 6 pilot respondents       |      5           |              |  
|4. Summary of pilot feedback            |      5           |              |               
|5. Import of Qualtrics data into R      |      5           |              |   
|6. Explanation to grader                |      5           |              |       
|**Totals**                              |     20           |              |          



```{r include=FALSE}
sessionInfo()
```




