% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  11pt,
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.14,0.14,0.14}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.43,0.43,0.43}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.43,0.43,0.43}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={ReCentering Psych Stats: Psychometrics},
  pdfauthor={Lynette H. Bikos, PhD, ABPP (she/her)},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}

\title{ReCentering Psych Stats: Psychometrics}
\author{Lynette H. Bikos, PhD, ABPP (she/her)}
\date{27 May 2024}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}
\hypertarget{book-cover}{%
\chapter*{BOOK COVER}\label{book-cover}}


\begin{figure}
\centering
\includegraphics{images/ReCenterPsychStats-Psychometrics-bookcover.png}
\caption{An image of the book cover. It includes four quadrants of non-normal distributions representing gender, race/ethnicity, sustainability/global concerns, and journal articles.}
\end{figure}

This open education resource (OER) is available in two formats:

\begin{itemize}
\tightlist
\item
  Formatted as an \href{https://lhbikos.github.io/ReC_Psychometrics/}{html book} via GitHub Pages
\item
  As a \href{https://github.com/lhbikos/ReC_Psychometrics/blob/main/ReC_Psychometrics.pdf}{PDF}
\end{itemize}

All materials used in creating this OER are available at its \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub repo}.

As a perpetually-in-progress, open education resource, feedback is always welcome. This IRB-approved (SPU IRB \#202102010R, no expiration) \href{https://spupsych.az1.qualtrics.com/jfe/form/SV_0OnBLfut3VIOIS2}{Qualtrics-hosted survey} includes formal rating scales, open-ended text boxes, and a portal for uploading attachments (e.g., marked up PDFs). You are welcome to complete only the portions that are relevant to you.

\hypertarget{preface}{%
\chapter*{PREFACE}\label{preface}}


\textbf{If you are viewing this document, you should know that this is a book-in-progress. Early drafts are released for the purpose teaching my classes and gaining formative feedback from a host of stakeholders. The document was last updated on 27 May 2024}. Emerging volumes on other statistics are posted on the \href{https://lhbikos.github.io/BikosRVT/ReCenter.html}{ReCentering Psych Stats} page at my research team's website.

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=c932455e-ef06-444a-bdca-acf7012d759a}{Screencasted Lecture Link}

To \emph{center} a variable in regression means to set its value at zero and interpret all other values in relation to this reference point. Regarding race and gender, researchers often center male and White at zero. Further, it is typical that research vignettes in statistics textbooks are similarly seated in a White, Western (frequently U.S.), heteronormative, framework. The purpose of this project is to create a set of open educational resources (OER) appropriate for doctoral and post-doctoral training that contribute to a socially responsive pedagogy -- that is, it contributes to justice, equity, diversity, and inclusion.

Statistics training in doctoral programs are frequently taught with fee-for-use programs (e.g., SPSS/AMOS, SAS, MPlus) that may not be readily available to the post-doctoral professional. In recent years, there has been an increase and improvement in R packages (e.g., \emph{psych}, \emph{lavaan}) used for in analyses common to psychological research. Correspondingly, many graduate programs are transitioning to statistics training in R (free and open source). This is a challenge for post-doctoral psychologists who were trained with other software. This OER will offer statistics training with R and be freely available (specifically in a GitHub repository and posted through GitHub Pages) under a Creative Commons Attribution-NonCommercial-ShareAlike license {[}CC BY-NC-SA 4.0{]}.

Training models for doctoral programs in health service psychology are commonly scholar-practitioner, scientist-practitioner, or clinical-scientist. An emerging model, the \emph{scientist-practitioner-advocacy} training model, incorporates social justice advocacy so that graduates are equipped to recognize and address the sociocultural context of oppression and unjust distribution of resources and opportunities \citep{mallinckrodt_scientist-practitioner-advocate_2014}. In statistics textbooks, the use of research vignettes engages the learner around a tangible scenario for identifying independent variables, dependent variables, covariates, and potential mechanisms of change. Many students recall examples in Field's \citeyearpar{field_discovering_2012} popular statistics text: Viagra to teach one-way ANOVA, beer goggles for two-way ANOVA, and bushtucker for repeated measures. What if the research vignettes were more socially responsive?

In this OER, research vignettes will be from recently published articles where:

\begin{itemize}
\tightlist
\item
  the author's identity is from a group where scholarship is historically marginalized (e.g., BIPOC, LGBTQ+, LMIC {[}low-middle income countries{]}),
\item
  the research is responsive to issues of justice, equity, inclusion, diversity,
\item
  the lesson's statistic is used in the article, and
\item
  there is sufficient information in the article to simulate the data for the chapter example(s) and practice problem(s); or it is publicly available.
\end{itemize}

In training for multicultural competence, the saying, ``A fish doesn't know that it's wet'' is often used to convey the notion that we are often unaware of our own cultural characteristics. In recent months and years, there has been an increased awakening to the institutional and systemic racism that our systems are perpetuating. Queuing from the water metaphor, I am hopeful that a text that is recentered in the ways I have described can contribute to \emph{changing the water} in higher education and in the profession of psychology.

\hypertarget{copyright-with-open-access}{%
\section*{Copyright with Open Access}\label{copyright-with-open-access}}


This book is published under a \href{http://creativecommons.org/licenses/by-nc-sa/4.0/}{Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License}. This means that this book can be reused, remixed, retained, revised, and redistributed (including commercially) as long as appropriate credit is given to the authors. If you remix, or modify the original version of this open textbook, you must redistribute all versions of this open textbook under the same license: CC BY-SA 4.0.

A \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub open-source repository} contains all of the text and source code for the book, including data and images.

\hypertarget{acknowledgements}{%
\chapter*{ACKNOWLEDGEMENTS}\label{acknowledgements}}


As a doctoral student at the University of Kansas (1992-1996), I learned that ``a foreign language'' was a graduation requirement. \emph{Please note that as one who studies the intersections of global, vocational, and sustainable psychology, I regret that I do not have language skills beyond English.} This could have been met with credit from high school, but my rural, mid-Missouri high school did not offer such classes. This requirement would have typically been met with courses taken during an undergraduate program -- but my non-teaching degree in the University of Missouri's School of Education was exempt from this. The requirement could have also been met with a computer language (FORTRAN, C++) -- but I did not have any of those either. There was a tiny footnote on my doctoral degree plan that indicated that a 2-credit course, ``SPSS for Windows'' would substitute for the language requirement. Given that it was taught by my one of my favorite professors, I readily signed up. As it turns out, Samuel B. Green, PhD, was using the course to draft chapters in the textbook \citep{green_using_2017} that has been so helpful for so many. Unfortunately, Drs. Green (1947 - 2018) and Salkind (1947 - 2017) are no longer with us. I have worn out numerous versions of their text. Another favorite text of mine has been Dr.~Barbara Byrne's \citeyearpar{byrne_structural_2016}, ``Structural Equation Modeling with AMOS.'' I loved the way she worked through each problem and paired it with a published journal article, so that the user could see how the statistical evaluation fit within the larger project/article. I took my tea-stained text with me to a workshop she taught at APA and was proud of the signature she added to it. Dr.~Byrne created SEM texts for a number of statistical programs (e.g., LISREL, EQS, MPlus). As I was learning R, I wrote Dr.~Byrne, asking if she had an edition teaching SEM/CFA with R. She promptly wrote back, saying that she did not have the bandwidth to learn a new statistics package. We lost Dr.~Byrne in December 2020. I am so grateful to these role models for their contributions to my statistical training. I am also grateful for the doctoral students who have taken my courses and are continuing to provide input for how to improve the materials.

The inspiration for training materials that re*center statistics and research methods came from the \href{https://www.academics4blacklives.com/}{Academics for Black Survival and Wellness Initiative}. This project, co-founded by Della V. Mosley, Ph.D., and Pearis L. Bellamy, M.S., made clear the necessity and urgency for change in higher education and the profession of psychology.

At very practical levels, I am indebted to SPU's Library, and more specifically, SPU's Education, Technology, and Media Department. Assistant Dean for Instructional Design and Emerging Technologies, R. John Robertson, MSc, MCS, has offered unlimited consultation, support, and connection. Senior Instructional Designer in Graphics \& Illustrations, Dominic Wilkinson, designed the logo and book cover. Psychology and Scholarly Communications Librarian, Kristin Hoffman, MLIS, has provided consultation on topics ranging from OERS to citations. I am also indebted to Associate Vice President, Teaching and Learning at Kwantlen Polytechnic University, Rajiv Jhangiani, PhD. Dr.~Jhangiani's text \citeyearpar{jhangiani_research_2019} was the first OER I ever used, and I was grateful for his encouraging conversation.

Throughout the ReCentering Psych Stats series, students (both paid and volunteer) have served as editorial assistants in copyediting the text and correcting the text captions. In the Psychometrics volume, these have included Layla Hakim and Kelly Ng.

Financial support for this project has been provided the following:

\begin{itemize}
\tightlist
\item
  \emph{Call to Action on Equity, Inclusion, Diversity, Justice, and Social Responsivity Request for Proposals} grant from the Association of Psychology Postdoctoral and Internship Centers (2021-2022).
\item
  \emph{Diversity Seed Grant}, Office of Inclusive Excellence and Advisory Council for Diversity and Reconciliation (ACDR), Seattle Pacific University.
\item
  \emph{ETM Open Textbook \& OER Development Funding}, Office of Education, Technology, \& Media, Seattle Pacific University.
\end{itemize}

\hypertarget{ReCintro}{%
\chapter{Introduction}\label{ReCintro}}

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=cc9b7c0d-e5c3-4e4e-a469-acf7013ee761}{Screencasted Lecture Link}

\hypertarget{what-to-expect-in-each-chapter}{%
\section{What to expect in each chapter}\label{what-to-expect-in-each-chapter}}

This textbook is intended as \emph{applied,} in that a primary goal is to help the scientist-practitioner-advocate use a variety of statistics in research problems and \emph{writing them up} for a program evaluation, dissertation, or journal article. In support of that goal, I try to provide just enough conceptual information so that the researcher can select the appropriate statistic (i.e., distinguishing between when ANOVA is appropriate and when regression is appropriate) and assign variables to their proper role (e.g., covariate, moderator, mediator).

This conceptual approach does include occasional, step-by-step, \emph{hand-calculations} (only we calculate them arithmetically in R) to provide a \emph{visceral feeling} of what is happening within the statistical algorithm that may be invisible to the researcher. Additionally, the conceptual review includes a review of the assumptions about the characteristics of the data and research design that are required for the statistic. Statistics can be daunting, so I have worked hard to establish a \emph{workflow} through each analysis. When possible, I include a flowchart that is referenced frequently in each chapter and assists the researcher keep track of their place in the many steps and choices that accompany even the simplest of analyses.

As with many statistics texts, each chapter includes a \emph{research vignette.} Somewhat unique to this resource is that the vignettes are selected from recently published articles. Each vignette is chosen with the intent to meet as many of the following criteria as possible:

\begin{itemize}
\tightlist
\item
  the statistic that is the focus of the chapter was properly used in the article,
\item
  the author's identity is from a group where scholarship is historically marginalized (e.g., BIPOC, LGBTQ+, LMIC {[}low middle income countries{]}),
\item
  the research has a justice, equity, inclusion, diversity, and social responsivity focus and will contribute positively to a social justice pedagogy, and
\item
  the data is available in a repository or there is sufficient information in the article to simulate the data for the chapter example(s) and practice problem(s).
\end{itemize}

In each chapter we employ \emph{R} packages that will efficiently calculate the statistic and the dashboard of metrics (e.g., effect sizes, confidence intervals) that are typically reported in psychological science.

\hypertarget{strategies-for-accessing-and-using-this-oer}{%
\section{Strategies for Accessing and Using this OER}\label{strategies-for-accessing-and-using-this-oer}}

There are a number of ways you can access this resource. You may wish to try several strategies and then select which works best for you. I demonstrate these in the screencast that accompanies this chapter.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simply follow along in the .html formatted document that is available on via GitHub Pages, and then

  \begin{itemize}
  \tightlist
  \item
    open a fresh .rmd file of your own, copying (or retyping) the script and running it
  \end{itemize}
\item
  Locate the original documents at the \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub repository}. You can:

  \begin{itemize}
  \tightlist
  \item
    open them to simply take note of the ``behind the scenes'' script
  \item
    copy/download individual documents that are of interest to you
  \item
    fork a copy of the entire project to your own GitHub site and further download it (in its entirety) to your personal workspace. The \href{https://desktop.github.com/}{GitHub Desktop app} makes this easy!
  \end{itemize}
\item
  Listen to the accompanying lectures (I think sound best when the speed is 1.75). The lectures are being recorded in Panopto and should include the closed captioning.
\item
  Provide feedback to me! If you fork a copy to your own GitHub repository, you can:

  \begin{itemize}
  \tightlist
  \item
    open up an editing tool and mark up the document with your edits,
  \item
    start a discussion by leaving comments/questions, and then
  \item
    sending them back to me by committing and saving. I get an e-mail notifying me of this action. I can then review (accepting or rejecting) them and, if a discussion is appropriate, reply back to you.
  \end{itemize}
\end{enumerate}

\hypertarget{if-you-are-new-to-r}{%
\section{If You are New to R}\label{if-you-are-new-to-r}}

R can be oveRwhelming. Jumping right into advanced statistics might not be the easiest way to start. However, in these chapters, I provide complete code for every step of the process, starting with uploading the data. To help explain what R script is doing, I sometimes write it in the chapter text; sometimes leave hashtagged-comments in the chunks; and, particularly in the accompanying screen casted lectures, try to take time to narrate what the R script is doing.

I've found that, somewhere on the internet, there's almost always a solution to what I'm trying to do. I am frequently stuck and stumped and have spent hours searching the internet for even the tiniest of things. When you watch my videos, you may notice that in my R studio, there is a ``scRiptuRe'' file. I takes notes on the solutions and scripts here -- using keywords that are meaningful to me so that when I need to repeat the task, I can hopefully search my own prior solutions and find a fix or a hint.

\hypertarget{base-r}{%
\subsection{Base R}\label{base-r}}

The base program is free and is available here: \url{https://www.r-project.org/}

Because R is already on my machine (and because the instructions are sufficient), I will not walk through the installation, but I will point out a few things.

\begin{itemize}
\tightlist
\item
  Follow the instructions for your operating system (Mac, Windows, Linux)
\item
  The ``cran'' (I think ``cranium'') is the \emph{Comprehensive R Archive Network.} In order for R to run on your computer, you have to choose a location. Because proximity is somewhat related to processing speed, select one that is geographically ``close to you.''
\item
  You will see the results of this download on your desktop (or elsewhere if you chose to not have it appear there) but you won't ever use R through this platform.
\end{itemize}

\hypertarget{r-studio}{%
\subsection{R Studio}\label{r-studio}}

\emph{R Studio} is the desktop application I work in R. It's a separate download. Choose the free, desktop, option that is appropriate for your operating system: \url{https://www.rstudio.com/products/RStudio/}

\begin{itemize}
\tightlist
\item
  The \textbf{upper right window} includes several tabs:

  \begin{itemize}
  \tightlist
  \item
    Environment: it lists the \emph{objects} that are available to you (e.g., dataframes)
  \end{itemize}
\item
  The \textbf{lower right window} has a number of helpful tab:

  \begin{itemize}
  \tightlist
  \item
    Files: Displays the file structure in your computer's environment. Make it a practice to (a) organize your work in small folders and (b) navigating to that small folder that is holding your project when you are working on it.
  \item
    Packages: Lists the packages that have been installed. If you navigate to it, you can see if it is ``on.'' You can also access information about the package (e.g., available functions, examples of script used with the package) in this menu. This information opens in the Help window.
  \item
    Viewer and Plots are helpful, later, when we can simultaneously look at our output and still work on our script.
  \end{itemize}
\item
  The \textbf{primary window} is where we work:

  \begin{itemize}
  \tightlist
  \item
    R Studio runs in the background (i.e., in the console). Occasionally, I can find useful troubleshooting information here.
  \item
    More commonly, I open my R Markdown document so that it takes the whole screen and I work directly, right here.
  \end{itemize}
\item
  \textbf{R Markdown} is the way that many analysts write \emph{script}, conduct analyses, and even write up results. These are saved as .rmd files.

  \begin{itemize}
  \tightlist
  \item
    In R Studio, open an R Markdown document through File/New File/R Markdown
  \item
    Specify the details of your document (title, author, desired output)
  \item
    In a separate step, SAVE this document (File/Save{]} into a NEW FILE FOLDER that will contain anything else you need for your project (e.g., the data).
  \item
    \emph{Packages} are at the heart of working in R. Installing and activating packages require writing script.
  \end{itemize}
\end{itemize}

\hypertarget{r-hygiene}{%
\subsection{R Hygiene}\label{r-hygiene}}

Many initial problems in R can be solved with good R hygiene. Here are some suggestions for basic practices. It can be tempting to ``skip this.'' However, in the first few weeks of class, these are the solutions I am presenting to my students.

\hypertarget{everything-is-documented-in-the-.rmd-file}{%
\subsubsection{Everything is documented in the .rmd file}\label{everything-is-documented-in-the-.rmd-file}}

Although others do it differently, everything is in my .rmd file. That is, for uploading data and opening packages I write the code in my .rmd file. Why? Because when I read about what I did hours or years later, I have a permanent record of very critical things like (a) where my data is located, (b) what version I was using, and (c) what package was associated with the functions.

\hypertarget{file-organization}{%
\subsubsection{File organization}\label{file-organization}}

File organization is a critical key to this:

\begin{itemize}
\tightlist
\item
  Create a project file folder.
\item
  Put the data file in it.
\item
  Open an R Markdown file.
\item
  Save it in the same file folder.
\item
  When your data and .rmd files are in the same folder (not your desktop, but a shared folder), they can be connected.
\end{itemize}

\hypertarget{chunks}{%
\subsubsection{Chunks}\label{chunks}}

The R Markdown document is an incredible tool for integrating text, tables, and analyses. This entire OER is written in R Markdown. A central feature of this is ``chunks.''

The easiest way to insert a chunk is to use the INSERT/R command at the top of this editor box. You can also insert a chunk with the keyboard shortcut: CTRL/ALT/i

``Chunks'' start and end with those three tic marks and will show up in a shaded box, like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# hashtags let me write comments to remind myself what I did here I}
\CommentTok{\# am simply demonstrating arithmetic (but I would normally be running}
\CommentTok{\# code)}
\DecValTok{2021} \SpecialCharTok{{-}} \DecValTok{1966}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 55
\end{verbatim}

Each chunk must open and close. If one or more of your tic marks get deleted, your chunk won't be read as such, and your script will not run. The only thing in the chunks should be script for running R; you can hashtag-out script, so it won't run.

Although unnecessary, you can add a brief title for the chunk in the opening row, after the ``r.'' These create something of a table of contents of all the chunks -- making it easier to find what you did. You can access them in the ``Chunks'' tab at the bottom left of R Studio. If you wish to knit a document, you cannot have identical chunk titles.

You can put almost anything you want in the space outside of tics. Syntax for simple formatting in the text areas (e.g., using italics, making headings, bold, etc.) is found here: \url{https://rmarkdown.rstudio.com/authoring_basics.html}

\hypertarget{packages}{%
\subsubsection{Packages}\label{packages}}

As scientist-practitioners (and not coders), we will rely on \emph{packages} to do our work for us. At first you may feel overwhelmed about the large number of packages that are available. Soon, though, you will become accustomed to the ones most applicable to our work (e.g., psych, tidyverse, lavaan, apaTables).

Researchers treat packages differently. In these lectures, I list all the packages we will use in an opening chunk that asks R to check to see if the package is installed, and if not, installs it.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(psych)) \{}
    \FunctionTok{install.packages}\NormalTok{(}\StringTok{"psych"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: psych
\end{verbatim}

To make a package operable, you need to open it through the library. This process must be repeated each time you restart R. I don't open the package (through the ``library(package\_name)'') command until it is time to use it. Especially for new users, I think it's important to connect the functions with the specific packages.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages (\textquotesingle{}psych\textquotesingle{})}
\FunctionTok{library}\NormalTok{(psych)}
\end{Highlighting}
\end{Shaded}

If you type in your own ``install.packages'' code, hashtag it out once it's been installed. It is problematic to continue to re-run this code.

\hypertarget{knitting}{%
\subsubsection{Knitting}\label{knitting}}

An incredible feature of R Markdown is its capacity to \emph{knit} to HTML, PowerPoint, or word. If you access the .rmd files for this OER, you can use annotate or revise them to suit your purposes. If you redistribute them, though, please honor the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License with a citation.

\hypertarget{troubleshooting-in-r-markdown}{%
\subsection{tRoubleshooting in R maRkdown}\label{troubleshooting-in-r-markdown}}

Hiccups are normal. Here are some ideas that I have found useful in getting unstuck.

\begin{itemize}
\tightlist
\item
  In an R script, you must have everything in order -- Every. Single. Time.

  \begin{itemize}
  \tightlist
  \item
    All the packages must be in your library and activated; if you restart R, you need to reload each package.
  \item
    If you open an .rmd file and want a boxplot, you cannot just scroll down to that script. You need to run any \emph{prerequisite} script (like loading the package, importing data, putting the data in the global environment, etc.)
  \item
    Do you feel lost? clear your global environment (broom) and start at the top of the R script. Frequent, fresh starts are good.
  \end{itemize}
\item
  Your .rmd file and your data need to be stored in the same file folder. These should be separate for separate projects, no matter how small.
\item
  Type any warnings you get into a search engine. Odds are, you'll get some decent hints in a manner of seconds. Especially at first, these are common errors:

  \begin{itemize}
  \tightlist
  \item
    The package isn't loaded (if you restarted R, you need to reload your packages)
  \item
    The .rmd file has been saved yet, or isn't saved in the same folder as the data
  \item
    Errors of punctuation or spelling
  \end{itemize}
\item
  Restart R (it's quick -- not like restarting your computer)
\item
  If you receive an error indicating that a function isn't working or recognized, and you have loaded the package, type the name of the package in front of the function with two colons (e.g., psych::describe(df). If multiple packages are loaded with functions that have the same name, R can get confused.
\end{itemize}

\hypertarget{strategies-for-success}{%
\subsection{stRategies for success}\label{strategies-for-success}}

\begin{itemize}
\tightlist
\item
  Engage with R, but don't let it overwhelm you.

  \begin{itemize}
  \tightlist
  \item
    The \emph{mechanical is also the conceptual}. Especially when it is \emph{simpler}, do try to retype the script into your own .rmd file and run it. Track down the errors you are making and fix them.
  \item
    If this stresses you out, move to simply copying the code into the .rmd file and running it. If you continue to have errors, you may have violated one of the best practices above (Is the package loaded? Are the data and .rmd files in the same place? Is all the prerequisite script run?).
  \item
    Still overwhelmed? Keep moving forward by downloading a copy of the .rmd file that accompanies any given chapter and just ``run it along'' with the lecture. Spend your mental power trying to understand what each piece does. Then select a practice problem that is appropriate for your next level of growth.
  \end{itemize}
\item
  Copy script that works elsewhere and replace it with your datafile, variables, etc.\\
\item
  The learning curve is steep, but not impossible. Gladwell\citeyearpar{gladwell_outliers_2008} reminds us that it takes about 10,000 hours to get GREAT at something (2,000 to get reasonably competent). Practice. Practice. Practice.
\item
  Updates to R, R Studio, and the packages are NECESSARY, but can also be problematic. It could very well be that updates cause programs/script to fail (e.g., ``X has been deprecated for version X.XX''). Moreover, this very well could have happened between my distribution of these resources and your attempt to use it. My personal practice is to update R, R Studio, and the packages a week or two before each academic term.
\item
  Embrace your downward dog. Also, walk away, then come back.
\end{itemize}

\hypertarget{resources-for-getting-started}{%
\subsection{Resources for getting staRted}\label{resources-for-getting-started}}

R for Data Science: \url{https://r4ds.had.co.nz/}

R Cookbook: \url{http://shop.oreilly.com/product/9780596809164.do}

R Markdown homepage with tutorials: \url{https://rmarkdown.rstudio.com/index.html}

R has cheat sheets for everything, here's one for R Markdown: \url{https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf}

R Markdown Reference guide: \url{https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf}

Using R Markdown for writing reproducible scientific papers: \url{https://libscie.github.io/rmarkdown-workshop/handout.html}

LaTeX equation editor: \url{https://www.codecogs.com/latex/eqneditor.php}

\hypertarget{QuestCon}{%
\chapter{Questionnaire Construction: The Fundamentals}\label{QuestCon}}

\href{https://www.youtube.com/playlist?list=PLtz5cFLQl4KNoMWlGfDS31jNYqW_J541F}{Screencasted Lecture Link}

The focus of this chapter is on the technical issues of constructing a survey. I found this lesson to be more of a struggle to prepare than I expected. Why? There is a great deal of lore about what increases response rates and participation. Yet, research over the years, has both supported, contradicted, or not addressed these claims. One example is where to include ``sensitive items.'' Historically, textbook authors have recommended that these should come last so that respondents would be engaged in the process and be more willing to complete the survey \citep{krathwohl_methods_2009, rowley_designing_2014}. Yet, research has shown that this has not held up in employee groups \citep{roberson_questionnaire_1990} nor among members of the National Association of Social Workers \citep{robert_g._green_should_2000}.

Given these contradictions, this lecture starts with the overall structure of a survey. The core of the lecture focuses on recent, evidence-based support for item-level decisions. I briefly discuss construct-specific guidance and discuss specific considerations for the on-line environment. I then close by addressing some of the decisions that I routinely make in survey construction and provide my rationale for why. Because this lesson occurs at the beginning of a text on psychometrics -- this ``skips over and around'' reliability and validity. These important issues will be addressed in subsequent lessons.

\hypertarget{navigating-this-lesson}{%
\section{Navigating this Lesson}\label{navigating-this-lesson}}

There is just under one hour of lecture.

While the majority of R objects and data you will need are created within the R script that sources the chapter, there are a few resources that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives}{%
\subsection{Learning Objectives}\label{learning-objectives}}

Focusing on this week's materials, make sure you can:

\begin{itemize}
\tightlist
\item
  Outline the overall structure/components of a questionnaire,
\item
  Articulate test construction myths (e.g., location of sensitive items, ``requirement'' to have reverse scored items) and their evidence-based solutions (when they have them)
\item
  List elements to consider when the questionnaire is administered online
\end{itemize}

\hypertarget{planning-for-practice}{%
\subsection{Planning for Practice}\label{planning-for-practice}}

This is a two-part lesson on questionnaire construction. After the second lesson, a detailed suggestion for practice will be provided that lists criteria for creating and piloting a survey of your own.

\hypertarget{readings-resources}{%
\subsection{Readings \& Resources}\label{readings-resources}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Chyung, S. Y., Roberts, K., Swanson, I., \& Hankinson, A. (2017). Evidence-Based Survey Design: The Use of a Midpoint on the Likert Scale. Performance Improvement, 56(10), 15--23. \url{https://doi.org/10.1002/pfi.21727}
\item
  Chyung, S. Y., Barkin, J. R., \& Shamsy, J. A. (2018a). Evidence‐Based Survey Design: The Use of Negatively Worded Items in Surveys. Performance Improvement, 57(3), 16--25. \url{https://doi.org/10.1002/pfi.21749}
\item
  Chung, S. Y., Kennedy, M., \& Campbell, I (2018b). Evidence-based survey design: The use of ascending or descending order of Likert-type response options. Performance Improvement, 57(9), 9-16. \url{https://doi.org/10.1002/pfi.21800}
\item
  Chyung, S. Y., Swanson, I., Roberts, K., \& Hankinson, A. (2018c). Evidence‐Based Survey Design: The Use of Continuous Rating Scales in Surveys. Performance Improvement, 57(5), 38--48. \url{https://doi.org/10.1002/pfi.21763}

  \begin{itemize}
  \tightlist
  \item
    Finding the Chyung et al.~series was like finding a pot of gold! They provide empirical support for guiding choices about survey construction. And they are current! If you don't have time to read them in detail, I recommend you scan them and archive them for future reference.
  \end{itemize}
\end{itemize}

\hypertarget{components-of-the-questionnaire}{%
\section{Components of the Questionnaire}\label{components-of-the-questionnaire}}

Let's start by examining the components of a questionnaire and the general guidelines for their construction\citep{colton_designing_2015, pershing_ineffective_2001}:

\textbf{Title}

\begin{itemize}
\tightlist
\item
  reflect the content of the instrument
\item
  be concisely worded
\item
  be written in language easily understood by the respondents
\item
  should not be offensive or off-putting
\item
  should be formatted clearly at the top/beginning of the document
\end{itemize}

\textbf{Introductory Statement}

\begin{itemize}
\tightlist
\item
  include a brief summary of the instrument's purpose
\item
  contain an appropriate statement concerning the confidentiality of the respondent's information (informed consent)
\item
  be motivating such that respondents are inspired/willing to complete the items
\item
  specify the approximate amount of time required to complete the instrument
\end{itemize}

\textbf{Directions}

\begin{itemize}
\tightlist
\item
  complete, unambiguous, concise
\item
  written at a language level appropriate to the respondents
\item
  tell the respondents how to return the instrument once they have completed it (surprisingly, in Qualtrics, this is also important; submission requires hitting that last little ``--\textgreater\textgreater{}'')
\end{itemize}

\textbf{Items}

\begin{itemize}
\tightlist
\item
  discussed throughout this textbook
\end{itemize}

\textbf{Closing Statement}

\begin{itemize}
\tightlist
\item
  thank the participants for their participation
\item
  remind participants that their information is valuable and perhaps remind about

  \begin{itemize}
  \tightlist
  \item
    next steps or follow-up
  \item
    confidentiality
  \end{itemize}
\end{itemize}

\textbf{Overall Structure/Look}

\begin{itemize}
\tightlist
\item
  should be coherent with an easy-to-follow layout
\item
  professional appearance

  \begin{itemize}
  \tightlist
  \item
    not crowded, plenty of white space
  \item
    avoiding a ``slick look''
  \item
    numbering and headings to provide a sense of progress
  \item
    breaks between every 4-6 questions (or shading alternate items)
  \item
    in a sense, inviting and ``easy on the eye''
  \end{itemize}
\end{itemize}

Pershing and Pershing \citeyearpar{pershing_ineffective_2001} reviewed 50 \emph{reactionnaires} (i.e., like ``questionnaires'' specifically intended to collect reactions) that were used by training evaluators at a ``prestigious medical school.'' Their purpose was to determine the degree to which the survey design adhered to the recommendations. The results suggested that:

\begin{itemize}
\tightlist
\item
  72\% did not include an introductory statement; an additional 16\% were ``minimal''
\item
  78\% had no closing statement
\item
  30\% had no directions; another 54\% of directions were ``minimal''
\item
  8\% were professional in appearance
\end{itemize}

In summary, the formatting of the reactionnaires were not designed in a way that would maximize respondent engagement. In turn, we might expect this to threaten the psychometric reliability and validity.

\hypertarget{what-improves-or-threatens-response-rates-and-bias}{%
\section{What Improves (or Threatens) Response Rates and Bias?}\label{what-improves-or-threatens-response-rates-and-bias}}

When we design survey instruments based on our own preference rather than research-based evidence, we may get less than optimal data. Chyung et al. \citeyearpar{chyung_evidence-based_2018} reviewed the five steps \citep{schwarz_asking_2001} that survey respondents engage when answering structured, closed-ended survey items.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Interpreting the question.
\item
  Retrieving information from their memory.
\item
  Integrating the retrieved information with the item prompt.
\item
  Selecting one of the given response options.
\item
  Editing the answer for reasons of social desirability.
\end{enumerate}

Chyung and colleagues appear to be starting such a systematic review. What follows are their evidence-based evaluations regarding some of the most common questions about questionnaire construction.

\hypertarget{should-likert-type-scales-include-a-midpoint}{%
\subsection{Should Likert-type scales include a midpoint?}\label{should-likert-type-scales-include-a-midpoint}}

Likert-type scales, named after Rensis Likert, include a set of questions or statements that can be responded to with a consistent set of response options. The response options are frequently scaled with intensity options ranging from 2 to 11; four and five point ``agreement'' scales str shown below. There are many variants of response options (i.e., frequency, like me, degree of stressfulness). Regarding the issue of a \emph{midpoint} (``neutral'' or ``neither disagree nor agree''), Chyung et al. \citeyearpar{chyung_evidence-based_2017} reviewed the literature. Examining their article, we can see variants of Likert-style scaling for a scale of agreement. They look something like this:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1959}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1856}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0928}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2680}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0928}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1649}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
No midpoint (4 pt.) & Strongly Disagree & Disagree & \emph{skipped} & Agree & Strongly Agree \\
Midpoint (5 pt.) & Strongly Disagree & Disagree & Neither Disagree nor Agree & Agree & Strongly Agree \\
\end{longtable}

Chyung and colleagues quickly suggest that the question is not ``Should I use a midpoint?'' but rather ``When should I use a midpoint?''

The article is more detailed, but essentially, a midpoint is appropriate when:

\begin{itemize}
\tightlist
\item
  the measurement scale is interval (instead of ordinal; this is a good statistical property to have)
\item
  the question content is such that the midpoint is a \emph{true} midpoint and not a point for hedging or avoiding
\end{itemize}

If a true midpoint is impossible, then consider adding an option such as ``I don't know'' or ``It depends.'' If the ``I don't know'' option is used, the researcher needs a plan for recoding the data as missing and having a plan for managing missingness

\hypertarget{should-continuous-rating-scales-be-used-in-surveys}{%
\subsection{\texorpdfstring{Should \emph{continuous rating scales} be used in surveys?}{Should continuous rating scales be used in surveys?}}\label{should-continuous-rating-scales-be-used-in-surveys}}

First, let's consider the distinction between \emph{discrete}, \emph{continuous}, and \emph{numerical} scales. Figure 4 in the Chyung, Swanson, Roberts, and Hankinson \citeyearpar{chyung_evidencebased_2018-1} article illustrate the major differences and some variations.

\begin{itemize}
\tightlist
\item
  \textbf{Discrete} scales are Likert-type scales that range between 2 and 11 \emph{discrete} options. Classically, respondents pick \emph{words} (e.g., pain rated as \emph{no pain}, \emph{mild}, \emph{moderate}, \emph{severe}, \emph{extreme}, \emph{worst pain possible}).

  \begin{itemize}
  \tightlist
  \item
    Six-point discrete rating scales result in a collection of six \emph{ordered values}.
  \item
    The measurement scale for discrete scales is \emph{ordinal}.
  \item
    Ordinal scales should be analyzed with non-parametric statistical procedures, however parametric approaches can be used if the data are normally distributed and there is a mid-point.
  \end{itemize}
\item
  \textbf{Continuous} scales allow respondents to indicate a response anywhere within a given range -- usually by marking a place on a horizontal line on a continuum of a minimum of 100 points. There are no discrete categories defined by words or numbers.

  \begin{itemize}
  \tightlist
  \item
    Continuous scales result in precise numbers (e.g., 26 or 26.8 if the scale is 0 to 100).
  \item
    The measurement scale for continuous scales is \emph{interval}.
  \item
    Interval scales can be evaluated with parametric statistics.
  \item
    \emph{Visual analog scales (VAS; aka graphic rating scales, GRS)} are another variant of continuous rating scales if they allow the participants to make ``anywhere on the line.'' Some VAS scales have verbal descriptors to guide the marking; some have numbers (hence, \emph{numerical response scales}). In Qualtrics there is a slider option that serves this function.
  \end{itemize}
\end{itemize}

Which is better? The mixed results are summarized in Chyung et al's \citeyearpar{chyung_evidencebased_2018-1} Table 1. With a focus on the types of research I encounter in my program, here is my take-away:

\begin{itemize}
\tightlist
\item
  Continuous scales provide better data (i.e., more precise/full information, more likely to be normally distributed, better reliability) for statistical analysis.

  \begin{itemize}
  \tightlist
  \item
    \emph{Caveat:} If the response scale on a Likert scale is increased to 11, there is a better chance to have normally distributed responses.
  \item
    \emph{Caveat:} When ``simple descriptive statistics'' are desired (e.g., histograms, frequency distributions) the discrete scale may be the best choice.
  \end{itemize}
\item
  Discrete and continuous options (including sliders) are easy to use, except in the case where respondents complete the surveys on mobile devices.

  \begin{itemize}
  \tightlist
  \item
    \emph{Caveat:} There has been more missing data with sliders (compared to radio buttons).
  \item
    \emph{Caveat:} Respondents are more likely to change their responses on sliders. If this means there is greater accuracy or more careful responding, this is desirable.
  \end{itemize}
\item
  In both circumstances adding ``don't know,'' ``prefer not to respond,'' or ``not applicable'' may improve the validity of the responses.

  \begin{itemize}
  \tightlist
  \item
    \emph{Caveat:} When using one of these response options, plan ahead for coding as missing and managing the missingness.
  \end{itemize}
\end{itemize}

\hypertarget{should-likert-type-response-options-use-an-ascending-or-descending-order}{%
\subsection{Should Likert-type response options use an ascending or descending order?}\label{should-likert-type-response-options-use-an-ascending-or-descending-order}}

Let's first look at the difference between ascending and descending order \citep{chyung_evidence-based_2018}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1596}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1915}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1596}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1596}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1596}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1702}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Ascending} & Strongly Disagree & Disagree & Neither Disagree nor Agree & Agree & Strongly Agree \\
\textbf{Descending} & Strongly Agree & Agree & Neither Agree nor Disagree & Disagree & Strongly Disagree \\
\end{longtable}

In the consideration of the choice between ascending/descending, we are concerned with \emph{response-order effects}. Let's first examine these conceptually/theoretically.

\textbf{Recency effect} is the tendency of survey respondents to select the options that they see at the end of the response-option list. This is expected when options are presented orally (e.g., during interviews, people tend to choose from the last-offered options).

\textbf{Primacy effect} is the survey respondents' tendency to select the options that are presented at the beginning of the response-option list. This is expected when options are presented visually. For example, people tend to choose among the first-presented categories in self-administered written survey questionnaires.

\begin{itemize}
\tightlist
\item
  \emph{Left-sided selection bias} occurs when respondents read text from left-to-right and are more inclined to select options from the left.
\item
  \emph{Satisficing theory} occurs when individuals seek solutions that are ``simply satisfactory'' so as to minimize psychological costs. Thus, respondents may

  \begin{itemize}
  \tightlist
  \item
    select the first option that seems ``reasonable enough'',
  \item
    select the ``I don't know'' response, or
  \item
    randomly select one of the options.
  \end{itemize}
\item
  \emph{Acquiescence bias} is the tendency for respondents to agree with the statement provided---aka yea-saying bias (e.g., being polite).

  \begin{itemize}
  \tightlist
  \item
    Closely related is \emph{social-desirability bias,} the tendency for respondents to select among the options they think are more socially acceptable or desirable (instead of true responses).
  \item
    In surveys, this generally is selecting \emph{agree} or \emph{strongly agree}.
  \end{itemize}
\end{itemize}

Considering these response biases together, Chyung et al.~suggest that when the response options are presented in descending order (\emph{Strongly agree, Agree, Neutral, Disagree, Strongly disagree}), respondents would (theoretically) see a positive option immediately on the left side of the response scale and perceive it to be socially desirable and satisfactory. As a result, they may to select it without having to spend more time to choose a more accurate response. After reviewing 13 studies, Chyung et al.~observed that many studies (paper and web based, with children and adults, in English and other language):

\begin{itemize}
\tightlist
\item
  Revealed response-order effects in self-administered surveys, especially the primacy effect, associated with left-side selection bias, acquiescence bias, and satisficing.
\item
  Showed more positive average scores from descending-ordered scales.
\end{itemize}

Recommendations:

\begin{itemize}
\tightlist
\item
  Present response scales in ascending order.

  \begin{itemize}
  \tightlist
  \item
    When a number line is used, lower and negative numbers should be on the left.
  \end{itemize}
\item
  When using descended order scales:

  \begin{itemize}
  \tightlist
  \item
    keep respondents motivated to complete items accurately,
  \item
    present half items with descended-ordered scales and the other half with ascended-ordered scales,
  \item
    assign half of participants with descended-ordered scales; half with ascended-ordered scales, and
  \item
    present response options vertically rather than horizontally.
  \end{itemize}
\end{itemize}

\hypertarget{should-surveys-include-negatively-worded-items}{%
\subsection{Should surveys include negatively worded items?}\label{should-surveys-include-negatively-worded-items}}

In examining this question, Chyung et al. \citep{chyung_evidencebased_2018} made a distinction between (see Table 1 in the article):

\begin{itemize}
\tightlist
\item
  \textbf{Statement format} with a consistent response scale (e.g., strongly disagree to strongly agree).
\item
  \textbf{Question format} with variable response scales that are tailored to individual survey questions.

  \begin{itemize}
  \tightlist
  \item
    A challenge with this format is the difficulty in calculating an average score of data obtained from multiple survey items.
  \end{itemize}
\end{itemize}

The advent of negatively worded items began with Rensis Likert in 1932. He was an American social psychologist who, in attempt to mitigate acquiescence/yea-saying biases, recommended designing one half of survey items to be associated with agreement and the other half with disagreement. Although Likert recommended ``straightforward statements,'' incorporating negative words can become quickly complicated. Table 2 in the Chyung paper shows that there are four ways of wording survey statements:

\textbf{Reverse-coding}, which is necessary when including negatively worded items in a scale, assumes that agreeing to a positively worded statement and disagreeing to its negatively worded counterpart are the same. Tables 3 and 4 in the Chyung et al., manuscript \citeyearpar{chyung_evidencebased_2018} show how this assumption may be faulty. A review of the negatively-worded-item literature suggested the following:

\begin{itemize}
\tightlist
\item
  Scales with all positively worded items yielded greater accuracy when compared with all negatively worded items or mixed worded items.
\item
  Scores on positively and negatively worded items are not the same (e.g., strongly disagreeing to a positively worded statement is different from strongly agreeing to a negatively worded statement)
\item
  Positively worded items produce higher means than negatively worded items. This may be due to

  \begin{itemize}
  \tightlist
  \item
    carelessness and fatigue in reading items,
  \item
    the cognitive processing of positive and negative items may be different.
  \end{itemize}
\item
  A \emph{method factor} has shown itself where exploratory approaches to factor analysis have produced separate factors with the negatively worded (or otherwise ambiguous) items creating their own factor. This results in a threat to construct validity and reliability.
\end{itemize}

Chyung, Barkin, and Ramsey \citeyearpar{chyung_evidencebased_2018} noted that respondent performance declines approximately 12 minutes after starting a survey. It appears that respondents increasingly fail to notice negatively worded statements even when there are efforts to draw their attention to them via bolding, underlining, or capitalizing the negated element (e.g., \textbf{not}). Thus, when negatively worded items are used, they should probably be presented early in the protocol.

Chyung et al \citeyearpar{chyung_evidencebased_2018} also cautioned about a response set bias that can occur when using all positively worded items. They recommended making design choices that enhance bias-free and accurate responding based on the research design.

\begin{itemize}
\tightlist
\item
  For example, attributes to be measured in some constructs (e.g., depression, anxiety) are, themselves, negative and so a negatively worded item may be most clear and appropriate.
\item
  The inclusion (and subsequent analysis) of negatively phrased items may help \emph{detect} acquiescence bias.
\item
  Table 5 in the Chyung et al \citeyearpar{chyung_evidencebased_2018} manuscript provides some guidelines that are more nuanced when negative items must be included. For example,

  \begin{itemize}
  \tightlist
  \item
    Ensure that negatively worded items are true polar opposites and symmetrical (so they can be analyzed with the positively worded items).
  \item
    Group negative items together (and forewarn/format so they are recognized as such).
  \item
    Administer the survey when respondents are not fatigued.
  \item
    Analyze the effect of the negatively worded items.
  \end{itemize}
\end{itemize}

\hypertarget{construct-specific-guidance}{%
\section{Construct-specific guidance}\label{construct-specific-guidance}}

Across disciplines and constructs, there may be localized guidance. One domain-specific example is \emph{self-efficacy}. In this case, construct-specific guidance addresses both the (a) content of the items and (b) formatting of the scales. Regrading content, even though there are some \emph{general self-efficacy scales} Bandura's original definition suggests that scales and their items should be task specific (i.e., career decision-making self-efficacy, math self-efficacy). Further, Bandura \citeyearpar{bandura_guide_2006}, recommended the following for self-efficacy scales:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Phrase items as ``can do'' rather than ``will do.''
\item
  Maintain consistency with the self-efficacy construct definition (e.g., domain specific, a focus on capability rather than self-worth).
\item
  Include items that reflect gradations of challenge.
\item
  Ask individuals to rate their current (as opposed to future) operative capabilities.
\item
  Use 100-point continuous scaling.
\end{enumerate}

\hypertarget{surveying-in-the-online-environment}{%
\section{Surveying in the Online Environment}\label{surveying-in-the-online-environment}}

Nearly a decade ago, a survey of human subjects review boards suggested that 94\% of the IRB applications reviewed involved online or Web-based surveys \citep{buchanan_online_2009}. Thus, it is important to understand the online environment. A first set of considerations involve data security, identity, and permission (implicit and explicit).

The \textbf{IP address} as well as \textbf{longitude/latitude} has been a contentious issue for a number of years \citep{buchanan_online_2009}. EU data protection laws consider IP addresses as personally identifiable data; in the U.S., IP addresses typically fall outside the definition of ``personal information.'' In Qualtrics, the default is to collect and download the IP address (the ``anonymize response'' option can prevent this data from being collected). On the one hand it is helpful to know geographically ``from where'' participants are responding; on the other, some consider its capture to be a violation of privacy. Relatedly, \textbf{paradata} and \textbf{metadata} are data such as typing speed, changed answers, response times, and time spent on the survey. For both geolocation and paradata/metadata, a strong consideration is \textbf{fully informed consent} \citep{conrad_survey_2007}. Is it ethical to capture this information without explicitly saying so? A best practice is to provide complete descriptions of what data is being collected and provide a rationale (in the IRB application, if not directly in the informed consent) for why it is necessary. Survey tools like Qualtrics have strong options for anonymizing data (i.e., permanently deleting identifying information). If this option is being used, this information should be included in the informed consent (and, perhaps, recruiting materials).

The specific \textbf{survey tool} being used should be evaluated. Buchanan and Hvizdak \citeyearpar{buchanan_online_2009} argued that until each tool is vetted and its privacy policies and data security policies are understood, we cannot be certain how security, consent, and privacy are operationalized within the individual tools. For example, it is possible that tool creators \emph{could} gather respondent data and repurpose it for their own marketing, for sale to other researchers, and so forth.

Online and web-based protocols increase our reach geographically and cross-culturally. A first impression might be that the online environment increases \textbf{access} \citep{conrad_survey_2007}. We should probably think twice about this presumption. Consider the decades of psychological research based on White, college-educated, males. Does the online environment create another strata of privileged research with technology that may not be accessible in terms of both internet/technology as well as capacity/fluency with the tool? Additionally, does the way that the survey is promoted result in invitations that only occur within certain segments of the internet? If so, the results may not be representative. On the other hand, what are the risks of not adopting new technologies before everyone has them. Another consideration is cultural and language translation. These issues are addressed more completely in the lesson on \protect\hyperlink{Invariance}{invariance testing}.

When paper/pencil measures were administered in face-to-face settings (individually or in auditoriums of students) there was some degree of a \textbf{standardized protocol.} This is lost when surveys are administered online. Further, we cannot guarantee \emph{who} is taking the survey. Increasingly, when surveys are offered through fee-based programs like mTurk and Prolific, bots have been trained to take the surveys and receive the incentive. Survey programs like Qualtrics now offer additional packages to help with security and bot-prevention. It is probably also wise to add attention-check items (e.g., ``This is an attention check. Please answer `3'.''). Another option is to include a final question that asks the respondent to ensure the integrity of the response. An example last item might be, ``To what degree is the statement below true of you: \emph{I read each question/item and provided answers that were true for me}.'' The scaling for this item was a 100-point slider with ``Untrue,'' ``Neither true or untrue,'' and ``True''.

When respondents are remote, what happens if they have a \textbf{negative reaction to the survey}? In a face-to-face context, debriefings can occur, and referrals can be made. IRB committees are likely to consider the degree to which surveys may be upsetting and require resources for referral and assistance in the case of an adverse event.

\textbf{Security of test items} might also be concerning. It is inappropriate to use proprietary items without the permission of its author. If the security of items is important (e.g., SAT/GRE, intelligence test items, inkblots) because they are central to administration, how can they be protected in the virtual environment?

Consequently, when students in our programs write doctoral dissertations they are to include the following in their Method section.

\begin{itemize}
\tightlist
\item
  Describe how informed consent will be obtained in the online environment.
\item
  Describe the level of identification that is collected. If the claim of ``anonymous'' or ``de-identified'' indicate whether/not this includes capturing the IP address; some researchers believe that capturing a computer's IP address threatens anonymity.
\item
  Describe the steps to be taken to ensure that respondents met the inclusion/exclusion criteria of the study.
\item
  Anticipate and describe how the online (e.g., uncontrolled, public, distractions) setting might affect responses.
\item
  Particularly if the survey contained sensitive materials, describe how respondents might access resources for debriefing or referral.
\item
  Identify the permissions (from original authors or copyright holders) granted to reformat and post (on the internet) existing surveys. If items are considered to be secure (e.g., those on the MMPI or WAIS), identify steps taken to protect them.
\end{itemize}

\hypertarget{in-my-surveys}{%
\section{In my Surveys}\label{in-my-surveys}}

Because there isn't empirical data on every decision that we make in survey construction, I thought it might be useful for me to address some of the decisions that I find myself making in the online surveys I use in my own research.

\hypertarget{demographics-and-background-information}{%
\subsection{Demographics and Background Information}\label{demographics-and-background-information}}

A core value that I hope to reflect in the \emph{ReCentering Psych Stats} series is to promote socially and culturally responsive research. Correspondingly, the information we collect from participants should ensure that they feel that their identities are authentically reflected in the survey. Naively, when I first considered how to capture race/ethnicity in my surveys, I looked to the categories used in the U.S. Census. Immediately, I learned that this is problematic. Rather than elaborating here, I invite you to take a listen to NPR's \href{https://www.npr.org/podcasts/510312/codeswitch}{Code Switch} podcast. Two of the episodes review how the assessment of race and ethnicity has evolved and explain why it is problematic: \href{https://www.npr.org/transcripts/607553683}{Census Watch 2020} and \href{https://www.npr.org/transcripts/540671012}{The U.S. Census and Our Sense of Us}. As made clear in the Code Switch podcasts, the assessment of race and ethnicity in the U.S. Census \emph{erases} people when their identities are not included.

My last few surveys have captured race/ethnicity data differently. Each time, I engage in several practices that (I hope) will continue to shape the item in a socially and culturally responsive way. Systematically, I:

\begin{itemize}
\tightlist
\item
  conduct a quick internet search to see if there is an emerging best practice (even though I may have also searched weeks or months prior),
\item
  consider who the intended research population is in relationship to the topic of investigation,
\item
  look to recently published, similar, research to see what other researchers are doing, and
\item
  ask for a collegial, formative review from individuals who hold marginalized identities, whose data will be requested in the survey.
\end{itemize}

When I engage in research, I try to balance the need to quantify (with discrete categories) who is participating in the survey and inviting respondents to state (in their own words) their identity. This is consistent with my view that variables like race, ethnicity, and gender identity are socially constructed. In addition to this particular worldview, Parent \citeyearpar{parent_handling_2013} has suggested that the worst possible kind of missing data pattern (MNAR -- missing not at random) may be caused when items are \emph{unanswerable} to particular person. Therefore, it is essential that all individuals recognize themselves in the items that assess demographic variables.

A recent survey of mine was directed toward community members (including students, alumni, staff, faculty) of my predominantly White, liberal arts, Christian institution. After reviewing recently published research articles and consulting with a handful of individuals, I chose to include the following categories -- \emph{each with a text write-in box} so that individuals could select the category(ies) that fit best and have the opportunity to refine it(them). I am excited to review this data because such responses may inform my next survey. The categories included:

\begin{itemize}
\tightlist
\item
  Asian or Pacific Islander
\item
  Black or African American
\item
  Hispanic or Latino
\item
  Native American or Alaskan Native
\item
  White or Caucasian
\item
  Biracial or multiracial
\item
  An international/global identity that does not fit in the U.S. categorization of race/ethnicity
\item
  A race/ethnicity not listed above
\end{itemize}

Respondents could select multiple categories. Additionally, they could write in an identity that better aligned with their self-understanding.

\begin{figure}
\centering
\includegraphics{images/QuestCon/RaceEthnicity.jpg}
\caption{Image of a survey item inquiring about race/ethnicity from a survey. Each option has an option for the respondent to clarify.}
\end{figure}

The option to select multiple boxes results in some extra coding when preparing the data for analysis. I am taking approach that we will \emph{listen} to the data and decide, based on the results, how to report the findings in a way that will efficiently fit into an APA style empirical paper and honor the respondents.

The population of interest for this particular study are those who are engaged in protest activities regarding hiring practices and policies that result in discrimination to members of the LGBTQIA+ community. This means that questions of gender identity, pronouns, and relationship to the LGBTQIA+ community are important to the research and need to be asked sensitively and with great security of the data.

Regarding gender identity, I used a similar approach, allowing individuals to select multiple categories and offering write-in boxes for each. The categories included:

\begin{itemize}
\tightlist
\item
  female
\item
  male
\item
  nonbinary
\item
  trans woman
\item
  trans man
\item
  another identity (with a write-in box)
\item
  prefer not to say
\end{itemize}

Additionally, I invited individuals to identify their pronouns. Again, write-in boxes were offered with each option.

\begin{itemize}
\tightlist
\item
  they/them/theirs
\item
  she/her/hers
\item
  he/him/his
\item
  they/she/he
\item
  neo pronouns (e.g., xe/xem/xyr, ze/hir/hirs, ey/em/eir)
\item
  another identity (with a write-in box)
\end{itemize}

Finally, we wanted individuals to indicate their relationship to the LGBTQIA+ community. We asked them to select all that apply. Only the ``something else'' box had a write-in option:

\begin{itemize}
\tightlist
\item
  Member
\item
  Exploring/Questioning
\item
  Ally
\item
  Not related
\item
  Something else (with a write-in box)
\end{itemize}

I expect that my future surveys may inquire about these variables differently. If you have found a different way to ask, please consider e-mailing me. I would love to provide different options and give credit to contributors.

\hypertarget{survey-order}{%
\subsection{Survey Order}\label{survey-order}}

Historically, demographic information has been first or last in the survey. Although some research has reported no differences in response rates when demographic and sensitive data are at the beginning or end \citep{krathwohl_methods_2009, rowley_designing_2014}, I am inclined to open the survey with questionnaire items that are closely related to the topic listed on the recruitment materials and end the survey with the demographic information. Why? It makes sense to me that if someone has responded positively to the survey topic, they expect to answer questions (right away) about that topic.

In between that opening survey and closing demographic items, I consider if there are any \emph{order effects} that would engage in undue \emph{priming} of responses. If there are such concerns, I think through the order to minimize these biasing effects. If there are no such concerns, I put my surveys in blocks and then ask my survey program to randomly present the blocks. This serves two purposes:

\begin{itemize}
\tightlist
\item
  counterbalancing possible order effects, and
\item
  distributing missingness for individuals who do not complete the survey.
\end{itemize}

\hypertarget{forced-responses}{%
\subsection{Forced Responses}\label{forced-responses}}

Programs like Qualtrics are able to engage in a variety of \emph{content validation} procedures. If these are in place, they may require the person to enter a properly formatted response (e.g., phone number, e-mail address, numerical response between 0 and 100) before responding. These are extremely helpful tools in collecting data that will be closest-to-being-ready-for-analysis. These same procedures can \emph{force} or \emph{request} a response.

\emph{Requiring} a response is tempting. However, doing so violates IRB requirements that allow a person to skip or ``quit at any time without penalty.'' They may also anger a person such that they stop responding. Some researchers get around this by \emph{requiring} the response but including a ``Not applicable'' or ``Prefer to not answer'' column. Because I worry that (a) the respondent may confuse that option with one extreme of the scale and/or (b) my research team and I will forget to code it as missing data, I prefer the \emph{request} response alternative.

In Qualtrics in particular, I turn on the ``Request response'' feature for each of the questions. If an item is skipped, a simple warning is displayed that invites the respondent to review the page of answers to see if they would like to answer the question. If not, they can simply move forward.

\hypertarget{practice-problems}{%
\section{Practice Problems}\label{practice-problems}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. With each of these options I encourage you to:

This is a two-part lesson on questionnaire construction. After the \protect\hyperlink{qualTRIX}{second lesson}, a detailed suggestion for practice will be provided that lists criteria for creating and piloting a survey of your own.

\hypertarget{qualTRIX}{%
\chapter{Be a QualTRIXter}\label{qualTRIX}}

\href{https://youtube.com/playlist?list=PLtz5cFLQl4KOnQOjm2NTnYWWqSlKLJcsi\&si=TwT0NXKiLK-pJEWq}{Screencasted Lecture Link}

The focus of this lecture is on the technical and mechanical tools available in Qualtrics (and likely other survey platforms) to increase the effectiveness of your survey.

\hypertarget{navigating-this-lesson-1}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-1}}

This lecture is just under one hour. Plan for another 30 minutes for \emph{intRavenous qualtRics} practice.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-1}{%
\subsection{Learning Objectives}\label{learning-objectives-1}}

Focusing on this week's materials, make sure you can:

\begin{itemize}
\tightlist
\item
  Utilize basic Qualtrics tools (e.g., question type, use of headers) so that surveys are present materials clearly to the respondent.
\item
  Incorporate more advanced tools (e.g., display logic, randomization) that may increase the respondent's ability to complete the survey and provide accurate responses.
\item
  Provide a rationale for survey options that protect (or possibly reveal) an individual's identity.
\end{itemize}

\hypertarget{planning-for-practice-1}{%
\subsection{Planning for Practice}\label{planning-for-practice-1}}

This is the second of a two-part lesson on questionnaire construction. At the end of this lesson is a detailed suggestion for practice that lists criteria for creating and piloting a survey of your own. There are four essential criteria for your survey:

\begin{itemize}
\tightlist
\item
  Adhere to the evidence-based practices identified in the lesson on \protect\hyperlink{QuestCon}{questionnaire construction}.
\item
  Utilize four techniques (in the context of Qualtrics, I term these \emph{qualTRIXter skills}) that increase the flow, effectiveness, and appearance of your survey.
\item
  Pilot and consider feedback provided by those who took the survey.
\item
  Import the data into the R environment.
\end{itemize}

\hypertarget{readings-resources-1}{%
\subsection{Readings \& Resources}\label{readings-resources-1}}

In preparing this chapter, I drew heavily from the tutorials available at the \href{https://www.qualtrics.com/support/}{Qualtrics support site}. I have tried to link them throughout the presentation. It is likely they could change at any time and/or they might not work on your particular browser.

\hypertarget{packages-1}{%
\subsection{Packages}\label{packages-1}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# will install the package if not already installed}
\CommentTok{\# if(!require(qualtRics))\{install.packages(\textquotesingle{}qualtRics\textquotesingle{})\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{research-vignette}{%
\section{Research Vignette}\label{research-vignette}}

I will demonstrate the qual''TRIX'' by using a Qualtrics account hosted at Seattle Pacific University. The only surveys in this account are for the \emph{Recentering Psych Stats} chapters and lessons. All surveys are designed to not capture personally identifying information and not collecting IP addresses nor longitude/latitude. I use this survey in several lessons in this OER. If you haven't taken the survey yet, \href{https://spupsych.az1.qualtrics.com/jfe/form/SV_b2cClqAlLGQ6nLU}{I invite you to do so, now}.

As a teaching activity for the ReCentering Psych Stats OER, the topic of the survey was selected to be consistent with the overall theme of OER. Specifically, the purpose of this study is to understand the campus climate for students whose identities make them vulnerable to bias and discrimination. These include students who are Black, non-Black students of color, LGBTQ+ students, international students, and students with disabilities.

After consulting with a diverse group of stakeholders and subject matter experts (and revising the response options numerous times) I have attempted to center anti-Black racism in the U.S. \citep{mosley_critical_2021, mosley_radical_2020, singh_building_2020}. In fact, the display logic does not present the race items when the course is offered outside the U.S. There are only five options for race: \emph{biracial/multiracial}, \emph{Black}, \emph{non-Black person(s) of color}, \emph{White}, and \emph{I did not notice} (intended to capture a color-blind response). One unintended negative consequence of this design is that the response options could contribute to \emph{colorism} \citep{adames_fallacy_2021, capielo_rosario_acculturation_2019}. Another possibility is that the limited options may erase, or make invisible, other identities. At the time that I wrote up the first description of this survey, the murder of six Asian American women in Atlanta had just occurred. The Center for the Study of Hate and Extremism has documented that while overall hate crimes dropped by 7\% in 2020, anti-Asian hate crimes reported to the police in America's largest cities increased by 149\% \citep{noauthor_fact_nodate}. These incidents have occurred not only in cities, but in our neighborhoods and on our campuses \citep{kim_guest_2021, kim_yes_2021, noauthor_stop_nodate}. While this survey is intended to assess campus climate as a function of race, it unfortunately does not distinguish between many identities that experience marginalization.

Although the dataset should provide the opportunity to test a number of statistical models, one working hypothesis that framed the study is that the there will be a greater sense of belonging and less bias and discrimination when there is similar representation (of identities that are often marginalized) in the instructional faculty and student body. Termed, ``structural diversity'' \citep{lewis_black_2019} this is likely an oversimplification. In fact, an increase in diverse representation without attention to interacting factors can increase hostility on campus \citep{hurtado_linking_2007}. Thus, the task of rating of a single course relates to the larger campus along the dimensions of belonging and bias/discrimination. For example, if a single class has higher ratings on issues of inclusivity, diversity, and respect, we would expect that sentiment to be echoed in the broader institution.

The survey design has notable limitations You will likely notice that we ask about demographic characteristics of the instructional staff and classmates in the course rated, but we do not ask about the demographic characteristics of the respondent. In making this decision, we likely lose important information. For example, Iacovino and James \citeyearpar{iacovino_retaining_2016} have noted that White students perceive campus more favorably than Black student counterparts.

The decision to not collect demographic details about the respondent was about protecting their (your) identity. As you will see, you have the opportunity to download and analyze the data. If a faculty member asked an entire class to take the survey, the date stamp and a handful of demographic identifiers could very likely identify a student. In certain circumstances, this might be risky in that private information (i.e., gender nonconformity, disclosure of a disability) along with course evaluation data and a date stamp could identify the respondent.

Further, the items that ask respondents to \emph{guess} the identities of the instructional staff and classmates are limited, and contrary to best practices in survey construction that recommend providing the option of a ``write-in'' a response.

In parallel, the items asking respondents to identity characteristics of the instructional staff along dimensions of gender, international status, and disability are ``large buckets'' and do not include ``write-in'' options. Similarly, there was no intent to cause harm by erasing or making invisible individuals whose identities are better defined by different descriptors. Further, no write-in items were allowed. This was also intentional to prevent potential harm caused by people who could leave inappropriate, racist, or otherwise harmful comments.

As I review Qualtrics essentials and trix, I will their use (if used) in the ReCentering Psych Stats survey.

\hypertarget{qualtrics-essentials}{%
\section{Qualtrics Essentials}\label{qualtrics-essentials}}

Qualtrics is a powerful program and I find that many of the surveys we distribute don't capitalize on the features Qualtrics has to offer. Qualtrics has detailed tutorials and instructions that are well worth the investment of a weekend to review them.

In this lecture I will point you to the elements that I think are critical to constructing online surveys. Because Qualtrics tutorials are (a) clear and thorough and (b) frequently updated, I will (a) point you to the tutorials that are available at the time of this lecture prep, (b) tell you why I think they are appropriate, and (c) show you how we have used them in some of our own surveys.

Even if you think you know what you are doing, start here (and then always take the time to ``look around'' at all the options on each window):

\textbf{Survey Basic Overview}: Qualtrics' \href{https://www.qualtrics.com/support/survey-platform/survey-module/survey-module-overview/}{Survey Basic Overview} tutorial is a great place to start. From there, you can follow all kinds of leads, looking for things you want to do with your survey -- and getting ideas for what will improve it.

\href{https://www.qualtrics.com/support/survey-platform/survey-module/block-options/block-options-overview/}{\textbf{Blocks}} are the basic organizational tool in Qualtrics surveys. Blocks have two purposes: (a) grouping items shown on ``one page,'' and (b) specifying ordering and/or random selection/presentation in the survey flow.

\href{https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/question-types-guide/question-types-overview/}{\textbf{Question types}}: Take time to look at all the options. You might be surprised to learn that there is a better choice than you might have imagined.

Let's take a look at super basic/helpful question types:

\begin{itemize}
\tightlist
\item
  \href{https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/question-types-guide/static-content/descriptive-text-and-graphic/}{\textbf{Text/graphic}}: These are the types you should use for providing information (e.g., informed consent) to the participants or displaying a logo or graphic stimulus.\\
\item
  \href{https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/question-types-guide/standard-content/matrix-table/}{\textbf{Matrix table}}: The matrix table is a more efficient way to use the Likert-style items (than multiple choice). There is some controversy about whether not to use matrix tables vs.~multiple choice dropdowns. As both a survey developer and a respondent, I prefer the matrix table.

  \begin{itemize}
  \tightlist
  \item
    Make sure to select a reasonable amount of header repetitions. This allows the respondent the maximum opportunity to see the column descriptors (and avoid guessing/remembering) while they are responding.
  \end{itemize}
\item
  \href{https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/question-types-guide/standard-content/slider/}{\textbf{Slider}} : The slider is designed for obtaining truly continuous data on a 1 to 100 scale. This range can be adapted to any interval you choose, and you can add anchors to the scale. If the scale you are using is already published, and has not been psychometrically evaluated for slider use, you should probably stick with the format recommended in the publication. But if you are writing test items, consider this option.
\item
  \href{https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/question-types-guide/standard-content/text-entry/}{\textbf{Text Entry Questions}}: Text boxes have multiple options for answer length.\\
\item
  \href{https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/validation/}{\textbf{Validation}}: Content validation allows the user to permit certain types of information and specify their formats (e.g., numbers, e-mail addresses, dates). There is art to balancing between being overly restricting and ensuring that the data is entered in the most clear and consistent way possible with honoring the uniqueness of each respondent. Another validation option I frequently use is one that asks individuals if they intended to leave something blank. This is tool that helps prevent missingness without forcing an individual to respond to an item that (a) might not be clear to them, (b) might not be appropriate or them, and/or (c) might result in an answer that is untrue for their unique circumstance.
\end{itemize}

\hypertarget{qual-trix}{%
\section{Qual-TRIX}\label{qual-trix}}

\href{https://www.qualtrics.com/support/survey-platform/my-projects/sharing-a-project/}{\textbf{Collaborating}} with other Qualtrics users in your institution is easy! Scroll down to ``Collaborating Inside Your Organization'' and follow the instructions for adding individuals to your survey (you must ``own'' the survey; your collaborators will not be able to add others).

The ability to \textbf{schedule survey distributions} is like having your very own assistant! If you have a roster (contact list) you can schedule distributions, reminders, and thank you's. Qualtrics will keep track of who responds and send reminders to the non-responders. Here are resources for

\begin{itemize}
\tightlist
\item
  \href{https://www.qualtrics.com/support/survey-platform/distributions-module/email-distribution/emails/emails-overview/}{E-mail overview}\\
\item
  \href{https://www.qualtrics.com/support/survey-platform/distributions-module/email-distribution/emails/email-distribution-management/}{E-mail distribution management}\\
\item
  \href{https://www.qualtrics.com/support/survey-platform/contacts/creating-a-contact-list/}{Directories}
\end{itemize}

\textbf{Personalizing} invitations and surveys. \href{https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/piped-text/piped-text-overview/}{Piped text} is a way to personalize invitations and/or ``carry forward'' prior responses into new questions.

\href{https://www.qualtrics.com/support/survey-platform/survey-module/survey-flow/standard-elements/randomizer/}{\textbf{Randomization} of blocks} (or a subset of blocks) can be use for several purposes such as: (a) using random selection to display one or more blocks to respondents -- as in a random clinical trial, (b) to randomly display a percentage of blocks or items to shorten the survey in a planned missing design, and (c) randomly display some or all of the blocks of the survey to all respondents so that when respondents experience test fatigue, when they quit responding, ``the last items/surveys'' aren't always the same ones. This functions to distribute missingness across surveys.

\href{https://www.qualtrics.com/support/survey-platform/survey-module/block-options/question-randomization/}{\textbf{Randomization} of items} within a block can be used for similar purposes. You can also use this to display only some of the items (e.g., planned missingness).

\href{https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/question-types-guide/advanced/file-upload/}{\textbf{File upload} from respondents} is an additional package that requires the institution to pay a higher fee. If available, this allows respondents to upload some sort of file (photo, PowerPoint, .pdf). We use it for poster contests at professional contests (where students upload their poster for online judging in advance of the conference). A colleague of mine uses this function to collect application elements (i.e., resumes, cover letters, reference letters) to a fellowship program.

\begin{itemize}
\tightlist
\item
  As researchers, we can also upload files (e.g., hardcopy of informed consent, documents to be reviewed) for use by the respondent.
\end{itemize}

\href{https://www.qualtrics.com/support/survey-platform/survey-module/question-options/display-logic/}{\textbf{Display, Skip, and/or Branch Logic}} can be used to help display to respondents \emph{only} the items that pertain to them. There are multiple approaches to doing this. Using a display logic approach may feel a bit \emph{backward} where the logic is applied \emph{from} the landing spot. We did this extensively in as study that involved two language versions and three age options.

Two other approaches for these issues are \href{https://www.qualtrics.com/support/survey-platform/survey-module/question-options/skip-logic/}{skip logic} and \href{https://www.qualtrics.com/support/survey-platform/survey-module/survey-flow/standard-elements/branch-logic/}{branch logic}

\hypertarget{even-more-particularly-relevant-to-irb}{%
\section{Even moRe, particularly relevant to iRb}\label{even-more-particularly-relevant-to-irb}}

We can use Qualtrics tools for purposes beyond collecting and downloading data. These tools are especially useful when I think about IRB applications and ethics related to data collection.

\href{https://www.qualtrics.com/support/survey-platform/survey-module/survey-tools/import-and-export-surveys/}{\textbf{Exporting to Word}}: Helpful for your IRB application (and perhaps in a cloud so that a team can use track changes to edit), it is super simple to export the survey to Microsoft Word. Additionally, you can specify options for including question numbers, recode values, logic, and so forth. This works well to create a codebook for your research team.

\href{https://www.qualtrics.com/support/survey-platform/distributions-module/web-distribution/anonymous-link/}{\textbf{Anonymizing responses}}: Another step toward an anonymous response is to withhold the IP address, latitude/longitude, and any contact information (e.g., e-mail, name) that you may have uploaded in an e-mail distribution directory. This is accomplished in the Survey Options menu. Do be careful -- while anonymizing responses is an ethical, best practice, the deleted information cannot be recovered.

\href{https://www.qualtrics.com/support/survey-platform/survey-module/survey-options/survey-protection/\#PreventingRespondentsFromTakingYourSurveyMoreThanOnce}{\textbf{Prevent ballot box stuffing}}: Want to make sure that respondents only answer once? In the same Survey Options window, you can prevent ballot box stuffing. This is helpful when surveys are distributed with an \emph{anonymous link}. The tool prevents more than one survey from the same IP address.

Other security options include

\begin{itemize}
\tightlist
\item
  Password protection
\item
  HTTP Referrer verification
\end{itemize}

Look also at:

\begin{itemize}
\tightlist
\item
  \textbf{Progress bar} to provide participants hope (or despair) for ``how much longer.''
\item
  \textbf{Survey termination} to connect custom endings and thank-you notes.
\item
  \href{https://www.qualtrics.com/support/survey-platform/survey-module/survey-options/partial-completion/}{\textbf{Partial completion}} to specify how long the respondent has to complete the survey (after opening it) and whether it is recorded or deleted if it is not completed.

  \begin{itemize}
  \tightlist
  \item
    Related to this, back on the \emph{Data \& Analysis} tab, you can see both the numbers of \href{https://www.qualtrics.com/support/survey-platform/data-and-analysis-module/data/responses-in-progress/}{recorded responses and responses in progress}. You also have options to manually determine how you want to include/exclude the responses in progress.
  \item
    Failure of the respondent to click the final ``--\textgreater{}'' submit and progress symbol is often the reason that surveys that are \textgreater{} 90\% complete aren't counted as ``complete.'' What to do? Options: (a) don't say ``Thanks and goodbye'' on a page that has any items, and (b) provide instructions to look for the ``--\textgreater{}'' symbol to continue.
  \end{itemize}
\end{itemize}

Finally, \textbf{PREVIEW PREVIEW PREVIEW}! There is no better way check your work than with previews.

\hypertarget{intravenous-qualtrics}{%
\section{intRavenous Qualtrics}\label{intravenous-qualtrics}}

Access credentials for the institutional account, individual user's account, and survey are essential for getting the survey items and/or results to export into R. The Qualtrics website provides a tutorial for \href{https://www.qualtrics.com/support/integrations/api-integration/overview/\#GeneratingAnAPIToken}{generating an API token}.

We need two pieces of information: the \textbf{root\_url} and an \textbf{API token}.

\begin{itemize}
\tightlist
\item
  Log into your respective qualtrics.com account.
\item
  Select Account Settings
\item
  Choose ``Qualtrics IDs'' from the username dropdown
\end{itemize}

We need the \textbf{root\_url}. This is the first part of the web address for the Qualtrics account. For our institution it is: spupsych.az1.qualtrics.com

The API token is in the box labeled, ``API.'' If it is empty, select, ``Generate Token.'' If you do not have this option, locate the \emph{brand administrator} for your Qualtrics account. They will need to set up your account so that you have API privileges.

\emph{BE CAREFUL WITH THE API TOKEN} This is the key to your Qualtrics accounts. If you leave it in an .rmd file that you forward to someone else, this key and the base URL gives access to every survey in your account. If you share it, you could be releasing survey data to others that would violate confidentiality promises in an IRB application.

If you mistakenly give out your API token, you can generate a new one within your Qualtrics account and re-protect all its contents.

You do need to change the API key/token if you want to download data from a different Qualtrics account. If your list of surveys generates the wrong set of surveys, restart R, make sure you have the correct API token and try again.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# only have to run this ONCE to draw from the same Qualtrics}
\CommentTok{\# account...but will need to get different \#token if you are changing}
\CommentTok{\# between accounts.}
\NormalTok{qualtRics}\SpecialCharTok{::}\FunctionTok{qualtrics\_api\_credentials}\NormalTok{(}\AttributeTok{api\_key =} \StringTok{"oEwd9qu9xJOf3RoE9iiCZKSs2sfNuSbvy8LnFYxo"}\NormalTok{,}
    \AttributeTok{base\_url =} \StringTok{"spupsych.az1.qualtrics.com"}\NormalTok{, }\AttributeTok{overwrite =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{install =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{readRenviron}\NormalTok{(}\StringTok{"\textasciitilde{}/.Renviron"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\emph{all\_surveys()} generates a dataframe containing information about all the surveys stored on your Qualtrics account.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{surveys }\OtherTok{\textless{}{-}}\NormalTok{ qualtRics}\SpecialCharTok{::}\FunctionTok{all\_surveys}\NormalTok{()}
\CommentTok{\# View this as an object (found in the right: Environment).  Get}
\CommentTok{\# survey id \# for the next command If this is showing you the WRONG}
\CommentTok{\# list of surveys, you are pulling from the wrong Qualtrics account}
\CommentTok{\# (i.e., maybe this one instead of your own). Go back and change your}
\CommentTok{\# API token (it saves your old one). Changing the API likely requires}
\CommentTok{\# a restart of R.}
\NormalTok{surveys}
\end{Highlighting}
\end{Shaded}

To retrieve the survey, use the \emph{fetch\_survey()} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# obtained with the survey ID}
\CommentTok{\#\textquotesingle{}surveyID\textquotesingle{} should be the ID from above}
\CommentTok{\#\textquotesingle{}verbose\textquotesingle{} prints messages to the R console}
\CommentTok{\#\textquotesingle{}label\textquotesingle{}, when TRUE, imports data as text responses; if FALSE prints the data as numerical responses}
\CommentTok{\#\textquotesingle{}convert\textquotesingle{}, when TRUE, attempts to convert certain question types to the \textquotesingle{}proper\textquotesingle{} data type in R; because I don\textquotesingle{}t like guessing, I want to set up my own factors.}
\CommentTok{\#\textquotesingle{}force\_request\textquotesingle{}, when TRUE, always downloads the survey from the API instead of from a temporary directory (i.e., it always goes to the primary source)}
\CommentTok{\# \textquotesingle{}import\_id\textquotesingle{}, when TRUE includes the unique Qualtrics{-}assigned ID;}
\CommentTok{\# since I have provided labels, I want false}

\CommentTok{\# Out of the blue, I started getting an error, that R couldn\textquotesingle{}t find}
\CommentTok{\# function \textquotesingle{}fetch\_survey.\textquotesingle{}  After trying a million things, adding}
\CommentTok{\# qualtRics:: to the front of it solved the problem}
\NormalTok{QTRX\_df }\OtherTok{\textless{}{-}}\NormalTok{ qualtRics}\SpecialCharTok{::}\FunctionTok{fetch\_survey}\NormalTok{(}\AttributeTok{surveyID =} \StringTok{"SV\_b2cClqAlLGQ6nLU"}\NormalTok{, }\AttributeTok{time\_zone =} \ConstantTok{NULL}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{label =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{convert =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{force\_request =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{import\_id =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# useLocalTime = TRUE,}
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either a .csv file (think ``Excel lite'') or .rds object (preserves any formatting you might do).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(QTRX\_df,}
\CommentTok{\# file=\textquotesingle{}QTRX\_df.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE) bring}
\CommentTok{\# back the simulated dat from a .csv file QTRX\_df \textless{}{-} read.csv}
\CommentTok{\# (\textquotesingle{}QTRX\_df.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(QTRX\_df, \textquotesingle{}QTRX\_df.rds\textquotesingle{}) bring back the simulated dat}
\CommentTok{\# from an .rds file QTRX\_df \textless{}{-} readRDS(\textquotesingle{}QTRX\_df.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-codebook}{%
\subsection{The Codebook}\label{the-codebook}}

In order to prepare data from a survey, it is critical to know about its content, scoring directions for scales/subscales, and its design. As I demonstrated above, we can export a \href{./Rate-a-Course_Codebook.pdf}{codebook}, that is, a Word (or PDF) version of the survey with all the coding. In Qualtrics the protocol is: Survey/Tools/ImportExport/Export Survey to Word. Then select all the options you want (especially ``Show Coded Values''). A tutorial provided by Qualtrics can be found \href{https://www.qualtrics.com/support/survey-platform/survey-module/survey-tools/import-and-export-surveys/}{here}. This same process can be used to print the PDF example I used above.

I recommend providing custom variable names and recode values directly in Qualtrics before exporting them into R (and before exporting the codebook). A Qualtrics tutorial for this is provided \href{https://www.qualtrics.com/support/survey-platform/survey-module/question-options/recode-values/}{here}. In general, consider these qualities when creating variable names:

\begin{itemize}
\tightlist
\item
  Brevity: historically, SPSS variable names could be a maximum of 8 characters.
\item
  Intuitive: although variables can be renamed in R (e.g., for use in charts and tables), it is helpful when the name imported from Qualtrics provides some indication of what the variable is.
\item
  Systematic: start items in a scale with the same stem, followed by the item number -- ITEM1, ITEM2, ITEM3.
\item
  Do not include special characters or spaces in variable names; this is problematic for R.
\item
  Do not start variable names with numerals; this is problematic for R.
\end{itemize}

More complete information about data preparation is covered in chapters in the \href{https://lhbikos.github.io/ReC_MultivModel/}{ReCentering Psych Stats: Multivariate Modeling} text.

\hypertarget{using-data-from-an-exported-qualtrics-.csv-file}{%
\subsection{Using data from an exported Qualtrics .csv file}\label{using-data-from-an-exported-qualtrics-.csv-file}}

It is also possible to download the Qualtrics data in a variety of formats (e.g., CSV, Excel, SPSS). Since my R and Qualtrics history began by using files with the CSV extension (think ``Excel'' lite), that is my preference.

In Qualtrics, these are the steps to download the data: Projects/YOURsurvey/Data \& Analysis/Export \& Import/Export data/CSV/Use numeric values. In order to import this data into R, it is critical that to save this file in the same folder as the .rmd file that you will use with the data.

R is sensitive to characters used filenames. As downloaded, my Qualtrics .csv file had a long name with spaces and symbols that are not allowed. Therefore, I gave it a simple, sensible, filename, ``ReC\_Download210319.csv''. An idiosyncrasy of mine is to datestamp filenames. I use two-digit representations of the year, month, and date so that if the letters preceding the date are the same, the files would alphabetize automatically.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# QTRX\_csv \textless{}{-} qualtRics::read\_survey(\textquotesingle{}ReC\_Download210319.csv\textquotesingle{},}
\CommentTok{\# strip\_html = TRUE, import\_id = FALSE, time\_zone=NULL, legacy =}
\CommentTok{\# FALSE)}
\end{Highlighting}
\end{Shaded}

Although minor tweaking may be required, the same script above should be applicable to this version of the data.

\hypertarget{tweaking-data-format}{%
\subsection{Tweaking Data Format}\label{tweaking-data-format}}

Two general approaches:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Inside Qualtrics: Use the \href{https://www.qualtrics.com/support/survey-platform/survey-module/question-options/recode-values/}{recode values} option (found in the item's gearbox, to the left of the block) to specify variable names and recode values. These should be preserved on the download.
\item
  In the R script: In another lecture I demonstrate how to change the formats of data (character, string), selecting only the variables in which we are interested (e.g., excluding the meta-data), and renaming variables sensibly.
\end{enumerate}

Both work! You can choose your preference. When you are working with a team, map out an explicit process with your collaborators.

\hypertarget{practice-problems-1}{%
\section{Practice Problems}\label{practice-problems-1}}

The suggestion for practice is to develop a questionnaire, format it, pilot it, and download it. Essentially you will be

\begin{itemize}
\tightlist
\item
  Formatting a survey on Qualtrics using all the best practices identified in the lecture. These include:

  \begin{itemize}
  \tightlist
  \item
    Having an introductory statement (to include statement of confidentiality), directions for each sub-survey (if more than one), and a closing statement.
  \item
    Selecting the most appropriate question type for the items. For example, matrix instead of multiple choice.
  \item
    Within the question type, using the appropriate options for proper formatting (e.g., the anchors in a matrix should be topically consistent and equal interval).
  \end{itemize}
\item
  The survey should include minimum of 3 of the qualTRIXter skills (identified in lecture). Choose from:

  \begin{itemize}
  \tightlist
  \item
    establishing collaboration
  \item
    scheduling e-mail distribution and follow-up
  \item
    personalizing the survey in some way
  \item
    randomization of blocks or items
  \item
    integrating display, skip, or branch logic (e.g., having males and females take a different route)
  \item
    exporting the survey to Word
  \item
    recoding variables in the item controls
  \item
    anonymizing the responses
  \item
    preventing ballot box stuffing
  \item
    including a progress bar
  \item
    creating a custom ending, e-mail, or thank-you note
  \item
    something else that YOU discovered that isn't in the lecture
  \end{itemize}
\item
  Piloting it, getting their feedback, and identifying what problems are (and how you might fix them).

  \begin{itemize}
  \tightlist
  \item
    with 3 folks from your research team, cohort, or this class
  \item
    with 3 additional folks who aren't quite as ``research savvy''
  \item
    collect their feedback (ideally in a text-item directly on the survey itself) and write a brief summary (3 paragraphs max) of their impressions and how you might improve the survey
  \end{itemize}
\item
  Import the Qualtrics data directly R

  \begin{itemize}
  \tightlist
  \item
    preferably, directly from Qualtrics with the API token, base URL, and survey ID
  \item
    alternatively (for the same \# of points) from the exported CSV file \emph{via the qualtRics package} (required)
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5493}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1972}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Possible
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Earned
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1. Qualtrics survey best practices & 5 & \\
2. QualTRIXter skills (at least 3) & 5 & \\
3. Minimum of 6 pilot respondents & 5 & \\
4. Summary of pilot feedback & 5 & \\
5. Import of Qualtrics data into R & 5 & \\
6. Explanation to grader & 5 & \\
\textbf{Totals} & 20 & \\
\end{longtable}

\hypertarget{rxy}{%
\chapter{Psychometric Validity: Basic Concepts}\label{rxy}}

\href{https://youtube.com/playlist?list=PLtz5cFLQl4KPWUS9MmUEu5XJqr5qLzwUJ\&si=ciHvbMInpwWSLxuM}{Screencasted Lecture Link}

The focus of this lecture is to provide an introduction to validity. This includes understanding some of the concerns of validity, different aspects of validity, and factors as they affect validity coefficients.

\hypertarget{navigating-this-lesson-2}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-2}}

There is just over one hour of lecture. If you work through the materials with me, plan for an additional hour.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-2}{%
\subsection{Learning Objectives}\label{learning-objectives-2}}

Focusing on this week's materials, make sure you can:

\begin{itemize}
\tightlist
\item
  Distinguish between different types of validity based on short descriptions.
\item
  Compute and interpret validity coefficients.
\item
  Evaluate the incremental validity of an instrument-of-interest.
\item
  Define and interpret the standard error of estimate.
\item
  Develop a rationale that defends importance of establishing the validity of a measuring instrument.
\end{itemize}

\hypertarget{planning-for-practice-2}{%
\subsection{Planning for Practice}\label{planning-for-practice-2}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. The least complex is to change the random seed in the research and rework the problem demonstrated in the lesson. The results \emph{should} map onto the ones obtained in the lecture.

The second option involves utilizing one of the simulated datasets available in this OER. The \protect\hyperlink{sims}{last lesson} in the OER contains three simulations that could be used for all of the statistics-based practice suggestions. At this very first lesson which involves statistics, I strongly encourage you to select one of these sets and use it for all of the statistics-based homework. Working a single instrument ``all the way through'' a psychometric evaluation is a great way to (a) understand the psychometric evaluation workflow and (b) get to know an instrument.

Alternatively, Lewis and Neville's \citeyearpar{lewis_construction_2015} Gendered Racial Microaggressions Scale for Black Women will be used in the lessons on exploratory factor analysis; Keum et al.'s Gendered Racial Microaggressions Scale for Asian American Women \citep{keum_gendered_2018} will be used in the lessons on confirmatory factor analysis; and Conover et al.'s \citeyearpar{conover_development_2017} Ableist Microaggressions Scale is used in the lesson on invariance testing. Any of these would be suitable for the PCA and PAF homework assignments.

As a third option, you are welcome to use data to which you have access and is suitable for validity testing. In any case, please select a scale that has item-level data for which there is a theorized total scale score as well as two or more subscales (three subscales is ideal). With each of these options, plan to:

\begin{itemize}
\tightlist
\item
  Create a correlation matrix that includes the instrument-of-interest and the variables that will have varying degrees of relation*.
\item
  With convergent and discriminant validity in mind, interpret the validity coefficients; this should include an assessment about whether the correlation coefficients (at least two different pairings) are statistically significantly different from each other.\\
\item
  With at least three variables, evaluate the degree to which the instrument demonstrates incremental validity
\end{itemize}

\hypertarget{readings-resources-2}{%
\subsection{Readings \& Resources}\label{readings-resources-2}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Jhangiani, R. S., Chiang, I.-C. A., Cuttler, C., \& Leighton, D. C. (2019). Reliability and Validity. In \emph{Research Methods in Psychology}. \url{https://doi.org/10.17605/OSF.IO/HF7DQ}
\item
  Clark, L. A. \& Watson, D. (1995). Constructing validity: Basic issues in objective scale development. Psychological Assessment, 7, 309-319.

  \begin{itemize}
  \tightlist
  \item
    In this manuscript, Clark and Watson (1995) create a beautiful blend of theoretical issues and practical suggestions for creating measures that evidence construct validity. From the practical perspective, the authors first guide potential scale constructors through the literature review and creating an item pool (including tips on writing items). The authors address structural validity by first beginning with strategies for constructing the test. In this section, the authors revisit the issue of dimensionality (i.e., alpha vs.~factor analysis). Finally, the authors look at initial data collection (addressing sample size) and psychometric evaluation.
  \end{itemize}
\end{itemize}

\hypertarget{packages-2}{%
\subsection{Packages}\label{packages-2}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# will install the package if not already installed}
\CommentTok{\# if(!require(tidyverse))\{install.packages(\textquotesingle{}tidyverse\textquotesingle{})\}}
\CommentTok{\# if(!require(MASS))\{install.packages(\textquotesingle{}MASS\textquotesingle{})\}}
\CommentTok{\# if(!require(psych))\{install.packages(\textquotesingle{}psych\textquotesingle{})\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{research-vignette-1}{%
\section{Research Vignette}\label{research-vignette-1}}

This lesson provides descriptions of numerous pathways for establishing an instrument's validity. In fact, best practices involving numerous demonstrations of validity. Across several lessons, we will rework several of the correlational analyses reported in the research vignette. For this lesson in particular, the research vignette allows demonstrations of convergent/discriminant validity and incremental validity.

The research vignette for this lesson is the development and psychometric evaluation of the Perceptions of the LGBTQ College Campus Climate Scale \citep{szymanski_perceptions_2020}. The scale is six items with responses rated on a 7-point Likert scale ranging from 1 (\emph{strongly disagree}) to 7 (\emph{strongly agree}). Higher scores indicate more negative perceptions of the LGBTQ campus climate. Szymanski and Bissonette have suggested that the psychometric evaluation supports using the scale in its entirety or as subscales composed of the following items:

\begin{itemize}
\tightlist
\item
  College Response to LGBTQ students:

  \begin{itemize}
  \tightlist
  \item
    My university/college is cold and uncaring toward LGBTQ students.
  \item
    My university/college is unresponsive to the needs of LGBTQ students.
  \item
    My university/college provides a supportive environment for LGBTQ students. {[}un{]}supportive; must be reverse-scored
  \end{itemize}
\item
  LGBTQ Stigma:

  \begin{itemize}
  \tightlist
  \item
    Negative attitudes toward LGBTQ persons are openly expressed on my university/college campus.
  \item
    Heterosexism, homophobia, biphobia, transphobia, and cissexism are visible on my university/college campus.
  \item
    LGBTQ students are harassed on my university/college campus.
  \end{itemize}
\end{itemize}

A \href{https://www.researchgate.net/publication/332062781_Perceptions_of_the_LGBTQ_College_Campus_Climate_Scale_Development_and_Psychometric_Evaluation/link/5ca0bef945851506d7377da7/download}{preprint} of the article is available at ResearchGate.

Because data is collected at the item level (and I want this resource to be as practical as possible), I have simulated the data for each of the scales utilized in the research vignette at the item level. Simulating the data involved using factor loadings, means, standard deviations, and correlations between the scales. Because the simulation will produce ``out-of-bounds'' values, the code below re-scales the scores into the range of the Likert-type scaling and rounds them to whole values.

Five additional scales were reported in the Szymanski and Bissonette article \citeyearpar{szymanski_perceptions_2020}. Unfortunately, I could not locate factor loadings for all of them; and in two cases, I used estimates from a more recent psychometric analysis. When the individual item and their factor loadings were known, I assigned names based on item content (e.g., ``lo\_energy'') rather than using item numbers (e.g., ``PHQ4''). When I am doing psychometric analyses, I prefer item-level names so that I can quickly see (without having to look up the item content) how the items are behaving. While the focus of this series of chapters is on the LGBTQ Campus Climate scale, this simulated data might be useful to you in one or more of the suggestions for practice (e.g., examining the psychometric characteristics of one or the other scales). The scales, their original citation, and information about how I simulated data for each are listed below.

\begin{itemize}
\tightlist
\item
  \textbf{Sexual Orientation-Based Campus Victimization Scale} \citep{herek_documenting_1993} is a 9-item item scale with Likert scaling ranging from 0 (\emph{never}) to 3 (\emph{two or more times}). Because I was not able to locate factor loadings from a psychometric evaluation, I simulated the data by specifying a 0.8 as a standardized factor loading for each of the items.
\item
  \textbf{College Satisfaction Scale} \citep{helm_relationship_1998} is a 5-item scale with Likert scaling ranging from 1 (\emph{strongly disagree}) to 7 (\emph{strongly agree}). Higher scores represent greater college satisfaction. Because I was not able to locate factor loadings from a psychometric evaluation, I simulated the data by specifying a 0.8 as a standardized factor loading for each of the items.
\item
  \textbf{Institutional and Goals Commitment} \citep{pascarella_predicting_1980} is a 6-item subscale from a 35-item measure assessing academic/social integration and institutional/goal commitment (5 subscales total). The measure had with Likert scaling ranging from 1 (\emph{strongly disagree}) to 5 (\emph{strongly agree}). Higher scores on the institutional and goals commitment subscale indicate greater intentions to persist in college. Data were simulated using factor loadings in the source article.
\item
  \textbf{GAD-7} \citep{spitzer_brief_2006} is a 7-item scale with Likert scaling ranging from 0 (\emph{not at all}) to 3 (\emph{nearly every day}). Higher scores indicate more anxiety. I simulated data by estimating factor loadings from Brattmyr et al. \citeyearpar{brattmyr_factor_2022}.
\item
  \textbf{PHQ-9} \citep{kroenke_phq-9_2001} is a 9-item scale with Likert scaling ranging from 0 (\emph{not at all}) to 3 (\emph{nearly every day}). Higher scores indicate higher levels of depression. I simulated data by estimating factor loadings from Brattmyr et al. \citeyearpar{brattmyr_factor_2022}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Entering the intercorrelations, means, and standard deviations from the journal article}

\NormalTok{Szymanski\_generating\_model }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}}
\StringTok{        \#measurement model}
\StringTok{        CollegeResponse  =\textasciitilde{} .88*cold + .73*unresponsive + .73*supportive }
\StringTok{        Stigma =\textasciitilde{} .86*negative + .76*heterosexism + .71*harassed}
\StringTok{        Victimization =\textasciitilde{} .8*Vic1 + .8*Vic2 + .8*Vic3 + .8*Vic4 + .8*Vic5 + .8*Vic6 + .8*Vic7 + .8*Vic8 + .8*Vic9}
\StringTok{        CollSat =\textasciitilde{} .8*Sat1 + .8*Sat2 + .8*Sat3 + .8*Sat4 + .8*Sat5}
\StringTok{        Persistence =\textasciitilde{} .69*graduation\_importance + .63*right\_decision + .62*will\_register + .59*not\_graduate + .45*undecided + .44*grades\_unimportant}
\StringTok{        Anxiety =\textasciitilde{} .851*nervous + .887*worry\_control + .894*much\_worry + 674*cant\_relax + .484*restless + .442*irritable + 716*afraid}
\StringTok{        Depression =\textasciitilde{} .798*anhedonia + .425*down +  .591*sleep +  .913*lo\_energy +  .441*appetite +  .519*selfworth +  .755*concentration +  .454*too\_slowfast + .695*s\_ideation}
\StringTok{   }
\StringTok{        \#Means}
\StringTok{         CollegeResponse \textasciitilde{} 2.71*1}
\StringTok{         Stigma \textasciitilde{}3.61*1}
\StringTok{         Victimization \textasciitilde{} 0.11*1}
\StringTok{         CollSat \textasciitilde{} 5.61*1}
\StringTok{         Persistence \textasciitilde{} 4.41*1}
\StringTok{         Anxiety \textasciitilde{} 1.45*1}
\StringTok{         Depression \textasciitilde{}1.29*1}

\StringTok{         }
\StringTok{        \#Correlations}
\StringTok{         CollegeResponse \textasciitilde{}\textasciitilde{} .58*Stigma}
\StringTok{         CollegeResponse \textasciitilde{}\textasciitilde{} {-}.25*Victimization}
\StringTok{         CollegeResponse \textasciitilde{}\textasciitilde{}  {-}.59*CollSat}
\StringTok{         CollegeResponse \textasciitilde{}\textasciitilde{}  {-}.29*Persistence}
\StringTok{         CollegeResponse \textasciitilde{}\textasciitilde{}  .17*Anxiety}
\StringTok{         CollegeResponse \textasciitilde{}\textasciitilde{}  .18*Depression}
\StringTok{         }
\StringTok{         Stigma \textasciitilde{}\textasciitilde{} .37*Victimization}
\StringTok{         Stigma \textasciitilde{}\textasciitilde{}  {-}.41*CollSat}
\StringTok{         Stigma \textasciitilde{}\textasciitilde{}  {-}.19*Persistence}
\StringTok{         Stigma \textasciitilde{}\textasciitilde{}  .27*Anxiety}
\StringTok{         Stigma \textasciitilde{}\textasciitilde{}  .24*Depression}
\StringTok{         }
\StringTok{         Victimization \textasciitilde{}\textasciitilde{}  {-}.22*CollSat}
\StringTok{         Victimization \textasciitilde{}\textasciitilde{}  {-}.04*Persistence}
\StringTok{         Victimization \textasciitilde{}\textasciitilde{}  .23*Anxiety}
\StringTok{         Victimization \textasciitilde{}\textasciitilde{}  .21*Depression}
\StringTok{         }
\StringTok{         CollSat \textasciitilde{}\textasciitilde{}  .53*Persistence}
\StringTok{         CollSat \textasciitilde{}\textasciitilde{}  {-}.29*Anxiety}
\StringTok{         CollSat \textasciitilde{}\textasciitilde{}  {-}.32*Depression}
\StringTok{         }
\StringTok{         Persistence \textasciitilde{}\textasciitilde{}  {-}.22*Anxiety}
\StringTok{         Persistence \textasciitilde{}\textasciitilde{}  {-}.26*Depression}
\StringTok{         }
\StringTok{         Anxiety \textasciitilde{}\textasciitilde{}  .76*Depression}
\StringTok{        \textquotesingle{}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240218}\NormalTok{)}
\NormalTok{dfSzy }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{simulateData}\NormalTok{(}\AttributeTok{model =}\NormalTok{ Szymanski\_generating\_model,}
                              \AttributeTok{model.type =} \StringTok{"sem"}\NormalTok{,}
                              \AttributeTok{meanstructure =}\NormalTok{ T,}
                              \AttributeTok{sample.nobs=}\DecValTok{646}\NormalTok{,}
                              \AttributeTok{standardized=}\ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\#used to retrieve column indices used in the rescaling script below}
\NormalTok{col\_index }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(dfSzy))}

\CommentTok{\#The code below loops through each column of the dataframe and assigns the scaling accordingly}
\CommentTok{\#Rows 1 thru 6 are the Perceptions of LGBTQ Campus Climate Scale}
\CommentTok{\#Rows 7 thru 15 are the Sexual Orientation{-}Based Campus Victimization Scale}
\CommentTok{\#Rows 16 thru 20 are the College Satisfaction Scale}
\CommentTok{\#Rows 21 thru 26 are the Institutional and Goals Commitment Scale }
\CommentTok{\#Rows 27 thru 33 are the GAD7}
\CommentTok{\#Rows 34 thru 42 are the PHQ9}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(dfSzy))\{  }
  \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{6}\NormalTok{)\{   }
\NormalTok{    dfSzy[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfSzy[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{7}\NormalTok{))}
\NormalTok{  \}}
    \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{7} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{15}\NormalTok{)\{   }
\NormalTok{    dfSzy[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfSzy[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{    \}}
      \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{16} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{20}\NormalTok{)\{  }
\NormalTok{    dfSzy[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfSzy[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{7}\NormalTok{))}
\NormalTok{      \}}
        \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{21} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{26}\NormalTok{)\{  }
\NormalTok{    dfSzy[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfSzy[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{        \}}
  \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{27} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{33}\NormalTok{)\{  }
\NormalTok{    dfSzy[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfSzy[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{  \}}
    \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{34} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{42}\NormalTok{)\{  }
\NormalTok{    dfSzy[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfSzy[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{  \}}
\NormalTok{\}}

\CommentTok{\#rounding to integers so that the data resembles that which was collected}
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{dfSzy }\OtherTok{\textless{}{-}}\NormalTok{ dfSzy }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{round}\NormalTok{(}\DecValTok{0}\NormalTok{) }

\CommentTok{\#quick check of my work}
\CommentTok{\#psych::describe(dfSzy) }

\CommentTok{\#Reversing the supportive item on the Perceptions of LGBTQ Campus Climate Scale so that the exercises will be consistent with the format in which the data was collected}

\NormalTok{dfSzy }\OtherTok{\textless{}{-}}\NormalTok{ dfSzy }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{supportiveNR =} \DecValTok{8} \SpecialCharTok{{-}}\NormalTok{ supportive)}

\CommentTok{\#Reversing three items on the Institutional and Goals Commitments scale so that the exercises will be consistent with the format in which the data was collected}

\NormalTok{dfSzy }\OtherTok{\textless{}{-}}\NormalTok{ dfSzy }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{not\_graduateNR =} \DecValTok{8} \SpecialCharTok{{-}}\NormalTok{ not\_graduate)}\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{undecidedNR =} \DecValTok{8} \SpecialCharTok{{-}}\NormalTok{ undecided)}\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{grades\_unimportantNR =} \DecValTok{8} \SpecialCharTok{{-}}\NormalTok{ grades\_unimportant)}

\NormalTok{dfSzy }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(dfSzy, }\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(supportive, not\_graduate, undecided, grades\_unimportant))}
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either an .rds object (preserves any formatting you might do) or a.csv file (think ``Excel lite'').

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(dfSzy, \textquotesingle{}SzyDF.rds\textquotesingle{}) bring back the simulated dat from}
\CommentTok{\# an .rds file dfSzy \textless{}{-} readRDS(\textquotesingle{}SzyDF.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(dfSzy,}
\CommentTok{\# file=\textquotesingle{}SzyDF.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE) bring}
\CommentTok{\# back the simulated dat from a .csv file}
\NormalTok{dfSzy }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"SzyDF.csv"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As we move into the lecture, allow me to provide a content advisory. Individuals who hold LGBTQIA+ identities are frequently the recipients of discrimination and harassment. If you are curious about why these items are considered to be stigmatizing or non-responsive, please do not ask a member of the LGBTQIA+ community to explain it to you; it is not their job to educate others on discrimination, harassment, and microaggressions. Rather, please read the article in its entirety. Additionally, resources such as \href{https://www.thetrevorproject.org/}{The Trevor Project}, \href{https://www.glsen.org/}{GLSEN}, and \href{https://www.campuspride.org/}{Campus Pride} are credible sources of information for learning more.

\hypertarget{fundamentals-of-validity}{%
\section{Fundamentals of Validity}\label{fundamentals-of-validity}}

\textbf{Validity} (the classic definition) is the ability of a test to measure what it purports to measure. Supporting that definition are these notions:

\begin{itemize}
\tightlist
\item
  Validity is extent of matching, congruence, or ``goodness of fit'' between the operational definition and concept it is supposed to measure.
\item
  An instrument is said to be valid if it taps the concept it claims to measure.
\item
  Validity is the appropriateness of the interpretation of the results of an assessment procedure for a given group of individuals, not to the procedure itself.
\item
  Validity is a matter of degree; it does not exist on an all-or-none basis.
\item
  Validity is always specific to some particular use or interpretation.
\item
  Validity is a unitary concept.
\item
  Validity involves an overall evaluative judgment.
\end{itemize}

Over the years (and, perhaps within each construct), validity has somewhat of an \emph{evolutionary} path from a focus on content, to prediction, to theory and hypothesis testing.

When the focus is on \textbf{content}, we are concerned with the:

\begin{itemize}
\tightlist
\item
  Assessment of what individuals had learned in specific content areas.
\item
  Relevance of its content (i.e., we compare the content to the content domain).
\end{itemize}

When the focus is on \textbf{prediction}, we are concerned with:

\begin{itemize}
\tightlist
\item
  How different persons respond in a given situation (now or later).
\item
  The correlation coefficient between test scores (predictor) and the assessment of a criterion (performance in a situation)
\end{itemize}

A focus on \textbf{theory and hypothesis testing} adds:

\begin{itemize}
\tightlist
\item
  A strengthened theoretical orientation.
\item
  A close linkage between psychological theory and verification through empirical and experimental hypothesis testing.
\item
  An emphasis on constructs in describing and understanding human behavior.
\end{itemize}

\textbf{Constructs} are broad categories, derived from the common features shared by directly observable behavioral variables. They are theoretical entities and not directly observable. \textbf{Construct validity} is at the heart of psychometric evaluation. We define \textbf{construct validity} as the fundamental and all-inclusive validity concept, insofar as it specifies what the test measures. Content and predictive validation procedures are among the many sources of information that contribute to the understanding of the constructs assessed by a test.

\hypertarget{validity-criteria}{%
\section{Validity Criteria}\label{validity-criteria}}

We have just stated that validity is an overall, evaluative judgment. Within that umbrella are different criteria by which we judge the validity of a measure. We casually refer to them as \emph{types}, but each speaks to that unitary concept.

\hypertarget{content-validity}{%
\subsection{Content Validity}\label{content-validity}}

Content validity is concerned with the representativeness of the domain being assessed. Content validation procedures may differ depending on whether the test is in the educational/achievement context or if it is more of an attitude/behavioral survey.

In the educational/achievement context, content validation seeks to ensure the items on an exam are appropriate for the content domain being assessed.

A \textbf{table of specifications} is a two-way chart which indicates the instructionally relevant learning tasks to be measured. Percentages in the table indicate the relative degree of emphasis that each content area.

Let's imagine that I was creating a table of specifications for items on a quiz for this very chapter. The columns represent the types of outcomes we might expect. The American Psychological Association often talks about \emph{KSAs} (knowledge, skills, attitudes), so I will utilize those as a framework. You'll notice that the number of items and percentages do not align mathematically. Because, in the exam, I would likely weight application items (e.g., ``work the problem'') more highly than knowledge items (e.g., multiple choice, true/false), the relative weighting may differ.

\textbf{Table of Specifications}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.4430}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1392}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1392}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1392}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1392}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Learning Objectives
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Knowledge
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Skills
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Attitudes
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\% of test
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Distinguish between different types of validity based on short descriptions. & 6 items & & & 30\% \\
Compute and interpret validity coefficients. & & 2 items & & 15\% \\
Evaluate the incremental validity of an instrument-of-interest. & & 1 item & & 20\% \\
Define and interpret the standard error of estimate. & 1 item & & & 15\% \\
Develop a rationale that defends importance of establishing the validity of a measuring instrument. & & & 1 item & 20\% \\
TOTALS & 7 items & 3 items & 1 item & 100\% \\
\end{longtable}

\textbf{Subject matter experts} (SMEs) are individuals chosen to evaluate items based on their degree of knowledge of the subject being assessed. If SMES are used, the researcher should:

\begin{itemize}
\tightlist
\item
  Report how many SMEs and list their professional qualifications.
\item
  Report any directions the SMEs were given; if they were used to evaluate items, report the extent of agreement.
\end{itemize}

Empirical procedures for enhancing content validity of educational assessments may include:

\begin{itemize}
\tightlist
\item
  Comparing item-level and total scores with grades; lower grades should get lower scores.
\item
  Analyzing individual errors.
\item
  Observing student work methods (have the students ``think aloud'' in front of an examiner).
\item
  Evaluating the role of speed, noting how many do not complete the test in the time allowed.
\item
  Correlating the scores with a reading comprehension test (if the exam is highly correlated, then it may be a test of reading and not another subject). Alternatively, if it is a reading comprehension test, give the student the questions (without the passage) to see how well they answered the questions on the basis of prior knowledge.
\end{itemize}

For surveys and tests outside of educational settings, content validation procedures ask, ``Does the test cover a representative sample of the specified skills and knowledge?'' and ``Is test performance reasonably free from the influence of irrelevant variables?'' Naturally, SMEs might be used.

An example of content validation from Industrial-Organizational Psychology is the job analysis which precedes the development of test for employee selection and classification. Not all tests require content analysis. In aptitude and personality tests we are probably more interested in other types of validity evaluation.

\hypertarget{face-validity-the-unvalidity}{%
\subsection{Face Validity: The ``Un''validity}\label{face-validity-the-unvalidity}}

Face validity is concerned with the question, ``How does an assessment look on the `face of it'?'' Let's imagine that on a qualification exam for electricians, a math item asks the electrician to estimate the amount of yarn needed to complete a project. The item may be more \emph{face valid} if the calculation was with wire. Thus, face validity can often be improved by reformulating test items in terms that appear relevant and plausible for the context.

Face validity should never be regarded as a substitute for objectively determined validity. In contrast, it should not be assumed that when a (valid and reliable) test has been modified to increase its face validity, that its objective validity and reliability is unaltered. That is, it should be reevaluated.

\hypertarget{criterion-related-validity}{%
\subsection{Criterion-Related Validity}\label{criterion-related-validity}}

Criterion-related validity has to do with the test's ability to \emph{predict} an outcome (the criterion). If the criterion is something that occurs simultaneously, it is an assessment of \textbf{concurrent validity}; if it is in the future, it is an assessment of \textbf{predictive validity.}

A \textbf{criterion} is the ``thing'' that the test should, theoretically, be able to \emph{predict}. That prediction could be occurring at the same time (\emph{concurrent validity}) or at a future time (\emph{predictive validity}). Regardless, the estimate of the criteria must be independent of the survey/assessment being evaluated. The table below provides examples of types of tests and concurrent and predictive validity criteria.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3514}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3243}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3243}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type of Test
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Concurrent Criteria Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Predictive Criteria Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
A shorter (or cheaper) standardized achievement test & school grades, existing standardized tests & subsequent graduation/college admissions, cumulative GPA \\
Employee selection tests & decision made by a search committee & subsequent retention or promotion of the selected employee \\
Assessment of depression severity (shorter or cheaper) & diagnosis from a mental health professional; correlation with an established measure & inpatient hospitalization or act of self-harm \\
\end{longtable}

\textbf{Contrasted groups} is a specific type of criterion-related validity. Clearly differentiated groups (e.g., salesclerks versus executives; engineers versus musicians) are chosen to see if exam performance or profiles differ in predictable ways.

\textbf{Criterion contamination} occurs when test scores, themselves, are used to make decisions about the criteria. To prevent this:

\begin{itemize}
\tightlist
\item
  No person who participates in the assignment of criterion ratings can have any knowledge of the examinee's test scores.\\
\item
  The test scores must be kept strictly confidential.
\end{itemize}

There are a number of issues related to criterion-related validity.

\begin{itemize}
\tightlist
\item
  Is the criterion choice appropriate?

  \begin{itemize}
  \tightlist
  \item
    Criterion validity is only as good as the validity of the criterion to which one is making a comparison.
  \item
    In the 1980s and 1990s there was more attention in this area; that is critics questioned the quality of the criterion being used.
  \end{itemize}
\item
  To what degree can the results of criterion-related validity be generalized?

  \begin{itemize}
  \tightlist
  \item
    Most tests are developed (intentionally) for a local context, setting, or audience. Consequently, in the local context, the criterion-prediction sample is usually too small (i.e., 50 cases or less).
  \item
    Those who want to generalize the test to a broader population should evaluate the test in relation to the new purpose.
  \end{itemize}
\item
  Is there a role for meta-analysis?

  \begin{itemize}
  \tightlist
  \item
    Repeated validation studies of our tests, on different samples, results in a number of small-scale studies, each with their own validity coefficients.
  \item
    We can use meta-analytic procedures in reporting the results of validity coefficients when they are used for establishing criterion validity.
  \end{itemize}
\end{itemize}

\hypertarget{construct-validity}{%
\subsection{Construct Validity}\label{construct-validity}}

\textbf{Construct validity} was introduced in 1954 in the first edition of APA's testing standards and is defined as the extent to which the test may be said to measure a theoretical construct or trait. The overarching focus is on the role of \emph{psychological theory} in test construction and the ability to formulate hypotheses that can be supported (or not) in the evaluation process. Construct validity is established by the accumulation of information from a variety of sources.

There are a number of sources that can be used to support construct validity.

\hypertarget{internal-consistency}{%
\subsection{Internal Consistency}\label{internal-consistency}}

In the next \protect\hyperlink{rxx}{chapter}, you will learn that \textbf{internal consistency} is generally considered to be an index of reliability. In the context of criterion-related validity, a goal is to ensure that the criterion is the total score on the test itself. To that end, some of the following could also support this aspect of validity:

\begin{itemize}
\tightlist
\item
  Comparing high and low scorers. Items that fail to show a significantly greater proportion of ``passes'' in the upper than the lower group are considered invalid and are modified or eliminated.
\item
  Computing a biserial correlation between the item and total score.\\
\item
  Correlating the subtest score with the total score. Any subtest whose correlation with the total score is too low is eliminated.
\end{itemize}

Although some take issue with this notion, the degree of \emph{homogeneity} (the degree to which items assess the same thing) has some bearing on construct validity. There is a tradeoff between items that measure a narrow slice of the construct definition (internal consistency estimates are likely to be higher) and those that sample the construct definition more broadly (internal consistency estimates are likely to be lower).

Admittedly, the contribution of internal consistency data is limited. In absence of external data, it tells us little about WHAT the test measures.

\hypertarget{structural-validity}{%
\subsection{Structural Validity}\label{structural-validity}}

\hypertarget{exploratory-factor-analysis}{%
\subsubsection{Exploratory Factor Analysis}\label{exploratory-factor-analysis}}

\textbf{Exploratory factor analysis} (EFA) is used to simplify the description of behavior by reducing the number of categories (factors or dimensions) to fewer than the number of items. In our research vignette the 6-item Perceptions of Campus Climate Scale will be represented by two factors \citep{szymanski_perceptions_2020} In instrument development, techniques like \emph{principal components analysis} or \emph{principal axis factoring} are used to identify clusters (latent factors) among items. We frequently treat these as scales and subscales.

Imagine the use of 20 tests to 300 people. There would be 190 correlations.

\begin{itemize}
\tightlist
\item
  Irrespective of content, we can probably summarize the intercorrelations of tests with 5-6 factors.
\item
  When the clustering of tests includes vocabulary, analogies, opposites, and sentence completions, we might suggest a ``verbal comprehension factor.''
\item
  Factorial validity is the correlation of the test with whatever is common to a group of tests or other indices of behavior. If our single test has a correlation of .66 with the factor on which it loads, then the ``factorial validity of the new test as a measure of the common trait is .66.''
\end{itemize}

When EFA is utilized, the items are ``fed'' into an iterative process that analyzes the relations and ``reveals'' (or suggests -- we are the ones who interpret the data) how many factors (think scales/subscales) and which items comprise them.

\hypertarget{confirmatory-factor-analysis}{%
\subsubsection{Confirmatory Factor Analysis}\label{confirmatory-factor-analysis}}

\textbf{Confirmatory factor analysis} (CFA) involves specifying, a priori, a proposed relationship of items, scales, and subscales and then testing its \emph{goodness of fit.} In CFA (a form of structural equation modeling {[}SEM{]}), the latent variables (usually the higher order scales and total scale score) are positioned to \emph{cause} the responses on the indicators/items.

Subsequent lessons provide examples of both EFA and CFA approaches to psychometrics.

\hypertarget{experimental-interventions}{%
\subsection{Experimental Interventions}\label{experimental-interventions}}

Construct validity is also supported by hypothesis testing and experimentation. If we expect that the construct assessed by the instrument is malleable (e.g., depression) and that an intervention could change it, then a random clinical trial that evaluated the effectiveness of an intervention (and it worked -- depression scores declined) would simultaneously provide support for the intervention as well as the instrument.

\hypertarget{convergent-and-discriminant-validity}{%
\subsection{Convergent and Discriminant Validity}\label{convergent-and-discriminant-validity}}

In a psychometric evaluation, we will often administer our instrument-of-interest along with a battery of instruments that are more-and-less related. \textbf{Convergent validity} is supported when there are \emph{moderately high} correlations between our tests and the instruments with which we expect moderately high correlations. In contrast, \textbf{discriminant validity} is established by low and/or non-significant correlations between our instrument-of-interest and instruments that should be unrelated. For example, we want a low and non-significant correlation between a quantitative reasoning test and scores on a reading comprehension test. Why? Because if the correlation is too high, the test cannot discriminate between reading comprehension and math.

There are no strict cut-offs to establish convergence or discrimination. We can even ask, ``Could a correlation intended to support convergence be too high?'' It is possible! Unless the instrument-of-interest offers advantages such as brevity or cost, then correlations that fall into the ranges of multicollinearity or singularity can indicate unnecessary duplication or redundancy.

In our research vignette, Szymanski and Bissonette \citeyearpar{szymanski_perceptions_2020} conducted a correlation matrix that reports the bivariate relations between the LGBTQ Campus Climate full-scale as well as the College Response and Stigma subscales with measures that assess (a) LGBTQ victimization, (b) satisfaction with college, (c) persistence attitudes, and (d) anxiety, and (e) depression.

In order to produce this correlation matrix, we must first score each of the scales. In the items we prepared, the Perceptions of LGBTQ Campus Climate scale had one reverse-scored item. Similarly, the Institutional Goals and Commitments Scale had three reversed items. A first step in scoring is reversing these items.

The naming conventions that researchers use vary. I added an ``NR'' (for ``needs reversing) to the original items so that I would remember to reverse-score them. I also am careful to reverse-score items into new variables. Otherwise, we risk getting confused about whether/not items are in their original or reversed formats.

Reverse-scoring the item is easily accomplished by subtracting the variable from ``one plus'' the scaling. Because both of these scales were on a 7-point scale, we will subtract the ``NR'' variables from 8.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Reverse scoring the single item from the LGBTQ Campus Climate Scale}
\NormalTok{dfSzy }\OtherTok{\textless{}{-}}\NormalTok{ dfSzy }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{unsupportive =} \DecValTok{8} \SpecialCharTok{{-}}\NormalTok{ supportiveNR)}

\CommentTok{\# Reversing three items on the Institutional and Goals Commitments}
\CommentTok{\# scale}

\NormalTok{dfSzy }\OtherTok{\textless{}{-}}\NormalTok{ dfSzy }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{not\_graduate =} \DecValTok{8} \SpecialCharTok{{-}}\NormalTok{ not\_graduateNR) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{undecided =} \DecValTok{8} \SpecialCharTok{{-}}\NormalTok{ undecidedNR) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{grades\_unimportant =} \DecValTok{8} \SpecialCharTok{{-}}\NormalTok{ grades\_unimportantNR)}
\end{Highlighting}
\end{Shaded}

Next, we create scale and/or subscale scores. The \emph{sjstats::mean\_n()} function allows us to specify how many items (whole number) or what percentage of items should be present in order to get the mean. It is customary to require 75-80\% of items to be present for scoring. Three-item variables might allow one missing (i.e., 66\%). In the code below, I first make lists of the variables that belong in each scale and subscale, then I create the new variables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Making the list of variables}
\NormalTok{LGBTQ\_Climate }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"cold"}\NormalTok{, }\StringTok{"unresponsive"}\NormalTok{, }\StringTok{"unsupportive"}\NormalTok{, }\StringTok{"negative"}\NormalTok{,}
    \StringTok{"heterosexism"}\NormalTok{, }\StringTok{"harassed"}\NormalTok{)}
\NormalTok{CollResponse }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"cold"}\NormalTok{, }\StringTok{"unresponsive"}\NormalTok{, }\StringTok{"unsupportive"}\NormalTok{)}
\NormalTok{Stigma }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"negative"}\NormalTok{, }\StringTok{"heterosexism"}\NormalTok{, }\StringTok{"harassed"}\NormalTok{)}
\NormalTok{Victimization }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Vic1"}\NormalTok{, }\StringTok{"Vic2"}\NormalTok{, }\StringTok{"Vic3"}\NormalTok{, }\StringTok{"Vic4"}\NormalTok{, }\StringTok{"Vic5"}\NormalTok{, }\StringTok{"Vic6"}\NormalTok{, }\StringTok{"Vic7"}\NormalTok{,}
    \StringTok{"Vic8"}\NormalTok{, }\StringTok{"Vic9"}\NormalTok{)}
\NormalTok{CampSat }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Sat1"}\NormalTok{, }\StringTok{"Sat2"}\NormalTok{, }\StringTok{"Sat3"}\NormalTok{, }\StringTok{"Sat4"}\NormalTok{, }\StringTok{"Sat5"}\NormalTok{)}
\NormalTok{Persist }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"graduation\_importance"}\NormalTok{, }\StringTok{"right\_decision"}\NormalTok{, }\StringTok{"will\_register"}\NormalTok{,}
    \StringTok{"not\_graduate"}\NormalTok{, }\StringTok{"undecided"}\NormalTok{, }\StringTok{"grades\_unimportant"}\NormalTok{)}
\NormalTok{GAD7 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"nervous"}\NormalTok{, }\StringTok{"worry\_control"}\NormalTok{, }\StringTok{"much\_worry"}\NormalTok{, }\StringTok{"cant\_relax"}\NormalTok{, }\StringTok{"restless"}\NormalTok{,}
    \StringTok{"irritable"}\NormalTok{, }\StringTok{"afraid"}\NormalTok{)}
\NormalTok{PHQ9 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"anhedonia"}\NormalTok{, }\StringTok{"down"}\NormalTok{, }\StringTok{"sleep"}\NormalTok{, }\StringTok{"lo\_energy"}\NormalTok{, }\StringTok{"appetite"}\NormalTok{, }\StringTok{"selfworth"}\NormalTok{,}
    \StringTok{"concentration"}\NormalTok{, }\StringTok{"too\_slowfast"}\NormalTok{, }\StringTok{"s\_ideation"}\NormalTok{)}

\CommentTok{\# Creating the new variables}
\NormalTok{dfSzy}\SpecialCharTok{$}\NormalTok{LGBTQclimate }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(dfSzy[, LGBTQ\_Climate], }\FloatTok{0.75}\NormalTok{)}
\NormalTok{dfSzy}\SpecialCharTok{$}\NormalTok{CollegeRx }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(dfSzy[, CollResponse], }\FloatTok{0.66}\NormalTok{)}
\NormalTok{dfSzy}\SpecialCharTok{$}\NormalTok{Stigma }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(dfSzy[, Stigma], }\FloatTok{0.66}\NormalTok{)}
\NormalTok{dfSzy}\SpecialCharTok{$}\NormalTok{Victimization }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(dfSzy[, Victimization], }\FloatTok{0.8}\NormalTok{)}
\NormalTok{dfSzy}\SpecialCharTok{$}\NormalTok{CampusSat }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(dfSzy[, CampSat], }\FloatTok{0.75}\NormalTok{)}
\NormalTok{dfSzy}\SpecialCharTok{$}\NormalTok{Persistence }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(dfSzy[, Persist], }\FloatTok{0.8}\NormalTok{)}
\NormalTok{dfSzy}\SpecialCharTok{$}\NormalTok{Anxiety }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(dfSzy[, GAD7], }\FloatTok{0.75}\NormalTok{)}
\NormalTok{dfSzy}\SpecialCharTok{$}\NormalTok{Depression }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(dfSzy[, PHQ9], }\FloatTok{0.8}\NormalTok{)}

\CommentTok{\# If the scoring code above does not work for you, try the format}
\CommentTok{\# below which involves inserting to periods in front of the variable}
\CommentTok{\# list. One example is provided. dfLewis$Belonging \textless{}{-}}
\CommentTok{\# sjstats::mean\_n(dfLewis[, ..Belonging\_vars], 0.80)}
\end{Highlighting}
\end{Shaded}

A correlation matrix of the scaled scores allows us to compare our scale(s) of interest to others within its nomological net.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(dfSzy[}\FunctionTok{c}\NormalTok{(}\StringTok{"LGBTQclimate"}\NormalTok{, }\StringTok{"CollegeRx"}\NormalTok{, }\StringTok{"Stigma"}\NormalTok{,}
    \StringTok{"Victimization"}\NormalTok{, }\StringTok{"CampusSat"}\NormalTok{, }\StringTok{"Persistence"}\NormalTok{, }\StringTok{"Anxiety"}\NormalTok{, }\StringTok{"Depression"}\NormalTok{)],}
    \AttributeTok{filename =} \StringTok{"SzyCor.doc"}\NormalTok{, }\AttributeTok{table.number =} \DecValTok{1}\NormalTok{, }\AttributeTok{show.sig.stars =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{landscape =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Table 1 

Means, standard deviations, and correlations with confidence intervals
 

  Variable         M    SD   1            2            3           
  1. LGBTQclimate  4.00 0.63                                       
                                                                   
  2. CollegeRx     4.04 0.77 .83**                                 
                             [.81, .85]                            
                                                                   
  3. Stigma        3.96 0.76 .83**        .37**                    
                             [.80, .85]   [.31, .44]               
                                                                   
  4. Victimization 1.55 0.33 .01          -.17**       .20**       
                             [-.06, .09]  [-.25, -.10] [.13, .27]  
                                                                   
  5. CampusSat     4.24 0.70 -.49**       -.46**       -.35**      
                             [-.55, -.43] [-.52, -.40] [-.41, -.28]
                                                                   
  6. Persistence   3.03 0.42 -.21**       -.17**       -.17**      
                             [-.28, -.13] [-.25, -.10] [-.25, -.10]
                                                                   
  7. Anxiety       1.49 0.38 .17**        .12**        .17**       
                             [.10, .25]   [.04, .19]   [.09, .24]  
                                                                   
  8. Depression    1.52 0.29 .18**        .14**        .15**       
                             [.10, .25]   [.07, .22]   [.08, .23]  
                                                                   
  4            5            6            7         
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
  -.17**                                           
  [-.25, -.10]                                     
                                                   
  -.04         .34**                               
  [-.11, .04]  [.27, .40]                          
                                                   
  .15**        -.20**       -.10*                  
  [.07, .23]   [-.28, -.13] [-.18, -.02]           
                                                   
  .15**        -.23**       -.10**       .54**     
  [.08, .23]   [-.30, -.15] [-.18, -.03] [.48, .59]
                                                   

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

Examination of these values follow some expected patterns. First, the LGBTQ climate score (i.e., the total scale score) is highly correlated with each of its subscales (College Response \emph{r} = .83, \emph{p} \textless{} 0.01; Stigma \emph{r} = .83, \emph{p} = 0.01). These strong correlations are somewhat misleading because half of the items on the total scale are the same items on each of the subscales. The correlation between the two subscales is still statistically significant, but much lower (\emph{r} = 0.37, \emph{p} = 0.01).

Convergent and discriminant validity are of interest when we compare the LGBTQ Climate total scale score and the College Response and Stigma subscales with the additional measures. Regarding the total LGBTQ Climate score, a very strong correlation was observed with campus satisfaction (\emph{r} = -0.49, \emph{p} \textless{} 0.01); less strong correlations were observed with and persistence (\emph{r} = -0.21, \emph{p} \textless{} 0.01), anxiety (\emph{r} = 0.17, \emph{p} \textless{} 0.01), and depression (\emph{r} = 0.18, \emph{p} \textless{} 0.01). Recalling that higher scores on the LGBTQ Campus Climate score indicate a negative climate, we see that as the LGBTQ campus climate becomes increasingly stigmatizing and nonresponsive, students experience lower overall campus satisfaction and are less likely to persist at that institution. The correlation between LGBTQ Campus Climate and victimization was non-significant (\emph{r} = 0.01, \emph{p} \textgreater{} 0.05).

In assessing patterns of convergent and discriminant validity, the researcher would also take the time to map out the subscales (i.e., College Response, Stigma) with the additional measures.

\hypertarget{determining-statistically-significant-differences-between-correlations}{%
\subsubsection{Determining Statistically Significant Differences Between Correlations}\label{determining-statistically-significant-differences-between-correlations}}

Without a formal test, it is inappropriate for researchers to declare that one correlation is stronger than another. The package \emph{cocor} allows the comparisons of \emph{dependent} (i.e., all respondents are from the same sample) and \emph{independent} (i.e., correlations are compared between two different samples) where the correlations themselves can be \emph{overlapping} (i.e., with one common variable) or \emph{non-overlapping} (i.e., the variables in both correlations are different).

Because all of the correlations we computed are within the same sample, they are \emph{dependent}. When assessing convergent and discriminant validity it is common to ask if the correlations between the additional measures are different between the subscales of the focal measure. Results could support the notion that the subscales are related and yet different. An example of this might be comparing the correlations between victimization with the College Response subscale (\emph{r} = -.17, \emph{p} \textless{} 0.01) and victimization with Stigma subscale (\emph{r} = 0.20, \emph{p} \textless{} 0.01). This test would be \emph{overlapping} because the victimization variable is in common. Here's the code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cocor}\SpecialCharTok{::}\FunctionTok{cocor}\NormalTok{(}\AttributeTok{formula =} \SpecialCharTok{\textasciitilde{}}\NormalTok{CollegeRx }\SpecialCharTok{+}\NormalTok{ Victimization }\SpecialCharTok{|}\NormalTok{ Stigma }\SpecialCharTok{+}\NormalTok{ Victimization,}
    \AttributeTok{data =}\NormalTok{ dfSzy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

  Results of a comparison of two overlapping correlations based on dependent groups

Comparison between r.jk (Victimization, CollegeRx) = -0.1741 and r.jh (Victimization, Stigma) = 0.2015
Difference: r.jk - r.jh = -0.3756
Related correlation: r.kh = 0.3734
Data: dfSzy: j = Victimization, k = CollegeRx, h = Stigma
Group size: n = 646
Null hypothesis: r.jk is equal to r.jh
Alternative hypothesis: r.jk is not equal to r.jh (two-sided)
Alpha: 0.05

pearson1898: Pearson and Filon's z (1898)
  z = -8.9455, p-value = 0.0000
  Null hypothesis rejected

hotelling1940: Hotelling's t (1940)
  t = -9.0342, df = 643, p-value = 0.0000
  Null hypothesis rejected

williams1959: Williams' t (1959)
  t = -9.0340, df = 643, p-value = 0.0000
  Null hypothesis rejected

olkin1967: Olkin's z (1967)
  z = -8.9455, p-value = 0.0000
  Null hypothesis rejected

dunn1969: Dunn and Clark's z (1969)
  z = -8.7137, p-value = 0.0000
  Null hypothesis rejected

hendrickson1970: Hendrickson, Stanley, and Hills' (1970) modification of Williams' t (1959)
  t = -9.0341, df = 643, p-value = 0.0000
  Null hypothesis rejected

steiger1980: Steiger's (1980) modification of Dunn and Clark's z (1969) using average correlations
  z = -8.6124, p-value = 0.0000
  Null hypothesis rejected

meng1992: Meng, Rosenthal, and Rubin's z (1992)
  z = -8.5080, p-value = 0.0000
  Null hypothesis rejected
  95% confidence interval for r.jk - r.jh: -0.4678 -0.2926
  Null hypothesis rejected (Interval does not include 0)

hittner2003: Hittner, May, and Silver's (2003) modification of Dunn and Clark's z (1969) using a backtransformed average Fisher's (1921) Z procedure
  z = -8.6124, p-value = 0.0000
  Null hypothesis rejected

zou2007: Zou's (2007) confidence interval
  95% confidence interval for r.jk - r.jh: -0.4568 -0.2921
  Null hypothesis rejected (Interval does not include 0)
\end{verbatim}

Fisher's z-test (\emph{z} = -8.6124, \emph{p} \textless{} 0.001) tells us that the correlations are statistically significantly different from each other; Zhou's confidence intervals provide the CI for the size of the difference between the two correlations. That is, the difference could be as large as -0.4568 or as small as -0.2921.

Another type of correlation comparison is with a the total and/or subscales, looking at the relative magnitude of their correlation with different variables. For example, we might wish to ask if the LGBTQ Campus Climate total scale is different degrees of correlation with anxiety (\emph{r} = 0.17, \emph{p} \textless{} 0.01) and depression (\emph{r} = .18, \emph{p} \textless{} 0.01).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cocor}\SpecialCharTok{::}\FunctionTok{cocor}\NormalTok{(}\AttributeTok{formula =} \SpecialCharTok{\textasciitilde{}}\NormalTok{LGBTQclimate }\SpecialCharTok{+}\NormalTok{ Anxiety }\SpecialCharTok{|}\NormalTok{ LGBTQclimate }\SpecialCharTok{+}\NormalTok{ Depression,}
    \AttributeTok{data =}\NormalTok{ dfSzy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

  Results of a comparison of two overlapping correlations based on dependent groups

Comparison between r.jk (LGBTQclimate, Anxiety) = 0.1724 and r.jh (LGBTQclimate, Depression) = 0.1795
Difference: r.jk - r.jh = -0.0071
Related correlation: r.kh = 0.5387
Data: dfSzy: j = LGBTQclimate, k = Anxiety, h = Depression
Group size: n = 646
Null hypothesis: r.jk is equal to r.jh
Alternative hypothesis: r.jk is not equal to r.jh (two-sided)
Alpha: 0.05

pearson1898: Pearson and Filon's z (1898)
  z = -0.1914, p-value = 0.8482
  Null hypothesis retained

hotelling1940: Hotelling's t (1940)
  t = -0.1911, df = 643, p-value = 0.8485
  Null hypothesis retained

williams1959: Williams' t (1959)
  t = -0.1909, df = 643, p-value = 0.8487
  Null hypothesis retained

olkin1967: Olkin's z (1967)
  z = -0.1914, p-value = 0.8482
  Null hypothesis retained

dunn1969: Dunn and Clark's z (1969)
  z = -0.1909, p-value = 0.8486
  Null hypothesis retained

hendrickson1970: Hendrickson, Stanley, and Hills' (1970) modification of Williams' t (1959)
  t = -0.1911, df = 643, p-value = 0.8485
  Null hypothesis retained

steiger1980: Steiger's (1980) modification of Dunn and Clark's z (1969) using average correlations
  z = -0.1909, p-value = 0.8486
  Null hypothesis retained

meng1992: Meng, Rosenthal, and Rubin's z (1992)
  z = -0.1909, p-value = 0.8486
  Null hypothesis retained
  95% confidence interval for r.jk - r.jh: -0.0825 0.0678
  Null hypothesis retained (Interval includes 0)

hittner2003: Hittner, May, and Silver's (2003) modification of Dunn and Clark's z (1969) using a backtransformed average Fisher's (1921) Z procedure
  z = -0.1909, p-value = 0.8486
  Null hypothesis retained

zou2007: Zou's (2007) confidence interval
  95% confidence interval for r.jk - r.jh: -0.0798 0.0656
  Null hypothesis retained (Interval includes 0)
\end{verbatim}

Fisher's z-test (\emph{z} = -0.1909, \emph{p} = 0.8486) tells us that the correlations are not statistically significantly different from each other; Zhou's confidence intervals indicate that the differences range between -0.0798 and 0.0656. Because this interval crosses zero, we know that the difference could be zero, or reversed in direction.

\hypertarget{multitrait-multimethod-matrix}{%
\subsubsection{Multitrait-Multimethod Matrix}\label{multitrait-multimethod-matrix}}

The \textbf{multitrait-multimethod matrix} is a systematic experimental design for the dual approach of convergent and discriminant validation, which requires the assessment of two or more traits (classically, math, English, and reading scores) by two more methods (self, parent, and teacher). Conducting a web-based image search on this term will show a matrix of alpha coefficients and correlation coefficients that are interpreted in relationship to each other. Roughly:

\begin{itemize}
\tightlist
\item
  alpha coefficients (internal consistency) should be the highest,
\item
  validity coefficients (correlations of the same trait assessed by different methods) should be higher than correlations between different traits measured by different methods,
\item
  validity coefficients (correlations of the same trait assessed by different methods) should be higher than different traits measured by the same method.
\end{itemize}

\hypertarget{incremental-validity}{%
\subsection{Incremental Validity}\label{incremental-validity}}

\textbf{Incremental validity} is the increase in predictive validity attributable to the test. It indicates the contribution the test makes to the selection of individuals who will meet the minimum standards in criterion performance. There are different ways to assess this -- one of the most common is to first enter known predictors and then see if the instrument-of-interest continues to account variance over-and-above those that are entered.

In the Szymanski and Bissonette \citeyearpar{szymanski_perceptions_2020} psychometric evaluation, the negative relations with satisfaction with college and intention to persist in college as well as positive relations with both anxiety and depression persisted even after controlling for LGBTQ victimization experiences.

I will demonstrate this procedure, predicting the contribution that the LGBTQ Campus Climate total scale score has on predicting intention to persist in college, over and above LGBTQ victimization.

The process is to use hierarchical linear regression. Two models are built. In the first mode (``PfV'' stands {[}in my mind{]} for ``Persistence from Victimization''), persistence is predicted from victimization. The second model adds the LGBTQ Campus Climate Scale. I asked for summaries of each model. Then the \emph{anova()} function compares the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PfV }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Persistence }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Victimization, }\AttributeTok{data =}\NormalTok{ dfSzy)}
\NormalTok{PfVC }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Persistence }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Victimization }\SpecialCharTok{+}\NormalTok{ LGBTQclimate, }\AttributeTok{data =}\NormalTok{ dfSzy)}
\FunctionTok{summary}\NormalTok{(PfV)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Persistence ~ Victimization, data = dfSzy)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.19281 -0.34150 -0.02281  0.29669  1.32226 

Coefficients:
              Estimate Std. Error t value            Pr(>|t|)    
(Intercept)    3.09906    0.07947  38.997 <0.0000000000000002 ***
Victimization -0.04566    0.05023  -0.909               0.364    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4224 on 644 degrees of freedom
Multiple R-squared:  0.001281,  Adjusted R-squared:  -0.0002694 
F-statistic: 0.8263 on 1 and 644 DF,  p-value: 0.3637
\end{verbatim}

From the PfV model we learn that victimization has a non-significant effect on intentions to persist in college (\emph{B} = -0.046, \emph{p} = 0.364). Further, the \(R^2\) is quite small (0.001).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(PfVC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Persistence ~ Victimization + LGBTQclimate, data = dfSzy)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.1696 -0.2842  0.0094  0.2569  1.3571 

Coefficients:
              Estimate Std. Error t value             Pr(>|t|)    
(Intercept)    3.64440    0.12795  28.483 < 0.0000000000000002 ***
Victimization -0.04183    0.04919  -0.850                0.395    
LGBTQclimate  -0.13788    0.02568  -5.369          0.000000111 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4135 on 643 degrees of freedom
Multiple R-squared:  0.04413,   Adjusted R-squared:  0.04116 
F-statistic: 14.84 on 2 and 643 DF,  p-value: 0.0000004991
\end{verbatim}

In the PfVC model, we see that the LGBTQ Campus Climate full scale score has a significant impact on intentions to persist. Specifically, for each additional point higher on the LGBTQ climate score, intentions to persist decrease by .14 points (\emph{p} \textless{} 0.001). Together, the model accounts for 4\% of the variance, representing a \(\Delta{R^2}\) of 4\%.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# calculating R2 change}
\FloatTok{0.04413} \SpecialCharTok{{-}} \FloatTok{0.001281}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.042849
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(PfV, PfVC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Model 1: Persistence ~ Victimization
Model 2: Persistence ~ Victimization + LGBTQclimate
  Res.Df    RSS Df Sum of Sq      F       Pr(>F)    
1    644 114.88                                     
2    643 109.95  1     4.929 28.824 0.0000001108 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We see that there is a statistically significant difference between the models \(F(1, 643) = 28.824, p < 0.001)\).

Let's try another model. With anxiety as our dependent variable, the code below asks if LGBTQ Campus Climate accounts for a proportion of the variance over-and-above victimization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AfV }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Victimization, }\AttributeTok{data =}\NormalTok{ dfSzy)}
\NormalTok{AfVC }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Victimization }\SpecialCharTok{+}\NormalTok{ LGBTQclimate, }\AttributeTok{data =}\NormalTok{ dfSzy)}
\FunctionTok{summary}\NormalTok{(AfV)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Anxiety ~ Victimization, data = dfSzy)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.05943 -0.30935 -0.01528  0.31306  1.24148 

Coefficients:
              Estimate Std. Error t value             Pr(>|t|)    
(Intercept)    1.21756    0.07131  17.073 < 0.0000000000000002 ***
Victimization  0.17427    0.04508   3.866             0.000122 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.379 on 644 degrees of freedom
Multiple R-squared:  0.02268,   Adjusted R-squared:  0.02116 
F-statistic: 14.95 on 1 and 644 DF,  p-value: 0.0001219
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(AfVC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Anxiety ~ Victimization + LGBTQclimate, data = dfSzy)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.00814 -0.29249 -0.02563  0.29877  1.20705 

Coefficients:
              Estimate Std. Error t value         Pr(>|t|)    
(Intercept)    0.81071    0.11561   7.012 0.00000000000595 ***
Victimization  0.17141    0.04444   3.857         0.000126 ***
LGBTQclimate   0.10287    0.02320   4.433 0.00001092659570 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.3736 on 643 degrees of freedom
Multiple R-squared:  0.05166,   Adjusted R-squared:  0.04872 
F-statistic: 17.52 on 2 and 643 DF,  p-value: 0.0000000392
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(AfV, AfVC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Model 1: Anxiety ~ Victimization
Model 2: Anxiety ~ Victimization + LGBTQclimate
  Res.Df    RSS Df Sum of Sq      F     Pr(>F)    
1    644 92.513                                   
2    643 89.770  1    2.7436 19.651 0.00001093 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

This model is a little more exciting in that our first model (AfV) is statistically significant \((B = 0.174, p < 0.001)\). That is, victimization has a statistically significant effect on anxiety, accounting for 2\% of the variance. In the second model, the LGBTQ Campus Climate total scale score is also significant \((B = 0.103, p < 0.001)\), and accounts for an additional 3\% of variance (\(\Delta{R^2} = 0.029\). There is a statistically significant difference between models (\emph{F}{[}1, 643{]} = 19.651, \emph{p} \textless{} .001).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# calculating change in R2}
\FloatTok{0.05166} \SpecialCharTok{{-}} \FloatTok{0.02268}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.02898
\end{verbatim}

\hypertarget{considering-the-individual-and-social-consequences-of-testing}{%
\subsection{Considering the Individual and Social Consequences of Testing}\label{considering-the-individual-and-social-consequences-of-testing}}

Messick \citep{messick_consequences_2000} and others recommend that the ``consequences of testing'' be included in the concept of test validity. Messick's point was to consider the the unintended consequences of specific uses. That is, their use may be detrimental to individuals or to members of certain ethnic or other populations with diverse experiential backgrounds. Examples of inappropriate use have included:

\begin{itemize}
\tightlist
\item
  The California Psychological Inventory (CPI) being used as a screening tool for employment as a security job. Two of its items inquired about same-sex activities and the employer was using this to screen out gay men. Applicants were able to demonstrate, in court, a consistent rejection of gay applicants.
\item
  While this is not a psychological test, urine samples are often collected as drug screening tools. In reality, urine can reveal a number of things, such as pregnancy.
\end{itemize}

The issue begs ``conflicting goals.'' In this case, the problem was not caused by the test but rather by its misuse. Studying the ``consequences'' of testing is one that is not necessarily answerable by empirical data/statistical analysis. It requires critical observation, human judgment, and systematic debate.

\hypertarget{factors-affecting-validity-coefficients}{%
\section{Factors Affecting Validity Coefficients}\label{factors-affecting-validity-coefficients}}

Keeping in mind that a \emph{validity coefficient} is merely the correlation between the test and some criteria, the same elements that impact the magnitude and significance of a correlation coefficient will similarly affect a validity coefficient.

\textbf{Nature of the group}. A test that has high validity in predicting a particular criterion in one population, may have little or no validity in predicting the same criterion in another population. If a test is designed for use in diverse populations, information about the population generalizability should be reported in the technical manuals.

\textbf{Sample heterogeneity}. Other things being equal, if there is a linear relationship between X and Y, it will have a greater magnitude when the sample is heterogeneous.

\textbf{Pre-selection}. Just like internal and external validity in a research design can be threatened by selection issues, pre-selection can also impact the validity coefficients of a measure. For example, if we are evaluating a new test for job selection, we may select a group of newly hired employees. We plan to collect some measure of job performance at a later date. Our results may be limited by the criteria used to select the employees. Were they volunteers? Were they only those hired? Were they ALL of the applicants?

\textbf{Validity coefficients may change over time}. Consider the relationship between the college boards and grade point average at Yale University. Fifty years ago, \(r_{xy} = .72\); today \(r_{xy} = .52\). Why? The nature of the student body has become more diverse (50 years ago, the student body was predominantly White, high SES, and male).

The \textbf{form of the relationship} matters. The Pearson R assumes the relationship between the predictor and criterion variables is linear, uniform, and homoscedastistic (equal variability throughout the range of a bivariate distribution). When the variability is unequal throughout the range of the distribution the relationship is heteroscedastic.

\begin{figure}
\centering
\includegraphics{04-Validity_files/figure-latex/unnamed-chunk-17-1.pdf}
\caption{\label{fig:unnamed-chunk-17}Illustration of heteroscedasticity}
\end{figure}

There could also be other factors involved in the relationship between the instrument and the criterion:

\begin{itemize}
\tightlist
\item
  curvilinearity
\item
  an undetected mechanism, such as a moderator
\end{itemize}

Finally, what is our threshold for acceptability?

\begin{itemize}
\tightlist
\item
  Consider statistical significance -- but also its limitations (e.g., power, Type I error, Type II error)
\item
  Consider the magnitude of the correlation; and also \(R^2\) (the proportion of variance accounted for)
\item
  Consider error:

  \begin{itemize}
  \tightlist
  \item
    The standard error of the estimate shows the margin of error to be expected in the individuals predicted criterion score as the result of the imperfect validity of the instrument.
  \end{itemize}
\end{itemize}

\[SE_{est} = SD_{y}\sqrt{1 - r_{xy}^{2}}\] Where:

\begin{itemize}
\tightlist
\item
  \(r_{xy}^{2}\) is the square of the validity coefficient, and
\item
  \(SD_{y}\) is the standard deviation of the criterion scores.
\end{itemize}

If the validity were perfect (\(r_{xy}^{2}\) = 1.00), the error of estimate would be 0.00. If the validity were zero, the error of estimate would equal \(SD_{y}\).

Interpreting \(SE_{est}\):

\begin{itemize}
\tightlist
\item
  If \(r_{xy}\) = .80, then \(\sqrt{1 - r_{xy}^{2}} = .60\).\\
\item
  Error is 60\% as large as it would be by chance. Stated another way, predicting an individual's criterion performance has a margin of error that is 40\% smaller than it would be by chance.
\end{itemize}

To obtain the \(SE_{est}\), we merely multiply by the \(SD_{y}\). This puts error in the metric of the criterion variable.

Your Turn:

\begin{itemize}
\tightlist
\item
  If \(r_{xy}\) = .25, then \(\sqrt{1 - r_{xy}^{2}} =\) ??
\end{itemize}

Make a statement about chance. Make a statement about margin of error.

\hypertarget{practice-problems-2}{%
\section{Practice Problems}\label{practice-problems-2}}

In each of these lessons, I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. With each of these options, I encourage you to interpret examine aspects of the construct validity through the creation and interpretation of validity coefficients. Ideally, you will examine both convergent/discriminant validity as well as incremental validity.

\hypertarget{problem-1-play-around-with-this-simulation.}{%
\subsection{Problem \#1: Play around with this simulation.}\label{problem-1-play-around-with-this-simulation.}}

Copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results.

If calculating is new to you, perhaps you just change the number in ``set.seed(210907)'' from 210907 to something else. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go.

\hypertarget{problem-2-conduct-the-reliability-analysis-selecting-different-variables.}{%
\subsection{Problem \#2: Conduct the reliability analysis selecting different variables.}\label{problem-2-conduct-the-reliability-analysis-selecting-different-variables.}}

The Szymanski and Bissonette \citeyearpar{szymanski_perceptions_2020} article conducted a handful of incremental validity assessments. Select different outcome variables (e.g., depression) and/or use the subscales as the instrument-of-interest.

\hypertarget{problem-3-try-something-entirely-new.}{%
\subsection{Problem \#3: Try something entirely new.}\label{problem-3-try-something-entirely-new.}}

Using data for which you have permission and access (e.g., IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), create validity coefficients and use three variables to estimate the incremental validity of the instrument-of-interest.

\hypertarget{grading-rubric}{%
\subsection{Grading Rubric}\label{grading-rubric}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5479}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2603}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1918}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Possible
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Earned
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1. Check and, if needed, format and score the data. & 5 & \_\_\_\_\_ \\
2. Create a correlation matrix that includes the instrument-of-interest and the variables that will have varying degrees of relation. & 5 & \_\_\_\_\_ \\
3. With convergent and discriminant validity in mind, interpret the validity coefficients; this should include an assessment about whether the correlation coefficients (at least two different pairings) are statistically significantly different from each other. & 5 & \_\_\_\_\_ \\
4. With at least three variables, evaluate the degree to which the instrument demonstrates incremental validity (this should involve two regression equations and their statistical comparison). & 5 & \_\_\_\_\_ \\
5. Explanation to grader. & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 25 & \_\_\_\_\_ \\
\end{longtable}

\hypertarget{homeworked-example}{%
\section{Homeworked Example}\label{homeworked-example}}

\href{https://www.youtube.com/watch?v=QPKej_cHCOk}{Screencast Link}

For more information about the data used in this homeworked example, please refer to the description and codebook located at the end of the \href{https://lhbikos.github.io/ReCenterPsychStats/ReCintro.html\#introduction-to-the-data-set-used-for-homeworked-examples}{introduction} in first volume of ReCentering Psych Stats.

As a brief review, this data is part of an IRB-approved study, with consent to use in teaching demonstrations and to be made available to the general public via the open science framework. Hence, it is appropriate to use in this context. You will notice there are student- and teacher- IDs. These numbers are not actual student and teacher IDs, rather they were further re-identified so that they could not be connected to actual people.

Because this is an actual dataset, if you wish to work the problem along with me, you will need to download the \href{https://github.com/lhbikos/ReC_Psychometrics/blob/main/Worked_Examples/ReC.rds}{ReC.rds} data file from the Worked\_Examples folder in the ReC\_Psychometrics project on the GitHub.

The course evaluation items can be divided into three subscales:

\begin{itemize}
\tightlist
\item
  \textbf{Valued by the student} includes the items: ValObjectives, IncrUnderstanding, IncrInterest
\item
  \textbf{Traditional pedagogy} includes the items: ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation
\item
  \textbf{Socially responsive pedagogy} includes the items: InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration
\end{itemize}

In this homework focused on validity we will score the total scale and subscales, create a correlation matrix of our scales with a different scale (or item), formally test to see if correlation coefficients are statistically significantly different from each other, conduct a test of incremental validity.

\hypertarget{check-and-if-needed-format-data}{%
\subsection{Check and, if needed, format data}\label{check-and-if-needed-format-data}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{big }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"ReC.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's check the structure\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(big)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Classes 'data.table' and 'data.frame':  310 obs. of  33 variables:
 $ deID                   : int  1 2 3 4 5 6 7 8 9 10 ...
 $ CourseID               : int  57085635 57085635 57085635 57085635 57085635 57085635 57085635 57085635 57085635 57085635 ...
 $ Dept                   : chr  "CPY" "CPY" "CPY" "CPY" ...
 $ Course                 : Factor w/ 3 levels "Psychometrics",..: 2 2 2 2 2 2 2 2 2 2 ...
 $ StatsPkg               : Factor w/ 2 levels "SPSS","R": 2 2 2 2 2 2 2 2 2 2 ...
 $ Centering              : Factor w/ 2 levels "Pre","Re": 2 2 2 2 2 2 2 2 2 2 ...
 $ Year                   : int  2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 ...
 $ Quarter                : chr  "Fall" "Fall" "Fall" "Fall" ...
 $ IncrInterest           : int  5 3 4 2 4 3 5 3 2 5 ...
 $ IncrUnderstanding      : int  2 3 4 3 4 4 5 2 4 5 ...
 $ ValObjectives          : int  5 5 4 4 5 5 5 5 4 5 ...
 $ ApprAssignments        : int  5 4 4 4 5 3 5 3 3 5 ...
 $ EffectiveAnswers       : int  5 3 5 3 5 3 4 3 2 3 ...
 $ Respectful             : int  5 5 4 5 5 4 5 4 5 5 ...
 $ ClearResponsibilities  : int  5 5 4 4 5 4 5 4 4 5 ...
 $ Feedback               : int  5 3 4 2 5 NA 5 4 4 5 ...
 $ OvInstructor           : int  5 4 4 3 5 3 5 4 3 5 ...
 $ MultPerspectives       : int  5 5 4 5 5 4 5 5 5 5 ...
 $ OvCourse               : int  3 4 4 3 5 3 5 3 2 5 ...
 $ InclusvClassrm         : int  5 5 5 5 5 4 5 5 4 5 ...
 $ DEIintegration         : int  5 5 5 5 5 4 5 5 5 5 ...
 $ ClearPresentation      : int  4 4 4 2 5 3 4 4 4 5 ...
 $ ApprWorkload           : int  5 5 3 4 4 2 5 4 4 5 ...
 $ MyContribution         : int  4 4 4 4 5 4 4 3 4 5 ...
 $ InspiredInterest       : int  5 3 4 3 5 3 5 4 4 5 ...
 $ Faith                  : int  5 NA 4 2 NA NA 4 4 4 NA ...
 $ EquitableEval          : int  5 5 3 5 5 3 5 5 3 5 ...
 $ SPFC.Decolonize.Opt.Out: chr  "" "" "" "" ...
 $ ProgramYear            : Factor w/ 3 levels "Second","Transition",..: 3 3 3 3 3 3 3 3 3 3 ...
 $ ClearOrganization      : int  3 4 3 4 4 4 5 4 4 5 ...
 $ RegPrepare             : int  5 4 4 4 4 3 4 4 4 5 ...
 $ EffectiveLearning      : int  2 4 3 4 4 2 5 3 2 5 ...
 $ AccessibleInstructor   : int  5 4 4 4 5 4 5 4 5 5 ...
 - attr(*, ".internal.selfref")=<externalptr> 
\end{verbatim}

We will need to create the three subscales. The codebook above, lists which variables go in each subscale score.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Making the list of variables}
\NormalTok{ValuedVars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"ValObjectives"}\NormalTok{, }\StringTok{"IncrUnderstanding"}\NormalTok{, }\StringTok{"IncrInterest"}\NormalTok{)}
\NormalTok{TradPedVars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"ClearResponsibilities"}\NormalTok{, }\StringTok{"EffectiveAnswers"}\NormalTok{, }\StringTok{"Feedback"}\NormalTok{,}
    \StringTok{"ClearOrganization"}\NormalTok{, }\StringTok{"ClearPresentation"}\NormalTok{)}
\NormalTok{SRPedVars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"InclusvClassrm"}\NormalTok{, }\StringTok{"EquitableEval"}\NormalTok{, }\StringTok{"MultPerspectives"}\NormalTok{, }\StringTok{"DEIintegration"}\NormalTok{)}
\NormalTok{Total }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"ValObjectives"}\NormalTok{, }\StringTok{"IncrUnderstanding"}\NormalTok{, }\StringTok{"IncrInterest"}\NormalTok{, }\StringTok{"ClearResponsibilities"}\NormalTok{,}
    \StringTok{"EffectiveAnswers"}\NormalTok{, }\StringTok{"Feedback"}\NormalTok{, }\StringTok{"ClearOrganization"}\NormalTok{, }\StringTok{"ClearPresentation"}\NormalTok{,}
    \StringTok{"InclusvClassrm"}\NormalTok{, }\StringTok{"EquitableEval"}\NormalTok{, }\StringTok{"MultPerspectives"}\NormalTok{, }\StringTok{"DEIintegration"}\NormalTok{)}

\CommentTok{\# Creating the new variables}
\NormalTok{big}\SpecialCharTok{$}\NormalTok{Valued }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(big[, ValuedVars], }\FloatTok{0.66}\NormalTok{)}
\NormalTok{big}\SpecialCharTok{$}\NormalTok{TradPed }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(big[, TradPedVars], }\FloatTok{0.75}\NormalTok{)}
\NormalTok{big}\SpecialCharTok{$}\NormalTok{SRPed }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(big[, SRPedVars], }\FloatTok{0.75}\NormalTok{)}
\NormalTok{big}\SpecialCharTok{$}\NormalTok{Total }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(big[, Total], }\FloatTok{0.8}\NormalTok{)}

\CommentTok{\# If the scoring code above does not work for you, try the format}
\CommentTok{\# below which involves inserting to periods in front of the variable}
\CommentTok{\# list. One example is provided. dfLewis$Belonging \textless{}{-}}
\CommentTok{\# sjstats::mean\_n(dfLewis[, ..Belonging\_vars], 0.80)}
\end{Highlighting}
\end{Shaded}

\hypertarget{create-a-correlation-matrix-that-includes-the-instrument-of-interest-and-the-variables-that-will-have-varying-degrees-of-relation}{%
\subsection{Create a correlation matrix that includes the instrument-of-interest and the variables that will have varying degrees of relation}\label{create-a-correlation-matrix-that-includes-the-instrument-of-interest-and-the-variables-that-will-have-varying-degrees-of-relation}}

Unfortunately, data from the course evals don't include any outside scales. However, I didn't include the ``Overall Instructor'' (OvInstructor) in any of the items, so we \emph{could} think of it as a way to look at convergent and discriminant validity.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(big[}\FunctionTok{c}\NormalTok{(}\StringTok{"Valued"}\NormalTok{, }\StringTok{"TradPed"}\NormalTok{, }\StringTok{"SRPed"}\NormalTok{, }\StringTok{"OvInstructor"}\NormalTok{)],}
    \AttributeTok{filename =} \StringTok{"ReC\_cortable.doc"}\NormalTok{, }\AttributeTok{table.number =} \DecValTok{1}\NormalTok{, }\AttributeTok{show.sig.stars =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{landscape =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Table 1 

Means, standard deviations, and correlations with confidence intervals
 

  Variable        M    SD   1          2          3         
  1. Valued       4.25 0.68                                 
                                                            
  2. TradPed      4.25 0.76 .70**                           
                            [.63, .75]                      
                                                            
  3. SRPed        4.52 0.58 .56**      .71**                
                            [.48, .64] [.65, .76]           
                                                            
  4. OvInstructor 4.37 0.94 .63**      .80**      .67**     
                            [.56, .70] [.76, .84] [.60, .73]
                                                            

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

All the correlations are strong and positive. However, look at the correlation between Overall Instructor and SCRPed!

\hypertarget{with-convergent-and-discriminant-validity-in-mind-interpret-the-validity-coefficients-this-should-include-an-assessment-about-whether-the-correlation-coefficients-at-least-two-different-pairings-are-statistically-significantly-different-from-each-other.}{%
\subsection{With convergent and discriminant validity in mind, interpret the validity coefficients; this should include an assessment about whether the correlation coefficients (at least two different pairings) are statistically significantly different from each other.}\label{with-convergent-and-discriminant-validity-in-mind-interpret-the-validity-coefficients-this-should-include-an-assessment-about-whether-the-correlation-coefficients-at-least-two-different-pairings-are-statistically-significantly-different-from-each-other.}}

We need to see if these correlations are statistically significantly different from each other. I am interested in knowing if the correlations between Overall Instructor and each of the three course dimensions (Valued {[}\emph{r} = 0.63, \emph{p} \textless{} 0.01{]}, TradPed {[}\emph{r} = 0.80, \emph{p} \textless{} 0.01{]}, SRPed {[}\emph{r} = 0.67, \emph{p} \textless{} 0.01{]}) are statistically significantly different from each other.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cocor}\SpecialCharTok{::}\FunctionTok{cocor}\NormalTok{(}\AttributeTok{formula =} \SpecialCharTok{\textasciitilde{}}\NormalTok{Valued }\SpecialCharTok{+}\NormalTok{ OvInstructor }\SpecialCharTok{|}\NormalTok{ TradPed }\SpecialCharTok{+}\NormalTok{ OvInstructor,}
    \AttributeTok{data =}\NormalTok{ big)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

  Results of a comparison of two overlapping correlations based on dependent groups

Comparison between r.jk (OvInstructor, Valued) = 0.6344 and r.jh (OvInstructor, TradPed) = 0.7997
Difference: r.jk - r.jh = -0.1652
Related correlation: r.kh = 0.697
Data: big: j = OvInstructor, k = Valued, h = TradPed
Group size: n = 307
Null hypothesis: r.jk is equal to r.jh
Alternative hypothesis: r.jk is not equal to r.jh (two-sided)
Alpha: 0.05

pearson1898: Pearson and Filon's z (1898)
  z = -5.4939, p-value = 0.0000
  Null hypothesis rejected

hotelling1940: Hotelling's t (1940)
  t = -6.2651, df = 304, p-value = 0.0000
  Null hypothesis rejected

williams1959: Williams' t (1959)
  t = -6.1447, df = 304, p-value = 0.0000
  Null hypothesis rejected

olkin1967: Olkin's z (1967)
  z = -5.4939, p-value = 0.0000
  Null hypothesis rejected

dunn1969: Dunn and Clark's z (1969)
  z = -5.9983, p-value = 0.0000
  Null hypothesis rejected

hendrickson1970: Hendrickson, Stanley, and Hills' (1970) modification of Williams' t (1959)
  t = -6.2651, df = 304, p-value = 0.0000
  Null hypothesis rejected

steiger1980: Steiger's (1980) modification of Dunn and Clark's z (1969) using average correlations
  z = -5.9444, p-value = 0.0000
  Null hypothesis rejected

meng1992: Meng, Rosenthal, and Rubin's z (1992)
  z = -5.9182, p-value = 0.0000
  Null hypothesis rejected
  95% confidence interval for r.jk - r.jh: -0.4644 -0.2333
  Null hypothesis rejected (Interval does not include 0)

hittner2003: Hittner, May, and Silver's (2003) modification of Dunn and Clark's z (1969) using a backtransformed average Fisher's (1921) Z procedure
  z = -5.8868, p-value = 0.0000
  Null hypothesis rejected

zou2007: Zou's (2007) confidence interval
  95% confidence interval for r.jk - r.jh: -0.2282 -0.1089
  Null hypothesis rejected (Interval does not include 0)
\end{verbatim}

Fisher's z-test (\emph{z} = -5.887, \emph{p} \textless{} 0.001) indicates that the correlation of overall instructor with the valued subscale (\emph{r} = 0.63) is lower than its correlation with the traditional pedagogy subscale (\emph{r} = 0.80).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cocor}\SpecialCharTok{::}\FunctionTok{cocor}\NormalTok{(}\AttributeTok{formula =} \SpecialCharTok{\textasciitilde{}}\NormalTok{TradPed }\SpecialCharTok{+}\NormalTok{ OvInstructor }\SpecialCharTok{|}\NormalTok{ SRPed }\SpecialCharTok{+}\NormalTok{ OvInstructor,}
    \AttributeTok{data =}\NormalTok{ big)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

  Results of a comparison of two overlapping correlations based on dependent groups

Comparison between r.jk (OvInstructor, TradPed) = 0.7962 and r.jh (OvInstructor, SRPed) = 0.6751
Difference: r.jk - r.jh = 0.1211
Related correlation: r.kh = 0.7091
Data: big: j = OvInstructor, k = TradPed, h = SRPed
Group size: n = 298
Null hypothesis: r.jk is equal to r.jh
Alternative hypothesis: r.jk is not equal to r.jh (two-sided)
Alpha: 0.05

pearson1898: Pearson and Filon's z (1898)
  z = 4.2785, p-value = 0.0000
  Null hypothesis rejected

hotelling1940: Hotelling's t (1940)
  t = 4.6684, df = 295, p-value = 0.0000
  Null hypothesis rejected

williams1959: Williams' t (1959)
  t = 4.5800, df = 295, p-value = 0.0000
  Null hypothesis rejected

olkin1967: Olkin's z (1967)
  z = 4.2785, p-value = 0.0000
  Null hypothesis rejected

dunn1969: Dunn and Clark's z (1969)
  z = 4.5174, p-value = 0.0000
  Null hypothesis rejected

hendrickson1970: Hendrickson, Stanley, and Hills' (1970) modification of Williams' t (1959)
  t = 4.6684, df = 295, p-value = 0.0000
  Null hypothesis rejected

steiger1980: Steiger's (1980) modification of Dunn and Clark's z (1969) using average correlations
  z = 4.4945, p-value = 0.0000
  Null hypothesis rejected

meng1992: Meng, Rosenthal, and Rubin's z (1992)
  z = 4.4834, p-value = 0.0000
  Null hypothesis rejected
  95% confidence interval for r.jk - r.jh: 0.1510 0.3855
  Null hypothesis rejected (Interval does not include 0)

hittner2003: Hittner, May, and Silver's (2003) modification of Dunn and Clark's z (1969) using a backtransformed average Fisher's (1921) Z procedure
  z = 4.4678, p-value = 0.0000
  Null hypothesis rejected

zou2007: Zou's (2007) confidence interval
  95% confidence interval for r.jk - r.jh: 0.0676 0.1802
  Null hypothesis rejected (Interval does not include 0)
\end{verbatim}

Fisher's z-test (\emph{z} = 4.4678, \emph{p} \textless{} 0.001) indicates that the correlation of overall instructor with the traditional pedagogy subscale (\emph{r} = 0.80) is higher than its correlation with the socially responsive pedagogy subscale (\emph{r} = 0.67).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cocor}\SpecialCharTok{::}\FunctionTok{cocor}\NormalTok{(}\AttributeTok{formula =} \SpecialCharTok{\textasciitilde{}}\NormalTok{Valued }\SpecialCharTok{+}\NormalTok{ OvInstructor }\SpecialCharTok{|}\NormalTok{ SRPed }\SpecialCharTok{+}\NormalTok{ OvInstructor, }\AttributeTok{data =}\NormalTok{ big)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

  Results of a comparison of two overlapping correlations based on dependent groups

Comparison between r.jk (OvInstructor, Valued) = 0.6338 and r.jh (OvInstructor, SRPed) = 0.6717
Difference: r.jk - r.jh = -0.0379
Related correlation: r.kh = 0.5624
Data: big: j = OvInstructor, k = Valued, h = SRPed
Group size: n = 299
Null hypothesis: r.jk is equal to r.jh
Alternative hypothesis: r.jk is not equal to r.jh (two-sided)
Alpha: 0.05

pearson1898: Pearson and Filon's z (1898)
  z = -1.0091, p-value = 0.3129
  Null hypothesis retained

hotelling1940: Hotelling's t (1940)
  t = -1.0355, df = 296, p-value = 0.3013
  Null hypothesis retained

williams1959: Williams' t (1959)
  t = -1.0071, df = 296, p-value = 0.3147
  Null hypothesis retained

olkin1967: Olkin's z (1967)
  z = -1.0091, p-value = 0.3129
  Null hypothesis retained

dunn1969: Dunn and Clark's z (1969)
  z = -1.0062, p-value = 0.3143
  Null hypothesis retained

hendrickson1970: Hendrickson, Stanley, and Hills' (1970) modification of Williams' t (1959)
  t = -1.0355, df = 296, p-value = 0.3013
  Null hypothesis retained

steiger1980: Steiger's (1980) modification of Dunn and Clark's z (1969) using average correlations
  z = -1.0060, p-value = 0.3144
  Null hypothesis retained

meng1992: Meng, Rosenthal, and Rubin's z (1992)
  z = -1.0058, p-value = 0.3145
  Null hypothesis retained
  95% confidence interval for r.jk - r.jh: -0.1948 0.0627
  Null hypothesis retained (Interval includes 0)

hittner2003: Hittner, May, and Silver's (2003) modification of Dunn and Clark's z (1969) using a backtransformed average Fisher's (1921) Z procedure
  z = -1.0058, p-value = 0.3145
  Null hypothesis retained

zou2007: Zou's (2007) confidence interval
  95% confidence interval for r.jk - r.jh: -0.1129 0.0360
  Null hypothesis retained (Interval includes 0)
\end{verbatim}

Fisher's z-test (\emph{z} = -1.006, \emph{p} = 0.315) indicates that the correlation of overall instructor with the valued subscale (\emph{r} = 0.4) is not statistically significantly different than its correlation with the socially responsive pedagogy subscale (\emph{r} = 0.67).

\hypertarget{with-at-least-three-variables-evaluate-the-degree-to-which-the-instrument-demonstrates-incremental-validity-this-should-involve-two-regression-equations-and-their-statistical-comparison}{%
\subsection{With at least three variables, evaluate the degree to which the instrument demonstrates incremental validity (this should involve two regression equations and their statistical comparison)}\label{with-at-least-three-variables-evaluate-the-degree-to-which-the-instrument-demonstrates-incremental-validity-this-should-involve-two-regression-equations-and-their-statistical-comparison}}

Playing around with these variables, let's presume our outcome of interested is the student's \emph{valuation of the class} (i.e., the ``Valued by the Student variable'') and we usually predict it through traditional pedagogy. What does SRPed contribute over-and-above?

\emph{Please understand, that we would normally have a more robust dataset with other indicators -- maybe predicting students' grades?}

\emph{Also, we are completely ignoring the multi-level nature of this data. The published manuscript takes a multi-level approach to analyzing the data and my lessons on multi-level modeling address this as well.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{big }\OtherTok{\textless{}{-}} \FunctionTok{na.omit}\NormalTok{(big)  }\CommentTok{\#included b/c there was uneven missingness and the subsequent comparison required equal sample sizes in the regression models}
\NormalTok{Step1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Valued }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TradPed, }\AttributeTok{data =}\NormalTok{ big)}
\NormalTok{Step2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Valued }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TradPed }\SpecialCharTok{+}\NormalTok{ SRPed, }\AttributeTok{data =}\NormalTok{ big)}
\FunctionTok{summary}\NormalTok{(Step1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Valued ~ TradPed, data = big)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.43330 -0.25471  0.04673  0.25388  1.79522 

Coefficients:
            Estimate Std. Error t value            Pr(>|t|)    
(Intercept)  1.67482    0.18581   9.014 <0.0000000000000002 ***
TradPed      0.61426    0.04191  14.656 <0.0000000000000002 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4274 on 213 degrees of freedom
Multiple R-squared:  0.5021,    Adjusted R-squared:  0.4998 
F-statistic: 214.8 on 1 and 213 DF,  p-value: < 0.00000000000000022
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(Step2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Valued ~ TradPed + SRPed, data = big)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.39671 -0.22675  0.03228  0.24841  1.71917 

Coefficients:
            Estimate Std. Error t value             Pr(>|t|)    
(Intercept)  1.44912    0.26349   5.500          0.000000109 ***
TradPed      0.56933    0.05602  10.162 < 0.0000000000000002 ***
SRPed        0.09116    0.07554   1.207                0.229    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.427 on 212 degrees of freedom
Multiple R-squared:  0.5055,    Adjusted R-squared:  0.5009 
F-statistic: 108.4 on 2 and 212 DF,  p-value: < 0.00000000000000022
\end{verbatim}

In the first step we see that traditional pedagogy had a statistically significant effect on the valued dimension \(B = 0.614, p < 0.001)\). This model accounted for 50\% of variance.

In the second step, socially responsive pedagogy was not a statistically significant predictor, over and above traditional pedagogy \(B = 0.091, p = 0.228\). This model accounted for 51\% of variance.

We can formally compare these two models with an the \emph{anova()} function in base R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(Step1, Step2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Model 1: Valued ~ TradPed
Model 2: Valued ~ TradPed + SRPed
  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1    213 38.918                           
2    212 38.652  1   0.26554 1.4564 0.2288
\end{verbatim}

We see that socially responsive pedagogy adds only a non-significant proportion of variance over traditional pedagogy \((F[1, 212] = 38.652, p = 0.229)\).

\hypertarget{rxx}{%
\chapter{Reliability}\label{rxx}}

\href{https://youtube.com/playlist?list=PLtz5cFLQl4KNt5HieVl6iSM7n_jMFadmb\&si=P-Prn9ZGxmIcvkg_}{Screencasted Lecture Link}

The focus of this lecture is the assessment of reliability. We start by defining \emph{classical test theory} and examining several forms of reliability. While the majority of our time is spent considering estimates of internal consistency, we also review retest reliability and inter-rater reliability.

\hypertarget{navigating-this-lesson-3}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-3}}

There is one hour and twenty minutes of lecture. If you work through the materials with me, plan for an additional hour.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-3}{%
\subsection{Learning Objectives}\label{learning-objectives-3}}

Focusing on this week's materials, make sure you can:

\begin{itemize}
\tightlist
\item
  Define ``reliability.''
\item
  Identify broad classes of reliability.
\item
  Interpret reliability coefficients.
\item
  Describe the strengths and limitations of the alpha coefficient.
\end{itemize}

\hypertarget{planning-for-practice-3}{%
\subsection{Planning for Practice}\label{planning-for-practice-3}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. The least complex is to change the random seed in the research and rework the problem demonstrated in the lesson. The results \emph{should} map onto the ones obtained in the lecture.

The second option involves utilizing one of the simulated datasets available in this OER. The \protect\hyperlink{sims}{last lesson} in the OER contains three simulations that could be used for all of the statistics-based practice suggestions. Especially if you started with one of these examples in an earlier lesson, I highly recommend you continue with that.

Alternatively, Lewis and Neville's \citeyearpar{lewis_construction_2015} Gendered Racial Microaggressions Scale for Black Women will be used in the lessons on exploratory factor analysis; Keum et al.'s Gendered Racial Microaggressions Scale for Asian American Women \citep{keum_gendered_2018} will be used in the lessons on confirmatory factor analysis; and Conover et al.'s \citeyearpar{conover_development_2017} Ableist Microaggressions Scale is used in the lesson on invariance testing. Any of these would be suitable for the PCA and PAF homework assignments.

As a third option, you are welcome to use data to which you have access and is suitable for PCA. In any case, please select a scale that has item-level data for which there is a theorized total scale score as well as two or more subscales (three subscales is ideal). With each of these options, plan to:

\begin{itemize}
\tightlist
\item
  Format (i.e., rescore, if necessary) a dataset so that it is possible to calculates estimates of internal consistency
\item
  Calculate and report the alpha coefficient for the total scale scores and subscales (if the scale has them)
\item
  Calculate and report \(\omega_{t}\) and \(\omega_{h}\). With these two determine what proportion of the variance is due to all the factors, error, and \emph{g}.
\item
  Calculate total and subscale scores.
\item
  Describe other reliability estimates that would be appropriate for the measure you are evaluating.
\end{itemize}

\hypertarget{readings-resources-3}{%
\subsection{Readings \& Resources}\label{readings-resources-3}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (and linked, when possible) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Jhangiani, R. S., Chiang, I.-C. A., Cuttler, C., \& Leighton, D. C. (2019). Reliability and Validity. In \emph{Research Methods in Psychology}. \url{https://doi.org/10.17605/OSF.IO/HF7DQ}
\item
  Revelle, W., \& Condon, D. M. (2019a). Reliability from α to ω: A tutorial. Psychological Assessment. \url{https://doi.org/10.1037/pas0000754}

  \begin{itemize}
  \tightlist
  \item
    A full-text preprint is available \href{https://personality-project.org/revelle/publications/rc.pa.19.pdf}{here}.
  \end{itemize}
\item
  Revelle, W., \& Condon, D. M. (2019b). Reliability from α to ω: A tutorial. Online supplement. Psychological Assessment. \url{https://doi.org/10.1037/pas0000754}
\item
  Revelle, William. (n.d.). Reliability. In An introduction to psychometric theory with applications in R. Retrieved from \url{http://www.personality-project.org/dev/r/book/\#chapter7}

  \begin{itemize}
  \tightlist
  \item
    All three documents provide a practical integration of conceptual and mechanical.
  \end{itemize}
\item
  Szymanski, D. M., \& Bissonette, D. (2020). Perceptions of the LGBTQ College Campus Climate Scale: Development and psychometric evaluation. \emph{Journal of Homosexuality}, 67(10), 1412--1428. \url{https://doi.org/10.1080/00918369.2019.1591788}

  \begin{itemize}
  \tightlist
  \item
    The research vignette for this lesson.
  \end{itemize}
\end{itemize}

\hypertarget{packages-3}{%
\subsection{Packages}\label{packages-3}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# will install the package if not already installed}
\CommentTok{\# if(!require(psych))\{install.packages(\textquotesingle{}psych\textquotesingle{})\}}
\CommentTok{\# if(!require(tidyverse))\{install.packages(\textquotesingle{}tidyverse\textquotesingle{})\}}
\CommentTok{\# if(!require(MASS))\{install.packages(\textquotesingle{}MASS\textquotesingle{})\}}
\CommentTok{\# if(!require(sjstats))\{install.packages(\textquotesingle{}sjstats\textquotesingle{})\}}
\CommentTok{\# if(!require(apaTables))\{install.packages(\textquotesingle{}apaTables\textquotesingle{})\}}
\CommentTok{\# if(!require(qualtRics))\{install.packages(\textquotesingle{}qualtRics\textquotesingle{})\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{defining-reliability}{%
\section{Defining Reliability}\label{defining-reliability}}

\hypertarget{begins-with-classical-test-theory-ctt}{%
\subsection{Begins with Classical Test Theory (CTT)}\label{begins-with-classical-test-theory-ctt}}

CTT is based on Spearman's (1904) \emph{true-score model} where:

\begin{itemize}
\tightlist
\item
  an observed score consists of two components -- a true component and an error component
\item
  X = T + E

  \begin{itemize}
  \tightlist
  \item
    X = the fallible, observed/manifest score, obtained under ideal or perfect conditions of measurement (these conditions never exist);
  \item
    T = the true/latent score (that will likely remain unknown); and
  \item
    E = random error
  \end{itemize}
\item
  In CTT, we assume that the traits measured are constant and the errors random.

  \begin{itemize}
  \tightlist
  \item
    Therefore, the mean of measurement errors for any individual (upon numerous repeated testing) would be zero.
  \end{itemize}
\item
  That said, in CTT, the true score would be equal to the mean of the observed scores over an indefinite number of repeated measures.

  \begin{itemize}
  \tightlist
  \item
    Caveat: this is based on the assumption that when individuals are repeatedly measured, their true scores remain unchanged.
  \end{itemize}
\item
  In classic test theory, true score can be estimated over multiple trials. However, if errors are systematically biased, the true score will remain unknown.
\end{itemize}

\hypertarget{why-are-we-concerned-with-reliability-error}{%
\subsection{Why are we concerned with reliability? Error!}\label{why-are-we-concerned-with-reliability-error}}

Measurements are imperfect and every observation has some unknown amount of error associated with it. There are two components in error:

\begin{itemize}
\tightlist
\item
  \textbf{random/unsystematic}: varies in unpredictable and inconsistent ways upon repeated measurements; sources are unknown
\item
  \textbf{systematic}: recurs upon repeated measurements reflecting situational or individual effects that, theoretically, could be specified.
\end{itemize}

Correlations are attenuated (i.e., smaller than) from the true correlation if the observations contain error. Knowing the reliability of an instruments allows us to:

\begin{itemize}
\tightlist
\item
  estimate the degree to which measured at one time and place with one instrument predict scores at another time and/or place and perhaps measured with a different instrument
\item
  estimate the consistency of scores
\item
  estimate ``\ldots the degree to which test scores are free from errors of measurement'' (APA, 1985, p.~19)
\end{itemize}

Figure 7.1a in \href{https://personality-project.org/r/book/Chapter7.pdf}{Revelle's chapter} illustrates the \emph{attenuation} of the correlation between the variables \emph{p} and \emph{q} as a function of reliability.

\begin{itemize}
\tightlist
\item
  circles (latent variables {[}LV{]}) represent the \emph{true score}
\item
  observed/measured/manifest variables are represented by squares, and each has an associated error; not illustrated are the \emph{random} and \emph{systematic} components of error
\item
  a true score is composed of a measured variable and its error
\item
  the relationship between the true scores would be stronger than the one between the measured variables
\item
  moving to 7.1b in Revelle's chapter, the correlation between LV \emph{p} and the observed '\,' can be estimated from the correlation of \emph{p'} with a parallel test (this is the reliability piece)
\end{itemize}

Figure 7.2 in Revelle's Chapter 7 \citeyearpar{revelle_introduction_nodate} illustrates the conceptual effect of reliability on the estimation of a true score. The figure is meant to demonstrate that when error variances are small and reliability is greater, the variance of the true scores more closely approximates that of observed scores.

\hypertarget{the-reliability-coefficient}{%
\subsection{The Reliability Coefficient}\label{the-reliability-coefficient}}

The symbol for reliability, \(r_{xx}\), sums up the big-picture definition that reliability is the correlation of a measure with itself. There are a number of ways to think about it:

\begin{itemize}
\tightlist
\item
  a ``theoretical validity'' of a measure because it refers to a relationship between observed scores and scores on a latent variable or construct,
\item
  represents the fraction of an observed score variance that is not error,
\item
  ranges from 0-1

  \begin{itemize}
  \tightlist
  \item
    1, when all observed variance is due to true-score variance; there are no random errors,
  \item
    0, when all observed variance is due to random errors of measurement,
  \end{itemize}
\item
  represents the squared correlation between observed scores and true scores,
\item
  the ratio between true-score variance and observed-score variance (for a formulaic rendition see \citep{pedhazur_measurement_1991}),
\end{itemize}

\[r_{xt}^{2}=r_{xx} =\frac{\sigma_{2}^{t}}{\sigma_{2}^{x}}\] where \(r_{xt}^{2}\) is the proportion of variance between observed scores (\emph{t} + \emph{e}) and true scores (\emph{t}); its square root is the correlation

\(r_{xx}\) is the reliability of a measure

\({\sigma_{2}^{t}}\) is the variance of true scores

\({\sigma_{2}^{x}}\) is the variance of observed scores

\begin{itemize}
\tightlist
\item
  The reliability coefficient is interpreted as the proportion of systematic variance in the observed score.

  \begin{itemize}
  \tightlist
  \item
    .8 means that 80\% of the variance of the observed scores is systematic;
  \item
    .2 (e.g., 1.00 - .8)is the proportion of variance due to random errors;
  \item
    the reliability coefficient is population specific.
  \end{itemize}
\end{itemize}

To restate the first portion of the formula: although reliability is expressed as a correlation between observed scores, it is also the ratio of reliable variance to total variance.

\hypertarget{research-vignette-2}{%
\section{Research Vignette}\label{research-vignette-2}}

The research vignette for this lesson is the development and psychometric evaluation of the Perceptions of the LGBTQ College Campus Climate Scale \citep{szymanski_perceptions_2020}. The scale is six items with responses rated on a 7-point Likert scale ranging from 1 (\emph{strongly disagree}) to 7 (\emph{strongly agree}). Higher scores indicate more negative perceptions of the LGBTQ campus climate. Szymanski and Bissonette \citeyearpar{szymanski_perceptions_2020} have suggested that the psychometric evaluation supports using the scale in its entirety or as subscales. Each item is listed below with its variable name in parentheses:

\begin{itemize}
\tightlist
\item
  College response to LGBTQ students:

  \begin{itemize}
  \tightlist
  \item
    My university/college is cold and uncaring toward LGBTQ students. (cold)
  \item
    My university/college is unresponsive to the needs of LGBTQ students. (unresponsive)
  \item
    My university/college provides a supportive environment for LGBTQ students. (unsupportive)

    \begin{itemize}
    \tightlist
    \item
      this item must be reverse-scored
    \end{itemize}
  \end{itemize}
\item
  LGBTQ Stigma:

  \begin{itemize}
  \tightlist
  \item
    Negative attitudes toward LGBTQ persons are openly expressed on my university/college campus. (negative)
  \item
    Heterosexism, homophobia, biphobia, transphobia, and cissexism are visible on my university/college campus. (heterosexism)
  \item
    LGBTQ students are harassed on my university/college campus. (harassed)
  \end{itemize}
\end{itemize}

A \href{https://www.researchgate.net/publication/332062781_Perceptions_of_the_LGBTQ_College_Campus_Climate_Scale_Development_and_Psychometric_Evaluation/link/5ca0bef945851506d7377da7/download}{preprint} of the article is available at ResearchGate. Below is the script for simulating item-level data from the factor loadings, means, and sample size presented in the published article.

Because data is collected at the item level (and I want this resource to be as practical as possible, I have simulated the data for each of the scales at the item level.

Simulating the data involved using factor loadings, means, and correlations between the scales. Because the simulation will produce ``out-of-bounds'' values, the code below rescales the scores into the range of the Likert-type scaling and rounds them to whole values.

Five additional scales were reported in the Szymanski and Bissonette article \citeyearpar{szymanski_perceptions_2020}. Unfortunately, I could not locate factor loadings for all of them; and in two cases, I used estimates from a more recent psychometric analysis. When the individual item and their factor loadings are known, I assigned names based on item content (e.g., ``lo\_energy'') rather than using item numbers (e.g., ``PHQ4''). When I am doing psychometric analyses, I prefer item-level names so that I can quickly see (without having to look up the item names) how the items are behaving. While the focus of this series of chapters is on the LGBTQ Campus Climate scale, this simulated data might be useful to you in one or more of the suggestions for practice (e.g., examining the psychometric characteristics of one or the other scales). The scales, their original citation, and information about how I simulated data for each are listed below.

\begin{itemize}
\tightlist
\item
  \textbf{Sexual Orientation-Based Campus Victimization Scale} \citep{herek_documenting_1993} is a 9-item item scale with Likert scaling ranging from 0 (\emph{never}) to 3 (\emph{two or more times}). Because I was not able to locate factor loadings from a psychometric evaluation, I simulated the data by specifying a 0.8 as a standardized factor loading for each of the items.
\item
  \textbf{College Satisfaction Scale} \citep{helm_relationship_1998} is a 5-item scale with Likert scaling ranging from 1 (\emph{strongly disagree}) to 7 (\emph{strongly agree}). Higher scores represent greater college satisfaction. Because I was not able to locate factor loadings from a psychometric evaluation, I simulated the data by specifying a 0.8 as a standardized factor loading for each of the items.
\item
  \textbf{Institutional and Goals Commitment} \citep{pascarella_predicting_1980} is a 6-item subscale from a 35-item measure assessing academic/social integration and institutional/goal commitment (5 subscales total). The measure had with Likert scaling ranging from 1 (\emph{strongly disagree}) to 5 (\emph{strongly agree}). Higher scores on the institutional and goals commitment subscale indicate greater intentions to persist in college. Data were simulated using factor loadings in the source article.
\item
  \textbf{GAD-7} \citep{spitzer_brief_2006} is a 7-item scale with Likert scaling ranging from 0 (\emph{not at all}) to 3 (\emph{nearly every day}). Higher scores indicate more anxiety. I simulated data by estimating factor loadings from Brattmyr et al. \citeyearpar{brattmyr_factor_2022}.
\item
  \textbf{PHQ-9} \citep{kroenke_phq-9_2001} is a 9-item scale with Likert scaling ranging from 0 (\emph{not at all}) to 3 (\emph{nearly every day}). Higher scores indicate higher levels of depression. I simulated data by estimating factor loadings from Brattmyr et al. \citeyearpar{brattmyr_factor_2022}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Entering the intercorrelations, means, and standard deviations from the journal article}

\NormalTok{Szymanski\_generating\_model }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}}
\StringTok{        \#measurement model}
\StringTok{        CollegeResponse  =\textasciitilde{} .88*cold + .73*unresponsive + .73*supportive }
\StringTok{        Stigma =\textasciitilde{} .86*negative + .76*heterosexism + .71*harassed}
\StringTok{        Victimization =\textasciitilde{} .8*Vic1 + .8*Vic2 + .8*Vic3 + .8*Vic4 + .8*Vic5 + .8*Vic6 + .8*Vic7 + .8*Vic8 + .8*Vic9}
\StringTok{        CollSat =\textasciitilde{} .8*Sat1 + .8*Sat2 + .8*Sat3 + .8*Sat4 + .8*Sat5}
\StringTok{        Persistence =\textasciitilde{} .69*graduation\_importance + .63*right\_decision + .62*will\_register + .59*not\_graduate + .45*undecided + .44*grades\_unimportant}
\StringTok{        Anxiety =\textasciitilde{} .851*nervous + .887*worry\_control + .894*much\_worry + 674*cant\_relax + .484*restless + .442*irritable + 716*afraid}
\StringTok{        Depression =\textasciitilde{} .798*anhedonia + .425*down +  .591*sleep +  .913*lo\_energy +  .441*appetite +  .519*selfworth +  .755*concentration +  .454*too\_slowfast + .695*s\_ideation}
\StringTok{   }
\StringTok{        \#Means}
\StringTok{         CollegeResponse \textasciitilde{} 2.71*1}
\StringTok{         Stigma \textasciitilde{}3.61*1}
\StringTok{         Victimization \textasciitilde{} 0.11*1}
\StringTok{         CollSat \textasciitilde{} 5.61*1}
\StringTok{         Persistence \textasciitilde{} 4.41*1}
\StringTok{         Anxiety \textasciitilde{} 1.45*1}
\StringTok{         Depression \textasciitilde{}1.29*1}

\StringTok{         }
\StringTok{        \#Correlations}
\StringTok{         CollegeResponse \textasciitilde{}\textasciitilde{} .58*Stigma}
\StringTok{         CollegeResponse \textasciitilde{}\textasciitilde{} {-}.25*Victimization}
\StringTok{         CollegeResponse \textasciitilde{}\textasciitilde{}  {-}.59*CollSat}
\StringTok{         CollegeResponse \textasciitilde{}\textasciitilde{}  {-}.29*Persistence}
\StringTok{         CollegeResponse \textasciitilde{}\textasciitilde{}  .17*Anxiety}
\StringTok{         CollegeResponse \textasciitilde{}\textasciitilde{}  .18*Depression}
\StringTok{         }
\StringTok{         Stigma \textasciitilde{}\textasciitilde{} .37*Victimization}
\StringTok{         Stigma \textasciitilde{}\textasciitilde{}  {-}.41*CollSat}
\StringTok{         Stigma \textasciitilde{}\textasciitilde{}  {-}.19*Persistence}
\StringTok{         Stigma \textasciitilde{}\textasciitilde{}  .27*Anxiety}
\StringTok{         Stigma \textasciitilde{}\textasciitilde{}  .24*Depression}
\StringTok{         }
\StringTok{         Victimization \textasciitilde{}\textasciitilde{}  {-}.22*CollSat}
\StringTok{         Victimization \textasciitilde{}\textasciitilde{}  {-}.04*Persistence}
\StringTok{         Victimization \textasciitilde{}\textasciitilde{}  .23*Anxiety}
\StringTok{         Victimization \textasciitilde{}\textasciitilde{}  .21*Depression}
\StringTok{         }
\StringTok{         CollSat \textasciitilde{}\textasciitilde{}  .53*Persistence}
\StringTok{         CollSat \textasciitilde{}\textasciitilde{}  {-}.29*Anxiety}
\StringTok{         CollSat \textasciitilde{}\textasciitilde{}  {-}.32*Depression}
\StringTok{         }
\StringTok{         Persistence \textasciitilde{}\textasciitilde{}  {-}.22*Anxiety}
\StringTok{         Persistence \textasciitilde{}\textasciitilde{}  {-}.26*Depression}
\StringTok{         }
\StringTok{         Anxiety \textasciitilde{}\textasciitilde{}  .76*Depression}
\StringTok{        \textquotesingle{}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240218}\NormalTok{)}
\NormalTok{dfSzy }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{simulateData}\NormalTok{(}\AttributeTok{model =}\NormalTok{ Szymanski\_generating\_model,}
                              \AttributeTok{model.type =} \StringTok{"sem"}\NormalTok{,}
                              \AttributeTok{meanstructure =}\NormalTok{ T,}
                              \AttributeTok{sample.nobs=}\DecValTok{646}\NormalTok{,}
                              \AttributeTok{standardized=}\ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\#used to retrieve column indices used in the rescaling script below}
\NormalTok{col\_index }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(dfSzy))}

\CommentTok{\#The code below loops through each column of the dataframe and assigns the scaling accordingly}
\CommentTok{\#Rows 1 thru 6 are the Perceptions of LGBTQ Campus Climate Scale}
\CommentTok{\#Rows 7 thru 15 are the Sexual Orientation{-}Based Campus Victimization Scale}
\CommentTok{\#Rows 16 thru 20 are the College Satisfaction Scale}
\CommentTok{\#Rows 21 thru 26 are the Institutional and Goals Commitment Scale }
\CommentTok{\#Rows 27 thru 33 are the GAD7}
\CommentTok{\#Rows 34 thru 42 are the PHQ9}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(dfSzy))\{  }
  \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{6}\NormalTok{)\{   }
\NormalTok{    dfSzy[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfSzy[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{7}\NormalTok{))}
\NormalTok{  \}}
    \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{7} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{15}\NormalTok{)\{   }
\NormalTok{    dfSzy[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfSzy[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{    \}}
      \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{16} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{20}\NormalTok{)\{  }
\NormalTok{    dfSzy[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfSzy[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{7}\NormalTok{))}
\NormalTok{      \}}
        \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{21} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{26}\NormalTok{)\{  }
\NormalTok{    dfSzy[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfSzy[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{        \}}
  \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{27} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{33}\NormalTok{)\{  }
\NormalTok{    dfSzy[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfSzy[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{  \}}
    \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{34} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{42}\NormalTok{)\{  }
\NormalTok{    dfSzy[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfSzy[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{  \}}
\NormalTok{\}}

\CommentTok{\#rounding to integers so that the data resembles that which was collected}
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{dfSzy }\OtherTok{\textless{}{-}}\NormalTok{ dfSzy }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{round}\NormalTok{(}\DecValTok{0}\NormalTok{) }

\CommentTok{\#quick check of my work}
\CommentTok{\#psych::describe(dfSzy) }

\CommentTok{\#Reversing the supportive item on the Perceptions of LGBTQ Campus Climate Scale so that the exercises will be consistent with the format in which the data was collected}

\NormalTok{dfSzy }\OtherTok{\textless{}{-}}\NormalTok{ dfSzy }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{supportiveNR =} \DecValTok{8} \SpecialCharTok{{-}}\NormalTok{ supportive)}

\CommentTok{\#Reversing three items on the Institutional and Goals Commitments scale so that the exercises will be consistent with the format in which the data was collected}

\NormalTok{dfSzy }\OtherTok{\textless{}{-}}\NormalTok{ dfSzy }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{not\_graduateNR =} \DecValTok{8} \SpecialCharTok{{-}}\NormalTok{ not\_graduate)}\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{undecidedNR =} \DecValTok{8} \SpecialCharTok{{-}}\NormalTok{ undecided)}\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{grades\_unimportantNR =} \DecValTok{8} \SpecialCharTok{{-}}\NormalTok{ grades\_unimportant)}

\NormalTok{dfSzy }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(dfSzy, }\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(supportive, not\_graduate, undecided, grades\_unimportant))}
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either an .rds object (preserves any formatting you might do) or a.csv file (think ``Excel lite'').

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(dfSzy, \textquotesingle{}SzyDF.rds\textquotesingle{}) bring back the simulated dat from}
\CommentTok{\# an .rds file dfSzy \textless{}{-} readRDS(\textquotesingle{}SzyDF.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(dfSzy,}
\CommentTok{\# file=\textquotesingle{}SzyDF.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE) bring}
\CommentTok{\# back the simulated dat from a .csv file dfSzy \textless{}{-}}
\CommentTok{\# read.csv(\textquotesingle{}SzyDF.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

If we look at the information about this particular scale, we recognize that the \emph{supportive} item is scaled in the opposite direction of the rest of the items. That is, a higher score on \emph{supportive} would indicate a positive perception of the campus climate for LGBTQ individuals, whereas higher scores on the remaining items indicate a more negative perception. Before moving forward, we must reverse score this item.

In this psychometrics example I have given my variables one-word names that represent each item. Many researchers (including myself) will often give variable names that are alpha numerical: LGBTQ1, LGBTQ2, LGBTQ\emph{n}. In the psychometric evaluations, the one-word names may be useful shortcuts as one begins to understand the inter-item relations.

In reverse-scoring the \emph{supportive} item, I will rename it ``unsupportive'' as an indication of its reversed direction.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{dfSzy }\OtherTok{\textless{}{-}}\NormalTok{ dfSzy }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{unsupportive =} \DecValTok{8} \SpecialCharTok{{-}}\NormalTok{ supportiveNR)  }\CommentTok{\#when reverse{-}coding, subtract the variable from one number higher than the scaling}

\CommentTok{\# When unhashtagged, this code provides item{-}level descriptive}
\CommentTok{\# statistics psych::describe(dfSzy)}
\end{Highlighting}
\end{Shaded}

Next, I will create dfs that each contain the items of the total and subscales. These will be useful in the reliability estimates that follow.

Note that I am adding ``T1'' (time 1) to the end of the variable names. Later in the lesson when we evaluate test-retest reliability, we will simulate and add a ``T2.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LGBTQT1 }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(dfSzy, cold, unresponsive, unsupportive, negative,}
\NormalTok{    heterosexism, harassed)}
\NormalTok{ResponseT1 }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(dfSzy, cold, unresponsive, unsupportive)}
\NormalTok{StigmaT1 }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(dfSzy, negative, heterosexism, harassed)}
\end{Highlighting}
\end{Shaded}

As we move into the lecture, allow me to provide a content advisory. Individuals who hold LGBTQIA+ identities are frequently the recipients of discrimination and harassment. If you are curious about why these items are considered to be stigmatizing or non-responsive, please do not ask a member of the LGBTQIA+ community to explain it to you; it is not their job to educate others on discrimination, harassment, and microaggressions. Rather, please read the article in its entirety. Additionally, resources such as \href{https://www.thetrevorproject.org/}{The Trevor Project}, \href{https://www.glsen.org/}{GLSEN}, and \href{https://www.campuspride.org/}{Campus Pride} are credible sources of information for learning more.

\hypertarget{a-parade-of-reliability-coefficients}{%
\section{A Parade of Reliability Coefficients}\label{a-parade-of-reliability-coefficients}}

While I cluster the reliability coefficients into large groups, please understand that these are somewhat overlapping.

Table 1 in Revelle and Condon's \citeyearpar{revelle_reliability_2019-1} article provides a summary of the type of reliability tested, the findings, and the function used in the \emph{psych} package.

\hypertarget{reliability-options-for-a-single-administration}{%
\subsection{Reliability Options for a Single Administration}\label{reliability-options-for-a-single-administration}}

If reliability is defined as the correlation between a test and a test just like it, how do we estimate the reliability of a single test, given only one time \citep{revelle_reliability_2019-1}? It may help to keep in mind that reliability is the ratio of true score variance to test score variance (or 1 - the ratio of error variance). Thus, the goal is to estimate the amount of error variance in the test. In this case we can investigate:

\begin{itemize}
\tightlist
\item
  a correlation between two random parts of the test
\item
  internal consistency
\item
  the internal structure of the test
\end{itemize}

\hypertarget{split-half-reliability}{%
\subsubsection{Split-half reliability}\label{split-half-reliability}}

\emph{Split-half reliability} is splitting a test into two random halves, correlating the two halves, and adjusting the correlation with the \emph{Spearman-Brown} prophecy formula. Abundant formulaic detail in Revelle's Chapter 7/Reliability \citeyearpar{revelle_william_personality_nodate}.

An important question to split-half is ``How to split?'' Revelle terms it a ``combinatorially difficult problem.'' There are 126 possible splits for a 10-item scale; 6,345 possible splits for a 16-item scale; and over 4.5 billion for a 36-item scale! The \emph{psych} package's \emph{splitHalf()} function will try all possible splits for scales of up to 16 items, then sample 10,000 splits for scales longer than that.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{split }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{splitHalf}\NormalTok{(LGBTQT1, }\AttributeTok{raw =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{brute =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{split  }\CommentTok{\#show the results of the analysis}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Split half reliabilities  
Call: psych::splitHalf(r = LGBTQT1, raw = TRUE, brute = TRUE)

Maximum split half reliability (lambda 4) =  0.73
Guttman lambda 6                          =  0.68
Average split half reliability            =  0.7
Guttman lambda 3 (alpha)                  =  0.7
Guttman lambda 2                          =  0.71
Minimum split half reliability  (beta)    =  0.54
Average interitem r =  0.28  with median =  0.25
                                             2.5% 50% 97.5%
 Quantiles of split half reliability      =  0.54 0.72 0.73
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(split}\SpecialCharTok{$}\NormalTok{raw, }\AttributeTok{breaks =} \DecValTok{101}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"Split{-}half reliability"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Split{-}half reliabilities of 6 LGBTQ items"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-Reliability_files/figure-latex/unnamed-chunk-8-1.pdf}

Results of the split-half can provide some indication of whether not the scale is unidimensional.

In this case, the maximum reliability coefficient is 0.73, the average 0.70, and the lowest is 0.54. Similarly, we can examine the quantiles: 0.54, 0.72, 0.73.

The split-half output also includes the classic Cronbach's (1951) alpha coefficient (0.70; aka Guttman lambda 3) and average interitem correlations (0.25). The figure plots the frequencies of the reliability coefficient values.

While I did not find guidelines on what constitutes a ``high enough lower bound'' to establish homogeneity, Revelle suggested that a scale with 0.85, 0.80, and 0.65 had ``strong evidence for a relatively homogeneous scale.'' When the values were 0.81, 0.73, 0.42, Revelle indicated that there was ``strong evidence for non-homogeneity'' \citep[p.~11]{revelle_reliability_2019}. In making this declaration, Revelle was also looking at the strength of the inter-item correlation and for a rather tight, bell-shaped, distribution at the higher (\textgreater{} 0.73) end of the figure.

What happens when we examine the split-half estimates of the subscales? With only three items, there's not much of a split and so the associated histogram will not be helpful.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{splitRx }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{splitHalf}\NormalTok{(ResponseT1, }\AttributeTok{raw =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{brute =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{splitRx  }\CommentTok{\#show the results of the analysis}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Split half reliabilities  
Call: psych::splitHalf(r = ResponseT1, raw = TRUE, brute = TRUE)

Maximum split half reliability (lambda 4) =  0.62
Guttman lambda 6                          =  0.57
Average split half reliability            =  0.59
Guttman lambda 3 (alpha)                  =  0.66
Guttman lambda 2                          =  0.66
Minimum split half reliability  (beta)    =  0.56
Average interitem r =  0.39  with median =  0.4
                                             2.5% 50% 97.5%
 Quantiles of split half reliability      =  0.57 0.58 0.62
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(splitRx}\SpecialCharTok{$}\NormalTok{raw, }\AttributeTok{breaks =} \DecValTok{101}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"Split{-}half reliability"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Split{-}half reliabilities of 3 items of the College Response subscale"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-Reliability_files/figure-latex/unnamed-chunk-9-1.pdf}

The alpha is 0.66. The range of splits for max, ave, and low are 0.62, 0.59, and 0.55 and the quantiles are 0.57, 0.58, 0.62. The inter-item correlations have an average of 0.40.

Let's look at the split-half reliability coefficients for the Stigma subscale.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{splitSt }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{splitHalf}\NormalTok{(StigmaT1, }\AttributeTok{raw =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{brute =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{splitSt  }\CommentTok{\#show the results of the analysis}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Split half reliabilities  
Call: psych::splitHalf(r = StigmaT1, raw = TRUE, brute = TRUE)

Maximum split half reliability (lambda 4) =  0.56
Guttman lambda 6                          =  0.53
Average split half reliability            =  0.56
Guttman lambda 3 (alpha)                  =  0.63
Guttman lambda 2                          =  0.63
Minimum split half reliability  (beta)    =  0.55
Average interitem r =  0.36  with median =  0.36
                                             2.5% 50% 97.5%
 Quantiles of split half reliability      =  0.55 0.56 0.56
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(splitRx}\SpecialCharTok{$}\NormalTok{raw, }\AttributeTok{breaks =} \DecValTok{101}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"Split{-}half reliability"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Split{-}half reliabilities of 3 items of the Stigma subscale"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-Reliability_files/figure-latex/unnamed-chunk-10-1.pdf} The alpha of this subscale is lower than the total scale score \((\alpha = 0.60\)). The maximum, average, and minimum split-half reliabilities were 0.56, 0.56, and 0.55; quantiles were at 0.55, 0.56, and 0.56. The average interitem correlation was 0.36.

Because the alpha coefficient can be defined as the ``average of all possible split-half coefficients'' for the groups tested, it is common for researchers to not provide split-half results in their papers -- this is true for our research vignette. I continue to teach the split-half because it can be a stepping stone in the conceptualization of internal consistency as an estimate of reliability.

\hypertarget{alpha-coefficients}{%
\subsubsection{Alpha coefficients}\label{alpha-coefficients}}

The most common methods to assess internal consistency are the \emph{KR20} (for dichotomous items) and \(\alpha\) (for Likert scaling); alpha has an alias, \(\lambda _{3}\) (i.e., the Guttman lambda 3).

Alpha and the Guttman 3 (used for scales with Likert-type scaling) may be thought of as:

\begin{itemize}
\tightlist
\item
  a function of the number of items and the average correlation between the items
\item
  the correlation of a test with a non-existent test just like it
\item
  average of all possible split-half coefficients for the groups tested
\end{itemize}

Although the \emph{psych} package has an incredible and thorough \emph{alpha()} function, Revelle is not a fan of alpha. In fact, his alpha function reports a 95\% CI around alpha as well as bootstrapped alpha results.

Let's grab alpha coefficients for our total and subscales.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(LGBTQT1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = LGBTQT1)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
       0.7       0.7    0.68      0.28 2.4 0.018    4 0.63     0.25

    95% confidence boundaries 
         lower alpha upper
Feldt     0.66   0.7  0.74
Duhachek  0.66   0.7  0.74

 Reliability if an item is dropped:
             raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r
cold              0.64      0.64    0.61      0.27 1.8    0.022 0.0066  0.22
unresponsive      0.66      0.66    0.63      0.28 2.0    0.021 0.0073  0.25
unsupportive      0.67      0.67    0.63      0.29 2.0    0.021 0.0058  0.25
negative          0.66      0.66    0.63      0.28 2.0    0.021 0.0084  0.25
heterosexism      0.66      0.66    0.63      0.28 2.0    0.021 0.0087  0.25
harassed          0.67      0.67    0.64      0.29 2.0    0.021 0.0078  0.25

 Item statistics 
               n raw.r std.r r.cor r.drop mean   sd
cold         646  0.68  0.68  0.59   0.49  4.1 1.03
unresponsive 646  0.63  0.63  0.51   0.43  4.3 0.99
unsupportive 646  0.62  0.62  0.51   0.42  3.7 0.98
negative     646  0.64  0.63  0.51   0.42  4.0 1.04
heterosexism 646  0.61  0.63  0.51   0.43  4.0 0.90
harassed     646  0.63  0.61  0.49   0.41  3.9 1.07

Non missing response frequency for each item
                1    2    3    4    5    6    7 miss
cold         0.00 0.04 0.22 0.40 0.23 0.09 0.00    0
unresponsive 0.00 0.03 0.17 0.37 0.33 0.09 0.01    0
unsupportive 0.01 0.07 0.35 0.37 0.17 0.02 0.01    0
negative     0.01 0.07 0.23 0.39 0.24 0.05 0.00    0
heterosexism 0.00 0.03 0.24 0.43 0.26 0.03 0.00    0
harassed     0.01 0.07 0.27 0.37 0.22 0.05 0.01    0
\end{verbatim}

The second screen of output shows the information we are interested in:

\begin{itemize}
\tightlist
\item
  \textbf{raw\_alpha}, .70 is based on the covariances
\item
  \textbf{std.apha}, .70 is based on correlations
\item
  \textbf{average\_r}, .28 is the average inter-item correlation (i.e., all possible pairwise combinations of items)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(ResponseT1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = ResponseT1)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
      0.66      0.66    0.57      0.39 1.9 0.023    4 0.77      0.4

    95% confidence boundaries 
         lower alpha upper
Feldt     0.61  0.66  0.70
Duhachek  0.62  0.66  0.71

 Reliability if an item is dropped:
             raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r
cold              0.52      0.52    0.35      0.35 1.1    0.038    NA  0.35
unresponsive      0.60      0.60    0.42      0.42 1.5    0.032    NA  0.42
unsupportive      0.58      0.58    0.40      0.40 1.4    0.033    NA  0.40

 Item statistics 
               n raw.r std.r r.cor r.drop mean   sd
cold         646  0.80  0.79  0.62   0.50  4.1 1.03
unresponsive 646  0.76  0.76  0.55   0.45  4.3 0.99
unsupportive 646  0.76  0.77  0.57   0.46  3.7 0.98

Non missing response frequency for each item
                1    2    3    4    5    6    7 miss
cold         0.00 0.04 0.22 0.40 0.23 0.09 0.00    0
unresponsive 0.00 0.03 0.17 0.37 0.33 0.09 0.01    0
unsupportive 0.01 0.07 0.35 0.37 0.17 0.02 0.01    0
\end{verbatim}

In the case of the College Response subscale:

\begin{itemize}
\tightlist
\item
  \textbf{raw\_alpha}, .66 is based on the covariances
\item
  \textbf{std.apha}, .66 is based on correlations
\item
  \textbf{average\_r}, .39 is the average interitem correlation
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(StigmaT1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = StigmaT1)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
      0.62      0.63    0.53      0.36 1.7 0.025    4 0.76     0.36

    95% confidence boundaries 
         lower alpha upper
Feldt     0.57  0.62  0.67
Duhachek  0.57  0.62  0.67

 Reliability if an item is dropped:
             raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r
negative          0.52      0.52    0.35      0.35 1.1    0.038    NA  0.35
heterosexism      0.53      0.53    0.36      0.36 1.1    0.037    NA  0.36
harassed          0.53      0.54    0.37      0.37 1.2    0.036    NA  0.37

 Item statistics 
               n raw.r std.r r.cor r.drop mean  sd
negative     646  0.77  0.76  0.56   0.44  4.0 1.0
heterosexism 646  0.73  0.76  0.55   0.44  4.0 0.9
harassed     646  0.77  0.75  0.54   0.43  3.9 1.1

Non missing response frequency for each item
                1    2    3    4    5    6    7 miss
negative     0.01 0.07 0.23 0.39 0.24 0.05 0.00    0
heterosexism 0.00 0.03 0.24 0.43 0.26 0.03 0.00    0
harassed     0.01 0.07 0.27 0.37 0.22 0.05 0.01    0
\end{verbatim}

In the case of the Stigma subscale:

\begin{itemize}
\tightlist
\item
  \textbf{raw\_alpha}, .62 is based on the covariances
\item
  \textbf{std.apha}, .63 is based on correlations
\item
  \textbf{average\_r}, .36 is the average interitem correlation
\end{itemize}

The documentation for this package is incredible. Scrolling down through the description of the \emph{alpha()} function provides a description of these different statistics.

Especially useful are item-level statistics:

\begin{itemize}
\tightlist
\item
  \textbf{r.drop} is the corrected item-total correlation (\protect\hyperlink{ItemAnalSurvey}{in the next lesson}) for this item against the remaining items in the scale
\item
  \textbf{mean} and \textbf{sd} are the mean and standard deviation of each item across all individuals.
\end{itemize}

The popularity of alpha emerged when tools available for calculation were less sophisticated; since then, we have learned that alpha can be misleading:

\begin{itemize}
\tightlist
\item
  Alpha inflates, somewhat artificially, even when inter-item correlations are low.

  \begin{itemize}
  \tightlist
  \item
    A 14-item scale will have an alpha of at least .70, even if it has two orthogonal (i.e., unrelated) scales \citep{cortina_what_1993}.
  \end{itemize}
\item
  Alpha assumes a unidimensional factor structure.
\item
  The same alpha can be obtained for dramatically different underlying factor structures (see graphs in \href{http://www.personality-project.org/dev/r/book/\#chapter7}{Revelle's Chapter 7}).
\end{itemize}

The proper use of alpha requires the following:

\begin{itemize}
\tightlist
\item
  \emph{Tau equivalence}, that is, equal covariances with the latent score represented by the test.
\item
  \emph{Unidimensionality}, equal factor loadings on the single factor of the test.
\end{itemize}

When either of these is violated, alpha underestimates reliability and overestimates the fraction of test variance that is associated with the general variance in the test.

Alpha and the split half are \emph{internal consistency} estimates. Moving to \emph{model-based} techniques allows us to take into consideration the factor structure of the scale. In the original article \citep{szymanski_perceptions_2020}, results were as follows:

\hypertarget{omega}{%
\subsubsection{Omega}\label{omega}}

Assessing reliability with \emph{omega} (\(\omega\)) statistics falls into a larger realm of \emph{composite reliability} where reliability is assessed from a ratio of the variability explained by the items compared with the total variance of the entire scale \citep{mcneish_thanks_2018}. Members of the omega family of reliability estimates come from factor exploratory (i.e., EFA) and confirmatory (i.e., CFA; structural equation modeling {[}SEM{]}) factor analytic approaches. This lesson precedes the lessons on CFA and SEM. Therefore, my explanations and demonstrations will be somewhat brief. I intend to revisit omega output in the CFA and SEM lessons and encourage you to review this section now, then return to this section again after learning more about CFA and SEM.

In the context of \emph{psychometrics}, it may be useful (albeit an oversimplification) to think of factors as scales/subscales where \emph{g} refers to the amount of variance in the \emph{general} factor (or total scale score) and subscales to be items that have something in common that is separate from what is \emph{g}.

Model-based estimates examine the correlations or covariances of the items and decompose the test variance into that which is:

\begin{itemize}
\tightlist
\item
  common to all items (\textbf{g}, a general factor),
\item
  specific to some items (\textbf{f}, orthogonal group factors), and
\item
  unique to each item (confounding \textbf{s} specific, and \textbf{e} error variance)
\end{itemize}

\(\omega\) is something of a shapeshifter. In the \emph{psych} package:

\begin{itemize}
\tightlist
\item
  \(\omega_{t}\) represents the total reliability of the test (\(\omega_{t}\))

  \begin{itemize}
  \tightlist
  \item
    In the \emph{psych} package, this is calculated from a bifactor model where there is one general \emph{g} factor (i.e., each item loads on the single general factor), one or more group factors (\emph{f}), and item-specific factors.
  \end{itemize}
\item
  \(\omega_{h}\) extracts a higher-order factor from the correlation matrix of lower-level factors, then applies the Schmid and Leiman \citeyearpar{schmid_development_1957} transformation to find the general loadings on the original items. Stated another way, it is a measure of the general factor saturation (\emph{g}; the amount of variance attributable to one common factor). The subscript ``h'' acknowledges the hierarchical nature of the approach.

  \begin{itemize}
  \tightlist
  \item
    the \(\omega_{h}\) approach is exploratory and defined if there are three or more group factors (with only two group factors, the default is to assume they are equally important, hence the factor loadings of those subscales will be equal)
  \item
    Najera Catalan \citep{najera_catalan_reliability_2019} suggests that \(\omega_{h}\) is the best measure of reliability when dealing with multiple dimensions.
  \end{itemize}
\item
  \(\omega_{g}\) is an estimate that uses a bifactor solution via the SEM package \emph{lavaan} and tends to be a larger (because it forces all the cross loadings of lower-level factors to be 0)

  \begin{itemize}
  \tightlist
  \item
    the \(\omega_{g}\) is confirmatory, requiring the specification of which variables load on each group factor
  \end{itemize}
\end{itemize}

Two commands in \emph{psych} get us the results:

\begin{itemize}
\tightlist
\item
  \emph{omega()} reports only the EFA solution
\item
  \emph{omegaSem()} reports both EFA and CFA solutions

  \begin{itemize}
  \tightlist
  \item
    We will use the \emph{omegaSem()} function
  \end{itemize}
\end{itemize}

Note that in our specification, we indicate there are two factors. We do not tell it what items belong to what factors (think, \emph{subscales}). One test will be to see if the items align with their respective factors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{omegaSem}\NormalTok{(LGBTQT1, }\AttributeTok{nfactors =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required namespace: GPArotation
\end{verbatim}

\begin{verbatim}

Three factors are required for identification -- general factor loadings set to be equal. 
Proceed with caution. 
Think about redoing the analysis with alternative values of the 'option' setting.
\end{verbatim}

\begin{verbatim}
Warning in lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats, : lavaan WARNING:
    Could not compute standard errors! The information matrix could
    not be inverted. This may be a symptom that the model is not
    identified.
\end{verbatim}

\includegraphics{05-Reliability_files/figure-latex/unnamed-chunk-14-1.pdf} \includegraphics{05-Reliability_files/figure-latex/unnamed-chunk-14-2.pdf}

\begin{verbatim}
 
Call: psych::omegaSem(m = LGBTQT1, nfactors = 2)
Omega 
Call: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, 
    digits = digits, title = title, sl = sl, labels = labels, 
    plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, 
    covar = covar)
Alpha:                 0.7 
G.6:                   0.68 
Omega Hierarchical:    0.54 
Omega H asymptotic:    0.73 
Omega Total            0.74 

Schmid Leiman Factor loadings greater than  0.2 
                g   F1*   F2*   h2   u2   p2
cold         0.53  0.45       0.49 0.51 0.59
unresponsive 0.45  0.37       0.34 0.66 0.60
unsupportive 0.45  0.41       0.37 0.63 0.55
negative     0.46        0.40 0.37 0.63 0.58
heterosexism 0.46        0.39 0.36 0.64 0.59
harassed     0.44        0.39 0.35 0.65 0.56

With Sums of squares  of:
   g  F1*  F2* 
1.31 0.51 0.46 

general/max  2.59   max/min =   1.1
mean percent general =  0.58    with sd =  0.02 and cv of  0.03 
Explained Common Variance of the general factor =  0.58 

The degrees of freedom are 4  and the fit is  0 
The number of observations was  646  with Chi Square =  2.59  with prob <  0.63
The root mean square of the residuals is  0.01 
The df corrected root mean square of the residuals is  0.02
RMSEA index =  0  and the 10 % confidence intervals are  0 0.049
BIC =  -23.3

Compare this with the adequacy of just a general factor and no group factors
The degrees of freedom for just the general factor are 9  and the fit is  0.18 
The number of observations was  646  with Chi Square =  115.31  with prob <  0.000000000000000000012
The root mean square of the residuals is  0.1 
The df corrected root mean square of the residuals is  0.13 

RMSEA index =  0.135  and the 10 % confidence intervals are  0.114 0.158
BIC =  57.08 

Measures of factor score adequacy             
                                                 g   F1*   F2*
Correlation of scores with factors            0.74  0.57  0.56
Multiple R square of scores with factors      0.54  0.33  0.31
Minimum correlation of factor score estimates 0.09 -0.34 -0.38

 Total, General and Subset omega for each subset
                                                 g  F1*  F2*
Omega total for total scores and subscales    0.74 0.66 0.63
Omega general for total scores and subscales  0.54 0.38 0.36
Omega group for total scores and subscales    0.20 0.28 0.27

 The following analyses were done using the  lavaan  package 

 Omega Hierarchical from a confirmatory model using sem =  0.54
 Omega Total  from a confirmatory model using sem =  0.74 
With loadings of 
                g  F1*  F2*   h2   u2   p2
cold         0.56 0.42      0.49 0.51 0.64
unresponsive 0.47 0.33      0.34 0.66 0.65
unsupportive 0.44 0.42      0.38 0.62 0.51
negative     0.45      0.42 0.37 0.63 0.55
heterosexism 0.46      0.39 0.36 0.64 0.59
harassed     0.43      0.40 0.34 0.66 0.54

With sum of squared loadings of:
   g  F1*  F2* 
1.32 0.46 0.49 

The degrees of freedom of the confirmatory model are  3  and the fit is  2.658827  with p =  0.4472696
general/max  2.71   max/min =   1.05
mean percent general =  0.58    with sd =  0.06 and cv of  0.1 
Explained Common Variance of the general factor =  0.58 

Measures of factor score adequacy             
                                                 g   F1*   F2*
Correlation of scores with factors            0.74  0.55  0.58
Multiple R square of scores with factors      0.55  0.31  0.33
Minimum correlation of factor score estimates 0.10 -0.39 -0.34

 Total, General and Subset omega for each subset
                                                 g  F1*  F2*
Omega total for total scores and subscales    0.74 0.66 0.63
Omega general for total scores and subscales  0.54 0.41 0.34
Omega group for total scores and subscales    0.20 0.26 0.28

To get the standard sem fit statistics, ask for summary on the fitted object
\end{verbatim}

There is a ton of output! How do we make sense of it?

First, our items aligned perfectly with their respective factors (subscales). That is, it would be problematic if the items switched factors.

Second, we can interpret our results. Like alpha, the omegas range from 0 to 1, where values closer to 1 represent good reliability \citep{najera_catalan_reliability_2019}. For unidimensional measures, \(\omega_{t}\) values above 0.80 indicate satisfactory reliability. For multidimensional measures with well-defined dimensions, we strive for \(\omega_{h}\) values above 0.65 (and \(\omega_{t}\) \textgreater{} 0.8). These recommendations are based on a Monte Carlo study that examined a host of reliability indicators and how their values corresponded with accurate predictions of poverty status. With this in mind, let's examine the output related to our simulated research vignette.

Let's start with the output in the lower portion where the values are ``from a confirmatory model using sem.''

Omega is a reliability estimate for factor analysis that represents the proportion of variance in the LGBTQ scale attributable to common variance rather than error. The omega for the total reliability of the test (\(\omega_{t}\); which included the general factors and the subscale factors) was .74, meaning that 74\% of the variance in the total scale is due to the factors and 26\% (100\% - 74\%) is attributable to error.

Omega hierarchical (\(\omega_{h}\)) estimates are the proportion of variance in the LGBTQ score attributable to the general factor, which in effect treats the subscales as error. \(\omega_{h}\) for the the LGBTQ total scale was .54. A quick calculation with \(\omega_{h}\) (.54) and \(\omega_{t}\) (.74; .54/.74 = .72) lets us know that that 73\% of the reliable variance in the LGBTQ total scale is attributable to the general factor.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{.}\DecValTok{54}\SpecialCharTok{/}\NormalTok{.}\DecValTok{74}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.7297297
\end{verbatim}

Amongst the output is the Cronbach's alpha coefficient (.70). Szymanski and Bissonette \citeyearpar{szymanski_perceptions_2020} did not report omega results; this may be because there were only two subfactors and/or they did not feel like a bifactor analysis would be appropriate. You might notice the lavaan warning indicating that three factors are needed in order to identify the CFA model. There is a longer explanation about factor identification. Stay tuned for CFA models.

\hypertarget{some-summary-statements-about-reliability-from-single-administrations}{%
\subsubsection{Some summary statements about reliability from single administrations}\label{some-summary-statements-about-reliability-from-single-administrations}}

\begin{itemize}
\tightlist
\item
  With the exception of the worst split-half reliability and \(\omega_{g}\) or \(\omega_{h}\), all of the reliability estimates are functions of test length and will tend asymptotically towards 1 as the number of items increases.
\item
  The omega output provides a great deal more information about reliability than a simple alpha.

  \begin{itemize}
  \tightlist
  \item
    Figure 7.5 in \href{http://www.personality-project.org/dev/r/book/\#chapter7}{Revelle's chapter} shows four different structural representations of measures that have equal alphas (all .72)
  \end{itemize}
\item
  \(\omega_{(h)}\), \(\beta\), and the worst split-half reliability are estimates of the amount of general factor variance in the test scores
\item
  In the case of low general factor saturation, the EFA based \(\omega_{(h)}\) is positively biased, so the CFA-based estimate, \(\omega_{(g)}\), should be used.
\item
  \(\omega_{(t)}\) is the model-based estimate of the greatest lower bound of the total reliability of the test; so is the best split-half reliability.
\end{itemize}

Revelle and Condon's \citeyearpar{revelle_reliability_2019} recommendations to researchers:

\begin{itemize}
\tightlist
\item
  Report at least two coefficients (e.g., \(\omega_{(h)}\) and \(\omega_{(t)}\)) and discuss why each is appropriate for the inference that is being made.
\item
  Report more than ``just alpha'' unless you can demonstrate that the measure is tau equivalent and unidimensional.
\end{itemize}

\hypertarget{reliability-options-for-two-or-more-administrations}{%
\subsection{Reliability Options for Two or more Administrations}\label{reliability-options-for-two-or-more-administrations}}

\hypertarget{test-retest-of-total-scores}{%
\subsubsection{Test-retest of total scores}\label{test-retest-of-total-scores}}

The purpose of test-retest reliability is to understand the stability of the measure over time. With two time points, T1 and T2, the test-retest correlation is an unknown mixture of trait, state, and specific variance, and is a function of the length of time between two measures.

\begin{itemize}
\tightlist
\item
  With two time points we cannot distinguish between trait and state effects, that said

  \begin{itemize}
  \tightlist
  \item
    we would expect a high degree of stability if the retest were (relatively) immediate
  \end{itemize}
\item
  With three time points we can leverage some SEM tools to distinguish between trait and state components
\item
  A large test-retest correlation over a long period of time indicates temporal stability. Temporal stability is:

  \begin{itemize}
  \tightlist
  \item
    expected if we are assessing something trait like (e.g., cognitive ability, personality trait)
  \item
    not expected if we are assessing something state like (e.g., emotional state, mood)
  \item
    not expected if there was an intervention (or condition) and the T1 and T2 administrations are part of a pre- and post-test design.
  \end{itemize}
\end{itemize}

There are some \emph{methodological} concerns about test-retest reliability. For example, owing to memory and learning effects, the average response time to a second administration of identical items takes about 80\% the time compared to the first administration.

Szymanski and Bissonette \citeyearpar{szymanski_perceptions_2020} did not assess retest reliability. We can, though, imagine how this might work. Let's imagine that both waves were taken in the same academic term, approximately two weeks apart.

With both sets of data we need to create scores for the total scale score and the two subscales. We would also need to join the two datasets into a single dataframe.

To demonstrate the retest reliability, I simulated a new dataset with total and subscale scores for our variables for Time 1 and Time 2. This next script is simply that simulation (i.e., you can skip over it). If this were your data, you would have item-level data and need to calculate total and subscale scores (as we did above).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SimCor\_mu }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{3.13}\NormalTok{, }\FloatTok{2.68}\NormalTok{, }\FloatTok{3.58}\NormalTok{, }\FloatTok{3.16}\NormalTok{, }\FloatTok{2.66}\NormalTok{, }\FloatTok{2.76}\NormalTok{)}
\NormalTok{SimCor\_sd }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.82}\NormalTok{, }\FloatTok{1.04}\NormalTok{, }\FloatTok{1.26}\NormalTok{, }\FloatTok{0.83}\NormalTok{, }\FloatTok{1.05}\NormalTok{, }\FloatTok{0.99}\NormalTok{)}
\NormalTok{simCor }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.64}\NormalTok{, }\FloatTok{0.77}\NormalTok{, }\FloatTok{0.44}\NormalTok{, }\FloatTok{0.33}\NormalTok{, }\FloatTok{0.29}\NormalTok{, }\FloatTok{0.64}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.53}\NormalTok{, }\FloatTok{0.35}\NormalTok{,}
    \FloatTok{0.46}\NormalTok{, }\FloatTok{0.34}\NormalTok{, }\FloatTok{0.77}\NormalTok{, }\FloatTok{0.53}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.27}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\FloatTok{0.47}\NormalTok{, }\FloatTok{0.44}\NormalTok{, }\FloatTok{0.35}\NormalTok{, }\FloatTok{0.27}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.63}\NormalTok{,}
    \FloatTok{0.62}\NormalTok{, }\FloatTok{0.33}\NormalTok{, }\FloatTok{0.46}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\FloatTok{0.63}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.57}\NormalTok{, }\FloatTok{0.29}\NormalTok{, }\FloatTok{0.34}\NormalTok{, }\FloatTok{0.47}\NormalTok{, }\FloatTok{0.62}\NormalTok{, }\FloatTok{0.57}\NormalTok{,}
    \DecValTok{1}\NormalTok{), }\AttributeTok{ncol =} \DecValTok{6}\NormalTok{)}
\NormalTok{scovMat }\OtherTok{\textless{}{-}}\NormalTok{ SimCor\_sd }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(SimCor\_sd) }\SpecialCharTok{*}\NormalTok{ simCor}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{210829}\NormalTok{)}
\NormalTok{retest\_df }\OtherTok{\textless{}{-}}\NormalTok{ MASS}\SpecialCharTok{::}\FunctionTok{mvrnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{646}\NormalTok{, }\AttributeTok{mu =}\NormalTok{ SimCor\_mu, }\AttributeTok{Sigma =}\NormalTok{ scovMat, }\AttributeTok{empirical =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{colnames}\NormalTok{(retest\_df) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"TotalT1"}\NormalTok{, }\StringTok{"ResponseT1"}\NormalTok{, }\StringTok{"StigmaT1"}\NormalTok{, }\StringTok{"TotalT2"}\NormalTok{,}
    \StringTok{"ResponseT2"}\NormalTok{, }\StringTok{"StigmaT2"}\NormalTok{)}
\NormalTok{retest\_df }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(retest\_df)  }\CommentTok{\#converts to a df so we can use in R}
\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{retest\_df }\OtherTok{\textless{}{-}}\NormalTok{ retest\_df }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{ID =} \FunctionTok{row\_number}\NormalTok{())  }\CommentTok{\#add ID to each row}
\NormalTok{retest\_df }\OtherTok{\textless{}{-}}\NormalTok{ retest\_df }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(ID, }\FunctionTok{everything}\NormalTok{())  }\CommentTok{\#moving the ID number to the first column; requires}
\end{Highlighting}
\end{Shaded}

Examing our df, we can see the ID variable and the three sets of scores for each wave of analysis. Now we simply ask for their correlations. There are a number of ways to do this -- the \emph{apaTables} package can do the calculations and pop it into a manuscript-ready table.

We won't want the ID variable to be in the table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{retest\_df2 }\OtherTok{\textless{}{-}}\NormalTok{ retest\_df }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{ID))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(}\AttributeTok{data =}\NormalTok{ retest\_df2, }\AttributeTok{landscape =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{table.number =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{filename =} \StringTok{"Table\_1\_Retest.doc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Table 1 

Means, standard deviations, and correlations with confidence intervals
 

  Variable      M    SD   1          2          3          4         
  1. TotalT1    3.13 0.82                                            
                                                                     
  2. ResponseT1 2.68 1.04 .64**                                      
                          [.59, .68]                                 
                                                                     
  3. StigmaT1   3.58 1.26 .77**      .53**                           
                          [.74, .80] [.47, .58]                      
                                                                     
  4. TotalT2    3.16 0.83 .44**      .35**      .27**                
                          [.38, .50] [.28, .42] [.20, .34]           
                                                                     
  5. ResponseT2 2.66 1.05 .33**      .46**      .40**      .63**     
                          [.26, .40] [.40, .52] [.33, .46] [.58, .67]
                                                                     
  6. StigmaT2   2.76 0.99 .29**      .34**      .47**      .62**     
                          [.22, .36] [.27, .41] [.41, .53] [.57, .67]
                                                                     
  5         
            
            
            
            
            
            
            
            
            
            
            
            
            
            
  .57**     
  [.52, .62]
            

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

As expected in this simulation,

\begin{itemize}
\tightlist
\item
  the strongest correlations are within each scale at their respective time, that is:

  \begin{itemize}
  \tightlist
  \item
    the T1 variables correlate with each other;
  \item
    the T2 variables correlate with each other.
  \end{itemize}
\item
  the next strongest correlations are with the same scale/subscale configuration across time, for example

  \begin{itemize}
  \tightlist
  \item
    TotalT1 with TotalT2 (\emph{r} = .44, \emph{p} \textless{} 0.01)
  \item
    ResponseT1 with ResponseT2 (\emph{r} = .46, \emph{p} \textless{} 0.01)
  \item
    StigmaT1 with StigmaT2 (\emph{r} = .47, \emph{p} \textless{} 0.01)
  \end{itemize}
\item
  the lowest correlations are different scales at T1 and T2

  \begin{itemize}
  \tightlist
  \item
    ResponseT1 with StigmaT2 (\emph{r} = .29)
  \end{itemize}
\end{itemize}

The range of retest correlations (e.g., .44 to .47 with \emph{p} \textless{} 0.01) are sufficient to be confident in test-retest reliability.

\hypertarget{test-retest-recap}{%
\subsubsection{Test-retest recap}\label{test-retest-recap}}

Here are some summary notions for retest reliability:

\begin{itemize}
\tightlist
\item
  Increases in the time interval will lower the reliability coefficient.
\item
  An experimental intervention that is designed to impact the retest assessment will lower the reliability coefficient.
\item
  State measures will have lower retest coefficients than trait measures.
\item
  The three phenomena above all interact with each other.
\end{itemize}

Revelle and Condon's \citetext{\citeyear{revelle_reliability_2019}; \citeyear{revelle_reliability_2019-1}} materials elaborate on this further. Their Table 1 is especially helpful. In addition to the myriad of vignettes used to illustrate issues with state, trait, items, whole scale, and so forth, there are demonstrations for duplicated items, assessing for consistency, and parallel/alternate forms.

If you are asking, ``Hey, is parallel/alternate forms really a variant of test retest?'' Great question! In fact, split-half could be seen as test-retest! Once you get in the weeds, the distinctions become less clear.

\hypertarget{interrater-reliability}{%
\subsection{Interrater Reliability}\label{interrater-reliability}}

\hypertarget{cohens-kappa}{%
\subsubsection{Cohen's kappa}\label{cohens-kappa}}

Cohen's kappa coefficient is used to calculate proportions of agreement corrected for chance. This type of analysis occurs in research designs where there is some kind of (usually) categorical designation of a response. I don't have an outside research vignette for this. In the past, I was involved in research where members of the research team coded counselor utterances according to Hill's \emph{helping skills} system designed by Clara Hill \citep{hill_helping_2020}. In the helping skills system, 15 different helping skills are divided into three larger groups that generally reflect the counseling trajectory: \emph{exploration}, \emph{insight}, \emph{action.} One of our analyses coded counselor utterances into these three categories. Let's look at a fabricated (not based on any real data) simulation where four raters each evaluated 12 counselor utterances (that represent the arch of a nonsensically speedy counseling session).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Rater1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{,}
    \StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"insight"}\NormalTok{, }\StringTok{"insight"}\NormalTok{, }\StringTok{"action"}\NormalTok{, }\StringTok{"action"}\NormalTok{,}
    \StringTok{"action"}\NormalTok{, }\StringTok{"action"}\NormalTok{)}
\NormalTok{Rater2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"insight"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{,}
    \StringTok{"insight"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"action"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{,}
    \StringTok{"action"}\NormalTok{)}
\NormalTok{Rater3 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"exploration"}\NormalTok{, }\StringTok{"insight"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{,}
    \StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"insight"}\NormalTok{, }\StringTok{"insight"}\NormalTok{, }\StringTok{"insight"}\NormalTok{, }\StringTok{"action"}\NormalTok{,}
    \StringTok{"action"}\NormalTok{)}
\NormalTok{Rater4 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{,}
    \StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{,}
    \StringTok{"action"}\NormalTok{, }\StringTok{"action"}\NormalTok{, }\StringTok{"action"}\NormalTok{)}
\NormalTok{ratings }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(Rater1, Rater2, Rater3, Rater4)}
\end{Highlighting}
\end{Shaded}

Historically, kappa could only be calculated for 2 raters at a time. Presently, though, it appears there can be any number of raters and the average agreement is reported.

Let's take a look at the data, then run the analysis, and interpret the results.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{cohen.kappa}\NormalTok{(ratings)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Cohen Kappa (below the diagonal) and Weighted Kappa (above the diagonal) 
For confidence intervals and detail print with all=TRUE
       Rater1 Rater2 Rater3 Rater4
Rater1   1.00   0.40   0.21   0.62
Rater2   0.14   1.00   0.00   0.57
Rater3   0.48   0.00   1.00   0.30
Rater4   0.54   0.45   0.43   1.00

Average Cohen kappa for all raters  0.34
Average weighted kappa for all raters  0.35
\end{verbatim}

Kappa can range from -1.00 to 1.00.

\begin{itemize}
\tightlist
\item
  K = .00 indicates that the observed agreement is exactly equal to the agreement that could be observed by chance.
\item
  Negative kappa indicates that observed kappa is less than the expected chance agreement.
\item
  K = 1.00 equals perfect agreement between judges.
\end{itemize}

There are commonly understood concerns about using kappa:

\begin{itemize}
\tightlist
\item
  Research teams typically set an expected standard (e.g., .85) and train raters until kappa is achieved.

  \begin{itemize}
  \tightlist
  \item
    In lengthy projects, rating agreement is rechecked periodically; if necessary there is retraining.
  \end{itemize}
\item
  Obtaining an acceptable kappa becomes difficult as the number of categories increases.

  \begin{itemize}
  \tightlist
  \item
    An example is Hill's \emph{Helping Skills System} when all 15 categories; we chose to use the three categories (into which the 15 categories are subsumed).
  \end{itemize}
\item
  It is also difficult to obtain an adequate kappa when \emph{infrequent} categories (e.g., ``insight'') exist.
\end{itemize}

Our kappa of .35 indicates that this rating team has a 35\% chance of agreement, corrected for by chance. This is substantially below the standard. Let's imagine that the team spends time with their dictionaries, examines common errors, and makes some decision rules.

Here's the resimulation of the ``improved'' agreement.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Rater1b }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{,}
    \StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"insight"}\NormalTok{, }\StringTok{"insight"}\NormalTok{, }\StringTok{"insight"}\NormalTok{, }\StringTok{"action"}\NormalTok{,}
    \StringTok{"action"}\NormalTok{, }\StringTok{"action"}\NormalTok{)}
\NormalTok{Rater2b }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{,}
    \StringTok{"exploration"}\NormalTok{, }\StringTok{"insight"}\NormalTok{, }\StringTok{"insight"}\NormalTok{, }\StringTok{"insight"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"action"}\NormalTok{,}
    \StringTok{"action"}\NormalTok{, }\StringTok{"action"}\NormalTok{)}
\NormalTok{Rater3b }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{,}
    \StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"insight"}\NormalTok{, }\StringTok{"insight"}\NormalTok{,}
    \StringTok{"insight"}\NormalTok{, }\StringTok{"action"}\NormalTok{, }\StringTok{"action"}\NormalTok{)}
\NormalTok{Rater4b }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{,}
    \StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"exploration"}\NormalTok{, }\StringTok{"insight"}\NormalTok{,}
    \StringTok{"action"}\NormalTok{, }\StringTok{"action"}\NormalTok{, }\StringTok{"action"}\NormalTok{)}
\NormalTok{after\_training }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(Rater1b, Rater2b, Rater3b, Rater4b)}
\end{Highlighting}
\end{Shaded}

Now run it again.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{cohen.kappa}\NormalTok{(after\_training)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels =
levels): upper or lower confidence interval exceed abs(1) and set to +/- 1.

Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels =
levels): upper or lower confidence interval exceed abs(1) and set to +/- 1.

Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels =
levels): upper or lower confidence interval exceed abs(1) and set to +/- 1.

Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels =
levels): upper or lower confidence interval exceed abs(1) and set to +/- 1.
\end{verbatim}

\begin{verbatim}

Cohen Kappa (below the diagonal) and Weighted Kappa (above the diagonal) 
For confidence intervals and detail print with all=TRUE
        Rater1b Rater2b Rater3b Rater4b
Rater1b    1.00    0.83    0.55    0.80
Rater2b    0.73    1.00    0.36    0.60
Rater3b    0.72    0.45    1.00    0.46
Rater4b    0.71    0.43    0.70    1.00

Average Cohen kappa for all raters  0.62
Average weighted kappa for all raters  0.6
\end{verbatim}

We observe improved scores, but this team needs more training if we aspire to a kappa of 0.85!

\hypertarget{intraclass-correlation-icc}{%
\subsubsection{Intraclass correlation (ICC)}\label{intraclass-correlation-icc}}

Another option for interrater reliability is the intraclass correlation (ICC). This is the same ICC we use in multilevel modeling! The ICC is used when we have numerical ratings.

In our fabricated vignette below, five raters are evaluating the campus climate for LGBTQIA+ individuals for 10 units/departments on a college campus. Using the ICC can help us determine the degree of leniency and variability within judges.

Below is a simulation of the data (you can ignore this)\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Rater1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{Rater2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{Rater3 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{Rater4 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{Rater5 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{ICC\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(Rater1, Rater2, Rater3, Rater4, Rater5)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#If the code below will not run remove the hashtags from the two lines of code below to install the Matrix package and then the lme4 package from its source}

\CommentTok{\#tools::package\_dependencies("Matrix", which = "LinkingTo", reverse = TRUE)[[1L]]}
\CommentTok{\#install.packages("lme4", type = "source")}
\end{Highlighting}
\end{Shaded}

We can use the \emph{psych::ICC} function to obtain the ICC values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# psych::ICC(ICC\_df [1:10,1:5], lmer = TRUE) \#find the ICCs for the}
\CommentTok{\# 10 campus units and 5 judges}
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{ICC}\NormalTok{(ICC\_df, }\AttributeTok{missing =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{lmer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{check.keys =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call: psych::ICC(x = ICC_df, missing = TRUE, alpha = 0.05, lmer = TRUE, 
    check.keys = FALSE)

Intraclass correlation coefficients 
                         type  ICC   F df1 df2       p lower bound upper bound
Single_raters_absolute   ICC1 0.34 3.5   9  40 0.00259       0.082        0.70
Single_random_raters     ICC2 0.37 5.4   9  36 0.00011       0.118        0.71
Single_fixed_raters      ICC3 0.47 5.4   9  36 0.00011       0.188        0.78
Average_raters_absolute ICC1k 0.72 3.5   9  40 0.00259       0.308        0.92
Average_random_raters   ICC2k 0.74 5.4   9  36 0.00011       0.400        0.92
Average_fixed_raters    ICC3k 0.81 5.4   9  36 0.00011       0.537        0.95

 Number of subjects = 10     Number of Judges =  5
See the help file for a discussion of the other 4 McGraw and Wong estimates,
\end{verbatim}

In the output, reliability for a single judge \(ICC_1\) is the ratio of person variance to total variance. Reliability for multiple judges \(ICC_1k\) adjusts the residual variance by the number of judges.

The ICC function reports six reliability coefficients: 3 for the case of single judges and 3 for the case of multiple judges. It also reports the results in terms of a traditional ANOVA as well as a mixed effects linear model. Additionally, confidence intervals are reported.

Like most correlation coefficients, the ICC ranges from 0 to 1.

\begin{itemize}
\tightlist
\item
  An ICC close to 1 indicates high similarity between values from the same group.
\item
  An ICC close to zero means that values from the same group are not similar.
\end{itemize}

\hypertarget{what-do-we-do-with-these-coefficients}{%
\section{What do we do with these coefficients?}\label{what-do-we-do-with-these-coefficients}}

\hypertarget{corrections-for-attenuation}{%
\subsection{Corrections for attenuation}\label{corrections-for-attenuation}}

Circa 1904, Spearman created the reliability coefficient out of a need to adjust observed correlations between related constructs for the error of measurement in each construct. This is only appropriate if the measure is seen as the expected value of a single underlying construct. However, ``under the hood,'' SEM programs model the pattern of observed correlations in terms of a measurement (reliability) model as well as a structural (validity) model.

\hypertarget{predicting-true-scores-and-their-cis}{%
\subsection{Predicting true scores (and their CIs)}\label{predicting-true-scores-and-their-cis}}

True scores remain unknown and so the reliability coefficient is used in a couple of ways to estimate the true score (and the confidence interval {[}CI{]} around that true score).

Take a quick look at the formula for predicting a true score and observe that the reliability coefficient is used within. It generally serves to nudge the observed score a bit closer to the mean: \(T'=(1-r_{xx})\bar{X}+r_{xx}X\)

The CI around that true score includes some estimate of standard error: \(CI_{95}=T'+/-z_{cv}(s_{e})\). Two estimates are commonly used. One is the standard error of estimate \(s_{e}=s_{x}\sqrt{r_{xx}(1-r_{xx})}\) (i.e., the standard deviation of predicted true scores for a given observed score). Another is the standard error of measurement (\(s_{m}=s_{x}\sqrt{(1-r_{xx})}\) (i.e., an estimate of the amount of variation to be expected in test scores; aka, the standard deviation of the errors of measurement).

\emph{I can hear you asking} What is the difference between \(s_{e}\) and \(s_{m}\)?

\begin{itemize}
\tightlist
\item
  Because \(r_{xx}\) is almost always a fraction, \(s_{e}\) is smaller than \(s_{m}\).
\item
  When the reliability is high, the two standard errors are fairly similar to each other.
\item
  Using \(s_{m}\) will result in wider confidence intervals.
\end{itemize}

\hypertarget{how-do-i-keep-it-all-straight}{%
\subsection{How do I keep it all straight?}\label{how-do-i-keep-it-all-straight}}

Table 1 in Revelle and Condon's \citep{revelle_reliability_2019} article helps us connect the type of reliability we are seeking with the statistic(s) and the R function within the \emph{psych} package.

\hypertarget{practice-problems-3}{%
\section{Practice Problems}\label{practice-problems-3}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. The practice problems are the start of a larger project that spans multiple lessons. Therefore, if possible, please use a dataset that has item-level data for which there is a theorized total scale score as well as two or more subscales. With each of these options I encourage you to:

\begin{itemize}
\tightlist
\item
  Format (i.e., rescore if necessary) a dataset so that it is possible to calculates estimates of internal consistency
\item
  Calculate and report the alpha coefficient for a total scale scores and subscales (if the scale has them)
\item
  Calculate and report \(\omega_{t}\) and \(\omega_{h}\). With these two determine what proportion of the variance is due to all the factors, error, and \emph{g}.
\item
  Calculate total and subscale scores.
\item
  Describe other reliability estimates that would be appropriate for the measure you are evaluating.
\end{itemize}

\hypertarget{problem-1-play-around-with-this-simulation.-1}{%
\subsection{Problem \#1: Play around with this simulation.}\label{problem-1-play-around-with-this-simulation.-1}}

If evaluating internal consistency is new to you, copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results. Perhaps you just change the number in ``set.seed(210827)'' from 210827 to something else. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go.

\hypertarget{problem-2-use-the-data-from-the-live-recentering-psych-stats-survey.}{%
\subsection{Problem \#2: Use the data from the live ReCentering Psych Stats survey.}\label{problem-2-use-the-data-from-the-live-recentering-psych-stats-survey.}}

The script below pulls live data directly from the ReCentering Psych Stats survey on Qualtrics. As described in the \href{https://lhbikos.github.io/ReC_MultivariateModeling/}{Scrubbing and Scoring chapters} of the ReCentering Psych Stats Multivariate Modeling volume, the Perceptions of the LGBTQ College Campus Climate Scale \citep{szymanski_perceptions_2020} was included (LGBTQ) and further adapted to assess perceptions of campus climate for Black students (BLst), non-Black students of color (nBSoC), international students (INTst), and students disabilities (wDIS). Consider conducting the analyses on one of these scales or merging them together.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\CommentTok{\# only have to run this ONCE to draw from the same Qualtrics}
\CommentTok{\# account...but will need to get different token if you are changing}
\CommentTok{\# between accounts}
\FunctionTok{library}\NormalTok{(qualtRics)}
\CommentTok{\# qualtrics\_api\_credentials(api\_key =}
\CommentTok{\# \textquotesingle{}mUgPMySYkiWpMFkwHale1QE5HNmh5LRUaA8d9PDg\textquotesingle{}, base\_url =}
\CommentTok{\# \textquotesingle{}spupsych.az1.qualtrics.com\textquotesingle{}, overwrite = TRUE, install = TRUE)}
\NormalTok{QTRX\_df }\OtherTok{\textless{}{-}}\NormalTok{ qualtRics}\SpecialCharTok{::}\FunctionTok{fetch\_survey}\NormalTok{(}\AttributeTok{surveyID =} \StringTok{"SV\_b2cClqAlLGQ6nLU"}\NormalTok{, }\AttributeTok{time\_zone =} \ConstantTok{NULL}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{label =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{convert =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{force\_request =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{import\_id =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{climate\_df }\OtherTok{\textless{}{-}}\NormalTok{ QTRX\_df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(}\StringTok{"Blst\_1"}\NormalTok{, }\StringTok{"Blst\_2"}\NormalTok{, }\StringTok{"Blst\_3"}\NormalTok{, }\StringTok{"Blst\_4"}\NormalTok{, }\StringTok{"Blst\_5"}\NormalTok{, }\StringTok{"Blst\_6"}\NormalTok{,}
        \StringTok{"nBSoC\_1"}\NormalTok{, }\StringTok{"nBSoC\_2"}\NormalTok{, }\StringTok{"nBSoC\_3"}\NormalTok{, }\StringTok{"nBSoC\_4"}\NormalTok{, }\StringTok{"nBSoC\_5"}\NormalTok{, }\StringTok{"nBSoC\_6"}\NormalTok{,}
        \StringTok{"INTst\_1"}\NormalTok{, }\StringTok{"INTst\_2"}\NormalTok{, }\StringTok{"INTst\_3"}\NormalTok{, }\StringTok{"INTst\_4"}\NormalTok{, }\StringTok{"INTst\_5"}\NormalTok{, }\StringTok{"INTst\_6"}\NormalTok{,}
        \StringTok{"wDIS\_1"}\NormalTok{, }\StringTok{"wDIS\_2"}\NormalTok{, }\StringTok{"wDIS\_3"}\NormalTok{, }\StringTok{"wDIS\_4"}\NormalTok{, }\StringTok{"wDIS\_5"}\NormalTok{, }\StringTok{"wDIS\_6"}\NormalTok{, }\StringTok{"LGBTQ\_1"}\NormalTok{,}
        \StringTok{"LGBTQ\_2"}\NormalTok{, }\StringTok{"LGBTQ\_3"}\NormalTok{, }\StringTok{"LGBTQ\_4"}\NormalTok{, }\StringTok{"LGBTQ\_5"}\NormalTok{, }\StringTok{"LGBTQ\_6"}\NormalTok{)}
\CommentTok{\# Item numbers are supported with the following items: \_1 \textquotesingle{}My campus}
\CommentTok{\# unit provides a supportive environment for \_\_\_ students\textquotesingle{} \_2}
\CommentTok{\# \textquotesingle{}\_\_\_\_\_\_\_\_ is visible in my campus unit\textquotesingle{} \_3 \textquotesingle{}Negative attitudes}
\CommentTok{\# toward persons who are \_\_\_\_ are openly expressed in my campus}
\CommentTok{\# unit.\textquotesingle{} \_4 \textquotesingle{}My campus unit is unresponsive to the needs of \_\_\_\_}
\CommentTok{\# students.\textquotesingle{} \_5 \textquotesingle{}Students who are\_\_\_\_\_ are harassed in my campus}
\CommentTok{\# unit.\textquotesingle{} \_6 \textquotesingle{}My campus unit is cold and uncaring toward \_\_\_\_}
\CommentTok{\# students.\textquotesingle{}}

\CommentTok{\# Item 1 on each subscale should be reverse coded. The College}
\CommentTok{\# Response scale is composed of items 1, 4, 6, The Stigma scale is}
\CommentTok{\# composed of items 2,3, 5}
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either a .csv file (think ``Excel lite'') or .rds object (preserves any formatting you might do).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(climate\_df,}
\CommentTok{\# file=\textquotesingle{}climate\_df.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE)}
\CommentTok{\# bring back the simulated dat from a .csv file climate\_df \textless{}{-}}
\CommentTok{\# read.csv (\textquotesingle{}climate\_df.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(climate\_df, \textquotesingle{}climate\_df.rds\textquotesingle{}) bring back the simulated}
\CommentTok{\# dat from an .rds file climate\_df \textless{}{-} readRDS(\textquotesingle{}climate\_df.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

\hypertarget{problem-3-try-something-entirely-new.-1}{%
\subsection{Problem \#3: Try something entirely new.}\label{problem-3-try-something-entirely-new.-1}}

Complete the same steps using data for which you have permission and access. This might be data of your own, from your lab, simulated from an article, or located on an open repository.

\hypertarget{grading-rubric-1}{%
\subsection{Grading Rubric}\label{grading-rubric-1}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5493}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1972}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Possible
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Earned
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1. Check and, if needed, format and score data & 5 & \_\_\_\_\_ \\
2. Calculate and report the alpha coefficient for a total scale scores and subscales (if the scale has them) & 5 & \_\_\_\_\_ \\
3.Calculate and report \(\omega_{t}\) and \(\omega_{h}\). With these two determine what proportion of the variance is due to all the factors, error, and \emph{g}. & 5 & \_\_\_\_\_ \\
4. Calculate total and subscale scores. & 5 & \_\_\_\_\_ \\
5.Describe other reliability estimates that would be appropriate for the measure you are evaluating. & 5 & \_\_\_\_\_ \\
6. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 30 & \_\_\_\_\_ \\
\end{longtable}

\hypertarget{homeworked-example-1}{%
\section{Homeworked Example}\label{homeworked-example-1}}

\href{https://youtu.be/CmbAeUUDJ6E}{Screencast Link}

For more information about the data used in this homeworked example, please refer to the description and codebook located at the end of the \href{https://lhbikos.github.io/ReCenterPsychStats/ReCintro.html\#introduction-to-the-data-set-used-for-homeworked-examples}{introduction} in first volume of ReCentering Psych Stats.

As a brief review, this data is part of an IRB-approved study, with consent to use in teaching demonstrations and to be made available to the general public via the open science framework. Hence, it is appropriate to use in this context. You will notice there are student- and teacher- IDs. These numbers are not actual student and teacher IDs, rather they were further re-identified so that they could not be connected to actual people.

Because this is an actual dataset, if you wish to work the problem along with me, you will need to download the \href{https://github.com/lhbikos/ReC_Psychometrics/blob/main/Worked_Examples/ReC.rds}{ReC.rds} data file from the Worked\_Examples folder in the ReC\_Psychometrics project on the GitHub.

The course evaluation items can be divided into three subscales:

\begin{itemize}
\tightlist
\item
  \textbf{Valued by the student} includes the items: ValObjectives, IncrUnderstanding, IncrInterest
\item
  \textbf{Traditional pedagogy} includes the items: ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation
\item
  \textbf{Socially responsive pedagogy} includes the items: InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration
\end{itemize}

In this homework focused on reliability we will report alpha coefficients for total scale score and subscale scores. We'll also calculate omega total and omega hierarchical and determine what proportion of variance is due to all the factors, error, and \emph{g}. Finally, we'll calculate total and subscale scores.

\hypertarget{check-and-if-needed-format-the-data}{%
\subsection{Check and, if needed, format the data}\label{check-and-if-needed-format-the-data}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{big }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"ReC.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's check the structure\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(big)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Classes 'data.table' and 'data.frame':  310 obs. of  33 variables:
 $ deID                   : int  1 2 3 4 5 6 7 8 9 10 ...
 $ CourseID               : int  57085635 57085635 57085635 57085635 57085635 57085635 57085635 57085635 57085635 57085635 ...
 $ Dept                   : chr  "CPY" "CPY" "CPY" "CPY" ...
 $ Course                 : Factor w/ 3 levels "Psychometrics",..: 2 2 2 2 2 2 2 2 2 2 ...
 $ StatsPkg               : Factor w/ 2 levels "SPSS","R": 2 2 2 2 2 2 2 2 2 2 ...
 $ Centering              : Factor w/ 2 levels "Pre","Re": 2 2 2 2 2 2 2 2 2 2 ...
 $ Year                   : int  2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 ...
 $ Quarter                : chr  "Fall" "Fall" "Fall" "Fall" ...
 $ IncrInterest           : int  5 3 4 2 4 3 5 3 2 5 ...
 $ IncrUnderstanding      : int  2 3 4 3 4 4 5 2 4 5 ...
 $ ValObjectives          : int  5 5 4 4 5 5 5 5 4 5 ...
 $ ApprAssignments        : int  5 4 4 4 5 3 5 3 3 5 ...
 $ EffectiveAnswers       : int  5 3 5 3 5 3 4 3 2 3 ...
 $ Respectful             : int  5 5 4 5 5 4 5 4 5 5 ...
 $ ClearResponsibilities  : int  5 5 4 4 5 4 5 4 4 5 ...
 $ Feedback               : int  5 3 4 2 5 NA 5 4 4 5 ...
 $ OvInstructor           : int  5 4 4 3 5 3 5 4 3 5 ...
 $ MultPerspectives       : int  5 5 4 5 5 4 5 5 5 5 ...
 $ OvCourse               : int  3 4 4 3 5 3 5 3 2 5 ...
 $ InclusvClassrm         : int  5 5 5 5 5 4 5 5 4 5 ...
 $ DEIintegration         : int  5 5 5 5 5 4 5 5 5 5 ...
 $ ClearPresentation      : int  4 4 4 2 5 3 4 4 4 5 ...
 $ ApprWorkload           : int  5 5 3 4 4 2 5 4 4 5 ...
 $ MyContribution         : int  4 4 4 4 5 4 4 3 4 5 ...
 $ InspiredInterest       : int  5 3 4 3 5 3 5 4 4 5 ...
 $ Faith                  : int  5 NA 4 2 NA NA 4 4 4 NA ...
 $ EquitableEval          : int  5 5 3 5 5 3 5 5 3 5 ...
 $ SPFC.Decolonize.Opt.Out: chr  "" "" "" "" ...
 $ ProgramYear            : Factor w/ 3 levels "Second","Transition",..: 3 3 3 3 3 3 3 3 3 3 ...
 $ ClearOrganization      : int  3 4 3 4 4 4 5 4 4 5 ...
 $ RegPrepare             : int  5 4 4 4 4 3 4 4 4 5 ...
 $ EffectiveLearning      : int  2 4 3 4 4 2 5 3 2 5 ...
 $ AccessibleInstructor   : int  5 4 4 4 5 4 5 4 5 5 ...
 - attr(*, ".internal.selfref")=<externalptr> 
\end{verbatim}

Let's create a df with the items only.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{items }\OtherTok{\textless{}{-}}\NormalTok{ big }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(ValObjectives, IncrUnderstanding, IncrInterest, ClearResponsibilities,}
\NormalTok{        EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation,}
\NormalTok{        MultPerspectives, InclusvClassrm, DEIintegration, EquitableEval)}
\end{Highlighting}
\end{Shaded}

\hypertarget{calculate-and-report-the-alpha-coefficient-for-a-total-scale-score-and-subscales-if-the-scale-has-them}{%
\subsection{Calculate and report the alpha coefficient for a total scale score and subscales (if the scale has them)}\label{calculate-and-report-the-alpha-coefficient-for-a-total-scale-score-and-subscales-if-the-scale-has-them}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(items)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = items)

  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r
      0.92      0.92    0.93      0.49  11 0.0065  4.3 0.61     0.48

    95% confidence boundaries 
         lower alpha upper
Feldt     0.90  0.92  0.93
Duhachek  0.91  0.92  0.93

 Reliability if an item is dropped:
                      raw_alpha std.alpha G6(smc) average_r  S/N alpha se var.r
ValObjectives              0.92      0.92    0.93      0.51 11.3   0.0067 0.016
IncrUnderstanding          0.91      0.91    0.92      0.49 10.6   0.0070 0.016
IncrInterest               0.91      0.91    0.92      0.49 10.4   0.0070 0.018
ClearResponsibilities      0.91      0.91    0.92      0.48 10.0   0.0073 0.015
EffectiveAnswers           0.91      0.91    0.92      0.48 10.0   0.0074 0.016
Feedback                   0.91      0.91    0.92      0.48 10.3   0.0071 0.018
ClearOrganization          0.91      0.91    0.92      0.48 10.2   0.0073 0.016
ClearPresentation          0.91      0.91    0.92      0.47  9.7   0.0076 0.015
MultPerspectives           0.91      0.91    0.92      0.48 10.0   0.0073 0.017
InclusvClassrm             0.91      0.91    0.92      0.49 10.6   0.0069 0.018
DEIintegration             0.92      0.92    0.93      0.52 11.8   0.0063 0.011
EquitableEval              0.91      0.91    0.93      0.49 10.5   0.0070 0.018
                      med.r
ValObjectives          0.53
IncrUnderstanding      0.50
IncrInterest           0.48
ClearResponsibilities  0.48
EffectiveAnswers       0.48
Feedback               0.48
ClearOrganization      0.48
ClearPresentation      0.47
MultPerspectives       0.47
InclusvClassrm         0.52
DEIintegration         0.53
EquitableEval          0.48

 Item statistics 
                        n raw.r std.r r.cor r.drop mean   sd
ValObjectives         309  0.59  0.61  0.55   0.53  4.5 0.61
IncrUnderstanding     309  0.71  0.70  0.67   0.64  4.3 0.82
IncrInterest          308  0.75  0.73  0.71   0.68  3.9 0.99
ClearResponsibilities 307  0.80  0.80  0.79   0.75  4.4 0.82
EffectiveAnswers      308  0.80  0.79  0.78   0.75  4.4 0.83
Feedback              304  0.75  0.75  0.72   0.69  4.2 0.88
ClearOrganization     309  0.79  0.77  0.75   0.72  4.0 1.08
ClearPresentation     309  0.85  0.84  0.83   0.80  4.2 0.92
MultPerspectives      305  0.79  0.80  0.78   0.75  4.4 0.84
InclusvClassrm        301  0.68  0.70  0.67   0.62  4.6 0.68
DEIintegration        273  0.51  0.53  0.49   0.42  4.5 0.74
EquitableEval         308  0.70  0.72  0.69   0.66  4.6 0.63

Non missing response frequency for each item
                         1    2    3    4    5 miss
ValObjectives         0.00 0.01 0.03 0.39 0.57 0.00
IncrUnderstanding     0.01 0.04 0.07 0.44 0.45 0.00
IncrInterest          0.02 0.09 0.14 0.44 0.31 0.01
ClearResponsibilities 0.01 0.02 0.07 0.31 0.59 0.01
EffectiveAnswers      0.01 0.02 0.08 0.36 0.53 0.01
Feedback              0.01 0.05 0.10 0.39 0.46 0.02
ClearOrganization     0.04 0.07 0.10 0.41 0.38 0.00
ClearPresentation     0.02 0.05 0.07 0.40 0.46 0.00
MultPerspectives      0.02 0.02 0.08 0.33 0.56 0.02
InclusvClassrm        0.01 0.01 0.05 0.23 0.70 0.03
DEIintegration        0.00 0.01 0.10 0.22 0.67 0.12
EquitableEval         0.00 0.01 0.03 0.32 0.63 0.01
\end{verbatim}

Total scale score alpha is 0.92

\hypertarget{subscale-alphas}{%
\subsection{Subscale alphas}\label{subscale-alphas}}

In the lecture, I created baby dfs of the subscales and ran the alpha on those; another option is to use concatenated lists of variables (i.e., variable vectors). Later, we can also use these to score the subscales.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ValuedVars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"ValObjectives"}\NormalTok{, }\StringTok{"IncrUnderstanding"}\NormalTok{, }\StringTok{"IncrInterest"}\NormalTok{)}
\NormalTok{TradPedVars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"ClearResponsibilities"}\NormalTok{, }\StringTok{"EffectiveAnswers"}\NormalTok{, }\StringTok{"Feedback"}\NormalTok{,}
    \StringTok{"ClearOrganization"}\NormalTok{, }\StringTok{"ClearPresentation"}\NormalTok{)}
\NormalTok{SRPedVars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"InclusvClassrm"}\NormalTok{, }\StringTok{"EquitableEval"}\NormalTok{, }\StringTok{"MultPerspectives"}\NormalTok{, }\StringTok{"DEIintegration"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(items[, ValuedVars])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = items[, ValuedVars])

  raw_alpha std.alpha G6(smc) average_r S/N  ase mean   sd median_r
      0.77      0.77    0.71      0.53 3.4 0.02  4.2 0.68     0.48

    95% confidence boundaries 
         lower alpha upper
Feldt     0.72  0.77  0.81
Duhachek  0.73  0.77  0.81

 Reliability if an item is dropped:
                  raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r
ValObjectives          0.80      0.81    0.68      0.68 4.3    0.022    NA
IncrUnderstanding      0.60      0.65    0.48      0.48 1.8    0.040    NA
IncrInterest           0.59      0.61    0.44      0.44 1.6    0.044    NA
                  med.r
ValObjectives      0.68
IncrUnderstanding  0.48
IncrInterest       0.44

 Item statistics 
                    n raw.r std.r r.cor r.drop mean   sd
ValObjectives     309  0.71  0.77  0.55   0.50  4.5 0.61
IncrUnderstanding 309  0.86  0.85  0.76   0.68  4.3 0.82
IncrInterest      308  0.90  0.87  0.79   0.70  3.9 0.99

Non missing response frequency for each item
                     1    2    3    4    5 miss
ValObjectives     0.00 0.01 0.03 0.39 0.57 0.00
IncrUnderstanding 0.01 0.04 0.07 0.44 0.45 0.00
IncrInterest      0.02 0.09 0.14 0.44 0.31 0.01
\end{verbatim}

Alpha for the Valued-by-Me dimension is .77

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(items[, TradPedVars])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = items[, TradPedVars])

  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r
      0.89       0.9    0.88      0.64 8.8 0.0094  4.3 0.76     0.65

    95% confidence boundaries 
         lower alpha upper
Feldt     0.87  0.89  0.91
Duhachek  0.88  0.89  0.91

 Reliability if an item is dropped:
                      raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r
ClearResponsibilities      0.86      0.86    0.84      0.62 6.4    0.013 0.0054
EffectiveAnswers           0.87      0.87    0.84      0.63 6.8    0.012 0.0045
Feedback                   0.89      0.89    0.87      0.68 8.4    0.010 0.0016
ClearOrganization          0.88      0.88    0.85      0.64 7.2    0.012 0.0044
ClearPresentation          0.86      0.87    0.83      0.62 6.5    0.013 0.0030
                      med.r
ClearResponsibilities  0.59
EffectiveAnswers       0.65
Feedback               0.69
ClearOrganization      0.66
ClearPresentation      0.62

 Item statistics 
                        n raw.r std.r r.cor r.drop mean   sd
ClearResponsibilities 307  0.87  0.87  0.84   0.79  4.4 0.82
EffectiveAnswers      308  0.84  0.85  0.81   0.76  4.4 0.83
Feedback              304  0.78  0.79  0.70   0.66  4.2 0.88
ClearOrganization     309  0.85  0.83  0.78   0.74  4.0 1.08
ClearPresentation     309  0.87  0.87  0.83   0.78  4.2 0.92

Non missing response frequency for each item
                         1    2    3    4    5 miss
ClearResponsibilities 0.01 0.02 0.07 0.31 0.59 0.01
EffectiveAnswers      0.01 0.02 0.08 0.36 0.53 0.01
Feedback              0.01 0.05 0.10 0.39 0.46 0.02
ClearOrganization     0.04 0.07 0.10 0.41 0.38 0.00
ClearPresentation     0.02 0.05 0.07 0.40 0.46 0.00
\end{verbatim}

Alpha for Traditional Pedagogy dimension is .90

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(items[, SRPedVars])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = items[, SRPedVars])

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
      0.81      0.81    0.78      0.52 4.3 0.017  4.5 0.58     0.54

    95% confidence boundaries 
         lower alpha upper
Feldt     0.77  0.81  0.84
Duhachek  0.77  0.81  0.84

 Reliability if an item is dropped:
                 raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r
InclusvClassrm        0.74      0.74    0.67      0.49 2.9    0.025 0.0120
EquitableEval         0.78      0.79    0.73      0.56 3.9    0.021 0.0034
MultPerspectives      0.73      0.74    0.67      0.49 2.8    0.026 0.0153
DEIintegration        0.78      0.78    0.71      0.54 3.6    0.021 0.0044
                 med.r
InclusvClassrm    0.50
EquitableEval     0.57
MultPerspectives  0.47
DEIintegration    0.57

 Item statistics 
                   n raw.r std.r r.cor r.drop mean   sd
InclusvClassrm   301  0.82  0.83  0.76   0.67  4.6 0.68
EquitableEval    308  0.75  0.76  0.64   0.58  4.6 0.63
MultPerspectives 305  0.85  0.83  0.76   0.68  4.4 0.84
DEIintegration   273  0.78  0.78  0.67   0.59  4.5 0.74

Non missing response frequency for each item
                    1    2    3    4    5 miss
InclusvClassrm   0.01 0.01 0.05 0.23 0.70 0.03
EquitableEval    0.00 0.01 0.03 0.32 0.63 0.01
MultPerspectives 0.02 0.02 0.08 0.33 0.56 0.02
DEIintegration   0.00 0.01 0.10 0.22 0.67 0.12
\end{verbatim}

Alpha for the SCR Pedagogy dimension is .81

\hypertarget{calculate-and-report-ux3c9t-and-ux3c9h}{%
\subsection{Calculate and report ωt and ωh}\label{calculate-and-report-ux3c9t-and-ux3c9h}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{omegaSem}\NormalTok{(items, }\AttributeTok{nfactors =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats, : lavaan WARNING:
    Could not compute standard errors! The information matrix could
    not be inverted. This may be a symptom that the model is not
    identified.
\end{verbatim}

\includegraphics{05-Reliability_files/figure-latex/unnamed-chunk-69-1.pdf} \includegraphics{05-Reliability_files/figure-latex/unnamed-chunk-69-2.pdf}

\begin{verbatim}
 
Call: psych::omegaSem(m = items, nfactors = 3)
Omega 
Call: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, 
    digits = digits, title = title, sl = sl, labels = labels, 
    plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, 
    covar = covar)
Alpha:                 0.92 
G.6:                   0.93 
Omega Hierarchical:    0.83 
Omega H asymptotic:    0.88 
Omega Total            0.94 

Schmid Leiman Factor loadings greater than  0.2 
                         g   F1*   F2*   F3*   h2   u2   p2
ValObjectives         0.47        0.32       0.33 0.67 0.67
IncrUnderstanding     0.57        0.60       0.69 0.31 0.47
IncrInterest          0.58        0.57       0.67 0.33 0.49
ClearResponsibilities 0.87                   0.78 0.22 0.98
EffectiveAnswers      0.79                   0.65 0.35 0.97
Feedback              0.73                   0.56 0.44 0.96
ClearOrganization     0.75        0.23       0.62 0.38 0.90
ClearPresentation     0.81        0.30       0.74 0.26 0.88
MultPerspectives      0.75              0.30 0.65 0.35 0.86
InclusvClassrm        0.56              0.48 0.57 0.43 0.55
DEIintegration        0.37              0.82 0.80 0.20 0.17
EquitableEval         0.70                   0.52 0.48 0.94

With Sums of squares  of:
   g  F1*  F2*  F3* 
5.51 0.04 0.98 1.06 

general/max  5.19   max/min =   26.42
mean percent general =  0.74    with sd =  0.26 and cv of  0.36 
Explained Common Variance of the general factor =  0.73 

The degrees of freedom are 33  and the fit is  0.25 
The number of observations was  310  with Chi Square =  76.65  with prob <  0.000025
The root mean square of the residuals is  0.02 
The df corrected root mean square of the residuals is  0.03
RMSEA index =  0.065  and the 10 % confidence intervals are  0.046 0.085
BIC =  -112.66

Compare this with the adequacy of just a general factor and no group factors
The degrees of freedom for just the general factor are 54  and the fit is  1.37 
The number of observations was  310  with Chi Square =  415  with prob <  0.0000000000000000000000000000000000000000000000000000000038
The root mean square of the residuals is  0.1 
The df corrected root mean square of the residuals is  0.11 

RMSEA index =  0.147  and the 10 % confidence intervals are  0.134 0.16
BIC =  105.22 

Measures of factor score adequacy             
                                                 g   F1*  F2*  F3*
Correlation of scores with factors            0.95  0.13 0.81 0.89
Multiple R square of scores with factors      0.91  0.02 0.66 0.79
Minimum correlation of factor score estimates 0.82 -0.96 0.32 0.58

 Total, General and Subset omega for each subset
                                                 g  F1*  F2*  F3*
Omega total for total scores and subscales    0.94 0.77 0.90 0.87
Omega general for total scores and subscales  0.83 0.76 0.69 0.64
Omega group for total scores and subscales    0.11 0.01 0.20 0.23

 The following analyses were done using the  lavaan  package 

 Omega Hierarchical from a confirmatory model using sem =  0.82
 Omega Total  from a confirmatory model using sem =  0.94 
With loadings of 
                         g  F1*  F2*  F3*   h2   u2   p2
ValObjectives         0.47      0.32      0.33 0.67 0.67
IncrUnderstanding     0.56      0.62      0.70 0.30 0.45
IncrInterest          0.57      0.56      0.64 0.36 0.51
ClearResponsibilities 0.86 0.32           0.84 0.16 0.88
EffectiveAnswers      0.80                0.65 0.35 0.98
Feedback              0.73                0.55 0.45 0.97
ClearOrganization     0.75      0.22      0.61 0.39 0.92
ClearPresentation     0.82      0.27      0.74 0.26 0.91
MultPerspectives      0.74           0.30 0.64 0.36 0.86
InclusvClassrm        0.56           0.49 0.56 0.44 0.56
DEIintegration        0.33           0.87 0.87 0.13 0.13
EquitableEval         0.69                0.51 0.49 0.93

With sum of squared loadings of:
   g  F1*  F2*  F3* 
5.46 0.10 0.94 1.14 

The degrees of freedom of the confirmatory model are  42  and the fit is  110.6184  with p =  0.00000004343729
general/max  4.8   max/min =   10.87
mean percent general =  0.73    with sd =  0.27 and cv of  0.37 
Explained Common Variance of the general factor =  0.71 

Measures of factor score adequacy             
                                                 g   F1*  F2*  F3*
Correlation of scores with factors            0.95  0.57 0.82 0.95
Multiple R square of scores with factors      0.91  0.32 0.67 0.90
Minimum correlation of factor score estimates 0.82 -0.36 0.35 0.80

 Total, General and Subset omega for each subset
                                                 g  F1*  F2*  F3*
Omega total for total scores and subscales    0.94 0.84 0.89 0.87
Omega general for total scores and subscales  0.82 0.74 0.70 0.62
Omega group for total scores and subscales    0.11 0.10 0.20 0.26

To get the standard sem fit statistics, ask for summary on the fitted object
\end{verbatim}

I'm reporting the values below the statement, ``The following analyses were done using the lavaan package'':

Omega total = .94 (omega total values \textgreater{} .80 are an indicator of good reliability). Interpretation: 94\% of the variance in the total scale is due to the factors and the balance (6\%) is due to error.

Omega hierarchical estimates the proportion of variance in the overall course evaluation score attributable to the general factors (thus treating the subscales as error). Omega h for the overall course evaluation score was .82

\hypertarget{with-these-two-determine-what-proportion-of-the-variance-is-due-to-all-the-factors-error-and-g.}{%
\subsection{With these two determine what proportion of the variance is due to all the factors, error, and g.}\label{with-these-two-determine-what-proportion-of-the-variance-is-due-to-all-the-factors-error-and-g.}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{.}\DecValTok{82}\SpecialCharTok{/}\NormalTok{.}\DecValTok{94}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.8723404
\end{verbatim}

A quick calculation with omega h (.82) and omega total (.94) lets us know that 87\% of the reliable variance in the overall course evaluation score is attributable to the general factor.

\hypertarget{calculate-total-and-subscale-scores.}{%
\subsection{Calculate total and subscale scores.}\label{calculate-total-and-subscale-scores.}}

This code uses the variable vectors I created above.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{items}\SpecialCharTok{$}\NormalTok{Valued }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(items[, ValuedVars], }\FloatTok{0.75}\NormalTok{)}
\NormalTok{items}\SpecialCharTok{$}\NormalTok{TradPed }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(items[, TradPedVars], }\FloatTok{0.75}\NormalTok{)}
\NormalTok{items}\SpecialCharTok{$}\NormalTok{SCRPed }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(items[, SRPedVars], }\FloatTok{0.75}\NormalTok{)}
\NormalTok{items}\SpecialCharTok{$}\NormalTok{Total }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(items, }\FloatTok{0.75}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scores }\OtherTok{\textless{}{-}}\NormalTok{ items }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Valued, TradPed, SCRPed, Total)}

\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(scores)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        vars   n mean   sd median trimmed  mad  min max range  skew kurtosis
Valued     1 309 4.25 0.68   4.33    4.32 0.50 1.67   5  3.33 -0.90     0.57
TradPed    2 307 4.25 0.76   4.40    4.37 0.59 1.00   5  4.00 -1.42     2.48
SCRPed     3 299 4.52 0.58   4.75    4.61 0.37 2.25   5  2.75 -1.25     1.33
Total      4 308 4.34 0.60   4.41    4.41 0.62 1.83   5  3.17 -1.07     1.12
          se
Valued  0.04
TradPed 0.04
SCRPed  0.03
Total   0.03
\end{verbatim}

\hypertarget{describe-other-reliability-estimates-that-would-be-appropriate-for-the-measure-you-are-evaluating.}{%
\subsection{Describe other reliability estimates that would be appropriate for the measure you are evaluating.}\label{describe-other-reliability-estimates-that-would-be-appropriate-for-the-measure-you-are-evaluating.}}

These scales are for the purposes of course evaluations. In their development, it might be helpful to give it at the end of a single course and then again a few weeks later to determine test-retest reliability.

\hypertarget{ItemAnalExam}{%
\chapter{Item Analysis for Educational Achievement Tests (Exams)}\label{ItemAnalExam}}

\href{https://youtube.com/playlist?list=PLtz5cFLQl4KMNtfAyQGc7Xv27O-bhoArX\&si=h7prXUG9TZunUrfI}{Screencasted Lecture Link}

In this lecture I walk through some procedures for analyzing the quality of multiple choice (including true/false) exam items. We look at item difficulty and item discrimination. We also look at item coverage as it relates to the learning objectives for an educational endeavor.

\hypertarget{navigating-this-lesson-4}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-4}}

There is about one hour of lecture. If you work through the materials with me it would be plan for an additional 30 minutes.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-4}{%
\subsection{Learning Objectives}\label{learning-objectives-4}}

Focusing on this week's materials, make sure you can:

\begin{itemize}
\tightlist
\item
  Provide a rationale for why having a \emph{test bank} might be a good idea.
\item
  Describe the effects of skewness on the interpretation of exam results.
\item
  Evaluate the quality of a multiple-choice item on the basis of item difficulty, correlation, and discrimination.
\item
  Discuss the challenges of identifying an \emph{ideal} difficulty level for test items. Further elaborate how guessing, speeded tests, interitem correlations, and the purposes of the test influence the \emph{ideal difficulty.}
\end{itemize}

\hypertarget{planning-for-practice-4}{%
\subsection{Planning for Practice}\label{planning-for-practice-4}}

Practice suggestions for this lesson encourage you to think about the exams in your life: those you might be taking; those you might be writing or proctoring.

\hypertarget{readings-resources-4}{%
\subsection{Readings \& Resources}\label{readings-resources-4}}

Classic psychometric texts tend to not cover item analysis for achievement tests and/or they skip over these fundamentals and move straight to item response theory/Rasch modeling (IRT). After scouring the internet, I landed on these two resources as concise, accessible, summaries.

\begin{itemize}
\tightlist
\item
  Understanding item analysis. Office of Educational Assessment, University of Washington. Retrieved September 20, 2019. Retrieved from \url{https://www.washington.edu/assessment/scanning-scoring/scoring/reports/item-analysis/}

  \begin{itemize}
  \tightlist
  \item
    It is common for excellent instructions/descriptions to accompany the scoring software used by institutions. UW appears to use ScorePak, and this resource provides both conceptual and interpretive information.
  \end{itemize}
\item
  Revelle, W. (2017). An overview of the psych package. Retrieved from \url{http://personality-project.org/r/overview.pdf}

  \begin{itemize}
  \tightlist
  \item
    Pages 85-85 provide a vignette for conducting item analysis on multiple choice items.
  \end{itemize}
\end{itemize}

\hypertarget{packages-4}{%
\subsection{Packages}\label{packages-4}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# will install the package if not already installed}
\CommentTok{\# if(!require(psych))\{install.packages(\textquotesingle{}psych\textquotesingle{})\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{research-vignette-3}{%
\section{Research Vignette}\label{research-vignette-3}}

This lesson's research vignette is from my own class. Especially in the early years of my teaching, I gave high(er) stakes mid-term and final exams. There were usually 40 (or so) multiple choice or true/false items, 2-3 applied problems or short essays, and 1 longer essay. Today's vignette is an array of exam items from a statistics exam that demonstrate the desirable and undesirable elements we want in objective items.

\hypertarget{item-analysis-in-the-educationalachievement-context}{%
\section{Item Analysis in the Educational/Achievement Context}\label{item-analysis-in-the-educationalachievement-context}}

Multiple choice, true/false, and other \emph{objectively} formatted/scored items are part-n-parcel to educational/achievement assessment. But how do we know if the items are performing the way they should? This lecture focuses on item analysis in the context of multiple choice and true/false items. Using these practices can help you identify what selection of items you'd like for your exams. These can be critical tools in helping you improve your ability to assess student performance. In-so-doing, we walk through a bit of ``what we used to do,'' to current common practices, to a glimpse of our future. We owe much of this to rapid advances in technology.

\emph{Test banks} are instructor-created resources for developing/storing/protecting items for use in future exams. We create test banks when we carefully distribute/collect/protect items ``that work'' (from statistical perspective). Why would we want to do this?

\begin{itemize}
\tightlist
\item
  Once a test is ``out'' it's out. Instructors can presume that resourceful students are using it to study; yet all students won't have equal access to it.
\item
  Developing ``good'' items takes a good deal of time; does the instructor want to redo this each term?
\item
  Should we be piloting \emph{all new items} on students each term and then having the debates about whether the item should be rescored?

  \begin{itemize}
  \tightlist
  \item
    Better is to introduce a proportion of new items each year and evaluate them for inclusion in the test bank; EPPP, SAT, GRE do this.
  \end{itemize}
\item
  A challenge is providing students appropriate study tools -- old exams are favorites of students (but maybe there are other ways -- worksheets, Jeopardy).
\end{itemize}

The conceptual portions of this lecture, particularly the interpretation of the difficulty and discrimination statistics are based in Anastasi's work \citep{anastasi_psychological_1997}

\hypertarget{and-now-a-quiz-please-take-it.}{%
\subsection{And now a quiz! Please take it.}\label{and-now-a-quiz-please-take-it.}}

Let's start with some items from an early version of the exam I gave when I taught CPY7020/Statistical Methods.

\textbf{Item 5} A grouping variable such as men or women that uses dummy coding of 1 and 0 to categorize the groups is an example of \_\_\_\_\_ scaling.

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    Nominal
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Ordinal
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    Interval
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    Ratio
  \end{enumerate}
\end{itemize}

\textbf{Item 11} The term ``grade inflation'' has frequently been applied to describe the distribution of grades in graduate school. Which of the following best describes this distribution.

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    negatively skewed
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    uniform/rectangular
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    positively skewed and leptokurtic
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    uniform and platykurtic
  \end{enumerate}
\end{itemize}

\textbf{Item 19} All distributions of Z-scores will have the identical

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    Mean
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Variance
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    Standard deviation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    All of the above
  \end{enumerate}
\end{itemize}

\textbf{Item 21} The most appropriate score for comparing scores across two or more distributions (e.g., exam scores in math and art classes) is the:

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    mean
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    percentile rank
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    raw score
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    z-score
  \end{enumerate}
\end{itemize}

\textbf{Item 37 } Of the following, what statement best describes \(r^2\) = .49

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    strong positive correlation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    strong positive or negative correlation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    weak positive or negative correlation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    weak negative correlation
  \end{enumerate}
\end{itemize}

\textbf{Item 38} When there are no ties among ranks, what is the relationship between the Spearman rho (\(\rho\)) and the Pearson (\(r\))?

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    \(\rho\) = \(r\)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\roman{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    \(\rho\) \textgreater{} \(r\)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    \(\rho\) \textless{} \(r\)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    no relationship
  \end{enumerate}
\end{itemize}

\hypertarget{item-difficulty}{%
\section{Item Difficulty}\label{item-difficulty}}

\hypertarget{percent-passing}{%
\subsection{Percent passing}\label{percent-passing}}

\textbf{Item difficulty index} is the proportion of test takers who answer an item correctly. It is calculated by dividing the number of people who passed the item (e.g., 55) by the total number of people (e.g., 100).

\begin{itemize}
\tightlist
\item
  If 55\% pass an item, we write \(p\) = .55
\item
  The easier the item, the larger the percentage will be.
\end{itemize}

What is an ideal pass rate (and this ``ideal'' is the \emph{statistical ideal} mostly for norm-referenced tests like the ACT, SAT, GRE)?

\begin{itemize}
\tightlist
\item
  The closer the difficulty of an item approaches 1.00 or 0, the less differential information about test takers it contributes.

  \begin{itemize}
  \tightlist
  \item
    If, out of 100 people, 50 pass an item and 50 fail (\(p\) = .50)\ldots we have 50 X 50 or 2,500 paired comparisons or differential bits of information.
  \end{itemize}
\item
  How much information would we have for an item passed by:

  \begin{itemize}
  \tightlist
  \item
    70\% of the people (70 * 30 = ???)
  \item
    90\% of the people (90 * 10 = ???)
  \end{itemize}
\item
  For maximum differentiation, one would choose all items at the .50 level (but hold up\ldots)
\end{itemize}

\hypertarget{several-factors-prevent-.50-from-being-the-ideal-difficulty-level}{%
\subsection{Several factors prevent .50 from being the ideal difficulty level}\label{several-factors-prevent-.50-from-being-the-ideal-difficulty-level}}

\textbf{Speeded tests} complicate the interpretation of item difficulty because items are usually of equivalent difficulty and there are so many that no one could complete them all. Thus, later items should be considered to be more difficult -- but item difficulty is probably not the best assessment of item/scale quality.

\textbf{Guessing} the correct answer in true/false and multiple-choice contexts interfere with the goal of \(p\) - .50. In a 1952 issue of \emph{Psychometrika}, Lord provided this guide for optimal \(p\) values based on the number of choices in the objective context:

\begin{longtable}[]{@{}l@{}}
\toprule\noalign{}
Optimal \emph{p} values \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\end{longtable}

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
Number of Choices & Optimal Mean Difficulty Level \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
2 (T/F) & 0.85 \\
3 & 0.77 \\
4 & 0.74 \\
5 & 0.69 \\
Constructed response essay & 0.5 \\
\end{longtable}

\textbf{The purpose} of the testing changes the ideal difficulty level.

\begin{itemize}
\tightlist
\item
  If the test is \emph{norm-referenced} (ACT, SAT, GRE), .50 is very useful.
\item
  If the test is mastery oriented, \(p\) values may be be as high as 0.90 since student performance is a function of repeated attempts with feedback.
\end{itemize}

\textbf{Item intercorrelations} impacts interpretation of item difficulty.

\begin{itemize}
\tightlist
\item
  The more homogeneous the test, the higher these intercorrelations will be. If all items were perfectly intercorrelated and all were of the .50 difficulty level:

  \begin{itemize}
  \tightlist
  \item
    the same 50 persons out of 100 would pass each item, that is,
  \item
    half of the test takers would obtain perfect scores, the other half zero scores
  \end{itemize}
\item
  It is best to select items with a moderate spread of difficulty but whose AVERAGE difficulty level is .50
\item
  The percentage of persons passing an item expresses the item difficulty in terms of which statistical scale of measurement? Is it nominal, ordinal, interval, or ratio?

  \begin{itemize}
  \tightlist
  \item
    Because of this issue, we can correctly indicate the rank order or relative difficulty of the items
  \item
    However, we cannot infer that the difference in difficulty between Items 1 and 2 is equal to the difference between Items 2 and 3.
  \end{itemize}
\item
  We can make an \emph{equal-interval inference} with the table of normal curve frequencies (i.e., translating the proportion to z-scores). Z-scores would be used as the units if an equal interval inference was required in the analysis. For example,

  \begin{itemize}
  \tightlist
  \item
    \emph{p} = .84 is equal to -1 \emph{SD}
  \item
    \emph{p} = .16 is equal to +1 \emph{SD}
  \end{itemize}
\end{itemize}

\emph{Seem a little upside down? Recall that we are calculating the percent passing and starting the count ``from the top.'' So a relatively easy item where 84\% passed, would have an standard deviation of -1.}

\begin{figure}
\centering
\includegraphics{images/ItemAnalExam/p84p16.jpg}
\caption{Image of graphs where p = .84 and p = .16}
\end{figure}

\hypertarget{item-discrimination}{%
\section{Item Discrimination}\label{item-discrimination}}

The degree to which an item differentiates correctly among test takers in the behavior that the test is designed to measure.

\begin{itemize}
\tightlist
\item
  The \emph{criterion} can be internal or external to the test itself.

  \begin{itemize}
  \tightlist
  \item
    Under some conditions, the two approaches lead to opposite results because (a) items chosen to maximize the validity of the test tend to be the ones rejected on the basis of internal consistency, and (b) rejecting items with low correlations with the total score tends to homogenize the test (we are more likely to keep items with the highest average intercorrelations).
  \end{itemize}
\item
  \emph{Internal} criteria maximize internal consistency or homogeneity of the test.

  \begin{itemize}
  \tightlist
  \item
    Example: achievement test, where criteria is total score itself.
  \end{itemize}
\item
  \emph{External} criteria maximize the validity of an external criterion.

  \begin{itemize}
  \tightlist
  \item
    Example: a different assessment of the same ability being assessed.
  \end{itemize}
\end{itemize}

\hypertarget{index-of-discrimination}{%
\subsection{Index of Discrimination}\label{index-of-discrimination}}

\begin{itemize}
\tightlist
\item
  Compare the proportion of cases that pass an item in contrasting criterion groups

  \begin{itemize}
  \tightlist
  \item
    upper (U) and lower (L) criterion groups are selected from the extremes of the distribution
  \item
    traditionally these groups are created from the 27\% from each of those sides of the distribution
  \end{itemize}
\item
  This \emph{index of discrimination (D)} can be expressed as a difference of raw frequencies (U - L), or (more conventionally) as the difference of percentages of those who scored it correctly in the upper 27\% and lower 27\% groups

  \begin{itemize}
  \tightlist
  \item
    when all members of the U group and none of the members of the L group pass, D = 100
  \item
    when all members of the L group and none of the members of the U group pass, D = 0
  \item
    optimum point at which these two conditions reach balance is with the upper and lower 27\%
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}l@{}}
\toprule\noalign{}
Optimal Discrimination \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\end{longtable}

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
Difficulty & Discrimination \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0.40 and larger & Excellent \\
0.30 - 0.39 & Good \\
0.11 - 0.29 & Fair \\
0.00 -0.10 & Poor \\
Negative values & Mis-keyed or other major flaw \\
\end{longtable}

\hypertarget{application-of-item-difficulty-and-discrimination}{%
\subsection{Application of Item Difficulty and Discrimination}\label{application-of-item-difficulty-and-discrimination}}

Earlier I asked you to ``take the quiz.'' To keep it engaging, I encourage you to look at your own answers and compare them to ``what happened'' from in this actual exam administration. I will demonstrate how to evaluate my exam items with these indices of difficulty and discrimination. I have intentionally selected items with a variety of desirable (and undesirable) characteristics.

\begin{figure}
\hypertarget{id}{%
\centering
\includegraphics[width=4.16667in,height=3.125in]{images/ItemAnalExam/ULchart.jpg}
\caption{Image of scores and responses of 6 items from 12 students.}\label{id}
}
\end{figure}

\textbf{Item 5} A grouping variable such as men or women that uses dummy coding of 1 and 0 to categorize the groups is an example of \_\_\_\_\_ scaling.

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    Nominal
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Ordinal
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    Interval
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    Ratio
  \end{enumerate}
\end{itemize}

If we wanted to hand-calculate the index of discrimination for Item \#5, we find that 3 people (100\%) in the upper group selected the correct answer and 3 people (100\%) in the lower group selected the correct answer: 3 - 3 = 0. If you prefer percentages: 100\% - 100\% = 0\%. This means there is no discrimination in performance of the upper and lower performing groupings.

Older scoring systems (e.g., Scantron) used to provide this information.

\begin{figure}
\hypertarget{id}{%
\centering
\includegraphics[width=10.41667in,height=2.60417in]{images/ItemAnalExam/Item5.jpg}
\caption{Scantron image of item analysis for exam item \#5}\label{id}
}
\end{figure}

Considering what we have learned already, Item \#5 is:

\begin{itemize}
\tightlist
\item
  too easy
\item
  does not discriminate between upper and lower performance
\item
  \emph{Yes, there is more data on here, but we will save it for the next level of review\ldots just a few moments.}
\end{itemize}

\textbf{Item 11} The term ``grade inflation'' has frequently been applied to describe the distribution of grades in graduate school. Which of the following best describes this distribution.

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    negatively skewed
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    uniform/rectangular
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    positively skewed and leptokurtic
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    uniform and platykurtic
  \end{enumerate}
\end{itemize}

For Item \#11, 2 people (\textasciitilde66\%) from the upper group selected the correct answer, 1 person (\textasciitilde33\%) from the lower group selected the correct answer. Thus, the U-L was +1 (+33\%) and the item is working in the proper direction.

\begin{figure}
\centering
\includegraphics{images/ItemAnalExam/Item11.jpg}
\caption{Scantron image of item analysis for exam item \#11}
\end{figure}

Considering what we have learned already, Item \#11 is:

\begin{itemize}
\tightlist
\item
  difficult (50\% overall selected the correct item)
\item
  does discriminate between upper and lower performance, with more individuals in the upper groups selecting the correct answer than in the lower group
\end{itemize}

\textbf{Item 19} All distributions of Z-scores will have the identical

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    Mean
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Variance
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    Standard deviation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    All of the above
  \end{enumerate}
\end{itemize}

Hand calculation: Upper = 3 (100\%), Lower = 3 (100\%). Difference = 0.

\begin{figure}
\centering
\includegraphics{images/ItemAnalExam/Item19.jpg}
\caption{Scantron image of item analysis for exam item \#19}
\end{figure}

Considering what we have learned already, Item \#19 is:

\begin{itemize}
\tightlist
\item
  somewhat easy (92\% overall selected the correct item)
\item
  using the U - L discrimination index, it does not discriminate between upper and lower performance
\end{itemize}

\textbf{Item 21} The most appropriate score for comparing scores across two or more distributions (e.g., exam scores in math and art classes) is the:

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    mean
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    percentile rank
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    raw score
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    z-score
  \end{enumerate}
\end{itemize}

Hand calculation: Upper = 2 (66\%), Lower = 3 (100\%). Difference = -33\%. This item is upside down. This is different than the Scantron snip below because uppers and lowers were likely calculated on exam total that included subjectively scored items (essays; and I no longer have that data).

\begin{figure}
\centering
\includegraphics{images/ItemAnalExam/Item21.jpg}
\caption{Scantron image of item analysis for exam item \#21}
\end{figure}

Considering what we have learned already, Item \#21 is:

\begin{itemize}
\tightlist
\item
  somewhat difficult (58\% overall selected the correct item)
\item
  on the basis of the hand-calculations it does not discriminate between uppers and lowers
\end{itemize}

\textbf{Item 37} Of the following, what statement best describes \(r^2\) = .49

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    strong positive correlation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    strong positive or negative correlation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    weak positive or negative correlation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    weak negative correlation
  \end{enumerate}
\end{itemize}

Hand calculation: Upper = 2 (66\%), Lower = 0 (0\%). Difference = 66\%.

\begin{figure}
\centering
\includegraphics{images/ItemAnalExam/Item37.jpg}
\caption{Scantron image of item analysis for exam item \#37}
\end{figure}

Considering what we have learned already, Item \#37 is:

\begin{itemize}
\tightlist
\item
  very difficult (33\% overall selected the correct item)
\item
  on the basis of the hand-calculations, this completely discriminates the uppers from the lowers)
\end{itemize}

\textbf{Item 38} When there are no ties among ranks, what is the relationship between the Spearman rho (\(\rho\)) and the Pearson r (\(r\))?

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    \(\rho\) = \(r\)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\roman{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    \(\rho\) \textgreater{} \(r\)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    \(\rho\) \textless{} \(r\)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    no relationship
  \end{enumerate}
\end{itemize}

Hand calculation: Upper = 1 (33\%), Lower = 1 (33\%). Difference = 0\%.

\begin{figure}
\centering
\includegraphics{images/ItemAnalExam/Item38.jpg}
\caption{Scantron image of item analysis for exam item \#38}
\end{figure}

Considering what we have learned already, Item \#21 is:

\begin{itemize}
\tightlist
\item
  very difficult (25\% overall selected the correct item)
\item
  on the basis of the hand-calculations, this does not discriminate the uppers from the lowers
\end{itemize}

\hypertarget{in-the-psych-package}{%
\section{\texorpdfstring{In the \emph{psych} Package}{In the psych Package}}\label{in-the-psych-package}}

Using the \emph{score.multiple.choice()} function in the \emph{psych} package. Documentation is pp.~85-86 in \url{http://personality-project.org/r/overview.pdf}

A multiple-choice exam presumes that there is one correct response. We start with a dataset that records the students' responses. It \emph{appears} that the psych package requires these responses to be numerical (rather than A, B, C, D).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# For portability of the lesson, I hand{-}entered the exam score data.}
\CommentTok{\# Variables are items (not students), so the entry is the 41 items}
\CommentTok{\# for the 12 students}
\NormalTok{Item1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{Item2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{Item3 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{Item4 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{Item5 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{Item6 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{Item7 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{Item8 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{Item9 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{Item10 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{Item11 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{Item12 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{Item13 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{Item14 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{Item15 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{Item16 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{Item17 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{Item18 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{Item19 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{Item20 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{Item21 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{Item22 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{Item23 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{Item24 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{Item25 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{Item26 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{Item27 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{Item28 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{Item29 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{Item30 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{Item31 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{Item32 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{Item33 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{Item34 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{Item35 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{Item36 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{Item37 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{Item38 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{Item39 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{Item40 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{Item41 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{)}

\NormalTok{exam }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(Item1, Item2, Item3, Item4, Item5, Item6, Item7, Item8,}
\NormalTok{    Item9, Item10, Item11, Item12, Item13, Item14, Item15, Item16, Item17,}
\NormalTok{    Item18, Item19, Item20, Item21, Item22, Item23, Item24, Item25, Item26,}
\NormalTok{    Item27, Item28, Item29, Item30, Item31, Item32, Item33, Item34, Item35,}
\NormalTok{    Item36, Item37, Item38, Item39, Item40, Item41)}
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either a .csv file (think ``Excel lite'') or .rds object (preserves any formatting you might do).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(exam,}
\CommentTok{\# file=\textquotesingle{}exam.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE) bring}
\CommentTok{\# back the simulated dat from a .csv file exam \textless{}{-} read.csv}
\CommentTok{\# (\textquotesingle{}exam.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(exam, \textquotesingle{}exam.rds\textquotesingle{}) bring back the simulated dat from an}
\CommentTok{\# .rds file exam \textless{}{-} readRDS(\textquotesingle{}exam.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

We create a key of the correct answers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{exam.keys }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{)  }
\end{Highlighting}
\end{Shaded}

We then insert that key into the \emph{psych} package's \emph{score.multiple.choice()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{score.multiple.choice}\NormalTok{(exam.keys, exam, }\AttributeTok{score =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{short =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{skew =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in cor(items, scores, use = "pairwise"): the standard deviation is zero
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call: NULL

(Unstandardized) Alpha:
[1] 0.73

Average item correlation:
[1] 0.06

item statistics 
       key    1    2    3    4 miss     r  n mean   sd  skew kurtosis   se
Item1    1 0.92 0.00 0.00 0.08 0.00  0.65 12 0.92 0.29 -2.65     5.48 0.08
Item2    4 0.08 0.00 0.00 0.92 0.00  0.65 12 0.92 0.29 -2.65     5.48 0.08
Item3    1 0.83 0.00 0.08 0.08 0.00  0.34 12 0.83 0.39 -1.57     0.53 0.11
Item4    2 0.00 0.92 0.08 0.00 0.00 -0.11 12 0.92 0.29 -2.65     5.48 0.08
Item5    1 1.00 0.00 0.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item6    1 1.00 0.00 0.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item7    3 0.00 0.08 0.67 0.25 0.00  0.72 12 0.67 0.49 -0.62    -1.74 0.14
Item8    2 0.17 0.67 0.00 0.17 0.00 -0.13 12 0.67 0.49 -0.62    -1.74 0.14
Item9    1 0.67 0.00 0.00 0.33 0.00  0.81 12 0.67 0.49 -0.62    -1.74 0.14
Item10   3 0.00 0.33 0.67 0.00 0.00  0.18 12 0.67 0.49 -0.62    -1.74 0.14
Item11   1 0.50 0.00 0.33 0.17 0.00  0.42 12 0.50 0.52  0.00    -2.16 0.15
Item12   2 0.08 0.83 0.00 0.08 0.00  0.17 12 0.83 0.39 -1.57     0.53 0.11
Item13   2 0.08 0.75 0.17 0.00 0.00  0.85 12 0.75 0.45 -1.01    -1.04 0.13
Item14   2 0.00 0.92 0.08 0.00 0.00 -0.04 12 0.92 0.29 -2.65     5.48 0.08
Item15   2 0.17 0.58 0.08 0.17 0.00  0.32 12 0.58 0.51 -0.30    -2.06 0.15
Item16   2 0.08 0.67 0.00 0.25 0.00  0.40 12 0.67 0.49 -0.62    -1.74 0.14
Item17   1 1.00 0.00 0.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item18   3 0.00 0.00 1.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item19   4 0.08 0.00 0.00 0.92 0.00  0.04 12 0.92 0.29 -2.65     5.48 0.08
Item20   1 1.00 0.00 0.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item21   4 0.00 0.42 0.00 0.58 0.00 -0.19 12 0.58 0.51 -0.30    -2.06 0.15
Item22   3 0.08 0.00 0.92 0.00 0.00  0.34 12 0.92 0.29 -2.65     5.48 0.08
Item23   3 0.00 0.25 0.75 0.00 0.00  0.71 12 0.75 0.45 -1.01    -1.04 0.13
Item24   3 0.17 0.08 0.75 0.00 0.00  0.61 12 0.75 0.45 -1.01    -1.04 0.13
Item25   1 0.00 0.92 0.08 0.00 0.00    NA 12 0.00 0.00   NaN      NaN 0.00
Item26   4 0.17 0.00 0.00 0.83 0.00  0.34 12 0.83 0.39 -1.57     0.53 0.11
Item27   4 0.08 0.00 0.00 0.92 0.00  0.65 12 0.92 0.29 -2.65     5.48 0.08
Item28   1 1.00 0.00 0.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item29   1 0.92 0.00 0.08 0.00 0.00 -0.11 12 0.92 0.29 -2.65     5.48 0.08
Item30   2 0.00 1.00 0.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item31   1 0.75 0.17 0.08 0.00 0.00  0.41 12 0.75 0.45 -1.01    -1.04 0.13
Item32   1 0.83 0.00 0.17 0.00 0.00  0.45 12 0.83 0.39 -1.57     0.53 0.11
Item33   3 0.00 0.00 1.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item34   3 0.00 0.00 1.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item35   3 0.00 0.00 1.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item36   2 0.00 1.00 0.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item37   2 0.25 0.33 0.42 0.00 0.00  0.49 12 0.33 0.49  0.62    -1.74 0.14
Item38   1 0.27 0.00 0.00 0.73 0.08 -0.07 11 0.27 0.47  0.88    -1.31 0.14
Item39   3 0.00 0.00 1.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item40   3 0.00 0.00 0.92 0.08 0.00  0.65 12 0.92 0.29 -2.65     5.48 0.08
Item41   4 0.08 0.58 0.00 0.33 0.00  0.40 12 0.33 0.49  0.62    -1.74 0.14
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# short=FALSE allows us to produce scores; we will use these later in}
\CommentTok{\# some IRT analyses names(results)}
\end{Highlighting}
\end{Shaded}

The first screen of output provides an alpha. In this context, \emph{alpha} should tell us the consistency of getting answers right or wrong. Technically, the alpha is reduced to a KR-20 (Kuder Richardson 20). We interpret it the same. Alpha is directly affected by:

\begin{itemize}
\tightlist
\item
  \emph{interitem correlations} among the items -- a large number of positive correlations between items increases alpha
\item
  test length -- more items produce higher reliability (all things else equal)
\item
  test content -- the more diverse/broad, the lower the reliability coefficient
\end{itemize}

In the context of the classroom, reliabilities above .70 are probably adequate and above .80 are good. Reliabilities below .60 suggest that items should be investigated, and additional measures (tests, homework assignments) should be included in assigning grades.

Focus instead on the second screen of output.

\textbf{key} indicates which answer was correct.

\textbf{1, 2, 3, 4} (there would be as many as there are options in the multiple-choice exam) provide a \emph{distractor analysis} by indicating the percentage of time that answer was chosen. For item 1, option 1 was correct, and it was chosen 92\% of the time. No individuals chose options 2 or 3. Option 4 was chosen 8\% of the time.

\textbf{miss} indicates how many times the item was skipped.

\emph{r} is a point-biserial correlation with a dichotomous correct/incorrect correlated with the continuously scaled total scale score. Positively scored items let us know that the item is working in the proper direction; the students who got the item correct, did better on the overall total score and vice versa.

\begin{itemize}
\tightlist
\item
  One of the best indicators of an items ability to \emph{discriminate} (hence, \textbf{item discrimination}) among the criterion assessed on the test.
\item
  It is important to investigate those with values close to zero (no relation between item performance with overall test performance) and those with negative values (meaning that those who had the correct answer on the item were those who scored lower on the exam).
\end{itemize}

\emph{n} tells us how many participants completed the item (this would necessarily be the inverse of ``miss'').

\emph{mean} repeats the proportion of individuals who scored correctly; it would be the same as the percentage in the item keyed as the correct one. This is an indication of \textbf{item difficulty}.

\emph{sd} gives an indication of the variability around that mean

It is important to look at the \emph{r} and \emph{mean} columns, together to understand the degree of difficulty and how well each item is discriminating between performance levels.

\emph{skew} can provide an indication of ceiling and floor effects.

\begin{figure}
\hypertarget{id}{%
\centering
\includegraphics[width=6.25in,height=2.08333in]{images/ItemAnalExam/skew.jpg}
\caption{Image of two graphs illustrating positive and negative skew}\label{id}
}
\end{figure}

If a score has a significant negative skew (long tail to the left), then there may be a piling up of items at the upper end of the scale. This would indicate an \emph{insufficient ceiling} and make it more difficult to discriminate among differences among the higher performers.

If a score has a significant positive skew (long tail to the right), then there may be a piling up of items at the low end, indicating an \emph{insufficient floor.} That is, it lacks the ability to discriminate between poorer performers.

How do you tell what is significant?

A general rule of thumb says that anything greater or less than the absolute value of 1.0 is significantly skewed. A formal z-test can be conducted this way: \(z_{skewness}= \frac{S-0}{SE_{skewness}}\)

In our exam dataset, -2.65 is the most extremely negatively skewed item and its \emph{se} = 0.08.

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{{-}}\FloatTok{2.65}\SpecialCharTok{/}\FloatTok{0.08}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -33.125
\end{verbatim}

Considering that anything greater than +/- 1.96 is statistically significant, it is safe to say that this item has an insufficient ceiling.

What about the items with -0.30 (\emph{se} = 0.15)?

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{{-}}\NormalTok{.}\DecValTok{30}\SpecialCharTok{/}\NormalTok{.}\DecValTok{15}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -2
\end{verbatim}

This is not as extreme (and recall my \emph{N} = 12, so I should probably look up a critical \emph{t} value), but there is still some question about whether my exam items can discriminate among high performers.

Please note, because these distributions are \emph{dichotomous} (correct/incorrect) they will never be normally distributed, but, like the difficulty index, they give another glimpse of the ability to discriminate.

Before we look at the specific exam items and their output from the scoring function, let me introduce you to the features of the psych package that draw from \emph{item response theory} (IRT).

\hypertarget{a-mini-introduction-to-irt}{%
\subsection{A Mini-Introduction to IRT}\label{a-mini-introduction-to-irt}}

To recap -- at the instructional level, the combination of percent passing (mean) and point-biserial correlation (discrimination index) is status quo for evaluating/improving the items.

The \emph{psych} package draws from its IRT capacity to conduct distractor analysis. IRT models individual responses to items by estimating individual ability (theta) and item difficulty (diff) parameters.

In these graphs, theta is on the X axis. Theta is the standard unit of the IRT model that represents the level of the domain being measured. Like a z-score, a theta unit of ``1'' is the SD of the calibrated sample.

The pattern of responses to multiple choice ability items can show that some items have poor distractors. This may be done by using the irt.responses function. A good distractor is one that is negatively related to ability.

As we look at each of the exam items, we will look at the psych input from the scoring function as well as use the \emph{results} objects to create the IRT graphs.

\textbf{Item 5} A grouping variable such as men or women that uses dummy coding of 1 and 0 to categorize the groups is an example of \_\_\_\_\_ scaling.

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    Nominal
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Ordinal
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    Interval
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    Ratio
  \end{enumerate}
\end{itemize}

Mean = 1.0 (much too easy), \emph{r} = NA, Distractors: 1.00 0.00 0.00 0.00, skew = -2.65

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# irt.responses(scores$scores, exam[5], breaks = 2)}
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{irt.responses}\NormalTok{(results}\SpecialCharTok{$}\NormalTok{scores, exam[}\DecValTok{5}\NormalTok{], }\AttributeTok{breaks =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple
of vector length (arg 1)
\end{verbatim}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple
of vector length (arg 1)
\end{verbatim}

\includegraphics{06-ItemAnalExam_files/figure-latex/unnamed-chunk-9-1.pdf}

With Item \#5, 100\% responded correctly (the flat, solid line at the top); there is not much to see.

\textbf{Item 11} The term ``grade inflation'' has frequently been applied to describe the distribution of grades in graduate school. Which of the following best describes this distribution.

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    negatively skewed
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    uniform/rectangular
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    positively skewed and leptokurtic
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    uniform and platykurtic
  \end{enumerate}
\end{itemize}

Mean = .50, \emph{r} = .42, Distractors: 0.50 0.00 0.33 0.17, skew = 0.00

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{irt.responses}\NormalTok{(results}\SpecialCharTok{$}\NormalTok{scores, exam[}\DecValTok{11}\NormalTok{], }\AttributeTok{breaks =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple
of vector length (arg 1)
\end{verbatim}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple
of vector length (arg 1)
\end{verbatim}

\includegraphics{06-ItemAnalExam_files/figure-latex/unnamed-chunk-10-1.pdf}

With Item \#11, there is a positive relationship between 1/A (correct answer) and ability (theta), no relationship between 3/C and ability, and a negative relationship between 4/D and ability (indicating that 4/D is a good distractor). These map onto each of the point-biserial correlations associated with the distractors in the Scantron output.

\textbf{Item 19} All distributions of Z-scores will have the identical

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    Mean
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Variance
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    Standard deviation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    All of the above
  \end{enumerate}
\end{itemize}

Mean = .92, \emph{r} = .04, Distractors: 0.08 0.00 0.00 0.92 , skew = -2.65

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{irt.responses}\NormalTok{(results}\SpecialCharTok{$}\NormalTok{scores, exam[}\DecValTok{19}\NormalTok{], }\AttributeTok{breaks =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple
of vector length (arg 1)
\end{verbatim}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple
of vector length (arg 1)
\end{verbatim}

\includegraphics{06-ItemAnalExam_files/figure-latex/unnamed-chunk-11-1.pdf}

Item \#19 shows rather flat (no relationship) relations with ability for the correct item and the lone distractor.

\textbf{Item 21} The most appropriate score for comparing scores across two or more distributions (e.g., exam scores in math and art classes) is the:

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    mean
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    percentile rank
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    raw score
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    z-score
  \end{enumerate}
\end{itemize}

Mean = .58, \emph{r} = -.19, Distractors: 0.00 0.42 0.00 0.58, skew = -0.30

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{irt.responses}\NormalTok{(results}\SpecialCharTok{$}\NormalTok{scores, exam[}\DecValTok{21}\NormalTok{], }\AttributeTok{breaks =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple
of vector length (arg 1)
\end{verbatim}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple
of vector length (arg 1)
\end{verbatim}

\includegraphics{06-ItemAnalExam_files/figure-latex/unnamed-chunk-12-1.pdf}

For Item \#21, a positive relationship between the WRONG answer (2/B) and ability (theta) and a negative relationship between 4/D (incorrect answer) and ability. This makes sense as the point biserial for the overall item was 0-.13.

\textbf{Item 37} Of the following, what statement best describes \(r^2\) = .49

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    strong positive correlation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    strong positive or negative correlation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    weak positive or negative correlation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    weak negative correlation
  \end{enumerate}
\end{itemize}

Mean = .33, \emph{r} = .49, Distractors: 0.25 0.33 0.42 0.00, skew = .62

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{irt.responses}\NormalTok{(results}\SpecialCharTok{$}\NormalTok{scores, exam[}\DecValTok{37}\NormalTok{], }\AttributeTok{breaks =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple
of vector length (arg 1)
\end{verbatim}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple
of vector length (arg 1)
\end{verbatim}

\includegraphics{06-ItemAnalExam_files/figure-latex/unnamed-chunk-13-1.pdf}

For Item \#37, a negative relation between endorsing 1/A and ability (a good distractor). No relationshp with ability for endorsing 3/C. A positive relation with ability for those endorsing 2/B 9correct answer).

\textbf{Item 38} When there are no ties among ranks, what is the relationship between the Spearman rho (\(\rho\)) and the Pearson r (\(r\))?

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    \(\rho\) = \(r\)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\roman{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    \(\rho\) \textgreater{} \(r\)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    \(\rho\) \textless{} \(r\)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    no relationship
  \end{enumerate}
\end{itemize}

Mean = .27, \emph{r} = -.07, Distractors: 0.27 0.00 0.00 0.73, skew = .68\\
\emph{Notice anything else that's funky about Item \#38?}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{irt.responses}\NormalTok{(results}\SpecialCharTok{$}\NormalTok{scores, exam[}\DecValTok{38}\NormalTok{], }\AttributeTok{breaks =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple
of vector length (arg 1)
\end{verbatim}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple
of vector length (arg 1)
\end{verbatim}

\includegraphics{06-ItemAnalExam_files/figure-latex/unnamed-chunk-14-1.pdf}

For Item \#38, there is a positive relationship with ability for endorsing 1/A (correct answer) and a negative relationship with ability for 4/D (incorrect answer).

\textbf{Regarding overall test characteristics} \includegraphics{images/ItemAnalExam/examheader.jpg}

\hypertarget{closing-thoughts-on-developing-measures-in-the-educationachievement-context}{%
\section{Closing Thoughts on Developing Measures in the Education/Achievement Context}\label{closing-thoughts-on-developing-measures-in-the-educationachievement-context}}

Item analysis tends to be an assessment of \emph{reliability}. However, in the context of educational assessment and achievement exams, there are also \emph{validity} issues.

\textbf{Content validity} is concerned with whether or not the scale adequately represents the entirety of the \emph{domain} to be assessed.

In educational and achievement contexts, this is often accomplished with a \emph{table of specifications.} I introduced this in the \protect\hyperlink{rxy}{Validity lesson}. As a refresher, I will include another example -- imagining that I am going to write a quiz or short exam based on the learning objectives of this, single, lesson. There are a number of different ways to organize the types of knowledge that is being assessed. Since the American Psychological Association (and others) work in ``KSAs'' (knowledge, skills, attitudes) in their accreditation standards, I will use those.

In creating a table of specifications, we start with the learning objectives. Then we decide what type of items to write and what type of performance level they satisfy. This helps us ensure that all learning objectives are proportionately covered, using a variety of assessment approaches. Otherwise, we might be tempted to include the items that come easily to us or that are from our favorite topics. Personally, I find that when I work on the exam, and am informed by the learning objectives and table of specifications, I find myself tinkering with all three. I am inclined to believe that this results in an ever-increasingly-improved pedagogy.

\textbf{Table of Specifications}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.4430}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1392}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1392}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1392}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1392}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Learning Objectives
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Knowledge
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Skills
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Attitudes
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\% of test
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Provide a rationale for why having a \emph{test bank} might be a good idea. & & & 1 item & 30\% \\
Describe the effects of skewness on the interpretation of exam results. & 2 items & & & 10\% \\
Evaluate the quality of a multiple-choice item on the basis of item difficulty, correlation, and discrimination. & & 5 items & & 25\% \\
Discuss the challenges of identifying an \emph{ideal} difficulty level for test items. Further elaborate how guessing, speeded tests, interitem correlations, and the purposes of the test influence the \emph{ideal difficulty.} & 2 items & & 1 item & 35\% \\
TOTALS & 4 items & 5 items & 2 items & 100\% \\
\end{longtable}

There are a variety of free resources that help with this process. Below are some that I find helpful:

\begin{itemize}
\tightlist
\item
  \href{https://www.fractuslearning.com/blooms-taxonomy-verbs-free-chart/}{Bloom's Taxonomy Verbs} is freely available from Fractus Learning.
\item
  \href{https://wabisabilearning.com/blogs/literacy-numeracy/download-blooms-digital-taxonomy-verbs-poster}{The Bloom's Taxonomy Verbs Poster for Teachers} is a great starter.
\item
  If you have ``writer's block'' for writing objectives, here is a \href{https://elearn.sitehost.iu.edu/courses/tos/gen2/}{learning outcome generator} that may help get you started.
\item
  From APA's Education Directorate, \href{https://www.apa.org/ed/sponsor/resources/objectives.pdf}{Guidance for Writing Behavioral Learning Objectives}. The APA Guidance really emphasizes key components of well-written behavioral leaning objectives.

  \begin{itemize}
  \tightlist
  \item
    Objectives should be \textbf{observable and measurable}. Write them with action verbs (i.e., recite, compare, define) that Objectives measurable behaviors. The APA CE office disallows the use of ``understand'' as an action verb,
  \item
    Statements should clearly describe what the learner will know or be able to do \textbf{as a result} of having participated.
  \item
    Objectives should focus on the learner and learning (as opposed to what the trainer is doing or leading),
  \item
    Objectives should be appropriate in breadth (not too few or too many).
  \end{itemize}
\end{itemize}

\textbf{Takeaway message}: Together, mapping out exam coverage in a table of specifications PLUS item analysis (difficulty/discrimination) can be powerful tools in educational assessment.

\hypertarget{practice-problems-4}{%
\section{Practice Problems}\label{practice-problems-4}}

For this particular lesson, I think some of the most meaningful practice comes from multiple choice and true/false exams that occur in your life. If you are in a class, see if your instructor is willing to share item analysis information that they have received. Learning management systems like Canvas, automatically calculate these.

If you are an instructor, calculate and review item analysis data on your own items. Think about how you might improve items between exams and consider how the dificulty and discrimination capacity of the item changes.

\hypertarget{ItemAnalSurvey}{%
\chapter{Item Analysis for Likert Type Scale Construction}\label{ItemAnalSurvey}}

\href{https://youtube.com/playlist?list=PLtz5cFLQl4KOjH-HGCpixJAA41XlPJp_s\&si=sGZUYF6d-rH74wwi}{Screencasted Lecture Link}

The focus of this lecture is on item analysis for surveys. We use information about alpha coefficients and item-total correlations (within and across subscales) to help assess what we might consider to be \emph{within-scale convergent and discriminant validity} (although we tend to think of it as an assessment of reliability).

\hypertarget{navigating-this-lesson-5}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-5}}

There is about 45 minutes of lecture. If you work through the materials with me it would be plan for an additional hour.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-5}{%
\subsection{Learning Objectives}\label{learning-objectives-5}}

Focusing on this week's materials, make sure you can:

\begin{itemize}
\tightlist
\item
  Define the corrected item-total correlation and compare it to an item-total correlation.
\item
  List the preliminary steps essential for scale construction, beginning with item development.
\item
  Name the type(s; e.g., reliability, validity) of psychometric evaluation that item analytic procedures assess.
\item
  Identify threats to the interpretation of item-total correlations and alpha coefficients.
\item
  Make decisions about item retention, deletion, and revision that balances statistical output with construct definitions.
\end{itemize}

\hypertarget{planning-for-practice-5}{%
\subsection{Planning for Practice}\label{planning-for-practice-5}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. The least complex is to change the random seed in the research and rework the problem demonstrated in the lesson. The results \emph{should} map onto the ones obtained in the lecture.

The second option involves utilizing one of the simulated datasets available in this OER. The \protect\hyperlink{sims}{last lesson} in the OER contains three simulations that could be used for all of the statistics-based practice suggestions. Especially if you started with one of these examples in an earlier lesson, I highly recommend you continue with that.

Alternatively, Lewis and Neville's \citeyearpar{lewis_construction_2015} Gendered Racial Microaggressions Scale for Black Women will be used in the lessons on exploratory factor analysis; Keum et al.'s Gendered Racial Microaggressions Scale for Asian American Women \citep{keum_gendered_2018} will be used in the lessons on confirmatory factor analysis; and Conover et al.'s \citeyearpar{conover_development_2017} Ableist Microaggressions Scale is used in the lesson on invariance testing. Any of these would be suitable for the PCA and PAF homework assignments.

As a third option, you are welcome to use data to which you have access and is suitable for PCA. In any case, please plan to:

\begin{itemize}
\tightlist
\item
  Produce alpha coefficients, average inter-item correlations, and corrected item-total correlations for the total and subscales, separately.
\item
  Produce correlations between the individual items of one subscale and the subscale scores of all other scales.
\item
  Draft an APA style results section with an accompanying table.
\end{itemize}

In my example there were only two subscales. If you have more, you will need to compare each subscale with all the others. For example, if you had three subscales: A, B, C, you would need to compare A/B, B/C, and A/C.

\hypertarget{readings-resources-5}{%
\subsection{Readings \& Resources}\label{readings-resources-5}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Green \& Salkind (2017). Lesson 38: Item analysis using the reliability Procedure. In S.B. Green and N.J. Salkind's, ``Using SPSS for Windows and Macintosh: Analyzing and understanding data (8th ed). New York: Pearson.

  \begin{itemize}
  \tightlist
  \item
    Even though the operation of the chapter uses SPSS, the narration of the ``what'' and ``why'' of item analysis is clear and concise. Further, I have not found another chapter (not even in psychometrics texts) that addresses this as completely.
  \end{itemize}
\item
  Szymanski, D. M., \& Bissonette, D. (2020). Perceptions of the LGBTQ College Campus Climate Scale: Development and psychometric evaluation. \emph{Journal of Homosexuality}, 67(10), 1412--1428. \url{https://doi.org/10.1080/00918369.2019.1591788}

  \begin{itemize}
  \tightlist
  \item
    The research vignette for this lesson.
  \end{itemize}
\end{itemize}

\hypertarget{packages-5}{%
\subsection{Packages}\label{packages-5}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# will install the package if not already installed}
\CommentTok{\# if(!require(tidyverse))\{install.packages(\textquotesingle{}tidyverse\textquotesingle{})\}}
\CommentTok{\# if(!require(MASS))\{install.packages(\textquotesingle{}MASS\textquotesingle{})\}}
\CommentTok{\# if(!require(psych))\{install.packages(\textquotesingle{}psych\textquotesingle{})\}}
\CommentTok{\# if(!require(apaTables))\{install.packages(\textquotesingle{}apaTables\textquotesingle{})\}}
\CommentTok{\# if(!require(sjstats))\{install.packages(\textquotesingle{}sjstats\textquotesingle{})\}}
\CommentTok{\# if(!require(qualtRics))\{install.packages(\textquotesingle{}qualtRics\textquotesingle{})\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{introducing-item-analysis-for-survey-development}{%
\section{Introducing Item Analysis for Survey Development}\label{introducing-item-analysis-for-survey-development}}

Item analysis can be used to help determine which items to include and exclude from a scale or subscale. The goal is to select a set of items that yields a summary score (total or mean) that is strongly related to the construct identified and defined in the scale.

\begin{itemize}
\tightlist
\item
  Item analysis is somewhat limiting because we usually cannot relate our items to a direct (external) measure of a construct to select our items.
\item
  Instead, we \emph{trust} (term used lightly) that the items we have chosen, together, represent the construct and we make decisions about the relative strength of each item's correlation to the total score.
\item
  This makes it imperative that we look to both statistics and our construct definition (e.g., how well does each item map onto the construct definition)
\end{itemize}

If this is initial scale development, the researchers are wise to write more items than needed so that there is flexibility in selecting items with optimal functioning. Szymanski and Bissonette \citeyearpar{szymanski_perceptions_2020} do this. Their article narrates how they began with 36 items, narrowed it to 24, and -- on the basis of subject matter expertise and peer review -- further narrowed it to 10. The reduction of additional items happened on the basis of exploratory factor analysis.

\hypertarget{workflow-for-item-analysis}{%
\subsection{Workflow for Item Analysis}\label{workflow-for-item-analysis}}

\begin{figure}
\centering
\includegraphics{images/ItemAnalysis/ItemAnalysisFlowchart.png}
\caption{Image of workflow for item analysis for survey development.}
\end{figure}

Step I: Calculate corrected item-total correlations. This involves:

\begin{itemize}
\tightlist
\item
  Reverse-scoring items that are negatively worded.
\item
  Ensuring proper formatting of variables (i.e., numerical and integer formats).
\item
  Evaluating the corrected item-total correlations (``r.drop'' in the \emph{psych::alpha} function)
\item
  Consider deleting items with low item-total correlations.

  \begin{itemize}
  \tightlist
  \item
    Consider the how deleting items might create too narrow of a construct definition. If so, hesitate before deleting.
  \item
    Re-run and re-evaluate the \emph{r.drop} values and alpha coefficients after each change.
  \item
    This is an iterative process and may involve ``adding back'' previously deleted items.
  \end{itemize}
\item
  Calculate correlations of items with other subscale scores.

  \begin{itemize}
  \tightlist
  \item
    Calculate the mean scores for each of the subscales of a measure.
  \item
    Focusing on one subscale at a time, correlate each of the subscale's items with the total score of all the other subscales.
  \end{itemize}
\item
  Compare the corrected item-total correlations to the correlations of items with other subscale scores.

  \begin{itemize}
  \tightlist
  \item
    The corrected item-total correlations should be stronger/higher than the correlations of items with other subscales.
  \end{itemize}
\end{itemize}

\hypertarget{research-vignette-4}{%
\section{Research Vignette}\label{research-vignette-4}}

The research vignette for this lesson is the development and psychometric evaluation of the Perceptions of the LGBTQ College Campus Climate Scale \citep{szymanski_perceptions_2020}. The scale is six items with responses rated on a 7-point Likert scale ranging from 1 (\emph{strongly disagree}) to 7 (\emph{strongly agree}). Higher scores indicate more negative perceptions of the LGBTQ campus climate. Szymanski and Bissonette \citeyearpar{szymanski_perceptions_2020} have suggested that the psychometric evaluation supports using the scale in its entirety or as subscales. Each item is listed below with its variable name in parentheses:

\begin{itemize}
\tightlist
\item
  College response to LGBTQ students:

  \begin{itemize}
  \tightlist
  \item
    My university/college is cold and uncaring toward LGBTQ students. (cold)
  \item
    My university/college is unresponsive to the needs of LGBTQ students. (unresponsive)
  \item
    My university/college provides a supportive environment for LGBTQ students. (unsupportive)

    \begin{itemize}
    \tightlist
    \item
      this item must be reverse-scored
    \end{itemize}
  \end{itemize}
\item
  LGBTQ Stigma:

  \begin{itemize}
  \tightlist
  \item
    Negative attitudes toward LGBTQ persons are openly expressed on my university/college campus. (negative)
  \item
    Heterosexism, homophobia, biphobia, transphobia, and cissexism are visible on my university/college campus. (heterosexism)
  \item
    LGBTQ students are harassed on my university/college campus. (harassed)
  \end{itemize}
\end{itemize}

A \href{https://www.researchgate.net/publication/332062781_Perceptions_of_the_LGBTQ_College_Campus_Climate_Scale_Development_and_Psychometric_Evaluation/link/5ca0bef945851506d7377da7/download}{preprint} of the article is available at ResearchGate. Below is the script for simulating item-level data from the factor loadings, means, and sample size presented in the published article.

Because data is collected at the item level (and I want this resource to be as practical as possible, I have simulated the data for each of the scales at the item level.

Simulating the data involved using factor loadings, means, and correlations between the scales. Because the simulation will produce ``out-of-bounds'' values, the code below rescales the scores into the range of the Likert-type scaling and rounds them to whole values.

Five additional scales were reported in the Szymanski and Bissonette article \citeyearpar{szymanski_perceptions_2020}. Unfortunately, I could not locate factor loadings for all of them; and in two cases, I used estimates from a more recent psychometric analysis. When the individual item and their factor loadings are known, I assigned names based on item content (e.g., ``lo\_energy'') rather than using item numbers (e.g., ``PHQ4''). When I am doing psychometric analyses, I prefer item-level names so that I can quickly see (without having to look up the item names) how the items are behaving. While the focus of this series of chapters is on the LGBTQ Campus Climate scale, this simulated data might be useful to you in one or more of the suggestions for practice (e.g., examining the psychometric characteristics of one or the other scales). The scales, their original citation, and information about how I simulated data for each are listed below.

\begin{itemize}
\tightlist
\item
  \textbf{Sexual Orientation-Based Campus Victimization Scale} \citep{herek_documenting_1993} is a 9-item item scale with Likert scaling ranging from 0 (\emph{never}) to 3 (\emph{two or more times}). Because I was not able to locate factor loadings from a psychometric evaluation, I simulated the data by specifying a 0.8 as a standardized factor loading for each of the items.
\item
  \textbf{College Satisfaction Scale} \citep{helm_relationship_1998} is a 5-item scale with Likert scaling ranging from 1 (\emph{strongly disagree}) to 7 (\emph{strongly agree}). Higher scores represent greater college satisfaction. Because I was not able to locate factor loadings from a psychometric evaluation, I simulated the data by specifying a 0.8 as a standardized factor loading for each of the items.
\item
  \textbf{Institutional and Goals Commitment} \citep{pascarella_predicting_1980} is a 6-item subscale from a 35-item measure assessing academic/social integration and institutional/goal commitment (5 subscales total). The measure had with Likert scaling ranging from 1 (\emph{strongly disagree}) to 5 (\emph{strongly agree}). Higher scores on the institutional and goals commitment subscale indicate greater intentions to persist in college. Data were simulated using factor loadings in the source article.
\item
  \textbf{GAD-7} \citep{spitzer_brief_2006} is a 7-item scale with Likert scaling ranging from 0 (\emph{not at all}) to 3 (\emph{nearly every day}). Higher scores indicate more anxiety. I simulated data by estimating factor loadings from Brattmyr et al. \citeyearpar{brattmyr_factor_2022}.
\item
  \textbf{PHQ-9} \citep{kroenke_phq-9_2001} is a 9-item scale with Likert scaling ranging from 0 (\emph{not at all}) to 3 (\emph{nearly every day}). Higher scores indicate higher levels of depression. I simulated data by estimating factor loadings from Brattmyr et al. \citeyearpar{brattmyr_factor_2022}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Entering the intercorrelations, means, and standard deviations from the journal article}

\NormalTok{Szymanski\_generating\_model }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}}
\StringTok{        \#measurement model}
\StringTok{        CollegeResponse  =\textasciitilde{} .88*cold + .73*unresponsive + .73*supportive }
\StringTok{        Stigma =\textasciitilde{} .86*negative + .76*heterosexism + .71*harassed}
\StringTok{        Victimization =\textasciitilde{} .8*Vic1 + .8*Vic2 + .8*Vic3 + .8*Vic4 + .8*Vic5 + .8*Vic6 + .8*Vic7 + .8*Vic8 + .8*Vic9}
\StringTok{        CollSat =\textasciitilde{} .8*Sat1 + .8*Sat2 + .8*Sat3 + .8*Sat4 + .8*Sat5}
\StringTok{        Persistence =\textasciitilde{} .69*graduation\_importance + .63*right\_decision + .62*will\_register + .59*not\_graduate + .45*undecided + .44*grades\_unimportant}
\StringTok{        Anxiety =\textasciitilde{} .851*nervous + .887*worry\_control + .894*much\_worry + 674*cant\_relax + .484*restless + .442*irritable + 716*afraid}
\StringTok{        Depression =\textasciitilde{} .798*anhedonia + .425*down +  .591*sleep +  .913*lo\_energy +  .441*appetite +  .519*selfworth +  .755*concentration +  .454*too\_slowfast + .695*s\_ideation}
\StringTok{   }
\StringTok{        \#Means}
\StringTok{         CollegeResponse \textasciitilde{} 2.71*1}
\StringTok{         Stigma \textasciitilde{}3.61*1}
\StringTok{         Victimization \textasciitilde{} 0.11*1}
\StringTok{         CollSat \textasciitilde{} 5.61*1}
\StringTok{         Persistence \textasciitilde{} 4.41*1}
\StringTok{         Anxiety \textasciitilde{} 1.45*1}
\StringTok{         Depression \textasciitilde{}1.29*1}

\StringTok{         }
\StringTok{        \#Correlations}
\StringTok{         CollegeResponse \textasciitilde{}\textasciitilde{} .58*Stigma}
\StringTok{         CollegeResponse \textasciitilde{}\textasciitilde{} {-}.25*Victimization}
\StringTok{         CollegeResponse \textasciitilde{}\textasciitilde{}  {-}.59*CollSat}
\StringTok{         CollegeResponse \textasciitilde{}\textasciitilde{}  {-}.29*Persistence}
\StringTok{         CollegeResponse \textasciitilde{}\textasciitilde{}  .17*Anxiety}
\StringTok{         CollegeResponse \textasciitilde{}\textasciitilde{}  .18*Depression}
\StringTok{         }
\StringTok{         Stigma \textasciitilde{}\textasciitilde{} .37*Victimization}
\StringTok{         Stigma \textasciitilde{}\textasciitilde{}  {-}.41*CollSat}
\StringTok{         Stigma \textasciitilde{}\textasciitilde{}  {-}.19*Persistence}
\StringTok{         Stigma \textasciitilde{}\textasciitilde{}  .27*Anxiety}
\StringTok{         Stigma \textasciitilde{}\textasciitilde{}  .24*Depression}
\StringTok{         }
\StringTok{         Victimization \textasciitilde{}\textasciitilde{}  {-}.22*CollSat}
\StringTok{         Victimization \textasciitilde{}\textasciitilde{}  {-}.04*Persistence}
\StringTok{         Victimization \textasciitilde{}\textasciitilde{}  .23*Anxiety}
\StringTok{         Victimization \textasciitilde{}\textasciitilde{}  .21*Depression}
\StringTok{         }
\StringTok{         CollSat \textasciitilde{}\textasciitilde{}  .53*Persistence}
\StringTok{         CollSat \textasciitilde{}\textasciitilde{}  {-}.29*Anxiety}
\StringTok{         CollSat \textasciitilde{}\textasciitilde{}  {-}.32*Depression}
\StringTok{         }
\StringTok{         Persistence \textasciitilde{}\textasciitilde{}  {-}.22*Anxiety}
\StringTok{         Persistence \textasciitilde{}\textasciitilde{}  {-}.26*Depression}
\StringTok{         }
\StringTok{         Anxiety \textasciitilde{}\textasciitilde{}  .76*Depression}
\StringTok{        \textquotesingle{}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240218}\NormalTok{)}
\NormalTok{dfSzy }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{simulateData}\NormalTok{(}\AttributeTok{model =}\NormalTok{ Szymanski\_generating\_model,}
                              \AttributeTok{model.type =} \StringTok{"sem"}\NormalTok{,}
                              \AttributeTok{meanstructure =}\NormalTok{ T,}
                              \AttributeTok{sample.nobs=}\DecValTok{646}\NormalTok{,}
                              \AttributeTok{standardized=}\ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\#used to retrieve column indices used in the rescaling script below}
\NormalTok{col\_index }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(dfSzy))}

\CommentTok{\#The code below loops through each column of the dataframe and assigns the scaling accordingly}
\CommentTok{\#Rows 1 thru 6 are the Perceptions of LGBTQ Campus Climate Scale}
\CommentTok{\#Rows 7 thru 15 are the Sexual Orientation{-}Based Campus Victimization Scale}
\CommentTok{\#Rows 16 thru 20 are the College Satisfaction Scale}
\CommentTok{\#Rows 21 thru 26 are the Institutional and Goals Commitment Scale }
\CommentTok{\#Rows 27 thru 33 are the GAD7}
\CommentTok{\#Rows 34 thru 42 are the PHQ9}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(dfSzy))\{  }
  \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{6}\NormalTok{)\{   }
\NormalTok{    dfSzy[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfSzy[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{7}\NormalTok{))}
\NormalTok{  \}}
    \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{7} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{15}\NormalTok{)\{   }
\NormalTok{    dfSzy[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfSzy[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{    \}}
      \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{16} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{20}\NormalTok{)\{  }
\NormalTok{    dfSzy[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfSzy[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{7}\NormalTok{))}
\NormalTok{      \}}
        \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{21} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{26}\NormalTok{)\{  }
\NormalTok{    dfSzy[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfSzy[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{        \}}
  \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{27} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{33}\NormalTok{)\{  }
\NormalTok{    dfSzy[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfSzy[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{  \}}
    \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{34} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{42}\NormalTok{)\{  }
\NormalTok{    dfSzy[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfSzy[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{  \}}
\NormalTok{\}}

\CommentTok{\#rounding to integers so that the data resembles that which was collected}
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{dfSzy }\OtherTok{\textless{}{-}}\NormalTok{ dfSzy }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{round}\NormalTok{(}\DecValTok{0}\NormalTok{) }

\CommentTok{\#quick check of my work}
\CommentTok{\#psych::describe(dfSzy) }

\CommentTok{\#Reversing the supportive item on the Perceptions of LGBTQ Campus Climate Scale so that the exercises will be consistent with the format in which the data was collected}

\NormalTok{dfSzy }\OtherTok{\textless{}{-}}\NormalTok{ dfSzy }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{supportiveNR =} \DecValTok{8} \SpecialCharTok{{-}}\NormalTok{ supportive)}

\CommentTok{\#Reversing three items on the Institutional and Goals Commitments scale so that the exercises will be consistent with the format in which the data was collected}

\NormalTok{dfSzy }\OtherTok{\textless{}{-}}\NormalTok{ dfSzy }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{not\_graduateNR =} \DecValTok{8} \SpecialCharTok{{-}}\NormalTok{ not\_graduate)}\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{undecidedNR =} \DecValTok{8} \SpecialCharTok{{-}}\NormalTok{ undecided)}\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{grades\_unimportantNR =} \DecValTok{8} \SpecialCharTok{{-}}\NormalTok{ grades\_unimportant)}

\NormalTok{dfSzy }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(dfSzy, }\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(supportive, not\_graduate, undecided, grades\_unimportant))}
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either an .rds object (preserves any formatting you might do) or a.csv file (think ``Excel lite'').

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(dfSzy, \textquotesingle{}SzyDF.rds\textquotesingle{}) bring back the simulated dat from}
\CommentTok{\# an .rds file dfSzy \textless{}{-} readRDS(\textquotesingle{}SzyDF.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(dfSzy,}
\CommentTok{\# file=\textquotesingle{}SzyDF.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE) bring}
\CommentTok{\# back the simulated dat from a .csv file dfSzy \textless{}{-}}
\CommentTok{\# read.csv(\textquotesingle{}SzyDF.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

Although Szymanski and Bissonette report inter-item correlations, it does not appear that they used item analysis to guide their selection of items. In fact, it is not necessary to do so. I teach item analysis because I think it provides a conceptual grounding for future lessons on exploratory and confirmatory factor analysis.

\hypertarget{step-i-corrected-item-total-correlations}{%
\section{Step I: Corrected item-total correlations}\label{step-i-corrected-item-total-correlations}}

\begin{figure}
\centering
\includegraphics{images/ItemAnalysis/StepOne.png}
\caption{Image of the first step in the workflow for item analysis for survey development.}
\end{figure}

You might think of corrected item-total correlations as form \emph{a within-scale of convergent validity.}

\begin{itemize}
\tightlist
\item
  If needed, transform any items (i.e., reverse-coding) and calculate a total score.
\item
  Calculate \emph{corrected item-total correlations} by correlating each item to the total score \emph{excluding} the item being evaluated.

  \begin{itemize}
  \tightlist
  \item
    to the degree that the item total represents the construct of interest, the items should be strongly correlated with the corrected total score.
  \end{itemize}
\item
  Make decisions about items and scales. For items that have low or negative correlations

  \begin{itemize}
  \tightlist
  \item
    consider deletion,
  \item
    consider revision (requires new data collection).
  \end{itemize}
\item
  Each time an item is deleted, the item analysis needs to be repeated because it changes the total-scale score.

  \begin{itemize}
  \tightlist
  \item
    In fact, it's a very iterative process. At times, researchers ``add back'' a previously deleted item (once others are deleted) because with each deletion/addition the statistical construct definition is evolving.
  \end{itemize}
\item
  In multidimensional scales, if the total-scale score is ever used, researchers should conduct item analyses separately for both the total- and the sub- scale scores.
\end{itemize}

There are reasons to not ``blindly follow the results of an item analysis'' \citep{green_using_2017}.

\begin{itemize}
\tightlist
\item
  \textbf{Method factors} (aka \emph{method effects}) are common \emph{methods} that are irrelevant to the characteristics or traits being measured -- yet when analyzed they share variance. Examples of these include negatively word items and common phrasing such as ``My supervisor tells me'' versus ``I receive feedback'' \citep{chyung_evidencebased_2018}.
\item
  Adequacy of construct representation. That is, how broad is the construct and to what degree do the items represent the entire construct? Threats to the adequacy of the construct representation include:

  \begin{itemize}
  \tightlist
  \item
    Writing items on a particular, narrow, aspect of the construct, ignoring others.
  \item
    Retaining items that are strongly correlated while deleting those that whose correlations are less strong (although they represent a different aspect of the construct).
  \end{itemize}
\end{itemize}

This means we should think carefully and simultaneously about:

\begin{itemize}
\tightlist
\item
  statistical properties of the item and overall scale,
\item
  construct definition,
\item
  scale structure (unidimensional? multidimensional? hierarchical?).
\end{itemize}

\hypertarget{data-prep}{%
\subsection{Data Prep}\label{data-prep}}

Let's do the operational work to get all the pieces we need:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Reverse-code the \emph{supportive} variable.
\item
  From the raw data calculate

  \begin{itemize}
  \tightlist
  \item
    total-scale score,
  \item
    college response subscale,
  \item
    stigma subscale.
  \end{itemize}
\item
  The result is dataset with the item-level data and the three mean scores (total, college response, stigma).
\end{enumerate}

When we review the information about this scale, we learn that the \emph{supportive} item is scaled in the opposite direction of the rest of the items. That is, a higher score on \emph{supportive} would indicate a positive perception of the campus climate for LGBTQ individuals whereas higher scores on the remaining items indicate a more negative perception. Before moving forward, we must reverse score this item.

In doing this, I will briefly note that in this case I have given my variables one-word names that represent each item. Many researchers (including myself) will often give variable names that are alpha numerical: LGBTQ1, LGBTQ2, LGBTQ\emph{n}. Either is acceptable. In the psychometrics case, I find the one-word names to be useful shortcuts as I begin to understand the inter-item relations.

In reverse-scoring the \emph{supportive} item, I will rename it ``unsupportive'' as an indication of its reversed direction.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dfSzy }\OtherTok{\textless{}{-}}\NormalTok{ dfSzy }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{unsupportive =} \DecValTok{8} \SpecialCharTok{{-}}\NormalTok{ supportiveNR)  }\CommentTok{\#scaling 1 to 7; so we subtract from 8}

\CommentTok{\# psych::describe(dfSzy)}
\end{Highlighting}
\end{Shaded}

Next, we score the items. In our simulation, we have no missing data. Using an available information approach (AIA; \citep{parent_handling_2013}) where it is common to allow 20-25\% missingness, we might allow the total-scale score to calculate if there is one variable missing. I am inclined to also score the subscales if there is one missing; thus, I set the threshold at 66\%. The \emph{mean\_n()} function in the \emph{sjstats} packages is especially helpful for this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LGBTQvars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"cold"}\NormalTok{, }\StringTok{"unresponsive"}\NormalTok{, }\StringTok{"negative"}\NormalTok{, }\StringTok{"heterosexism"}\NormalTok{, }\StringTok{"harassed"}\NormalTok{,}
    \StringTok{"unsupportive"}\NormalTok{)}
\NormalTok{ResponseVars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"cold"}\NormalTok{, }\StringTok{"unresponsive"}\NormalTok{, }\StringTok{"unsupportive"}\NormalTok{)}
\NormalTok{Stigmavars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"negative"}\NormalTok{, }\StringTok{"heterosexism"}\NormalTok{, }\StringTok{"harassed"}\NormalTok{)}

\NormalTok{dfSzy}\SpecialCharTok{$}\NormalTok{Total }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(dfSzy[, LGBTQvars], }\FloatTok{0.8}\NormalTok{)  }\CommentTok{\#will create the mean for each individual if 80\% of variables are present (this means there must be at least 5 of 6)}
\NormalTok{dfSzy}\SpecialCharTok{$}\NormalTok{Response }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(dfSzy[, ResponseVars], }\FloatTok{0.66}\NormalTok{)  }\CommentTok{\#will create the mean for each individual if 66\% of variables are present (in this case 1 variable can be missing)}
\NormalTok{dfSzy}\SpecialCharTok{$}\NormalTok{Stigma }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(dfSzy[, Stigmavars], }\FloatTok{0.66}\NormalTok{)  }\CommentTok{\#will create the mean for each individual if 66\% of variables are present (in this case 1 variable can be missing)}

\CommentTok{\# If the scoring code above does not work for you, try the format}
\CommentTok{\# below which involves inserting to periods in front of the variable}
\CommentTok{\# list. One example is provided. dfLewis$Belonging \textless{}{-}}
\CommentTok{\# sjstats::mean\_n(dfLewis[, ..Belonging\_vars], 0.80)}
\end{Highlighting}
\end{Shaded}

While we are at it, let's just create tiny dfs with just our variables of interest.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LGBTQ }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(dfSzy, cold, unresponsive, unsupportive, negative,}
\NormalTok{    heterosexism, harassed)}
\NormalTok{Response }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(dfSzy, cold, unresponsive, unsupportive)}
\NormalTok{Stigma }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(dfSzy, negative, heterosexism, harassed)}
\end{Highlighting}
\end{Shaded}

\hypertarget{calculating-item-total-correlation-coefficients}{%
\subsection{Calculating Item-Total Correlation Coefficients}\label{calculating-item-total-correlation-coefficients}}

Let's first ask, ``Is there support for this instrument as a unidimensional measure?'' To do that, we get an alpha for the whole scale score.

The easiest way to do this is apply the \emph{alpha()} function to a tiny df with the variables in that particular scale or subscale. Any variables should be pre-reversed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LGBTQalpha }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(LGBTQ)  }\CommentTok{\#Although unnecessary, I have saved the output as objects because I will use the objects to create a table }
\NormalTok{LGBTQalpha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = LGBTQ)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
       0.7       0.7    0.68      0.28 2.4 0.018    4 0.63     0.25

    95% confidence boundaries 
         lower alpha upper
Feldt     0.66   0.7  0.74
Duhachek  0.66   0.7  0.74

 Reliability if an item is dropped:
             raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r
cold              0.64      0.64    0.61      0.27 1.8    0.022 0.0066  0.22
unresponsive      0.66      0.66    0.63      0.28 2.0    0.021 0.0073  0.25
unsupportive      0.67      0.67    0.63      0.29 2.0    0.021 0.0058  0.25
negative          0.66      0.66    0.63      0.28 2.0    0.021 0.0084  0.25
heterosexism      0.66      0.66    0.63      0.28 2.0    0.021 0.0087  0.25
harassed          0.67      0.67    0.64      0.29 2.0    0.021 0.0078  0.25

 Item statistics 
               n raw.r std.r r.cor r.drop mean   sd
cold         646  0.68  0.68  0.59   0.49  4.1 1.03
unresponsive 646  0.63  0.63  0.51   0.43  4.3 0.99
unsupportive 646  0.62  0.62  0.51   0.42  3.7 0.98
negative     646  0.64  0.63  0.51   0.42  4.0 1.04
heterosexism 646  0.61  0.63  0.51   0.43  4.0 0.90
harassed     646  0.63  0.61  0.49   0.41  3.9 1.07

Non missing response frequency for each item
                1    2    3    4    5    6    7 miss
cold         0.00 0.04 0.22 0.40 0.23 0.09 0.00    0
unresponsive 0.00 0.03 0.17 0.37 0.33 0.09 0.01    0
unsupportive 0.01 0.07 0.35 0.37 0.17 0.02 0.01    0
negative     0.01 0.07 0.23 0.39 0.24 0.05 0.00    0
heterosexism 0.00 0.03 0.24 0.43 0.26 0.03 0.00    0
harassed     0.01 0.07 0.27 0.37 0.22 0.05 0.01    0
\end{verbatim}

Examining our list, the overall alpha is 0.70. Further, the average inter-item correlation (\emph{average\_r}) is .28.\\
\emph{And just hold up a minute, I thought you told us alpha was bad!}

\begin{itemize}
\tightlist
\item
  While it is less than ideal, we still use it all the time:

  \begin{itemize}
  \tightlist
  \item
    keeping in mind its relative value (does it increase/decrease, holding other things {[}like sample size{]} constant) and
  \item
    examining alpha alternatives (such as we obtained from the omega output)
  \end{itemize}
\item
  Why alpha in this context? Its information about \emph{consistency} is essential. In evaluating a scale's reliability, we do want to know if items (unidimensionally or across subscales) are responding consistently high/middle/low.
\end{itemize}

We take note of two columns:

\begin{itemize}
\tightlist
\item
  \emph{r.cor} is the correlation between the item and the total-scale score with the row-item included. When our focus is on the contribution of a specific item, this information is not helpful since this column gets ``extra credit'' for the redundancy of the duplicated item.
\item
  \emph{r.drop} is the corrected item-total correlation. This is the better choice because it excludes the row-item being evaluated (eliminating the redundancy) prior to conducting the correlation.

  \begin{itemize}
  \tightlist
  \item
    Looking at the two columns, notice that the \emph{r.drop} correlations are lower. This is the more honest correlation of the item with the \emph{other} items.
  \item
    In item analysis, we look for items that have relatively high (assessing redundancy or duplication) of items and relatively low (indicating they are unlike the other items) values.
  \end{itemize}
\end{itemize}

If we thought an item was problematic, we could eliminate it and rerun the analysis. Because we are looking at a list of items that ``made the cut,'' we don't have any items that are concerningly high or low. For demonstration purposes, though, the corrected item-total correlation (\emph{r.drop}) of the \emph{harassed} variable was the lowest (0.40). Let's re-run the analysis excluding this item.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{minus\_harassed }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(dfSzy, cold, unresponsive, unsupportive,}
\NormalTok{    negative, heterosexism)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(minus\_harassed)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = minus_harassed)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
      0.67      0.67    0.64      0.29   2 0.021    4 0.65     0.25

    95% confidence boundaries 
         lower alpha upper
Feldt     0.63  0.67  0.71
Duhachek  0.63  0.67  0.71

 Reliability if an item is dropped:
             raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r
cold              0.58      0.58    0.53      0.26 1.4    0.027 0.0060  0.22
unresponsive      0.62      0.62    0.56      0.29 1.6    0.025 0.0079  0.25
unsupportive      0.61      0.61    0.56      0.28 1.6    0.025 0.0072  0.25
negative          0.65      0.64    0.58      0.31 1.8    0.022 0.0093  0.30
heterosexism      0.64      0.64    0.58      0.31 1.8    0.023 0.0097  0.30

 Item statistics 
               n raw.r std.r r.cor r.drop mean   sd
cold         646  0.72  0.71  0.61   0.50  4.1 1.03
unresponsive 646  0.66  0.66  0.53   0.43  4.3 0.99
unsupportive 646  0.67  0.67  0.55   0.45  3.7 0.98
negative     646  0.63  0.62  0.46   0.37  4.0 1.04
heterosexism 646  0.60  0.62  0.47   0.38  4.0 0.90

Non missing response frequency for each item
                1    2    3    4    5    6    7 miss
cold         0.00 0.04 0.22 0.40 0.23 0.09 0.00    0
unresponsive 0.00 0.03 0.17 0.37 0.33 0.09 0.01    0
unsupportive 0.01 0.07 0.35 0.37 0.17 0.02 0.01    0
negative     0.01 0.07 0.23 0.39 0.24 0.05 0.00    0
heterosexism 0.00 0.03 0.24 0.43 0.26 0.03 0.00    0
\end{verbatim}

The alpha \((\alpha = 0.67)\) decreases; the overall inter-item correlations increase, slightly (\emph{average\_r}; 0.29). This decrease in alpha is an example of how sample size can affect the result.

Examining item-level statistics, we do see greater variability (0.37 to 0.50) in the corrected item-total correlations (\emph{r.drop}). What might this mean?

\begin{itemize}
\tightlist
\item
  The item we dropped (\emph{harassed}) may be clustering with \emph{negative} and \emph{heterosexism} in a subordinate factor (think subscale).
\item
  Although item analysis is more of a tool in assessing reliability, the statistical information that \emph{harassed} provided may broaden the construct definition (definitions are a concern of \emph{validity}) of perceptions of campus climate such that it is necessary to ground/anchor \emph{negative} and \emph{heterosexism}.
\end{itemize}

Tentative conclusion: there is evidence that this is not a unidimensional measure. Let's move on to inspect similar data for each of the subscales. We'll start with the College Response subscale.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RESPalpha }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(Response)}
\NormalTok{RESPalpha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = Response)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
      0.66      0.66    0.57      0.39 1.9 0.023    4 0.77      0.4

    95% confidence boundaries 
         lower alpha upper
Feldt     0.61  0.66  0.70
Duhachek  0.62  0.66  0.71

 Reliability if an item is dropped:
             raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r
cold              0.52      0.52    0.35      0.35 1.1    0.038    NA  0.35
unresponsive      0.60      0.60    0.42      0.42 1.5    0.032    NA  0.42
unsupportive      0.58      0.58    0.40      0.40 1.4    0.033    NA  0.40

 Item statistics 
               n raw.r std.r r.cor r.drop mean   sd
cold         646  0.80  0.79  0.62   0.50  4.1 1.03
unresponsive 646  0.76  0.76  0.55   0.45  4.3 0.99
unsupportive 646  0.76  0.77  0.57   0.46  3.7 0.98

Non missing response frequency for each item
                1    2    3    4    5    6    7 miss
cold         0.00 0.04 0.22 0.40 0.23 0.09 0.00    0
unresponsive 0.00 0.03 0.17 0.37 0.33 0.09 0.01    0
unsupportive 0.01 0.07 0.35 0.37 0.17 0.02 0.01    0
\end{verbatim}

The alpha for the College Response subscale is 0.66; this is a bit lower than the alpha for the total scale score \((\alpha = 0.70)\). The average inter-item correlation (\emph{average\_r}) is higher somewhat higher than the that of the total scale score (0.39 versus 0.28).

Examining the corrected item-total correlations (r.drop) indicates strong correlations between the row-item with the remaining variables (0.45 to 0.50).

Let's examine at the Stigma subscale.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{STIGalpha }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(Stigma)}
\NormalTok{STIGalpha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = Stigma)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
      0.62      0.63    0.53      0.36 1.7 0.025    4 0.76     0.36

    95% confidence boundaries 
         lower alpha upper
Feldt     0.57  0.62  0.67
Duhachek  0.57  0.62  0.67

 Reliability if an item is dropped:
             raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r
negative          0.52      0.52    0.35      0.35 1.1    0.038    NA  0.35
heterosexism      0.53      0.53    0.36      0.36 1.1    0.037    NA  0.36
harassed          0.53      0.54    0.37      0.37 1.2    0.036    NA  0.37

 Item statistics 
               n raw.r std.r r.cor r.drop mean  sd
negative     646  0.77  0.76  0.56   0.44  4.0 1.0
heterosexism 646  0.73  0.76  0.55   0.44  4.0 0.9
harassed     646  0.77  0.75  0.54   0.43  3.9 1.1

Non missing response frequency for each item
                1    2    3    4    5    6    7 miss
negative     0.01 0.07 0.23 0.39 0.24 0.05 0.00    0
heterosexism 0.00 0.03 0.24 0.43 0.26 0.03 0.00    0
harassed     0.01 0.07 0.27 0.37 0.22 0.05 0.01    0
\end{verbatim}

The alpha for the Stigma subscale is 0.63; this is a bit lower than the alpha for the total-scale \((\alpha = 0.70)\). In contrast, the inter-item correlation (\emph{average\_r}) is a bit higher than the same for the total scale score (0.36 versus 0.28).

Examining the corrected item-total correlations (r.drop) indicates a strong correlation between the row-item with the remaining variables (0.43 to 0.44).

In addition to needing strong inter-item correlations (which we just assessed) we want the individual items to correlate more strongly with themselves than with the other scale. Let's do that next.

\hypertarget{step-ii-correlating-items-with-other-scale-totals}{%
\section{Step II: Correlating Items with Other Scale Totals}\label{step-ii-correlating-items-with-other-scale-totals}}

You might think of this step as analyzing a within-scale version of discriminant validity. That is, we do not want individual items from one scale to correlate more highly with subscale scores of other scales, than it does with its own.

\begin{figure}
\centering
\includegraphics{images/ItemAnalysis/StepTwo.png}
\caption{Image of the second step in the workflow for item analysis for survey development.}
\end{figure}

\begin{itemize}
\tightlist
\item
  Calculate scale scores for each of the subscales of a measure.
\item
  Focusing on one subscale at a time, correlate each of the subscale's items with the total scores of all the other subscales.
\item
  Comparing to the results of Step I's corrected item-total process, each item should have stronger correlations with its own items (i.e., the corrected item-total correlations) than with the other subscale total scores.
\end{itemize}

In this first analysis, we will correlate the individual \emph{items} from the College Response subscale to the Stigma subscale \emph{score.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(dfSzy[}\FunctionTok{c}\NormalTok{(}\StringTok{"cold"}\NormalTok{, }\StringTok{"unresponsive"}\NormalTok{, }\StringTok{"unsupportive"}\NormalTok{,}
    \StringTok{"Stigma"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Means, standard deviations, and correlations with confidence intervals
 

  Variable        M    SD   1          2          3         
  1. cold         4.11 1.03                                 
                                                            
  2. unresponsive 4.31 0.99 .40**                           
                            [.34, .47]                      
                                                            
  3. unsupportive 3.69 0.98 .42**      .35**                
                            [.36, .49] [.28, .42]           
                                                            
  4. Stigma       3.96 0.76 .33**      .28**      .26**     
                            [.26, .39] [.21, .35] [.18, .33]
                                                            

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

We want the corrected item-total correlations of the College Response scale (0.45 to 0.50; retrieved from the \emph{r.drop} column above)to be higher than their correlations with the Stigma scale (0.28 to 0.33) with all three items). These items follow the pattern.

Let's examine the individual items from the Stigma scale with the College Response subscale score.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(dfSzy[}\FunctionTok{c}\NormalTok{(}\StringTok{"negative"}\NormalTok{, }\StringTok{"heterosexism"}\NormalTok{, }\StringTok{"harassed"}\NormalTok{,}
    \StringTok{"Response"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Means, standard deviations, and correlations with confidence intervals
 

  Variable        M    SD   1          2          3         
  1. negative     3.96 1.04                                 
                                                            
  2. heterosexism 4.00 0.90 .37**                           
                            [.30, .43]                      
                                                            
  3. harassed     3.91 1.07 .36**      .35**                
                            [.29, .42] [.28, .42]           
                                                            
  4. Response     4.04 0.77 .29**      .29**      .27**     
                            [.22, .36] [.22, .36] [.20, .34]
                                                            

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

Similarly, the corrected item-total correlations (i.e., \emph{r.drop}) from the Stigma subscale (0.43 to 0.44) are stronger than their correlation with the College Response subscale (0.27 to 0.29).

\hypertarget{step-iii-interpreting-and-writing-up-the-results}{%
\section{Step III: Interpreting and Writing up the Results}\label{step-iii-interpreting-and-writing-up-the-results}}

Now it's time to make sense of the results. Here's a reminder from the workflow:

\begin{figure}
\centering
\includegraphics{images/ItemAnalysis/StepThree.png}
\caption{Image of the third step in the workflow for item analysis for survey development.}
\end{figure}

Tabling these results can be really useful to present them effectively. As is customary in APA style tables, when the item is in bold, the value represents its relationship with its own factor. These values come from the corrected item-total (\emph{r.drop}) values where the item is singled out and correlated with the remaining items in its subscale.

\begin{longtable}[]{@{}l@{}}
\toprule\noalign{}
Item-Total Correlations of Items with their Own and Other Subscales \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\end{longtable}

\begin{longtable}[]{@{}lcc@{}}
\toprule\noalign{}
Variables & College Response & Stigma \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
cold & \textbf{.50} & 0.33 \\
unresponsive & \textbf{.45} & 0.28 \\
unsupportive & \textbf{.46} & 0.26 \\
negative & 0.29 & \textbf{0.44} \\
heterosexism & 0.29 & \textbf{0.44} \\
harassed & 0.27 & \textbf{0.43} \\
\end{longtable}

Although I pitched this type of item-analysis as \emph{reliability}, to some degree it assesses within-scale \textbf{convergent and discriminant validity} because we can see the item relates more strongly to members of its own scale (higher correlation coefficients indicate \emph{convergence}) than to the subscale scores of the other scales. When this pattern occurs, we can argue that the items \emph{discriminate} well.

\textbf{Results}

Item analyses were conducted on the six items hypothesized to assess perceptions of campus climate for members of the LGBTQ community. To assess the within-scale convergent and discriminant validity of the College Response and Stigma subscales, each item was correlated with its own scale (with the item removed) and with the other subscale (see Table 1). In all cases, items were more highly correlated with their own scale than with the other scale. Coefficient alphas were 0.66, 0.63, and 0.70 for the College Response, Stigma, and total-scale scores, respectively. We concluded that the within-scale convergent and discriminant validity of this measure is strong.

For your consideration: You are at your dissertation defense. For one of your measures, the Cronbach's alpha is .45. A committee member asks, ``So why was the alpha coefficient so low?'' On the basis of what you have learned in this lesson, how do you respond?

\hypertarget{a-conversation-with-dr.-szymanski}{%
\section{A Conversation with Dr.~Szymanski}\label{a-conversation-with-dr.-szymanski}}

Doctoral students Julian Williams (Industrial-Organizational Psychology), Jaylee York (Clinical Psychology), and I were able to interview the first author (Dawn Szymanski, PhD) about the article \citep{szymanski_perceptions_2020} and what it means. Here's a direct \href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=0f9696ab-df9a-452b-8ccd-aee101271054}{link} to that interview.

Among other things, we asked:

\begin{itemize}
\tightlist
\item
  How were you able to create such an efficient (6 items) survey?
\item
  What were the decisions around a potential third factor of \emph{visibility}?
\item
  What would you say to senior leadership on a college campus (where there hiring policies that discriminate against LGBTQIA+ applicants) who will acknowledge the research that indicates that the existence of such policies are associated with reduced well-being for members of the LGBTQIA+ community but who insists that their campus is different?
\item
  How would you like to see the article used?
\end{itemize}

\hypertarget{practice-problems-5}{%
\section{Practice Problems}\label{practice-problems-5}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. For this lesson, please locate item-level data for a scale that has the potential for at least two subscales and a total-scale score. Ideally, you selected such data for practice from the prior lesson. Then, please examine the following:

\begin{itemize}
\tightlist
\item
  produce alpha coefficients, average inter-item correlations, and corrected item-total correlations for the total and subscales, separately
\item
  produce correlations between the individual items of one subscale and the subscale scores of all other scales
\item
  draft an APA style results section with an accompanying table.
\end{itemize}

In my example, there were only two subscales. If you have more, you will need to compare each subscale with all the others. For example, if you had three subscales: A, B, C, you would need to compare A/B, B/C, and A/C.

\hypertarget{problem-1-play-around-with-this-simulation.-2}{%
\subsection{Problem \#1: Play around with this simulation.}\label{problem-1-play-around-with-this-simulation.-2}}

Copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results.

If item analysis is new to you, copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results. Perhaps you just change the number in ``set.seed(210827)'' from 210827 to something else. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go.

\hypertarget{problem-2-use-raw-data-from-the-recentering-psych-stats-survey-on-qualtrics.}{%
\subsection{Problem \#2: Use raw data from the ReCentering Psych Stats survey on Qualtrics.}\label{problem-2-use-raw-data-from-the-recentering-psych-stats-survey-on-qualtrics.}}

The script below pulls live data directly from the ReCentering Psych Stats survey on Qualtrics. As described in the \href{https://lhbikos.github.io/ReC_MultivariateModeling/}{Scrubbing and Scoring chapters} of the ReCentering Psych Stats Multivariate Modeling volume, the Perceptions of the LGBTQ College Campus Climate Scale \citep{szymanski_perceptions_2020} was included (LGBTQ) and further adapted to assess perceptions of campus climate for Black students (BLst), non-Black students of color (nBSoC), international students (INTst), and students with disabilities (wDIS). Consider conducting the analyses on one of these scales or merging them together and imagining subscales according to identity/group (LGBTQ, Black, non-Black, disability, international) or College Response and Stigma across the different groups.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\CommentTok{\# only have to run this ONCE to draw from the same Qualtrics}
\CommentTok{\# account...but will need to get different token if you are changing}
\CommentTok{\# between accounts}
\FunctionTok{library}\NormalTok{(qualtRics)}
\CommentTok{\# qualtrics\_api\_credentials(api\_key =}
\CommentTok{\# \textquotesingle{}mUgPMySYkiWpMFkwHale1QE5HNmh5LRUaA8d9PDg\textquotesingle{}, base\_url =}
\CommentTok{\# \textquotesingle{}spupsych.az1.qualtrics.com\textquotesingle{}, overwrite = TRUE, install = TRUE)}
\NormalTok{QTRX\_df }\OtherTok{\textless{}{-}}\NormalTok{ qualtRics}\SpecialCharTok{::}\FunctionTok{fetch\_survey}\NormalTok{(}\AttributeTok{surveyID =} \StringTok{"SV\_b2cClqAlLGQ6nLU"}\NormalTok{, }\AttributeTok{time\_zone =} \ConstantTok{NULL}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{label =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{convert =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{force\_request =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{import\_id =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{climate\_df }\OtherTok{\textless{}{-}}\NormalTok{ QTRX\_df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(}\StringTok{"Blst\_1"}\NormalTok{, }\StringTok{"Blst\_2"}\NormalTok{, }\StringTok{"Blst\_3"}\NormalTok{, }\StringTok{"Blst\_4"}\NormalTok{, }\StringTok{"Blst\_5"}\NormalTok{, }\StringTok{"Blst\_6"}\NormalTok{,}
        \StringTok{"nBSoC\_1"}\NormalTok{, }\StringTok{"nBSoC\_2"}\NormalTok{, }\StringTok{"nBSoC\_3"}\NormalTok{, }\StringTok{"nBSoC\_4"}\NormalTok{, }\StringTok{"nBSoC\_5"}\NormalTok{, }\StringTok{"nBSoC\_6"}\NormalTok{,}
        \StringTok{"INTst\_1"}\NormalTok{, }\StringTok{"INTst\_2"}\NormalTok{, }\StringTok{"INTst\_3"}\NormalTok{, }\StringTok{"INTst\_4"}\NormalTok{, }\StringTok{"INTst\_5"}\NormalTok{, }\StringTok{"INTst\_6"}\NormalTok{,}
        \StringTok{"wDIS\_1"}\NormalTok{, }\StringTok{"wDIS\_2"}\NormalTok{, }\StringTok{"wDIS\_3"}\NormalTok{, }\StringTok{"wDIS\_4"}\NormalTok{, }\StringTok{"wDIS\_5"}\NormalTok{, }\StringTok{"wDIS\_6"}\NormalTok{, }\StringTok{"LGBTQ\_1"}\NormalTok{,}
        \StringTok{"LGBTQ\_2"}\NormalTok{, }\StringTok{"LGBTQ\_3"}\NormalTok{, }\StringTok{"LGBTQ\_4"}\NormalTok{, }\StringTok{"LGBTQ\_5"}\NormalTok{, }\StringTok{"LGBTQ\_6"}\NormalTok{)}
\CommentTok{\# Item numbers are supported with the following items: \_1 \textquotesingle{}My campus}
\CommentTok{\# unit provides a supportive environment for \_\_\_ students\textquotesingle{} \_2}
\CommentTok{\# \textquotesingle{}\_\_\_\_\_\_\_\_ is visible in my campus unit\textquotesingle{} \_3 \textquotesingle{}Negative attitudes}
\CommentTok{\# toward persons who are \_\_\_\_ are openly expressed in my campus}
\CommentTok{\# unit.\textquotesingle{} \_4 \textquotesingle{}My campus unit is unresponsive to the needs of \_\_\_\_}
\CommentTok{\# students.\textquotesingle{} \_5 \textquotesingle{}Students who are\_\_\_\_\_ are harassed in my campus}
\CommentTok{\# unit.\textquotesingle{} \_6 \textquotesingle{}My campus unit is cold and uncaring toward \_\_\_\_}
\CommentTok{\# students.\textquotesingle{}}

\CommentTok{\# Item 1 on each subscale should be reverse coded. The College}
\CommentTok{\# Response scale is composed of items 1, 4, 6, The Stigma scale is}
\CommentTok{\# composed of items 2,3, 5}
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either a .csv file (think ``Excel lite'') or .rds object (preserves any formatting you might do).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(climate\_df,}
\CommentTok{\# file=\textquotesingle{}climate\_df.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE)}
\CommentTok{\# bring back the simulated dat from a .csv file climate\_df \textless{}{-}}
\CommentTok{\# read.csv (\textquotesingle{}climate\_df.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(climate\_df, \textquotesingle{}climate\_df.rds\textquotesingle{}) bring back the simulated}
\CommentTok{\# dat from an .rds file climate\_df \textless{}{-} readRDS(\textquotesingle{}climate\_df.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

\hypertarget{problem-3-try-something-entirely-new.-2}{%
\subsection{Problem \#3: Try something entirely new.}\label{problem-3-try-something-entirely-new.-2}}

Complete the same steps using data for which you have permission and access. This might be data of your own, from your lab, simulated from an article, or located on an open repository.

\hypertarget{grading-rubric-2}{%
\subsection{Grading Rubric}\label{grading-rubric-2}}

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5493}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1972}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Possible
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Earned
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1. Check and, if needed, format and score data & 5 & \_\_\_\_\_ \\
2. Report alpha coefficients and average inter-item correlations for the total and subscales & 5 & \_\_\_\_\_ \\
3. Produce and interpret corrected item-total correlations for total and subscales, separately & 5 & \_\_\_\_\_ \\
4. Produce and interpret correlations between the individual items of a given subscale and the subscale scores of all other subscales & 5 & \_\_\_\_\_ \\
5. APA style results section with table & 5 & \_\_\_\_\_ \\
6. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 30 & \_\_\_\_\_ \\
\end{longtable}

\hypertarget{bonus-reel}{%
\section{Bonus Reel:}\label{bonus-reel}}

\begin{figure}
\centering
\includegraphics{images/film-strip-1.jpg}
\caption{Image of a filmstrip}
\end{figure}

For our interpretation and results, I created the table by manually typing in the results. Since there were only two subscales, this was easy. However, it can be a very useful skill (and prevent typing errors) by leveraging R's capabilities to build a table.

The script below

\begin{itemize}
\tightlist
\item
  Creates a correlation matrix of the items of each scale and correlates them with the ``other'' subscale, separately for both subscales.
\item
  Extracts the r.drop from each subscale
\item
  Joins (adds more variables) the analyses across the corrected item-total and item-other subscale analyses
\item
  Binds (adds more cases) the two sets of items together
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Resp\_othR }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{corr.test}\NormalTok{(dfSzy[}\FunctionTok{c}\NormalTok{(}\StringTok{"negative"}\NormalTok{, }\StringTok{"heterosexism"}\NormalTok{, }\StringTok{"harassed"}\NormalTok{,}
    \StringTok{"Response"}\NormalTok{)])  }\CommentTok{\#Run the correlation of the subscale and the items that are *not* on the subscale}
\NormalTok{Resp\_othR }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(Resp\_othR}\SpecialCharTok{$}\NormalTok{r)  }\CommentTok{\#extracts the \textquotesingle{}r\textquotesingle{} matrix and makes it a df}
\NormalTok{Resp\_othR}\SpecialCharTok{$}\NormalTok{Items }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"negative"}\NormalTok{, }\StringTok{"heterosexism"}\NormalTok{, }\StringTok{"harassed"}\NormalTok{, }\StringTok{"Response"}\NormalTok{)  }\CommentTok{\#Assigning names to the items}
\NormalTok{Resp\_othR }\OtherTok{\textless{}{-}}\NormalTok{ Resp\_othR[}\SpecialCharTok{!}\NormalTok{Resp\_othR}\SpecialCharTok{$}\NormalTok{Items }\SpecialCharTok{==} \StringTok{"Response"}\NormalTok{, ]  }\CommentTok{\#Removing the subscale score as a a row in the df}
\NormalTok{Resp\_othR[, }\StringTok{"Stigma"}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}  \CommentTok{\#We need a column for this to bind the items later}
\NormalTok{Resp\_othR }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Resp\_othR, Items, Response, Stigma)  }\CommentTok{\#All we need is the item name and the correlations with the subscales}
\NormalTok{RESPalpha }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(RESPalpha}\SpecialCharTok{$}\NormalTok{item.stats)  }\CommentTok{\#Grabbing the alpha objet we created earlier and making it a df  }
\NormalTok{RESPalpha}\SpecialCharTok{$}\NormalTok{Items }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"cold"}\NormalTok{, }\StringTok{"unresponsive"}\NormalTok{, }\StringTok{"unsupportive"}\NormalTok{)}

\NormalTok{Stig\_othR }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{corr.test}\NormalTok{(dfSzy[}\FunctionTok{c}\NormalTok{(}\StringTok{"cold"}\NormalTok{, }\StringTok{"unresponsive"}\NormalTok{, }\StringTok{"unsupportive"}\NormalTok{,}
    \StringTok{"Stigma"}\NormalTok{)])  }\CommentTok{\#Run the correlation of the subscale and the items that are *not* on the subscale}
\NormalTok{Stig\_othR }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(Stig\_othR}\SpecialCharTok{$}\NormalTok{r)  }\CommentTok{\#extracts the \textquotesingle{}r\textquotesingle{} matrix and makes it a df}
\NormalTok{Stig\_othR}\SpecialCharTok{$}\NormalTok{Items }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"cold"}\NormalTok{, }\StringTok{"unresponsive"}\NormalTok{, }\StringTok{"unsupportive"}\NormalTok{, }\StringTok{"Stigma"}\NormalTok{)  }\CommentTok{\#Assigning names to the items}
\NormalTok{Stig\_othR }\OtherTok{\textless{}{-}}\NormalTok{ Stig\_othR[}\SpecialCharTok{!}\NormalTok{Stig\_othR}\SpecialCharTok{$}\NormalTok{Items }\SpecialCharTok{==} \StringTok{"Stigma"}\NormalTok{, ]  }\CommentTok{\#Removing the subscale score as a a row in the df}
\NormalTok{Stig\_othR[, }\StringTok{"Response"}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}  \CommentTok{\#We need a column for this to bind the items later}
\NormalTok{Stig\_othR }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Stig\_othR, Items, Response, Stigma)  }\CommentTok{\#All we need is the item name and the correlations with the subscales}
\NormalTok{STIGalpha }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(STIGalpha}\SpecialCharTok{$}\NormalTok{item.stats)  }\CommentTok{\#Grabbing the alpha objet we created earlier and making it a df  }
\NormalTok{STIGalpha}\SpecialCharTok{$}\NormalTok{Items }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"negative"}\NormalTok{, }\StringTok{"heterosexism"}\NormalTok{, }\StringTok{"harassed"}\NormalTok{)}

\CommentTok{\# Combining these four dfs}
\NormalTok{ResponseStats }\OtherTok{\textless{}{-}} \FunctionTok{full\_join}\NormalTok{(RESPalpha, Stig\_othR, }\AttributeTok{by =} \StringTok{"Items"}\NormalTok{)}
\NormalTok{ResponseStats}\SpecialCharTok{$}\NormalTok{Response }\OtherTok{\textless{}{-}}\NormalTok{ ResponseStats}\SpecialCharTok{$}\NormalTok{r.drop}
\NormalTok{ResponseStats }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(ResponseStats, Items, Response, Stigma)}
\NormalTok{StigmaStats }\OtherTok{\textless{}{-}} \FunctionTok{full\_join}\NormalTok{(STIGalpha, Resp\_othR, }\AttributeTok{by =} \StringTok{"Items"}\NormalTok{)}
\NormalTok{StigmaStats}\SpecialCharTok{$}\NormalTok{Stigma }\OtherTok{\textless{}{-}}\NormalTok{ StigmaStats}\SpecialCharTok{$}\NormalTok{r.drop}
\NormalTok{StigmaStats }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(StigmaStats, Items, Response, Stigma)}
\NormalTok{ItemAnalyses }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(ResponseStats, StigmaStats)}
\NormalTok{ItemAnalyses}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         Items  Response    Stigma
1         cold 0.5041391 0.3258401
2 unresponsive 0.4490390 0.2783617
3 unsupportive 0.4644473 0.2581391
4     negative 0.2874243 0.4401670
5 heterosexism 0.2925477 0.4365623
6     harassed 0.2704033 0.4291150
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Writing them to a .csv file allows post{-}r formatting}
\FunctionTok{write.csv}\NormalTok{(ItemAnalyses, }\AttributeTok{file =} \StringTok{"LGBTQ\_Climate\_ItemAnalyses.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{,}
    \AttributeTok{row.names =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{col.names =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{homeworked-example-2}{%
\section{Homeworked Example}\label{homeworked-example-2}}

\href{https://youtu.be/uElfCNI3TsE}{Screencast Link}

For more information about the data used in this homeworked example, please refer to the description and codebook located at the end of the \href{https://lhbikos.github.io/ReCenterPsychStats/ReCintro.html\#introduction-to-the-data-set-used-for-homeworked-examples}{introduction} in first volume of ReCentering Psych Stats.

As a brief review, this data is part of an IRB-approved study, with consent to use in teaching demonstrations and to be made available to the general public via the open science framework. Hence, it is appropriate to use in this context. You will notice there are student- and teacher- IDs. These numbers are not actual student and teacher IDs, rather they were further re-identified so that they could not be connected to actual people.

Because this is an actual dataset, if you wish to work the problem along with me, you will need to download the \href{https://github.com/lhbikos/ReC_Psychometrics/blob/main/Worked_Examples/ReC.rds}{ReC.rds} data file from the Worked\_Examples folder in the ReC\_Psychometrics project on the GitHub.

The course evaluation items can be divided into three subscales:

\begin{itemize}
\tightlist
\item
  \textbf{Valued by the student} includes the items: ValObjectives, IncrUnderstanding, IncrInterest
\item
  \textbf{Traditional pedagogy} includes the items: ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation
\item
  \textbf{Socially responsive pedagogy} includes the items: InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration
\end{itemize}

In this homework focused on validity we will score the total scale and subscales, create a correlation matrix of our scales with a different scale (or item), formally test to see if correlation coefficients are statistically significantly different from each other, conduct a test of incremental validity.

\hypertarget{check-and-if-needed-format-and-score-data}{%
\subsection{Check and, if needed, format and score data}\label{check-and-if-needed-format-and-score-data}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{big }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"ReC.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

With the next code I will create an item-level df with only the items used in the three scales.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{items }\OtherTok{\textless{}{-}}\NormalTok{ big}\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{ (ValObjectives, IncrUnderstanding, IncrInterest, ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation, MultPerspectives, InclusvClassrm, DEIintegration,EquitableEval)}
\end{Highlighting}
\end{Shaded}

Next I check the structure of the data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(items)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Classes 'data.table' and 'data.frame':  310 obs. of  12 variables:
 $ ValObjectives        : int  5 5 4 4 5 5 5 5 4 5 ...
 $ IncrUnderstanding    : int  2 3 4 3 4 4 5 2 4 5 ...
 $ IncrInterest         : int  5 3 4 2 4 3 5 3 2 5 ...
 $ ClearResponsibilities: int  5 5 4 4 5 4 5 4 4 5 ...
 $ EffectiveAnswers     : int  5 3 5 3 5 3 4 3 2 3 ...
 $ Feedback             : int  5 3 4 2 5 NA 5 4 4 5 ...
 $ ClearOrganization    : int  3 4 3 4 4 4 5 4 4 5 ...
 $ ClearPresentation    : int  4 4 4 2 5 3 4 4 4 5 ...
 $ MultPerspectives     : int  5 5 4 5 5 4 5 5 5 5 ...
 $ InclusvClassrm       : int  5 5 5 5 5 4 5 5 4 5 ...
 $ DEIintegration       : int  5 5 5 5 5 4 5 5 5 5 ...
 $ EquitableEval        : int  5 5 3 5 5 3 5 5 3 5 ...
 - attr(*, ".internal.selfref")=<externalptr> 
\end{verbatim}

\hypertarget{report-alpha-coefficients-and-average-inter-item-correlations-for-the-total-and-subscales}{%
\subsection{Report alpha coefficients and average inter-item correlations for the total and subscales}\label{report-alpha-coefficients-and-average-inter-item-correlations-for-the-total-and-subscales}}

This task is completed in the next section.

\hypertarget{produce-and-interpret-corrected-item-total-correlations-for-total-and-subscales-separately}{%
\subsection{Produce and interpret corrected item-total correlations for total and subscales, separately}\label{produce-and-interpret-corrected-item-total-correlations-for-total-and-subscales-separately}}

In the lecture, I created baby dfs of the subscales and ran the alphas on those; another option is to create lists of variables (i.e., variable vectors) and use that instead. We can later use those same variable vectors to score the items.

\hypertarget{all-course-evaluation-items}{%
\subsubsection{All Course Evaluation Items}\label{all-course-evaluation-items}}

First, I will calculate the alpha coefficient and inter-item correlations for the total score representing the course evaluation items.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(items)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = items)

  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r
      0.92      0.92    0.93      0.49  11 0.0065  4.3 0.61     0.48

    95% confidence boundaries 
         lower alpha upper
Feldt     0.90  0.92  0.93
Duhachek  0.91  0.92  0.93

 Reliability if an item is dropped:
                      raw_alpha std.alpha G6(smc) average_r  S/N alpha se var.r
ValObjectives              0.92      0.92    0.93      0.51 11.3   0.0067 0.016
IncrUnderstanding          0.91      0.91    0.92      0.49 10.6   0.0070 0.016
IncrInterest               0.91      0.91    0.92      0.49 10.4   0.0070 0.018
ClearResponsibilities      0.91      0.91    0.92      0.48 10.0   0.0073 0.015
EffectiveAnswers           0.91      0.91    0.92      0.48 10.0   0.0074 0.016
Feedback                   0.91      0.91    0.92      0.48 10.3   0.0071 0.018
ClearOrganization          0.91      0.91    0.92      0.48 10.2   0.0073 0.016
ClearPresentation          0.91      0.91    0.92      0.47  9.7   0.0076 0.015
MultPerspectives           0.91      0.91    0.92      0.48 10.0   0.0073 0.017
InclusvClassrm             0.91      0.91    0.92      0.49 10.6   0.0069 0.018
DEIintegration             0.92      0.92    0.93      0.52 11.8   0.0063 0.011
EquitableEval              0.91      0.91    0.93      0.49 10.5   0.0070 0.018
                      med.r
ValObjectives          0.53
IncrUnderstanding      0.50
IncrInterest           0.48
ClearResponsibilities  0.48
EffectiveAnswers       0.48
Feedback               0.48
ClearOrganization      0.48
ClearPresentation      0.47
MultPerspectives       0.47
InclusvClassrm         0.52
DEIintegration         0.53
EquitableEval          0.48

 Item statistics 
                        n raw.r std.r r.cor r.drop mean   sd
ValObjectives         309  0.59  0.61  0.55   0.53  4.5 0.61
IncrUnderstanding     309  0.71  0.70  0.67   0.64  4.3 0.82
IncrInterest          308  0.75  0.73  0.71   0.68  3.9 0.99
ClearResponsibilities 307  0.80  0.80  0.79   0.75  4.4 0.82
EffectiveAnswers      308  0.80  0.79  0.78   0.75  4.4 0.83
Feedback              304  0.75  0.75  0.72   0.69  4.2 0.88
ClearOrganization     309  0.79  0.77  0.75   0.72  4.0 1.08
ClearPresentation     309  0.85  0.84  0.83   0.80  4.2 0.92
MultPerspectives      305  0.79  0.80  0.78   0.75  4.4 0.84
InclusvClassrm        301  0.68  0.70  0.67   0.62  4.6 0.68
DEIintegration        273  0.51  0.53  0.49   0.42  4.5 0.74
EquitableEval         308  0.70  0.72  0.69   0.66  4.6 0.63

Non missing response frequency for each item
                         1    2    3    4    5 miss
ValObjectives         0.00 0.01 0.03 0.39 0.57 0.00
IncrUnderstanding     0.01 0.04 0.07 0.44 0.45 0.00
IncrInterest          0.02 0.09 0.14 0.44 0.31 0.01
ClearResponsibilities 0.01 0.02 0.07 0.31 0.59 0.01
EffectiveAnswers      0.01 0.02 0.08 0.36 0.53 0.01
Feedback              0.01 0.05 0.10 0.39 0.46 0.02
ClearOrganization     0.04 0.07 0.10 0.41 0.38 0.00
ClearPresentation     0.02 0.05 0.07 0.40 0.46 0.00
MultPerspectives      0.02 0.02 0.08 0.33 0.56 0.02
InclusvClassrm        0.01 0.01 0.05 0.23 0.70 0.03
DEIintegration        0.00 0.01 0.10 0.22 0.67 0.12
EquitableEval         0.00 0.01 0.03 0.32 0.63 0.01
\end{verbatim}

At the total scale level, \(\alpha = 0.92\); the average inter-item correlations are 0.487; and the corrected item-total correlations (\emph{r.drop}) range from 0.42 to 0.80.

To obtain this data at the subscale level I will first create variable vectors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Valued\_vars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}ValObjectives\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}IncrUnderstanding\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}IncrInterest\textquotesingle{}}\NormalTok{)}
\NormalTok{TradPed\_vars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}ClearResponsibilities\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}EffectiveAnswers\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Feedback\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}ClearOrganization\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}ClearPresentation\textquotesingle{}}\NormalTok{)}
\NormalTok{SCRPed\_vars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}MultPerspectives\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}InclusvClassrm\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}DEIintegration\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}EquitableEval\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

I can insert these variable vectors into the \emph{psych::alpha()} function to obtain the information.

\hypertarget{valued-by-student-subscale}{%
\subsubsection{Valued-by-Student Subscale}\label{valued-by-student-subscale}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(items[,Valued\_vars])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = items[, Valued_vars])

  raw_alpha std.alpha G6(smc) average_r S/N  ase mean   sd median_r
      0.77      0.77    0.71      0.53 3.4 0.02  4.2 0.68     0.48

    95% confidence boundaries 
         lower alpha upper
Feldt     0.72  0.77  0.81
Duhachek  0.73  0.77  0.81

 Reliability if an item is dropped:
                  raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r
ValObjectives          0.80      0.81    0.68      0.68 4.3    0.022    NA
IncrUnderstanding      0.60      0.65    0.48      0.48 1.8    0.040    NA
IncrInterest           0.59      0.61    0.44      0.44 1.6    0.044    NA
                  med.r
ValObjectives      0.68
IncrUnderstanding  0.48
IncrInterest       0.44

 Item statistics 
                    n raw.r std.r r.cor r.drop mean   sd
ValObjectives     309  0.71  0.77  0.55   0.50  4.5 0.61
IncrUnderstanding 309  0.86  0.85  0.76   0.68  4.3 0.82
IncrInterest      308  0.90  0.87  0.79   0.70  3.9 0.99

Non missing response frequency for each item
                     1    2    3    4    5 miss
ValObjectives     0.00 0.01 0.03 0.39 0.57 0.00
IncrUnderstanding 0.01 0.04 0.07 0.44 0.45 0.00
IncrInterest      0.02 0.09 0.14 0.44 0.31 0.01
\end{verbatim}

The alpha for Valued-by-the-Student subscale is 0.77. I will start building my homework table as I go along by adding the corrected item-total correlations (i.e., the \emph{r.drop}) column.

\begin{longtable}[]{@{}l@{}}
\toprule\noalign{}
Item-Total Correlations of Items with their Own and Other Subscales \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\end{longtable}

\begin{longtable}[]{@{}lccc@{}}
\toprule\noalign{}
Variables & Valued & TradPed & SCRPed \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ValObjectives & \textbf{.50} & & \\
IncrUnderstanding & \textbf{.68} & & \\
IncrInterest & \textbf{.70} & & \\
ClearResponsibilities & & & \\
EffectiveAnswers & & & \\
Feedback & & & \\
ClearOrganization & & & \\
ClearPresentation & & & \\
MultPerspectives & & & \\
DEIintegration & & & \\
EquitableEval & & & \\
Feedback & & & \\
\end{longtable}

\hypertarget{traditional-pedagogy-items}{%
\subsubsection{Traditional Pedagogy Items}\label{traditional-pedagogy-items}}

Next I will produce the output for the Traditional Pedagogy subscale.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(items[,TradPed\_vars])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = items[, TradPed_vars])

  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r
      0.89       0.9    0.88      0.64 8.8 0.0094  4.3 0.76     0.65

    95% confidence boundaries 
         lower alpha upper
Feldt     0.87  0.89  0.91
Duhachek  0.88  0.89  0.91

 Reliability if an item is dropped:
                      raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r
ClearResponsibilities      0.86      0.86    0.84      0.62 6.4    0.013 0.0054
EffectiveAnswers           0.87      0.87    0.84      0.63 6.8    0.012 0.0045
Feedback                   0.89      0.89    0.87      0.68 8.4    0.010 0.0016
ClearOrganization          0.88      0.88    0.85      0.64 7.2    0.012 0.0044
ClearPresentation          0.86      0.87    0.83      0.62 6.5    0.013 0.0030
                      med.r
ClearResponsibilities  0.59
EffectiveAnswers       0.65
Feedback               0.69
ClearOrganization      0.66
ClearPresentation      0.62

 Item statistics 
                        n raw.r std.r r.cor r.drop mean   sd
ClearResponsibilities 307  0.87  0.87  0.84   0.79  4.4 0.82
EffectiveAnswers      308  0.84  0.85  0.81   0.76  4.4 0.83
Feedback              304  0.78  0.79  0.70   0.66  4.2 0.88
ClearOrganization     309  0.85  0.83  0.78   0.74  4.0 1.08
ClearPresentation     309  0.87  0.87  0.83   0.78  4.2 0.92

Non missing response frequency for each item
                         1    2    3    4    5 miss
ClearResponsibilities 0.01 0.02 0.07 0.31 0.59 0.01
EffectiveAnswers      0.01 0.02 0.08 0.36 0.53 0.01
Feedback              0.01 0.05 0.10 0.39 0.46 0.02
ClearOrganization     0.04 0.07 0.10 0.41 0.38 0.00
ClearPresentation     0.02 0.05 0.07 0.40 0.46 0.00
\end{verbatim}

Traditional Pedagogy \(\alpha = 0.90\). I retrieve the corrected item-total correlations from the \emph{r.drop} column.

\begin{longtable}[]{@{}l@{}}
\toprule\noalign{}
Item-Total Correlations of Items with their Own and Other Subscales \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\end{longtable}

\begin{longtable}[]{@{}lccc@{}}
\toprule\noalign{}
Variables & Valued & TradPed & SCRPed \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ValObjectives & \textbf{.50} & & \\
IncrUnderstanding & \textbf{.67} & & \\
IncrInterest & \textbf{.70} & & \\
ClearResponsibilities & & \textbf{.79} & \\
EffectiveAnswers & & \textbf{.76} & \\
Feedback & & \textbf{.66} & \\
ClearOrganization & & \textbf{.74} & \\
ClearPresentation & & \textbf{.78} & \\
MultPerspectives & & & \\
DEIintegration & & & \\
EquitableEval & & & \\
Feedback & & & \\
\end{longtable}

\hypertarget{socially-responsive-pedagogy-items}{%
\subsubsection{Socially Responsive Pedagogy Items}\label{socially-responsive-pedagogy-items}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(items[,SCRPed\_vars])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = items[, SCRPed_vars])

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
      0.81      0.81    0.78      0.52 4.3 0.017  4.5 0.58     0.54

    95% confidence boundaries 
         lower alpha upper
Feldt     0.77  0.81  0.84
Duhachek  0.77  0.81  0.84

 Reliability if an item is dropped:
                 raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r
MultPerspectives      0.73      0.74    0.67      0.49 2.8    0.026 0.0153
InclusvClassrm        0.74      0.74    0.67      0.49 2.9    0.025 0.0120
DEIintegration        0.78      0.78    0.71      0.54 3.6    0.021 0.0044
EquitableEval         0.78      0.79    0.73      0.56 3.9    0.021 0.0034
                 med.r
MultPerspectives  0.47
InclusvClassrm    0.50
DEIintegration    0.57
EquitableEval     0.57

 Item statistics 
                   n raw.r std.r r.cor r.drop mean   sd
MultPerspectives 305  0.85  0.83  0.76   0.68  4.4 0.84
InclusvClassrm   301  0.82  0.83  0.76   0.67  4.6 0.68
DEIintegration   273  0.78  0.78  0.67   0.59  4.5 0.74
EquitableEval    308  0.75  0.76  0.64   0.58  4.6 0.63

Non missing response frequency for each item
                    1    2    3    4    5 miss
MultPerspectives 0.02 0.02 0.08 0.33 0.56 0.02
InclusvClassrm   0.01 0.01 0.05 0.23 0.70 0.03
DEIintegration   0.00 0.01 0.10 0.22 0.67 0.12
EquitableEval    0.00 0.01 0.03 0.32 0.63 0.01
\end{verbatim}

Alpha for the SCR Pedagogy dimension is .81

\begin{longtable}[]{@{}l@{}}
\toprule\noalign{}
Item-Total Correlations of Items with their Own and Other Subscales \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\end{longtable}

\begin{longtable}[]{@{}lccc@{}}
\toprule\noalign{}
Variables & Valued & TradPed & SCRPed \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ValObjectives & \textbf{.50} & & \\
IncrUnderstanding & \textbf{.67} & & \\
IncrInterest & \textbf{.70} & & \\
ClearResponsibilities & & \textbf{.79} & \\
EffectiveAnswers & & \textbf{.76} & \\
Feedback & & \textbf{.66} & \\
ClearOrganization & & \textbf{.74} & \\
ClearPresentation & & \textbf{.78} & \\
MultPerspectives & & & \textbf{.68} \\
DEIintegration & & & \textbf{.67} \\
EquitableEval & & & \textbf{.59} \\
Feedback & & & \textbf{.58} \\
\end{longtable}

\hypertarget{produce-and-interpret-correlations-between-the-individual-items-of-a-given-subscale-and-the-subscale-scores-of-all-other-subscales}{%
\subsection{Produce and interpret correlations between the individual items of a given subscale and the subscale scores of all other subscales}\label{produce-and-interpret-correlations-between-the-individual-items-of-a-given-subscale-and-the-subscale-scores-of-all-other-subscales}}

To do this we need to have subscale scores. Conveniently, I can use the variable vectors created in an earlier step. I will score each of the scales if 75\% of the items are non-missing.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{items}\SpecialCharTok{$}\NormalTok{Valued }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(items[,Valued\_vars], .}\DecValTok{75}\NormalTok{)}
\NormalTok{items}\SpecialCharTok{$}\NormalTok{TradPed }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(items[,TradPed\_vars], .}\DecValTok{75}\NormalTok{)}
\NormalTok{items}\SpecialCharTok{$}\NormalTok{SCRPed }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(items[,SCRPed\_vars], .}\DecValTok{75}\NormalTok{)}
\NormalTok{items}\SpecialCharTok{$}\NormalTok{Total }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(items, .}\DecValTok{75}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For each subscale, we can produce an apaTables::apa.cor.table for the items-and-OtherScales. Since there are 3 subscales, this is going to get spicy.

\hypertarget{valued-by-the-student-items}{%
\subsubsection{Valued-by-the-Student Items}\label{valued-by-the-student-items}}

First with the traditional pedagogy total scale score.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(items[}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}ValObjectives\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}IncrUnderstanding\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}IncrInterest\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}TradPed\textquotesingle{}}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Means, standard deviations, and correlations with confidence intervals
 

  Variable             M    SD   1          2          3         
  1. ValObjectives     4.52 0.61                                 
                                                                 
  2. IncrUnderstanding 4.28 0.82 .44**                           
                                 [.34, .52]                      
                                                                 
  3. IncrInterest      3.94 0.99 .48**      .68**                
                                 [.39, .56] [.62, .74]           
                                                                 
  4. TradPed           4.25 0.76 .50**      .62**      .61**     
                                 [.41, .58] [.55, .68] [.54, .68]
                                                                 

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

After each analysis, I can update my table with the data representing the correlations with the individual items of one scale with the total scale score included in the correlation matrix. Our hope is that the corrected item-total correlations that we collected above will be stronger than the individual items' correlations with the other scales' total scores.

\begin{longtable}[]{@{}l@{}}
\toprule\noalign{}
Item-Total Correlations of Items with their Own and Other Subscales \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\end{longtable}

\begin{longtable}[]{@{}lccc@{}}
\toprule\noalign{}
Variables & Valued & TradPed & SCRPed \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ValObjectives & \textbf{.50} & .50 & \\
IncrUnderstanding & \textbf{.67} & .62 & \\
IncrInterest & \textbf{.70} & .61 & \\
ClearResponsibilities & & \textbf{.79} & \\
EffectiveAnswers & & \textbf{.76} & \\
Feedback & & \textbf{.66} & \\
ClearOrganization & & \textbf{.74} & \\
ClearPresentation & & \textbf{.78} & \\
MultPerspectives & & & \textbf{.68} \\
DEIintegration & & & \textbf{.67} \\
EquitableEval & & & \textbf{.59} \\
Feedback & & & \textbf{.58} \\
\end{longtable}

Next, the Valued-by-the-Student Items with socially responsive pedagogy total scale score.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(items[}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}ValObjectives\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}IncrUnderstanding\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}IncrInterest\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}SCRPed\textquotesingle{}}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Means, standard deviations, and correlations with confidence intervals
 

  Variable             M    SD   1          2          3         
  1. ValObjectives     4.52 0.61                                 
                                                                 
  2. IncrUnderstanding 4.28 0.82 .44**                           
                                 [.34, .52]                      
                                                                 
  3. IncrInterest      3.94 0.99 .48**      .68**                
                                 [.39, .56] [.62, .74]           
                                                                 
  4. SCRPed            4.52 0.58 .41**      .44**      .54**     
                                 [.32, .50] [.35, .53] [.45, .61]
                                                                 

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

\begin{longtable}[]{@{}l@{}}
\toprule\noalign{}
Item-Total Correlations of Items with their Own and Other Subscales \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\end{longtable}

\begin{longtable}[]{@{}lccc@{}}
\toprule\noalign{}
Variables & Valued & TradPed & SCRPed \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ValObjectives & \textbf{.50} & .50 & .41 \\
IncrUnderstanding & \textbf{.67} & .62 & .44 \\
IncrInterest & \textbf{.70} & .61 & .54 \\
ClearResponsibilities & & \textbf{.79} & \\
EffectiveAnswers & & \textbf{.76} & \\
Feedback & & \textbf{.66} & \\
ClearOrganization & & \textbf{.74} & \\
ClearPresentation & & \textbf{.78} & \\
MultPerspectives & & & \textbf{.68} \\
DEIintegration & & & \textbf{.67} \\
EquitableEval & & & \textbf{.59} \\
Feedback & & & \textbf{.58} \\
\end{longtable}

\hypertarget{traditional-pedagogy-items-1}{%
\subsection{Traditional Pedagogy Items}\label{traditional-pedagogy-items-1}}

First, the traditional pedagogy items with the valued-by-the-student total scale score.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(items[}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}ClearResponsibilities\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}EffectiveAnswers\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Feedback\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}ClearOrganization\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}ClearPresentation\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Valued\textquotesingle{}}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Means, standard deviations, and correlations with confidence intervals
 

  Variable                 M    SD   1          2          3         
  1. ClearResponsibilities 4.44 0.82                                 
                                                                     
  2. EffectiveAnswers      4.36 0.83 .69**                           
                                     [.63, .74]                      
                                                                     
  3. Feedback              4.24 0.88 .63**      .58**                
                                     [.56, .70] [.50, .65]           
                                                                     
  4. ClearOrganization     4.01 1.08 .67**      .60**      .55**     
                                     [.61, .73] [.52, .67] [.46, .62]
                                                                     
  5. ClearPresentation     4.24 0.92 .69**      .72**      .55**     
                                     [.62, .74] [.66, .77] [.47, .62]
                                                                     
  6. Valued                4.25 0.68 .52**      .59**      .49**     
                                     [.43, .60] [.51, .66] [.40, .57]
                                                                     
  4          5         
                       
                       
                       
                       
                       
                       
                       
                       
                       
                       
                       
  .70**                
  [.63, .75]           
                       
  .63**      .69**     
  [.55, .69] [.63, .75]
                       

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

\begin{longtable}[]{@{}l@{}}
\toprule\noalign{}
Item-Total Correlations of Items with their Own and Other Subscales \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\end{longtable}

\begin{longtable}[]{@{}lccc@{}}
\toprule\noalign{}
Variables & Valued & TradPed & SCRPed \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ValObjectives & \textbf{.50} & .50 & .41 \\
IncrUnderstanding & \textbf{.67} & .62 & .44 \\
IncrInterest & \textbf{.70} & .61 & .54 \\
ClearResponsibilities & .52 & \textbf{.79} & \\
EffectiveAnswers & .59 & \textbf{.76} & \\
Feedback & .49 & \textbf{.66} & \\
ClearOrganization & .63 & \textbf{.74} & \\
ClearPresentation & .69 & \textbf{.78} & \\
MultPerspectives & & & \textbf{.68} \\
DEIintegration & & & \textbf{.67} \\
EquitableEval & & & \textbf{.59} \\
Feedback & & & \textbf{.58} \\
\end{longtable}

Next, the traditional pedagogy items with the socially responsive pedagogy total scale score.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(items[}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}ClearResponsibilities\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}EffectiveAnswers\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Feedback\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}ClearOrganization\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}ClearPresentation\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}SCRPed\textquotesingle{}}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Means, standard deviations, and correlations with confidence intervals
 

  Variable                 M    SD   1          2          3         
  1. ClearResponsibilities 4.44 0.82                                 
                                                                     
  2. EffectiveAnswers      4.36 0.83 .69**                           
                                     [.63, .74]                      
                                                                     
  3. Feedback              4.24 0.88 .63**      .58**                
                                     [.56, .70] [.50, .65]           
                                                                     
  4. ClearOrganization     4.01 1.08 .67**      .60**      .55**     
                                     [.61, .73] [.52, .67] [.46, .62]
                                                                     
  5. ClearPresentation     4.24 0.92 .69**      .72**      .55**     
                                     [.62, .74] [.66, .77] [.47, .62]
                                                                     
  6. SCRPed                4.52 0.58 .64**      .60**      .62**     
                                     [.56, .70] [.52, .67] [.55, .69]
                                                                     
  4          5         
                       
                       
                       
                       
                       
                       
                       
                       
                       
                       
                       
  .70**                
  [.63, .75]           
                       
  .51**      .62**     
  [.43, .59] [.54, .68]
                       

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

\begin{longtable}[]{@{}l@{}}
\toprule\noalign{}
Item-Total Correlations of Items with their Own and Other Subscales \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\end{longtable}

\begin{longtable}[]{@{}lccc@{}}
\toprule\noalign{}
Variables & Valued & TradPed & SCRPed \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ValObjectives & \textbf{.50} & .50 & .41 \\
IncrUnderstanding & \textbf{.67} & .62 & .44 \\
IncrInterest & \textbf{.70} & .61 & .54 \\
ClearResponsibilities & .52 & \textbf{.79} & .64 \\
EffectiveAnswers & .59 & \textbf{.76} & .60 \\
Feedback & .49 & \textbf{.66} & .62 \\
ClearOrganization & .63 & \textbf{.74} & .51 \\
ClearPresentation & .69 & \textbf{.78} & .62 \\
MultPerspectives & & & \textbf{.68} \\
DEIintegration & & & \textbf{.67} \\
EquitableEval & & & \textbf{.59} \\
Feedback & & & \textbf{.58} \\
\end{longtable}

\hypertarget{socially-responsive-pedagogy-items-1}{%
\subsubsection{Socially Responsive Pedagogy Items}\label{socially-responsive-pedagogy-items-1}}

First, the socially responsive pedagogy items with the valued-by-the-student total scale score.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(items[}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}MultPerspectives\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}InclusvClassrm\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}DEIintegration\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}EquitableEval\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Valued\textquotesingle{}}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Means, standard deviations, and correlations with confidence intervals
 

  Variable            M    SD   1          2          3          4         
  1. MultPerspectives 4.39 0.84                                            
                                                                           
  2. InclusvClassrm   4.61 0.68 .57**                                      
                                [.49, .64]                                 
                                                                           
  3. DEIintegration   4.53 0.74 .50**      .62**                           
                                [.41, .59] [.54, .69]                      
                                                                           
  4. EquitableEval    4.57 0.63 .59**      .47**      .37**                
                                [.51, .66] [.37, .55] [.27, .47]           
                                                                           
  5. Valued           4.25 0.68 .54**      .48**      .29**      .46**     
                                [.45, .61] [.38, .56] [.18, .40] [.37, .54]
                                                                           

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

\begin{longtable}[]{@{}l@{}}
\toprule\noalign{}
Item-Total Correlations of Items with their Own and Other Subscales \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\end{longtable}

\begin{longtable}[]{@{}lccc@{}}
\toprule\noalign{}
Variables & Valued & TradPed & SCRPed \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ValObjectives & \textbf{.50} & .50 & .41 \\
IncrUnderstanding & \textbf{.67} & .62 & .44 \\
IncrInterest & \textbf{.70} & .61 & .54 \\
ClearResponsibilities & .52 & \textbf{.79} & .64 \\
EffectiveAnswers & .59 & \textbf{.76} & .60 \\
Feedback & .49 & \textbf{.66} & .62 \\
ClearOrganization & .63 & \textbf{.74} & .51 \\
ClearPresentation & .69 & \textbf{.78} & .62 \\
MultPerspectives & .54 & & \textbf{.68} \\
DEIintegration & .48 & & \textbf{.67} \\
EquitableEval & .29 & & \textbf{.59} \\
Feedback & .46 & & \textbf{.58} \\
\end{longtable}

Next, the socially responsive pedagogy items with the traditional pedagogy total scale score.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(items[}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}MultPerspectives\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}InclusvClassrm\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}DEIintegration\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}EquitableEval\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}TradPed\textquotesingle{}}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Means, standard deviations, and correlations with confidence intervals
 

  Variable            M    SD   1          2          3          4         
  1. MultPerspectives 4.39 0.84                                            
                                                                           
  2. InclusvClassrm   4.61 0.68 .57**                                      
                                [.49, .64]                                 
                                                                           
  3. DEIintegration   4.53 0.74 .50**      .62**                           
                                [.41, .59] [.54, .69]                      
                                                                           
  4. EquitableEval    4.57 0.63 .59**      .47**      .37**                
                                [.51, .66] [.37, .55] [.27, .47]           
                                                                           
  5. TradPed          4.25 0.76 .71**      .54**      .34**      .65**     
                                [.64, .76] [.46, .62] [.23, .44] [.58, .71]
                                                                           

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

\begin{longtable}[]{@{}l@{}}
\toprule\noalign{}
Item-Total Correlations of Items with their Own and Other Subscales \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\end{longtable}

\begin{longtable}[]{@{}lccc@{}}
\toprule\noalign{}
Variables & Valued & TradPed & SCRPed \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ValObjectives & \textbf{.50} & .50 & .41 \\
IncrUnderstanding & \textbf{.67} & .62 & .44 \\
IncrInterest & \textbf{.70} & .61 & .54 \\
ClearResponsibilities & .52 & \textbf{.79} & .64 \\
EffectiveAnswers & .59 & \textbf{.76} & .60 \\
Feedback & .49 & \textbf{.66} & .62 \\
ClearOrganization & .63 & \textbf{.74} & .51 \\
ClearPresentation & .69 & \textbf{.78} & .62 \\
MultPerspectives & .54 & .71 & \textbf{.68} \\
InclusvClassrm & .48 & .54 & \textbf{.67} \\
DEIintegration & .29 & .34 & \textbf{.59} \\
EquitableEval & .46 & .65 & \textbf{.58} \\
\end{longtable}

\hypertarget{apa-style-results-section-with-table}{%
\subsection{APA style results section with table}\label{apa-style-results-section-with-table}}

\begin{itemize}
\tightlist
\item
  Brief description of each step
\item
  Brief instructions for interpreting results
\item
  Presentation of results
\end{itemize}

\begin{quote}
Item analyses were conducted on the 12 course evaluation items that were selected to represent the Valued-by-the-Student, Traditional Pedagogy, and Socially Responsive Pedagogy subscales. To assess the within-scale convergent and discriminant validity of these three subscales, each item was correlated with its own scale (with the item removed) and with the other course evaluation scales (see Table 1). For the Valued and Traditional Pedagogy dimensions, items were more highly correlated with their own scale than with the other scale. For the SCRPed subscale, two items (multiple perspectives, equitable evaluations) had higher correlations with Traditional Pedagogy than with their hypothesized subscale (Socially Responsive Pedagogy). Coefficient alphas were .77, .90, .81, and .92 for the Valued-by-the-Student, Traditional Pedagogy, Socially Responsive, and total-scale scores, respectively. We conclude that more work is needed to create distinct and stable subscales.
\end{quote}

\hypertarget{explanation-to-grader}{%
\subsection{Explanation to grader}\label{explanation-to-grader}}

\hypertarget{exploratory-factor-analysis-1}{%
\chapter*{\texorpdfstring{EXPLORATORY \emph{FACTOR} ANALYSIS}{EXPLORATORY FACTOR ANALYSIS}}\label{exploratory-factor-analysis-1}}


The next two lessons are devoted to exploratory \emph{factor} analysis. The two approaches are principal components analysis (PCA) and principal axis factoring (PAF). In truth, only PAF is considered \emph{factor} analysis. I will explain why in the lesson.

These approaches are loosely termed \emph{exploratory} because the statistical process (not the researcher) produces the factor (think scale or subscale) and identifies which items belong to it. This is contrasted with \emph{confirmatory} approaches (which use structural equation modeling) where the researcher assigns items to factors and analyzes the goodness of fit.

\hypertarget{PCA}{%
\chapter{Principal Components Analysis}\label{PCA}}

\href{https://youtube.com/playlist?list=PLtz5cFLQl4KNzmApvhfVE7gNpZOQL1OVR\&si=F65ij19VytmImI4Z}{Screencasted Lecture Link}

In this lesson on principal components analysis (PCA) I provide an introduction to the exploratory factor analysis (EFA) arena. We will review the theoretical and technical aspects of PCA, we will work through a research vignette, and then consider the relationship of PCA to item analysis and reliability coefficients.

Please note, although PCA is frequently grouped into EFA techniques, it is \emph{exploratory}, but it is not \emph{factor analysis}. We'll discuss the difference in the lecture.

\hypertarget{navigating-this-lesson-6}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-6}}

There are about two hours of lecture. If you work through the materials with me, I would be plan for an additional hour-and-a-half.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-6}{%
\subsection{Learning Objectives}\label{learning-objectives-6}}

Focusing on this week's materials, make sure you can:

\begin{itemize}
\tightlist
\item
  Distinguish between PCA and PAF on several levels:

  \begin{itemize}
  \tightlist
  \item
    which path diagram represents each best, and
  \item
    keywords associated with each: factor loadings, linear components, describe versus explain.
  \end{itemize}
\item
  Recognize/define an identity matrix -- what test would you use to diagnose it?
\item
  Recognize/define multicollinearity and singularity -- what test would you use to diagnose it?
\item
  Describe the pattern of ``loadings'' (i.e., the relative weights of an item on its own scale compared to other scales)that supports the structure of the instrument.
\item
  Compare the results from item analysis and PCA.
\end{itemize}

\hypertarget{planning-for-practice-6}{%
\subsection{Planning for Practice}\label{planning-for-practice-6}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. The least complex is to change the random seed in the research and rework the problem demonstrated in the lesson. The results \emph{should} map onto the ones obtained in the lecture.

The second option involves utilizing one of the simulated datasets available in this OER. The \protect\hyperlink{sims}{last lesson} in the OER contains three simulations that could be used for all of the statistics-based practice suggestions. Especially if you started with one of these examples in an earlier lesson, I highly recommend you continue with that.

Alternatively, Keum et al.'s Gendered Racial Microaggressions Scale for Asian American Women \citeyearpar{keum_gendered_2018} will be used in the lessons on confirmatory factor analysis and Conover et al.'s \citeyearpar{conover_development_2017} Ableist Microaggressions Scale is used in the lesson on invariance testing. Both of these would be suitable for the PCA and PAF homework assignments.

As a third option, you are welcome to use data to which you have access and is suitable for PCA. These could include other vignettes from this OER, other simulated data, or your own data (presuming you have permission to use it). In any case, please plan to:

\begin{itemize}
\tightlist
\item
  Properly format and prepare the data.
\item
  Conduct diagnostic tests to determine the suitability of the data for PCA.
\item
  Conducting tests to guide the decisions about number of components to extract.
\item
  Conducting orthogonal and oblique extractions (at least two each with different numbers of components).
\item
  Selecting one solution and preparing an APA style results section (with table and figure).
\item
  Compare your results in light of any other psychometrics lessons where you have used this data.
\end{itemize}

\hypertarget{readings-resources-6}{%
\subsection{Readings \& Resources}\label{readings-resources-6}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Revelle, William. (n.d.). Chapter 6: Constructs, components, and factor models. In \emph{An introduction to psychometric theory with applications in R}. Retrieved from \url{https://personality-project.org/r/book/\#chapter6}

  \begin{itemize}
  \tightlist
  \item
    pp.~145 to 150 (we'll continue with the rest in the next lecture). Stop at ``6.2 Exploratory Factor Analysis.''
  \item
    A simultaneously theoretical review of psychometric theory while working with R and data to understand the concepts.
  \end{itemize}
\item
  Revelle, W. (2019). \emph{How To: Use the psych package for Factor Analysis and data reduction}.

  \begin{itemize}
  \tightlist
  \item
    pp.~13 through 24 provide technical information about what we are doing
  \end{itemize}
\item
  Dekay, Nicole (2021). Quick Reference Guide: The statistics for psychometrics \url{https://www.humanalysts.com/quick-reference-guide-the-statistics-for-psychometrics}
\item
  Lewis, J. A., \& Neville, H. A. (2015). Construction and initial validation of the Gendered Racial Microaggressions Scale for Black Women. \emph{Journal of Counseling Psychology, 62}(2), 289--302. \url{https://doi.org/10.1037/cou0000062}

  \begin{itemize}
  \tightlist
  \item
    Our research vignette for this lesson.
  \end{itemize}
\end{itemize}

\hypertarget{packages-6}{%
\subsection{Packages}\label{packages-6}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# will install the package if not already installed}
\CommentTok{\# if(!require(psych))\{install.packages(\textquotesingle{}psych\textquotesingle{})\}}
\CommentTok{\# if(!require(tidyverse))\{install.packages(\textquotesingle{}tidyverse\textquotesingle{})\}}
\CommentTok{\# if(!require(MASS))\{install.packages(\textquotesingle{}MASS\textquotesingle{})\}}
\CommentTok{\# if(!require(sjstats))\{install.packages(\textquotesingle{}sjstats\textquotesingle{})\}}
\CommentTok{\# if(!require(apaTables))\{install.packages(\textquotesingle{}apaTables\textquotesingle{})\}}
\CommentTok{\# if(!require(qualtRics))\{install.packages(\textquotesingle{}qualtRics\textquotesingle{})\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{exploratory-principal-components-analysis}{%
\section{Exploratory Principal Components Analysis}\label{exploratory-principal-components-analysis}}

The psychometric version of \emph{parsimony} is seen in our attempt to \emph{describe} (components) or to \emph{explain} (factors) in the relationships between many observed variables in terms of a more limited set of components, latent factors, or dimensions.

That is, we are trying to:

\begin{itemize}
\tightlist
\item
  understand the structure of a set of variables,
\item
  construct a questionnaire to measure an underlying latent variable, and
\item
  reduce a data set to a more manageable size (e.g., representing bundles of items as subscale scores) while retaining as much of the information as possible
\end{itemize}

\hypertarget{some-framing-ideas-in-very-lay-terms}{%
\subsection{Some Framing Ideas (in very lay terms)}\label{some-framing-ideas-in-very-lay-terms}}

\emph{Exploratory} versus \emph{confirmatory} factor analysis.

\begin{itemize}
\item
  Both exploratory and confirmatory approaches to components/factor analysis are used in scale construction. Think of ``scales'' as being interchangeable with ``factors'' and ``components.''

  \begin{itemize}
  \tightlist
  \item
    That said, ``factors'' and ``components'' are not interchangeable terms.
  \end{itemize}
\item
  \textbf{Exploratory}: Even though we may have an a priori model in mind, we \emph{explore} the structure of the items by using diagnostics (KMO, Barlett's, determinant), factor extraction, and rotation to determine the number of scales (i.e., components or factors) that exist within the raw data or correlation matrix. The algorithms (including matrix algebra) determine the relationship of each item to its respective scales (i.e., components or factors).
\item
  \textbf{Confirmatory}: Starting with an a priori theory, we specify the structure (i.e., number and levels of factors) and which items belong to factors. We use structural equation modeling as the framework. We evaluate the quality of the model with a number of fit indices.
\end{itemize}

Within the \emph{exploratory} category we will focus on two further distinctions (there are even more). The first is principal components analysis (PCA). The second is principal axis factoring (PAF). PAF is one of the approaches that is commonly termed ``exploratory factor analysis'' (EFA). In this first lesson we focus on the differences between PCA and EFA.

\begin{itemize}
\item
  \textbf{Option \#1/Component model}: PCA approximates the correlation matrix in terms of the product of components where each is a weighted linear sum of the variables. In the figure below, note how the arrows in the components analysis (a \emph{path} model) point from variables to the component. Perhaps an oversimplification, think of each of these as a predictor variable contributing to an outcome.
\item
  \textbf{Option \#2/Factor model}: EFA (and in the next lesson, PAF/principal axis factoring) approximates the correlation matrix by the product of the two factors; this approach presumes that the factors are the causes (rather than as consequences). In the figure below, note how the arrows in the factor analysis model (a \emph{structural} model) point from latent variable (or factor) to the observed variables (items). Factor analysis has been termed \emph{causal modeling} because the latent variables are theorized to cause the responses to the individual items. There are other popular approaches, including parallel analysis (which is what the authors used in this lesson's research vignette).
\end{itemize}

Well-crafted figures provide important clues to the analyses. In structural models, rectangles and squares indicate the presence of \emph{observed} (also called \emph{manifest}) variables. These are variables that have a column in the dataset. In our particular case, they are the responses to the 25 items in the GRMS.

Circles or ovals represent latent variables or factors. These were never raw data, but are composed of the relations of variables that were collected. They are more complex than mean or sum scores. Rather, they represent what the variables that represent them share in common.

\begin{figure}
\centering
\includegraphics{images/PCA/PCAvPAF.png}
\caption{Comparison of path models for PCA and EFA for our research vignette}
\end{figure}

Our focus today is on the principal component analysis (PCA) approach to scale construction.

\hypertarget{pca-workflow}{%
\section{PCA Workflow}\label{pca-workflow}}

Below is a screenshot of the workflow. The original document is located in the \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub site} that hosts the ReCentering Psych Stats: Psychometrics OER.

\begin{figure}
\centering
\includegraphics{images/PCA/PCAworkflow.png}
\caption{Image of the workflow for PCA}
\end{figure}

Steps in the process include:

\begin{itemize}
\tightlist
\item
  Creating an \emph{items only} dataframe where any items are scaled in the same direction (e.g., negatively worded items are reverse scored).
\item
  Conducting tests that assess the statistical assumptions of PCA to ensure that the data is appropriate for PCA.
\item
  Determining the number of components (think ``subscales'') to extract.
\item
  Conducting the component extraction -- this process will likely occur iteratively,

  \begin{itemize}
  \tightlist
  \item
    exploring orthogonal (uncorrelated/independent) and oblique (correlated)components, and
  \item
    changing the number of components to extract
  \end{itemize}
\end{itemize}

Because the intended audience for the ReCentering Psych Stats OER is the scientist-practitioner-advocate, this lesson focuses on the workflow and decisions. As you might guess, the details of PCA can be quite complex. Some important notions to consider that may not be obvious from lesson, are these:

\begin{itemize}
\tightlist
\item
  The values of component loadings are directly related to the correlation (similarly, the covariance) matrix between the items.

  \begin{itemize}
  \tightlist
  \item
    Although I do not explain this in detail, nearly every analytic step attempts to convey this notion by presenting equivalent analytic options using the raw data and correlation matrix.
  \end{itemize}
\item
  PCA is about \emph{dimension reduction} -- our goal is fewer components (i.e., subscales) than there are items.

  \begin{itemize}
  \tightlist
  \item
    In this lesson's vignette there are 25 items on the scale, and we will end up with 4 subscales.
  \end{itemize}
\item
  Principal component analysis is \emph{exploratory}, but it is not ``factor analysis.''
\item
  Matrix algebra (e.g., using the transpose of a matrix, multiplying matrices together) plays a critical role in the analytic solution.
\end{itemize}

\hypertarget{research-vignette-5}{%
\section{Research Vignette}\label{research-vignette-5}}

This lesson's research vignette emerges from Lewis and Neville's Gendered Racial Microaggressions Scale for Black Women \citeyearpar{lewis_construction_2015}. The article reports on two separate studies that comprised the development, refinement, and psychometric evaluation of two parallel versions (stress appraisal, frequency) of the scale. Below, I simulate data from the final construction of the stress appraisal version as the basis of the lecture. Items were on a 6-point Likert scale ranging from 0 (\emph{not at all stressful}) to 5 (\emph{extremely stressful}).

Lewis and Neville \citeyearpar{lewis_construction_2015} reported support for a total scale score (25 items) and four subscales. Below, I list the four subscales along with the items and their abbreviation. At the outset, let me provide a content advisory. For those who hold this particular identity (or related identities) the content in the items may be upsetting. In other lessons, I often provide a variable name that gives an indication of the primary content of the item. In the case of the GRMS, I will simply provide an abbreviation of the subscale name and its respective item number. This will allow us to easily inspect the alignment of the item with its intended factor, and hopefully minimize discomfort.

If you are not a member of this particular identity, I encourage you to learn about these microaggressions by reading the article in its entirety. Please do not ask members of this group to explain why these microaggressions are harmful or ask if they have encountered them. The four factors, number of items, and sample item are as follows:

\begin{itemize}
\tightlist
\item
  Assumptions of Beauty and Sexual Objectification (10 items)

  \begin{itemize}
  \tightlist
  \item
    Unattractive because of size of butt (Obj1)
  \item
    Negative comments about size of facial features (Obj2)
  \item
    Imitated the way they think Black women speak (Obj3)
  \item
    Someone made me feel unattractive (Obj4)
  \item
    Negative comment about skin tone (Obj5)
  \item
    Someone assumed I speak a certain way (Obj6)
  \item
    Objectified me based on physical features(Obj7)
  \item
    Someone assumed I have a certain body type (Obj8; stress only)
  \item
    Made a sexually inappropriate comment (Obj9)
  \item
    Negative comments about my hair when natural (Obj10)
  \item
    Assumed I was sexually promiscuous (frequency only; not used in this simulation)
  \end{itemize}
\item
  Silenced and Marginalized (7 items)

  \begin{itemize}
  \tightlist
  \item
    I have felt unheard (Marg1)
  \item
    My comments have been ignored (Marg2)
  \item
    Someone challenged my authority (Marg3)
  \item
    I have been disrespected in workplace (Marg4)
  \item
    Someone has tried to ``put me in my place'' (Marg5)
  \item
    Felt excluded from networking opportunities (Marg6)
  \item
    Assumed I did not have much to contribute to the conversation (Marg7)
  \end{itemize}
\item
  Strong Black Woman Stereotype (5 items)

  \begin{itemize}
  \tightlist
  \item
    Someone assumed I was sassy and straightforward (Str1; stress only)
  \item
    I have been told that I am too independent (Str2)
  \item
    Someone made me feel exotic as a Black woman (Str2; stress only)
  \item
    I have been told that I am too assertive
  \item
    Assumed to be a strong Black woman
  \end{itemize}
\item
  Angry Black Woman Stereotype (3 items)

  \begin{itemize}
  \tightlist
  \item
    Someone has told me to calm down (Ang1)
  \item
    Perceived to be ``angry Black woman'' (Ang2)
  \item
    Someone accused me of being angry when speaking calm (Ang3)
  \end{itemize}
\end{itemize}

Three additional scales were reported in the Lewis and Neville article \citeyearpar{lewis_construction_2015}. Because (a) the focus of this lesson is on exploratory factor analytic approaches and, therefore, only requires item-level data for the scale, and (b) the article does not include correlations between the subscales/scales of all involved measures, I only simulated item-level data for the GRMS items.

Below, I walk through the data simulation. This is not an essential portion of the lesson, but I will lecture it in case you are interested. None of the items are negatively worded (relative to the other items), so there is no need to reverse-score any items.

Simulating the data involved using factor loadings, means, standard deviations, and correlations between the scales. Because the simulation will produce ``out-of-bounds'' values, the code below rescales the scores into the range of the Likert-type scaling and rounds them to whole values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Entering the intercorrelations means and standard deviations from}
\CommentTok{\# the journal article}

\NormalTok{LewisGRMS\_generating\_model }\OtherTok{\textless{}{-}} \StringTok{"}
\StringTok{        \#measurement model}
\StringTok{        Objectification =\textasciitilde{} .69*Obj1 + .69*Obj2 + .60*Obj3 + .59*Obj4 + .55*Obj5 + .55*Obj6 + .54*Obj7 + .50*Obj8 + .41*Obj9 + .41*Obj10}
\StringTok{        Marginalized =\textasciitilde{} .93*Marg1 + .81*Marg2 +.69*Marg3 + .67*Marg4 + .61*Marg5 + .58*Marg6 +.54*Marg7}
\StringTok{        Strong =\textasciitilde{} .59*Str1 + .55*Str2 + .54*Str3 + .54*Str4 + .51*Str5}
\StringTok{        Angry =\textasciitilde{} .70*Ang1 + .69*Ang2 + .68*Ang3}
\StringTok{        }
\StringTok{        \#Means}
\StringTok{         Objectification \textasciitilde{} 1.85*1}
\StringTok{         Marginalized \textasciitilde{} 2.67*1}
\StringTok{         Strong \textasciitilde{} 1.61*1}
\StringTok{         Angry \textasciitilde{} 2.29*1}
\StringTok{         }
\StringTok{        \#Correlations}
\StringTok{         Objectification \textasciitilde{}\textasciitilde{} .63*Marginalized}
\StringTok{         Objectification \textasciitilde{}\textasciitilde{} .66*Strong}
\StringTok{         Objectification \textasciitilde{}\textasciitilde{} .51*Angry}
\StringTok{         }
\StringTok{         Marginalized \textasciitilde{}\textasciitilde{} .59*Strong}
\StringTok{         Marginalized \textasciitilde{}\textasciitilde{} .62*Angry}

\StringTok{         Strong \textasciitilde{}\textasciitilde{} .61*Angry}
\StringTok{ }
\StringTok{        "}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{dfGRMS }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{simulateData}\NormalTok{(}\AttributeTok{model =}\NormalTok{ LewisGRMS\_generating\_model, }\AttributeTok{model.type =} \StringTok{"sem"}\NormalTok{,}
    \AttributeTok{meanstructure =}\NormalTok{ T, }\AttributeTok{sample.nobs =} \DecValTok{259}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# used to retrieve column indices used in the rescaling script below}
\NormalTok{col\_index }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(dfGRMS))}

\CommentTok{\# The code below loops through each column of the dataframe and}
\CommentTok{\# assigns the scaling accordingly Rows 1 thru 26 are the GRMS items}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(dfGRMS)) \{}
    \ControlFlowTok{if}\NormalTok{ (i }\SpecialCharTok{\textgreater{}=} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{25}\NormalTok{) \{}
\NormalTok{        dfGRMS[, i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfGRMS[, i], }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{    \}}
\NormalTok{\}}

\CommentTok{\# rounding to integers so that the data resembles that which was}
\CommentTok{\# collected}
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{dfGRMS }\OtherTok{\textless{}{-}}\NormalTok{ dfGRMS }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{round}\NormalTok{(}\DecValTok{0}\NormalTok{)}

\CommentTok{\# quick check of my work psych::describe(dfGRMS)}
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either an .rds object (preserves any formatting you might do) or a .csv file (think ``Excel lite'').

An .rds file preserves all formatting to variables prior to the export and re-import. For the purpose of this chapter, you don't need to do either. That is, you can re-simulate the data each time you work the problem.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(dfGRMS, \textquotesingle{}dfGRMS.rds\textquotesingle{}) bring back the simulated dat}
\CommentTok{\# from an .rds file dfGRMS \textless{}{-} readRDS(\textquotesingle{}dfGRMS.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

If you save the .csv file and bring it back in, you will lose any formatting (e.g., ordered factors will be interpreted as character variables).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(dfGRMS,}
\CommentTok{\# file=\textquotesingle{}dfGRMS.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE) bring}
\CommentTok{\# back the simulated dat from a .csv file dfGRMS \textless{}{-} read.csv}
\CommentTok{\# (\textquotesingle{}dfGRMS.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

Before moving on, I want to acknowledge that (at their first drafting), I try to select research vignettes that have been published within the prior 5 years. With a publication date of 2015, this article clearly falls outside that range. I have continued to include it because (a) the scholarship is superior -- especially as the measure captures an intersectional identity, (b) the article has been a model for research that follows (e.g., Keum et al's \citeyearpar{keum_gendered_2018} Gendered Racial Microaggression Scale for Asian American Women), and (c) there is often a time lag between the initial publication of a psychometric scale and its use. A key reason I have retained the GRMS as a psychometrics research vignette is that in \href{https://lhbikos.github.io/ReC_MultivModel/}{ReCentering Psych Stats: Multivariate Modeling}, GRMS scales are used in a couple of more recently published research vignettes.

\hypertarget{working-the-vignette}{%
\section{Working the Vignette}\label{working-the-vignette}}

Below we will create a correlation matrix of our items. Whether we are conducting PCA or PAF, the \emph{dimension-reduction} we are looking for clusters of correlated items in the \(R\)-matrix. Essentially, these are \citep{field_discovering_2012}:

\begin{itemize}
\tightlist
\item
  Statistical entities that can be plotted as classification axes where coordinates of variables along each axis represent the strength of the relationship between that variable to each component (later, factor).
\item
  Mathematical equations, resembling regression equations, where each variable is represented according to its relative weight.
\end{itemize}

PCA in particular establishes which linear components exist within the data and how a particular variable might contribute to that component.

Below is the correlation matrix of our items. I have saved it as an object so that I can show you that PCA (and later, PAF) can also be conducted with just correlation data. It would be quite a daunting exercise to visually inspect this and manually cluster the correlations of items.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GRMSmatrix }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(dfGRMS)  }\CommentTok{\#correlation matrix created and saved as object}
\FunctionTok{round}\NormalTok{(GRMSmatrix, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      Obj1 Obj2 Obj3 Obj4 Obj5 Obj6 Obj7  Obj8 Obj9 Obj10 Marg1 Marg2 Marg3
Obj1  1.00 0.35 0.25 0.27 0.28 0.25 0.28  0.35 0.15  0.24  0.19  0.25  0.17
Obj2  0.35 1.00 0.31 0.25 0.27 0.23 0.31  0.28 0.26  0.24  0.22  0.21  0.25
Obj3  0.25 0.31 1.00 0.24 0.28 0.28 0.20  0.25 0.21  0.22  0.17  0.23  0.17
Obj4  0.27 0.25 0.24 1.00 0.39 0.23 0.28  0.30 0.26  0.28  0.22  0.18  0.14
Obj5  0.28 0.27 0.28 0.39 1.00 0.15 0.18  0.29 0.25  0.20  0.17  0.20  0.23
Obj6  0.25 0.23 0.28 0.23 0.15 1.00 0.20  0.14 0.21  0.12  0.10  0.14  0.05
Obj7  0.28 0.31 0.20 0.28 0.18 0.20 1.00  0.31 0.19  0.28  0.30  0.21  0.20
Obj8  0.35 0.28 0.25 0.30 0.29 0.14 0.31  1.00 0.19  0.23  0.27  0.14  0.14
Obj9  0.15 0.26 0.21 0.26 0.25 0.21 0.19  0.19 1.00  0.20  0.10  0.12  0.21
Obj10 0.24 0.24 0.22 0.28 0.20 0.12 0.28  0.23 0.20  1.00  0.09  0.12  0.17
Marg1 0.19 0.22 0.17 0.22 0.17 0.10 0.30  0.27 0.10  0.09  1.00  0.43  0.41
Marg2 0.25 0.21 0.23 0.18 0.20 0.14 0.21  0.14 0.12  0.12  0.43  1.00  0.35
Marg3 0.17 0.25 0.17 0.14 0.23 0.05 0.20  0.14 0.21  0.17  0.41  0.35  1.00
Marg4 0.19 0.18 0.24 0.26 0.20 0.10 0.25  0.24 0.07  0.12  0.38  0.23  0.32
Marg5 0.17 0.22 0.21 0.27 0.25 0.16 0.23  0.19 0.19  0.11  0.41  0.40  0.25
Marg6 0.18 0.27 0.16 0.23 0.22 0.26 0.28  0.26 0.15  0.26  0.35  0.27  0.25
Marg7 0.13 0.19 0.14 0.19 0.06 0.17 0.16  0.14 0.10  0.11  0.31  0.33  0.20
Str1  0.22 0.18 0.14 0.06 0.23 0.07 0.25  0.17 0.19  0.10  0.19  0.25  0.20
Str2  0.19 0.18 0.19 0.19 0.12 0.15 0.13  0.06 0.18  0.19  0.12  0.18  0.17
Str3  0.10 0.09 0.09 0.08 0.11 0.09 0.19  0.05 0.12  0.10  0.13  0.18  0.10
Str4  0.09 0.14 0.18 0.15 0.12 0.08 0.07  0.13 0.05  0.02  0.08  0.12  0.08
Str5  0.20 0.15 0.15 0.08 0.19 0.11 0.15  0.04 0.07  0.09  0.10  0.23  0.12
Ang1  0.06 0.07 0.07 0.09 0.12 0.04 0.15  0.07 0.17  0.06  0.16  0.23  0.18
Ang2  0.06 0.15 0.08 0.06 0.09 0.20 0.13 -0.03 0.00  0.14  0.17  0.19  0.19
Ang3  0.21 0.13 0.11 0.14 0.11 0.16 0.23  0.07 0.06  0.08  0.28  0.28  0.11
      Marg4 Marg5 Marg6 Marg7 Str1 Str2 Str3 Str4 Str5 Ang1  Ang2 Ang3
Obj1   0.19  0.17  0.18  0.13 0.22 0.19 0.10 0.09 0.20 0.06  0.06 0.21
Obj2   0.18  0.22  0.27  0.19 0.18 0.18 0.09 0.14 0.15 0.07  0.15 0.13
Obj3   0.24  0.21  0.16  0.14 0.14 0.19 0.09 0.18 0.15 0.07  0.08 0.11
Obj4   0.26  0.27  0.23  0.19 0.06 0.19 0.08 0.15 0.08 0.09  0.06 0.14
Obj5   0.20  0.25  0.22  0.06 0.23 0.12 0.11 0.12 0.19 0.12  0.09 0.11
Obj6   0.10  0.16  0.26  0.17 0.07 0.15 0.09 0.08 0.11 0.04  0.20 0.16
Obj7   0.25  0.23  0.28  0.16 0.25 0.13 0.19 0.07 0.15 0.15  0.13 0.23
Obj8   0.24  0.19  0.26  0.14 0.17 0.06 0.05 0.13 0.04 0.07 -0.03 0.07
Obj9   0.07  0.19  0.15  0.10 0.19 0.18 0.12 0.05 0.07 0.17  0.00 0.06
Obj10  0.12  0.11  0.26  0.11 0.10 0.19 0.10 0.02 0.09 0.06  0.14 0.08
Marg1  0.38  0.41  0.35  0.31 0.19 0.12 0.13 0.08 0.10 0.16  0.17 0.28
Marg2  0.23  0.40  0.27  0.33 0.25 0.18 0.18 0.12 0.23 0.23  0.19 0.28
Marg3  0.32  0.25  0.25  0.20 0.20 0.17 0.10 0.08 0.12 0.18  0.19 0.11
Marg4  1.00  0.30  0.26  0.16 0.10 0.21 0.05 0.06 0.03 0.12  0.22 0.17
Marg5  0.30  1.00  0.29  0.28 0.16 0.13 0.16 0.14 0.18 0.12  0.14 0.21
Marg6  0.26  0.29  1.00  0.20 0.13 0.18 0.15 0.13 0.08 0.11  0.21 0.12
Marg7  0.16  0.28  0.20  1.00 0.14 0.05 0.04 0.02 0.12 0.17  0.13 0.09
Str1   0.10  0.16  0.13  0.14 1.00 0.21 0.30 0.23 0.23 0.18  0.05 0.10
Str2   0.21  0.13  0.18  0.05 0.21 1.00 0.20 0.20 0.12 0.16  0.12 0.16
Str3   0.05  0.16  0.15  0.04 0.30 0.20 1.00 0.27 0.18 0.20  0.07 0.15
Str4   0.06  0.14  0.13  0.02 0.23 0.20 0.27 1.00 0.12 0.15  0.03 0.02
Str5   0.03  0.18  0.08  0.12 0.23 0.12 0.18 0.12 1.00 0.22  0.15 0.11
Ang1   0.12  0.12  0.11  0.17 0.18 0.16 0.20 0.15 0.22 1.00  0.24 0.23
Ang2   0.22  0.14  0.21  0.13 0.05 0.12 0.07 0.03 0.15 0.24  1.00 0.25
Ang3   0.17  0.21  0.12  0.09 0.10 0.16 0.15 0.02 0.11 0.23  0.25 1.00
\end{verbatim}

This correlation matrix is so big that you might wish to write code so that you can examine it in sections.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# round(GRMSmatrix[,1:8], 2) round(GRMSmatrix[,9:16], 2)}
\CommentTok{\# round(GRMSmatrix[,17:25], 2)}
\end{Highlighting}
\end{Shaded}

With component and factor analytic procedures we can analyze the data with either raw data or correlation matrix. Using the correlation matrix helps us perceive how this is a \emph{structural} analysis. That is, we are trying to see if our more parsimonious extraction (i.e., our \emph{dimension reduction}) reproduces this original correlation matrix. In each of the analyses I will include code for running the analyses with raw data and the \emph{r}-matrix.

\hypertarget{three-diagnostic-tests-to-evaluate-the-appropriateness-of-the-data-for-component-or-factor-analysis}{%
\subsection{Three Diagnostic Tests to Evaluate the Appropriateness of the Data for Component-or-Factor Analysis}\label{three-diagnostic-tests-to-evaluate-the-appropriateness-of-the-data-for-component-or-factor-analysis}}

Below is a snip from the workflow to remind us of where we are in the steps to PCA.

\begin{figure}
\centering
\includegraphics{images/PCA/assumptions.png}
\caption{Image of an excerpt from the workflow}
\end{figure}

\hypertarget{is-my-sample-adequate-for-pca}{%
\subsubsection{Is my sample adequate for PCA?}\label{is-my-sample-adequate-for-pca}}

There have been a number of generic guidelines (some supported with empirical evidence, some not) about ``how big'' the sample size should be:

\begin{itemize}
\tightlist
\item
  10-15 participants per variable
\item
  10 times as many participants as variables (Nunnally, 1978)
\item
  5 and 10 participants per variable up to 300 (Kass \& Tinsley, 1979)
\item
  300 (Tabachnick \& Fidell, 2007)
\item
  1000 = excellent, 300 = good, 100 = poor (Comrey \& Lee, 1992)
\end{itemize}

Of course it is more complicated. Monte Carlo studies have shown that:

\begin{itemize}
\tightlist
\item
  if factor loadings are large (\textasciitilde.6), the solution is reliable regardless of size
\item
  if communalities are large (\textasciitilde.6), relatively small samples (\textasciitilde100) are sufficient, but when they are lower (well below .5), then larger samples (\textgreater500 are indicated).
\end{itemize}

The \textbf{Kaiser-Meyer-Olkin} index (KMO) is an index of \emph{sampling adequacy} that can be used with the actual sample to let us know if the sample size is sufficient relative to the statistical characteristics of the data. If the KMO is below the recommendations, we should probably collect more data to see if it can achieve a satisfactory value.

Kaiser's 1974 recommendations were:

\begin{itemize}
\tightlist
\item
  bare minimum of .5
\item
  values between .5 and .7 as mediocre
\item
  values between .7 and .8 as good
\item
  values above .9 are superb
\end{itemize}

Revelle has included a KMO test in the psych package. The function can use either raw or matrix data. Either way, the only variables in the matrix should be the items of interest. This means that everything else (e.g., total or subscale scores, ID numbers) should be removed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{KMO}\NormalTok{(dfGRMS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Kaiser-Meyer-Olkin factor adequacy
Call: psych::KMO(r = dfGRMS)
Overall MSA =  0.85
MSA for each item = 
 Obj1  Obj2  Obj3  Obj4  Obj5  Obj6  Obj7  Obj8  Obj9 Obj10 Marg1 Marg2 Marg3 
 0.87  0.91  0.88  0.85  0.85  0.80  0.90  0.85  0.81  0.85  0.86  0.89  0.86 
Marg4 Marg5 Marg6 Marg7  Str1  Str2  Str3  Str4  Str5  Ang1  Ang2  Ang3 
 0.86  0.90  0.89  0.84  0.83  0.85  0.82  0.74  0.84  0.78  0.76  0.81 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# psych::KMO(GRMSmatrix)}
\end{Highlighting}
\end{Shaded}

We examine the KMO values for both the overall matrix and the individual items.

At the matrix level, our \(KMO = .85\), which falls into Kaiser's definition of \emph{good}. You can locate this value as the ``Overall MSA.''

At the item level, the KMO should be \textgreater{} .50. Variables with values below .50 should be evaluated for exclusion from the analysis (or run the analysis with and without the variable and compare the difference). Because removing and adding variables impacts the KMO, be sure to re-evaluate the sampling adequacy if changes are made to the items (and/or sample size).

At the item level, our KMO values range between .74 (Str4) and .91 (Obj2).

Considering both item and matrix levels, we conclude that the sample size and the data are adequate for component (or factor) analysis.

\hypertarget{are-the-correlations-among-the-variables-large-enough-to-be-analyzed}{%
\subsubsection{Are the correlations among the variables large enough to be analyzed?}\label{are-the-correlations-among-the-variables-large-enough-to-be-analyzed}}

\textbf{Bartlett's test} lets us know if a matrix is an \emph{identity matrix.} In an identity matrix all correlation coefficients (everything on the off-diagonal) would be 0.0 (and everything on the diagonal would be 1.0.

A significant Barlett's (i.e., \(p < .05\)) tells that the \(R\)-matrix is not an identity matrix. That is, there are some relationships between variables that can be analyzed.

The \emph{cortest.bartlett()} function is in the \emph{psych} package and can be run either from the raw data or R matrix formats.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{cortest.bartlett}\NormalTok{(dfGRMS)  }\CommentTok{\#from the raw data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
R was not square, finding R from data
\end{verbatim}

\begin{verbatim}
$chisq
[1] 1217.508

$p.value
[1] 0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001107085

$df
[1] 300
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# raw data produces the warning \textquotesingle{}R was not square, finding R from}
\CommentTok{\# data.\textquotesingle{} This means nothing other than we fed it raw data and the}
\CommentTok{\# function is creating a matrix from which to do the analysis.}

\CommentTok{\# psych::cortest.bartlett(GRMSmatrix, n = 259) \#if using the matrix,}
\CommentTok{\# must specify sample}
\end{Highlighting}
\end{Shaded}

Our Bartlett's test is significant: \(\chi^{2}(300)=1217.508, p < .001\). This means that our sample correlation matrix is statistically significantly different than an identity matrix and, therefore, supports a component-or-factor analytic approach for investigating the data.

\hypertarget{is-there-multicollinearity-or-singularity-in-my-data}{%
\subsubsection{Is there multicollinearity or singularity in my data?}\label{is-there-multicollinearity-or-singularity-in-my-data}}

The \textbf{determinant of the correlation matrix} should be greater than 0.00001 (that would be 4 zeros, then the 1). If it is smaller than 0.00001 then we may have an issue with \emph{multicollinearity} (i.e., variables that are too highly correlated) or \emph{singularity} (variables that are perfectly correlated).

The determinant function we use comes from base R. It is easiest to compute when the correlation matrix is the object. However, it is also possible to specify the command to work with the raw data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{det}\NormalTok{(GRMSmatrix)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.007499909
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# det(cor(dfGRMS))\#if using the raw data}
\end{Highlighting}
\end{Shaded}

With a value of 0.0075, our determinant is greater than the 0.00001 requirement. If it were not, then we could identify problematic variables (i.e., those correlating too highly with others; those not correlating sufficiently with others) and re-run the diagnostic statistics.

\hypertarget{apa-style-summary-so-far}{%
\subsubsection{APA Style Summary So Far}\label{apa-style-summary-so-far}}

\begin{quote}
Data screening was conducted to determine the suitability of the data for principal components analyses. The Kaiser-Meyer-Olkin measure of sampling adequacy (KMO; Kaiser, 1970) represents the ratio of the squared correlation between variables to the squared partial correlation between variables. KMO ranges from 0.00 to 1.00; values closer to 1.00 indicate that the patterns of correlations are relatively compact, and that component analysis should yield distinct and reliable components (Field, 2012). In our dataset, the KMO value was .85, indicating acceptable sampling adequacy. The Barlett's Test of Sphericity examines whether the population correlation matrix resembles an identity matrix (Field, 2012). When the \emph{p} value for the Bartlett's test is \textless{} .05, we are fairly certain we have clusters of correlated variables. In our dataset, \(\chi^{2}(300)=1217.508, p < .001\), indicating the correlations between items are sufficiently large enough for principal components analysis. The determinant of the correlation matrix alerts us to any issues of multicollinearity or singularity and should be larger than 0.00001. Our determinant was 0.0075, supporting the suitability of our data for analysis.
\end{quote}

\hypertarget{principal-components-analysis}{%
\subsection{Principal Components Analysis}\label{principal-components-analysis}}

Below is a snip from the workflow to remind us where we are in the steps to PCA.

\begin{figure}
\centering
\includegraphics{images/PCA/NumComponents.png}
\caption{Image of an excerpt from the workflow}
\end{figure}

We can use the \emph{principal()} function from the \emph{psych} package with raw or matrix data.

We start by creating a principal components model that has the same number of components as there are variables in the data. This allows us to inspect the component's eigenvalues and make decisions about which to extract.

\begin{itemize}
\tightlist
\item
  Note, this is different than actual \emph{factor} analysis where you \emph{must} extract fewer factors than variables (e.g., extracting 18 {[}an arbitrary number{]} instead of 25).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# All of the code sets below are functionally identical. They simply}
\CommentTok{\# swap out using the df or r{-}matrix, and whether I specify the number}
\CommentTok{\# of factors or write code to instruct R to calculate it.}

\CommentTok{\# pca1 \textless{}{-} psych::principal(GRMSmatrix, nfactors=25, rotate = \textquotesingle{}none\textquotesingle{})}
\CommentTok{\# \#using the matrix form of the data and specifying the \# factors}

\CommentTok{\# pca1 \textless{}{-} psych::principal(GRMSmatrix,}
\CommentTok{\# nfactors=length(GRMSmatrix[,1]), rotate = \textquotesingle{}none\textquotesingle{}) \#using the matrix}
\CommentTok{\# form of the data and letting the length function automatically}
\CommentTok{\# calculate the \# factors as a function of how many columns in the}
\CommentTok{\# matrix}

\CommentTok{\# pca1 \textless{}{-} psych::principal(dfGRMS, nfactors=25, rotate=\textquotesingle{}none\textquotesingle{}) \#using}
\CommentTok{\# raw data and specifying \# factors}

\NormalTok{pca1 }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{principal}\NormalTok{(dfGRMS, }\AttributeTok{nfactors =} \FunctionTok{length}\NormalTok{(dfGRMS), }\AttributeTok{rotate =} \StringTok{"none"}\NormalTok{)  }\CommentTok{\# using raw data and letting the length function automatically calculate the \# factors as a function of how many columns in the raw data}
\NormalTok{pca1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = dfGRMS, nfactors = length(dfGRMS), rotate = "none")
Standardized loadings (pattern matrix) based upon correlation matrix
       PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9  PC10  PC11  PC12
Obj1  0.52 -0.29  0.06  0.08 -0.23 -0.12 -0.33 -0.15 -0.19 -0.18  0.04  0.09
Obj2  0.55 -0.28  0.00  0.07 -0.11  0.00  0.03  0.09 -0.28 -0.12  0.14 -0.16
Obj3  0.49 -0.26  0.06  0.06 -0.04  0.32  0.06 -0.24 -0.17 -0.13  0.15  0.01
Obj4  0.52 -0.35 -0.09  0.02  0.08  0.09  0.04 -0.15  0.40  0.18 -0.08  0.28
Obj5  0.51 -0.29  0.09 -0.12 -0.08 -0.04  0.14 -0.42  0.11  0.27 -0.23 -0.20
Obj6  0.40 -0.20  0.01  0.47 -0.14  0.44 -0.03  0.22  0.05 -0.12  0.00 -0.24
Obj7  0.56 -0.11 -0.03  0.06  0.02 -0.33 -0.28  0.26  0.07  0.01  0.04 -0.06
Obj8  0.48 -0.40 -0.13 -0.25  0.00 -0.14 -0.26  0.01  0.07  0.14  0.33 -0.05
Obj9  0.40 -0.28  0.16 -0.04 -0.07 -0.11  0.53  0.12  0.33 -0.29  0.00 -0.23
Obj10 0.42 -0.32  0.00  0.23  0.12 -0.35  0.14  0.21 -0.13  0.19 -0.18  0.39
Marg1 0.58  0.31 -0.36 -0.26  0.06 -0.02 -0.13  0.05  0.04 -0.06 -0.01 -0.07
Marg2 0.58  0.38 -0.08 -0.13 -0.21  0.09 -0.02 -0.03 -0.04 -0.13 -0.14  0.14
Marg3 0.51  0.23 -0.18 -0.23  0.11 -0.18  0.36 -0.08 -0.28 -0.10 -0.02 -0.16
Marg4 0.50  0.10 -0.35 -0.07  0.38  0.01 -0.05 -0.28 -0.10 -0.01  0.11  0.02
Marg5 0.56  0.19 -0.19 -0.20 -0.08  0.27 -0.01 -0.03  0.20  0.03 -0.31  0.01
Marg6 0.54 -0.01 -0.19  0.05  0.24  0.09  0.00  0.37 -0.09  0.27 -0.16 -0.17
Marg7 0.41  0.21 -0.27 -0.08 -0.38  0.20  0.16  0.34  0.04  0.01  0.25  0.36
Str1  0.43  0.12  0.45 -0.30 -0.11 -0.19 -0.06  0.11 -0.16 -0.13  0.00 -0.12
Str2  0.40  0.03  0.31  0.15  0.43  0.07  0.15 -0.05 -0.13 -0.37 -0.07  0.37
Str3  0.33  0.22  0.54 -0.08  0.17  0.00 -0.18  0.26  0.15  0.04 -0.22 -0.07
Str4  0.29  0.03  0.48 -0.23  0.31  0.41 -0.14 -0.01 -0.02  0.21  0.26  0.02
Str5  0.34  0.20  0.37  0.10 -0.46 -0.01  0.01 -0.19 -0.23  0.28 -0.16  0.11
Ang1  0.35  0.40  0.27  0.14 -0.04 -0.23  0.22 -0.10  0.30  0.17  0.47  0.04
Ang2  0.33  0.37 -0.10  0.57  0.13 -0.02  0.11 -0.06 -0.18  0.28  0.07 -0.19
Ang3  0.38  0.31 -0.04  0.37  0.00 -0.16 -0.35 -0.21  0.32 -0.29 -0.05 -0.05
       PC13  PC14  PC15  PC16  PC17  PC18  PC19  PC20  PC21  PC22  PC23  PC24
Obj1  -0.30 -0.09  0.04 -0.02  0.24  0.14  0.20 -0.19  0.14 -0.06  0.14 -0.24
Obj2   0.03 -0.28 -0.38  0.25 -0.05  0.21 -0.27 -0.01 -0.01  0.00 -0.22  0.07
Obj3   0.54  0.01  0.22 -0.15 -0.08 -0.03 -0.15  0.01  0.00 -0.03  0.11 -0.18
Obj4  -0.10  0.06 -0.10  0.23  0.18 -0.19 -0.08  0.02 -0.12 -0.17 -0.10 -0.09
Obj5  -0.14 -0.02  0.21  0.15 -0.07  0.00 -0.22 -0.03  0.03  0.18  0.09  0.05
Obj6  -0.12  0.20  0.15 -0.08  0.17 -0.15  0.10 -0.01  0.00  0.18 -0.23  0.10
Obj7   0.25  0.25 -0.26  0.05 -0.01 -0.29 -0.06 -0.23 -0.08  0.11  0.20  0.02
Obj8  -0.09  0.01  0.12 -0.22 -0.11  0.19  0.13  0.19 -0.29  0.07  0.03  0.19
Obj9   0.03  0.01 -0.11 -0.08 -0.04  0.06  0.21  0.07  0.02 -0.26  0.11  0.00
Obj10  0.22 -0.21  0.18 -0.03 -0.04 -0.02  0.20  0.06  0.12  0.07 -0.19  0.03
Marg1  0.00 -0.03 -0.02 -0.09  0.06 -0.09 -0.05  0.26 -0.12 -0.02 -0.20 -0.35
Marg2  0.00 -0.22  0.16 -0.11 -0.02 -0.10 -0.02 -0.33 -0.24 -0.23 -0.07  0.24
Marg3 -0.01 -0.10  0.01 -0.01  0.37 -0.19  0.07  0.08  0.01  0.21  0.10  0.09
Marg4  0.08  0.41 -0.02  0.05  0.06  0.17  0.06 -0.07  0.25 -0.18 -0.12  0.19
Marg5  0.11 -0.02 -0.22  0.00 -0.23  0.22  0.28 -0.14  0.05  0.27 -0.01 -0.08
Marg6 -0.24 -0.08  0.04 -0.30 -0.15 -0.02 -0.22 -0.04  0.21 -0.16  0.13 -0.05
Marg7 -0.06  0.08  0.12  0.25  0.02  0.09 -0.10  0.14  0.12  0.05  0.21  0.06
Str1  -0.10  0.23  0.25  0.27 -0.31 -0.16  0.07  0.02  0.08 -0.05 -0.15 -0.07
Str2  -0.26  0.13 -0.10 -0.10 -0.16  0.04 -0.12  0.05 -0.18  0.15  0.08 -0.01
Str3   0.19  0.04  0.14  0.07  0.35  0.36 -0.11  0.04 -0.07 -0.02  0.03  0.02
Str4   0.00 -0.28 -0.14  0.08  0.02 -0.22  0.21  0.03  0.11 -0.03  0.06  0.06
Str5   0.01  0.22 -0.30 -0.26  0.04 -0.04  0.01  0.24  0.02 -0.09 -0.01  0.07
Ang1  -0.05 -0.03  0.01 -0.20  0.02  0.07 -0.11 -0.22  0.09  0.13 -0.16 -0.10
Ang2  -0.02 -0.01  0.06  0.27 -0.10  0.10  0.20  0.02 -0.26 -0.10  0.13 -0.10
Ang3   0.04 -0.24  0.04  0.03 -0.11 -0.08 -0.05  0.26  0.19  0.00  0.08  0.15
       PC25 h2                   u2  com
Obj1   0.02  1 -0.00000000000000044  8.5
Obj2  -0.02  1 -0.00000000000000178  6.8
Obj3  -0.11  1  0.00000000000000056  6.0
Obj4  -0.26  1  0.00000000000000078  7.4
Obj5   0.26  1  0.00000000000000000  7.9
Obj6   0.05  1  0.00000000000000067  7.8
Obj7   0.11  1 -0.00000000000000022  7.0
Obj8  -0.10  1 -0.00000000000000089  8.8
Obj9   0.12  1  0.00000000000000022  6.9
Obj10  0.09  1  0.00000000000000289 10.6
Marg1  0.25  1  0.00000000000000011  5.8
Marg2  0.07  1  0.00000000000000022  6.2
Marg3 -0.21  1  0.00000000000000000  8.0
Marg4  0.09  1  0.00000000000000044  7.2
Marg5 -0.13  1 -0.00000000000000022  7.2
Marg6 -0.13  1  0.00000000000000078  7.5
Marg7  0.08  1 -0.00000000000000178 10.2
Str1  -0.17  1  0.00000000000000122  9.1
Str2   0.05  1  0.00000000000000133  8.8
Str3  -0.01  1  0.00000000000000000  7.1
Str4   0.13  1  0.00000000000000056  8.4
Str5   0.00  1  0.00000000000000100  9.2
Ang1  -0.04  1  0.00000000000000067  8.7
Ang2  -0.01  1  0.00000000000000022  6.5
Ang3  -0.08  1  0.00000000000000056 10.3

                       PC1  PC2  PC3  PC4  PC5  PC6  PC7  PC8  PC9 PC10 PC11
SS loadings           5.37 1.71 1.54 1.22 1.08 1.04 1.02 0.98 0.95 0.90 0.84
Proportion Var        0.21 0.07 0.06 0.05 0.04 0.04 0.04 0.04 0.04 0.04 0.03
Cumulative Var        0.21 0.28 0.34 0.39 0.44 0.48 0.52 0.56 0.60 0.63 0.67
Proportion Explained  0.21 0.07 0.06 0.05 0.04 0.04 0.04 0.04 0.04 0.04 0.03
Cumulative Proportion 0.21 0.28 0.34 0.39 0.44 0.48 0.52 0.56 0.60 0.63 0.67
                      PC12 PC13 PC14 PC15 PC16 PC17 PC18 PC19 PC20 PC21 PC22
SS loadings           0.83 0.75 0.73 0.69 0.68 0.64 0.61 0.58 0.55 0.51 0.48
Proportion Var        0.03 0.03 0.03 0.03 0.03 0.03 0.02 0.02 0.02 0.02 0.02
Cumulative Var        0.70 0.73 0.76 0.79 0.81 0.84 0.86 0.89 0.91 0.93 0.95
Proportion Explained  0.03 0.03 0.03 0.03 0.03 0.03 0.02 0.02 0.02 0.02 0.02
Cumulative Proportion 0.70 0.73 0.76 0.79 0.81 0.84 0.86 0.89 0.91 0.93 0.95
                      PC23 PC24 PC25
SS loadings           0.45 0.45 0.41
Proportion Var        0.02 0.02 0.02
Cumulative Var        0.97 0.98 1.00
Proportion Explained  0.02 0.02 0.02
Cumulative Proportion 0.97 0.98 1.00

Mean item complexity =  7.9
Test of the hypothesis that 25 components are sufficient.

The root mean square of the residuals (RMSR) is  0 
 with the empirical chi square  0  with prob <  NA 

Fit based upon off diagonal values = 1
\end{verbatim}

The total variance for a particular variable will have two components: some of it will be share with other variables (common variance, h2) and some of it will be specific to that measure (unique variance, u2). Random variance is also specific to one item, but not reliably so.

We can examine this most easily by examining the matrix (second screen).

The columns PC1 thru PC25 are the (uninteresting at this point) unrotated loadings. PC stands for ``principal component.'' Although these don't align with the specific items, at this point in the procedure, there are as many components as variables.

\textbf{Communalities} are represented as \(h^2\). These are the proportions of common variance present in the variables. A variable that has no specific (or random) variance would have a communality of 1.0. If a variable shares none of its variance with any other variable its communality would be 0.0.

Because we extracted the same number components as variables, they all equal 1.0. That is, we have explained all the variance in each variable. When we specify fewer components, the value of the communalities will decrease.

**Uniquenesses* are represented as \(u2\). These are the amount of unique variance for each variable. They are calculated as \(1 - h^2\) (or 1 minus the communality). Technically (at this point in the analysis where we have an equal number of components as items), they should all be zero, but the \emph{psych} package is very ``quantsy'' and decimals are reported to the 15th and 16th decimal places! (hence the u2 for Q1 is -0.0000000000000006661338).

The final column, \emph{com}, represents \emph{item complexity.} This is an indication of how well an item reflects a single construct. If it is 1.0 then the item loads only on one component, if it is 2.0, it loads evenly on two components, and so forth. For now, we can ignore this. \emph{I mostly wanted to reassure you that ``com'' is not ``communality''; h2 is communality}.

Let's switch to the first screen of output.

\textbf{Eigenvalues} are displayed in the row called \emph{SS loadings} (i.e., the sum of squared loadings). They represent the variance explained by the particular linear component. PC1 explains 5.37 units of variance (out of a possible 25, the total of components). As a proportion, this is 5.37/25 = 0.21 (reported in the \emph{Proportion Var} row).

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{5.37}\SpecialCharTok{/}\DecValTok{25}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.2148
\end{verbatim}

Note:

\begin{itemize}
\tightlist
\item
  \emph{Cumulative Var} is helpful in determining how many components we would like to retain to balance parsimony (where the goal is frequently ``as few as possible'') with the amount of variance we want to explain.
\item
  The eigenvalues are in descending order. If we were to use the \emph{eigenvalue \textgreater{} 1.0} (i.e., ``Kaiser's'') criteria to determine how many components to extract, we would select 7. Joliffe's criteria was 0.7 (thus, we would select 14 components). Eigenvalues are only one criteria, let's look at the scree plot.
\end{itemize}

\emph{Scree plot}: We can gain another view of how many components to extract by creating a scree plot.

Eigenvalues are stored in the pca1 object's variable, ``values''. We can see all the values captured by this object with the \emph{names()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(pca1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "values"       "rotation"     "n.obs"        "communality"  "loadings"    
 [6] "fit"          "fit.off"      "fn"           "Call"         "uniquenesses"
[11] "complexity"   "valid"        "chi"          "EPVAL"        "R2"          
[16] "objective"    "residual"     "rms"          "factors"      "dof"         
[21] "null.dof"     "null.model"   "criteria"     "STATISTIC"    "PVAL"        
[26] "weights"      "r.scores"     "Vaccounted"   "Structure"    "scores"      
\end{verbatim}

Plotting the eigen\emph{values} produces a scree plot. We can use this to further gauge the number of factors we should extract.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(pca1}\SpecialCharTok{$}\NormalTok{values, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{)  }\CommentTok{\#type = \textquotesingle{}b\textquotesingle{} gives us \textquotesingle{}both\textquotesingle{} lines and points;  type = \textquotesingle{}l\textquotesingle{} gives lines and is relatively worthless}
\end{Highlighting}
\end{Shaded}

\includegraphics{08-EFA_PCA_files/figure-latex/unnamed-chunk-14-1.pdf}

We look for the point of \emph{inflexion}. That is, where the baseline levels out into a plateau. It seems to me that there is only one clear component above the plateau. However, we see that components \#5 and 5 flatten out, and then there is another drop. So, it could be 1, 2, or 4.

\hypertarget{specifying-the-number-of-components}{%
\subsection{Specifying the Number of Components}\label{specifying-the-number-of-components}}

Below is a snip from the workflow to remind us of where we are in the steps to PCA.

\begin{figure}
\centering
\includegraphics{images/PCA/SpecifyCompNum.png}
\caption{Image of an excerpt from the workflow}
\end{figure}

Having determined the number of components, we re-run the analysis with this specification. Especially when researchers may not have a clear theoretical structure that guides the process, researchers may do this iteratively with varying numbers of factors. Lewis and Neville \citep{lewis_construction_2015} examined solutions with 2, 3, 4, and 5 factors. Further, they used a parallel \emph{factor} analysis, whereas we used a principal components analysis).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# pca2 \textless{}{-} psych::principal(GRMSmatrix, nfactors=4, rotate=\textquotesingle{}none\textquotesingle{})}
\NormalTok{pca2 }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{principal}\NormalTok{(dfGRMS, }\AttributeTok{nfactors =} \DecValTok{4}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"none"}\NormalTok{)  }\CommentTok{\#can copy prior script, but change nfactors and object name}
\NormalTok{pca2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = dfGRMS, nfactors = 4, rotate = "none")
Standardized loadings (pattern matrix) based upon correlation matrix
       PC1   PC2   PC3   PC4   h2   u2 com
Obj1  0.52 -0.29  0.06  0.08 0.37 0.63 1.7
Obj2  0.55 -0.28  0.00  0.07 0.39 0.61 1.5
Obj3  0.49 -0.26  0.06  0.06 0.32 0.68 1.6
Obj4  0.52 -0.35 -0.09  0.02 0.40 0.60 1.8
Obj5  0.51 -0.29  0.09 -0.12 0.36 0.64 1.8
Obj6  0.40 -0.20  0.01  0.47 0.42 0.58 2.3
Obj7  0.56 -0.11 -0.03  0.06 0.32 0.68 1.1
Obj8  0.48 -0.40 -0.13 -0.25 0.47 0.53 2.6
Obj9  0.40 -0.28  0.16 -0.04 0.27 0.73 2.2
Obj10 0.42 -0.32  0.00  0.23 0.33 0.67 2.5
Marg1 0.58  0.31 -0.36 -0.26 0.63 0.37 2.7
Marg2 0.58  0.38 -0.08 -0.13 0.50 0.50 1.9
Marg3 0.51  0.23 -0.18 -0.23 0.40 0.60 2.1
Marg4 0.50  0.10 -0.35 -0.07 0.39 0.61 2.0
Marg5 0.56  0.19 -0.19 -0.20 0.43 0.57 1.8
Marg6 0.54 -0.01 -0.19  0.05 0.33 0.67 1.2
Marg7 0.41  0.21 -0.27 -0.08 0.29 0.71 2.4
Str1  0.43  0.12  0.45 -0.30 0.50 0.50 2.9
Str2  0.40  0.03  0.31  0.15 0.28 0.72 2.2
Str3  0.33  0.22  0.54 -0.08 0.45 0.55 2.1
Str4  0.29  0.03  0.48 -0.23 0.37 0.63 2.1
Str5  0.34  0.20  0.37  0.10 0.30 0.70 2.7
Ang1  0.35  0.40  0.27  0.14 0.37 0.63 3.0
Ang2  0.33  0.37 -0.10  0.57 0.57 0.43 2.5
Ang3  0.38  0.31 -0.04  0.37 0.38 0.62 2.9

                       PC1  PC2  PC3  PC4
SS loadings           5.37 1.71 1.54 1.22
Proportion Var        0.21 0.07 0.06 0.05
Cumulative Var        0.21 0.28 0.34 0.39
Proportion Explained  0.55 0.17 0.16 0.12
Cumulative Proportion 0.55 0.72 0.88 1.00

Mean item complexity =  2.1
Test of the hypothesis that 4 components are sufficient.

The root mean square of the residuals (RMSR) is  0.06 
 with the empirical chi square  639.9  with prob <  0.00000000000000000000000000000000000000000000056 

Fit based upon off diagonal values = 0.89
\end{verbatim}

Our eigenvalues/SS loadings remain the same. With 4 components, we explain 39\% of the variance (we can see this in the ``Cumulative Var'' row.

\emph{Communality} is the proportion of common variance within a variable. Principal components analysis assumes that all variance is common. Before extraction, all variance was set at 1.0, therefore, changing from 25 to 4 components will change this value (\(h2\)) as well as its associated \emph{uniqueness} (\(u2\)), which is calculated as ``1.0 minus the communality.''

The \emph{communalities} (\(h2\)) and \emph{uniquenesses} (\(u2\)) have changed.

Now we see that 37\% of the variance associate with Obj1 is common/shared (the \(h2\) value).

Recall that we could represent this scale with all 25 items as components, but we want a more \emph{parsimonious} explanation. By respecifying a smaller number of components, we lose some information. That is, the retained components (now 4) cannot explain all of the variance present in the data (as we saw, it explains about 39\%, cumulatively). The amount of variance explained in each variable is represented by the communalities after extraction.

We can examine the communalities through the lens of Kaiser's criterion (the eigenvalue \textgreater{} 1 criteria) to see if we think that ``four'' was a good number of components to extract.

Kaiser's criterion is believed to be accurate if:

\begin{itemize}
\tightlist
\item
  when there are fewer than 30 variables (we had 25) and, after extraction, the communalities are greater than .70

  \begin{itemize}
  \tightlist
  \item
    looking at our data, none are \textgreater{} .70, so, this does not support extracting four components
  \end{itemize}
\item
  when the sample size is greater than 250 (ours was 259) and the average communality is \textgreater{} .60

  \begin{itemize}
  \tightlist
  \item
    we can extract the communalities from our object and calculate the mean the average communality
  \end{itemize}
\end{itemize}

Using the \emph{names()} function again, we see that ``communality'' is available. Thus, we can easily calculate their mean. To get this value let's first examine the possible contents of the object we created from this PCA analysis by asking for its names.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(pca2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "values"       "rotation"     "n.obs"        "communality"  "loadings"    
 [6] "fit"          "fit.off"      "fn"           "Call"         "uniquenesses"
[11] "complexity"   "valid"        "chi"          "EPVAL"        "R2"          
[16] "objective"    "residual"     "rms"          "factors"      "dof"         
[21] "null.dof"     "null.model"   "criteria"     "STATISTIC"    "PVAL"        
[26] "weights"      "r.scores"     "Vaccounted"   "Structure"    "scores"      
\end{verbatim}

We see that it includes communalities. Thus, we can easily calculate their mean.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(pca2}\SpecialCharTok{$}\NormalTok{communality)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.3932201
\end{verbatim}

We see that the average communality is 0.39. These two criteria would suggest that we may not have the best solution. That said (in our defense):

\begin{itemize}
\tightlist
\item
  We used the scree plot as a guide and there was support for four dimensions.
\item
  We have an adequate sample size and that was supported with the KMO.
\item
  Are the number of components consistent with theory? We have not yet inspected the component loadings. This will provide us with more information.
\end{itemize}

We could do several things:

\begin{itemize}
\tightlist
\item
  re-run with a different number of components (recall Lewis and Neville \citeyearpar{lewis_construction_2015} ran models with 2, 3, 4, and 5 factors)
\item
  conduct more diagnostics

  \begin{itemize}
  \tightlist
  \item
    reproduced correlation matrix
  \item
    the difference between the reproduced correlation matrix and the correlation matrix in the data
  \end{itemize}
\end{itemize}

The \emph{factor.model()} function in \emph{psych} produces the \emph{reproduced correlation matrix} by using the \emph{loadings} from our extracted object. Conceptually, this matrix is the correlations that should be produced if we did not have the raw data, but we only had the component loadings. We could do fancy matrix algebra and produce these.

The questions, though, is: How close did we get? How different is the \emph{reproduced correlation matrix} from \emph{GRMSmatrix} -- the \(R\)-matrix produced from our raw data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# produces the reproduced correlation matrix}
\FunctionTok{round}\NormalTok{(psych}\SpecialCharTok{::}\FunctionTok{factor.model}\NormalTok{(pca2}\SpecialCharTok{$}\NormalTok{loadings), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       Obj1  Obj2  Obj3  Obj4   Obj5  Obj6  Obj7   Obj8   Obj9 Obj10 Marg1
Obj1  0.368 0.375 0.343 0.372  0.346 0.308 0.324  0.340  0.298 0.329 0.171
Obj2  0.375 0.387 0.350 0.388  0.352 0.313 0.341  0.358  0.297 0.335 0.217
Obj3  0.343 0.350 0.321 0.346  0.326 0.281 0.305  0.320  0.280 0.303 0.169
Obj4  0.372 0.388 0.346 0.404  0.356 0.291 0.333  0.397  0.292 0.335 0.224
Obj5  0.346 0.352 0.326 0.356  0.365 0.208 0.303  0.377  0.305 0.277 0.205
Obj6  0.308 0.313 0.281 0.291  0.208 0.424 0.274  0.157  0.202 0.338 0.044
Obj7  0.324 0.341 0.305 0.333  0.303 0.274 0.325  0.298  0.247 0.280 0.285
Obj8  0.340 0.358 0.320 0.397  0.377 0.157 0.298  0.469  0.294 0.273 0.264
Obj9  0.298 0.297 0.280 0.292  0.305 0.202 0.247  0.294  0.268 0.247 0.101
Obj10 0.329 0.335 0.303 0.335  0.277 0.338 0.280  0.273  0.247 0.326 0.085
Marg1 0.171 0.217 0.169 0.224  0.205 0.044 0.285  0.264  0.101 0.085 0.633
Marg2 0.177 0.207 0.175 0.179  0.195 0.093 0.276  0.170  0.122 0.092 0.519
Marg3 0.171 0.203 0.169 0.201  0.206 0.047 0.250  0.234  0.124 0.088 0.493
Marg4 0.203 0.242 0.194 0.256  0.199 0.143 0.272  0.259  0.118 0.159 0.465
Marg5 0.211 0.244 0.205 0.242  0.238 0.090 0.284  0.268  0.152 0.129 0.505
Marg6 0.277 0.305 0.261 0.304  0.255 0.238 0.310  0.275  0.189 0.238 0.368
Marg7 0.130 0.163 0.127 0.165  0.133 0.081 0.207  0.166  0.068 0.086 0.418
Str1  0.189 0.180 0.188 0.133  0.260 0.009 0.191  0.172  0.225 0.070 0.203
Str2  0.229 0.222 0.216 0.174  0.204 0.227 0.218  0.103  0.197 0.189 0.095
Str3  0.130 0.113 0.131 0.044  0.162 0.052 0.135  0.020  0.162 0.045 0.088
Str4  0.150 0.132 0.148 0.089  0.207 0.006 0.125  0.119  0.194 0.057 0.061
Str5  0.148 0.140 0.144 0.078  0.136 0.148 0.162  0.010  0.138 0.100 0.101
Ang1  0.088 0.089 0.089 0.020  0.068 0.123 0.148 -0.065  0.066 0.045 0.197
Ang2  0.101 0.119 0.092 0.065 -0.017 0.321 0.180 -0.119 -0.009 0.145 0.196
Ang3  0.138 0.154 0.129 0.107  0.059 0.265 0.205 -0.027  0.050 0.145 0.238
      Marg2 Marg3  Marg4 Marg5 Marg6 Marg7   Str1  Str2  Str3   Str4  Str5
Obj1  0.177 0.171  0.203 0.211 0.277 0.130  0.189 0.229 0.130  0.150 0.148
Obj2  0.207 0.203  0.242 0.244 0.305 0.163  0.180 0.222 0.113  0.132 0.140
Obj3  0.175 0.169  0.194 0.205 0.261 0.127  0.188 0.216 0.131  0.148 0.144
Obj4  0.179 0.201  0.256 0.242 0.304 0.165  0.133 0.174 0.044  0.089 0.078
Obj5  0.195 0.206  0.199 0.238 0.255 0.133  0.260 0.204 0.162  0.207 0.136
Obj6  0.093 0.047  0.143 0.090 0.238 0.081  0.009 0.227 0.052  0.006 0.148
Obj7  0.276 0.250  0.272 0.284 0.310 0.207  0.191 0.218 0.135  0.125 0.162
Obj8  0.170 0.234  0.259 0.268 0.275 0.166  0.172 0.103 0.020  0.119 0.010
Obj9  0.122 0.124  0.118 0.152 0.189 0.068  0.225 0.197 0.162  0.194 0.138
Obj10 0.092 0.088  0.159 0.129 0.238 0.086  0.070 0.189 0.045  0.057 0.100
Marg1 0.519 0.493  0.465 0.505 0.368 0.418  0.203 0.095 0.088  0.061 0.101
Marg2 0.501 0.426  0.365 0.437 0.321 0.347  0.295 0.198 0.237  0.165 0.226
Marg3 0.426 0.398  0.356 0.409 0.298 0.321  0.234 0.122 0.139  0.118 0.128
Marg4 0.365 0.356  0.386 0.378 0.331 0.323  0.085 0.084 0.001 -0.011 0.052
Marg5 0.437 0.409  0.378 0.425 0.328 0.334  0.237 0.142 0.139  0.119 0.136
Marg6 0.321 0.298  0.331 0.328 0.329 0.265  0.132 0.166 0.072  0.054 0.119
Marg7 0.347 0.321  0.323 0.334 0.265 0.286  0.103 0.076 0.043  0.012 0.074
Str1  0.295 0.234  0.085 0.237 0.132 0.103  0.495 0.269 0.435  0.413 0.303
Str2  0.198 0.122  0.084 0.142 0.166 0.076  0.269 0.276 0.289  0.229 0.269
Str3  0.237 0.139  0.001 0.139 0.072 0.043  0.435 0.289 0.449  0.377 0.341
Str4  0.165 0.118 -0.011 0.119 0.054 0.012  0.413 0.229 0.377  0.366 0.255
Str5  0.226 0.128  0.052 0.136 0.119 0.074  0.303 0.269 0.341  0.255 0.298
Ang1  0.311 0.188  0.110 0.191 0.141 0.143  0.277 0.253 0.333  0.208 0.309
Ang2  0.262 0.138  0.198 0.157 0.219 0.192 -0.034 0.195 0.085 -0.074 0.205
Ang3  0.293 0.187  0.211 0.206 0.230 0.201  0.073 0.207 0.143  0.016 0.216
        Ang1   Ang2   Ang3
Obj1   0.088  0.101  0.138
Obj2   0.089  0.119  0.154
Obj3   0.089  0.092  0.129
Obj4   0.020  0.065  0.107
Obj5   0.068 -0.017  0.059
Obj6   0.123  0.321  0.265
Obj7   0.148  0.180  0.205
Obj8  -0.065 -0.119 -0.027
Obj9   0.066 -0.009  0.050
Obj10  0.045  0.145  0.145
Marg1  0.197  0.196  0.238
Marg2  0.311  0.262  0.293
Marg3  0.188  0.138  0.187
Marg4  0.110  0.198  0.211
Marg5  0.191  0.157  0.206
Marg6  0.141  0.219  0.230
Marg7  0.143  0.192  0.201
Str1   0.277 -0.034  0.073
Str2   0.253  0.195  0.207
Str3   0.333  0.085  0.143
Str4   0.208 -0.074  0.016
Str5   0.309  0.205  0.216
Ang1   0.372  0.312  0.299
Ang2   0.312  0.575  0.454
Ang3   0.299  0.454  0.383
\end{verbatim}

We're not really interested in this matrix. We just need it to compare it to the \emph{GRMSmatrix} to produce the residuals. We do that next.

\textbf{Residuals} are the difference between the reproduced (i.e., those created from our component loadings) and \(R\)-matrix produced by the raw data.

If we look at the \(r_{_{Obj1Obj2}}\) in our original correlation matrix (theoretically from the raw data {[}although we simulated data{]}), the value is 0.35 The reproduced correlation that we just calculated for this pair is 0.375. The diffference is -0.025.

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.35} \SpecialCharTok{{-}} \FloatTok{0.375}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.025
\end{verbatim}

By using the \emph{factor.residuals()} function R will calculate the residuals for each pair.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(psych}\SpecialCharTok{::}\FunctionTok{factor.residuals}\NormalTok{(GRMSmatrix, pca2}\SpecialCharTok{$}\NormalTok{loadings), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        Obj1   Obj2   Obj3   Obj4   Obj5   Obj6   Obj7   Obj8   Obj9  Obj10
Obj1   0.632 -0.020 -0.093 -0.101 -0.063 -0.057 -0.044  0.006 -0.145 -0.088
Obj2  -0.020  0.613 -0.038 -0.140 -0.085 -0.079 -0.029 -0.076 -0.035 -0.090
Obj3  -0.093 -0.038  0.679 -0.105 -0.043 -0.002 -0.103 -0.072 -0.070 -0.088
Obj4  -0.101 -0.140 -0.105  0.596  0.031 -0.060 -0.048 -0.100 -0.036 -0.055
Obj5  -0.063 -0.085 -0.043  0.031  0.635 -0.055 -0.126 -0.086 -0.053 -0.072
Obj6  -0.057 -0.079 -0.002 -0.060 -0.055  0.576 -0.075 -0.015  0.004 -0.217
Obj7  -0.044 -0.029 -0.103 -0.048 -0.126 -0.075  0.675  0.009 -0.054  0.004
Obj8   0.006 -0.076 -0.072 -0.100 -0.086 -0.015  0.009  0.531 -0.101 -0.043
Obj9  -0.145 -0.035 -0.070 -0.036 -0.053  0.004 -0.054 -0.101  0.732 -0.049
Obj10 -0.088 -0.090 -0.088 -0.055 -0.072 -0.217  0.004 -0.043 -0.049  0.674
Marg1  0.014 -0.002  0.000 -0.005 -0.039  0.058  0.011  0.004  0.000  0.009
Marg2  0.068  0.000  0.055 -0.002  0.002  0.045 -0.061 -0.034 -0.004  0.031
Marg3 -0.002  0.042  0.005 -0.064  0.020  0.005 -0.052 -0.095  0.091  0.077
Marg4 -0.015 -0.061  0.048  0.000 -0.002 -0.041 -0.021 -0.020 -0.051 -0.038
Marg5 -0.039 -0.025  0.007  0.026  0.014  0.072 -0.053 -0.081  0.037 -0.020
Marg6 -0.099 -0.036 -0.102 -0.076 -0.030  0.024 -0.025 -0.014 -0.039  0.018
Marg7 -0.001  0.028  0.015  0.029 -0.076  0.093 -0.052 -0.025  0.037  0.023
Str1   0.026  0.005 -0.046 -0.071 -0.029  0.057  0.056 -0.006 -0.034  0.031
Str2  -0.041 -0.038 -0.027  0.016 -0.088 -0.075 -0.083 -0.040 -0.014 -0.002
Str3  -0.033 -0.025 -0.040  0.036 -0.052  0.042  0.052  0.026 -0.046  0.053
Str4  -0.056  0.008  0.036  0.061 -0.087  0.077 -0.055  0.009 -0.141 -0.034
Str5   0.049  0.006  0.009  0.002  0.054 -0.035 -0.013  0.027 -0.072 -0.009
Ang1  -0.025 -0.021 -0.016  0.072  0.050 -0.083  0.003  0.138  0.103  0.018
Ang2  -0.043  0.026 -0.009 -0.005  0.104 -0.124 -0.050  0.093  0.005 -0.007
Ang3   0.068 -0.027 -0.017  0.033  0.046 -0.110  0.021  0.092  0.012 -0.062
       Marg1  Marg2  Marg3  Marg4  Marg5  Marg6  Marg7   Str1   Str2   Str3
Obj1   0.014  0.068 -0.002 -0.015 -0.039 -0.099 -0.001  0.026 -0.041 -0.033
Obj2  -0.002  0.000  0.042 -0.061 -0.025 -0.036  0.028  0.005 -0.038 -0.025
Obj3   0.000  0.055  0.005  0.048  0.007 -0.102  0.015 -0.046 -0.027 -0.040
Obj4  -0.005 -0.002 -0.064  0.000  0.026 -0.076  0.029 -0.071  0.016  0.036
Obj5  -0.039  0.002  0.020 -0.002  0.014 -0.030 -0.076 -0.029 -0.088 -0.052
Obj6   0.058  0.045  0.005 -0.041  0.072  0.024  0.093  0.057 -0.075  0.042
Obj7   0.011 -0.061 -0.052 -0.021 -0.053 -0.025 -0.052  0.056 -0.083  0.052
Obj8   0.004 -0.034 -0.095 -0.020 -0.081 -0.014 -0.025 -0.006 -0.040  0.026
Obj9   0.000 -0.004  0.091 -0.051  0.037 -0.039  0.037 -0.034 -0.014 -0.046
Obj10  0.009  0.031  0.077 -0.038 -0.020  0.018  0.023  0.031 -0.002  0.053
Marg1  0.367 -0.094 -0.086 -0.089 -0.098 -0.020 -0.110 -0.012  0.026  0.043
Marg2 -0.094  0.499 -0.078 -0.134 -0.034 -0.053 -0.022 -0.041 -0.016 -0.057
Marg3 -0.086 -0.078  0.602 -0.033 -0.158 -0.044 -0.117 -0.031  0.051 -0.043
Marg4 -0.089 -0.134 -0.033  0.614 -0.079 -0.073 -0.159  0.015  0.128  0.048
Marg5 -0.098 -0.034 -0.158 -0.079  0.575 -0.038 -0.052 -0.079 -0.009  0.019
Marg6 -0.020 -0.053 -0.044 -0.073 -0.038  0.671 -0.066 -0.003  0.018  0.075
Marg7 -0.110 -0.022 -0.117 -0.159 -0.052 -0.066  0.714  0.037 -0.024 -0.002
Str1  -0.012 -0.041 -0.031  0.015 -0.079 -0.003  0.037  0.505 -0.054 -0.133
Str2   0.026 -0.016  0.051  0.128 -0.009  0.018 -0.024 -0.054  0.724 -0.091
Str3   0.043 -0.057 -0.043  0.048  0.019  0.075 -0.002 -0.133 -0.091  0.551
Str4   0.023 -0.048 -0.039  0.073  0.021  0.080  0.005 -0.187 -0.026 -0.109
Str5  -0.001  0.003 -0.005 -0.020  0.040 -0.035  0.049 -0.073 -0.148 -0.161
Ang1  -0.034 -0.079 -0.006  0.008 -0.076 -0.036  0.028 -0.096 -0.093 -0.134
Ang2  -0.026 -0.077  0.054  0.024 -0.015 -0.006 -0.059  0.084 -0.074 -0.011
Ang3   0.041 -0.015 -0.077 -0.038  0.003 -0.114 -0.115  0.029 -0.052  0.004
        Str4   Str5   Ang1   Ang2   Ang3
Obj1  -0.056  0.049 -0.025 -0.043  0.068
Obj2   0.008  0.006 -0.021  0.026 -0.027
Obj3   0.036  0.009 -0.016 -0.009 -0.017
Obj4   0.061  0.002  0.072 -0.005  0.033
Obj5  -0.087  0.054  0.050  0.104  0.046
Obj6   0.077 -0.035 -0.083 -0.124 -0.110
Obj7  -0.055 -0.013  0.003 -0.050  0.021
Obj8   0.009  0.027  0.138  0.093  0.092
Obj9  -0.141 -0.072  0.103  0.005  0.012
Obj10 -0.034 -0.009  0.018 -0.007 -0.062
Marg1  0.023 -0.001 -0.034 -0.026  0.041
Marg2 -0.048  0.003 -0.079 -0.077 -0.015
Marg3 -0.039 -0.005 -0.006  0.054 -0.077
Marg4  0.073 -0.020  0.008  0.024 -0.038
Marg5  0.021  0.040 -0.076 -0.015  0.003
Marg6  0.080 -0.035 -0.036 -0.006 -0.114
Marg7  0.005  0.049  0.028 -0.059 -0.115
Str1  -0.187 -0.073 -0.096  0.084  0.029
Str2  -0.026 -0.148 -0.093 -0.074 -0.052
Str3  -0.109 -0.161 -0.134 -0.011  0.004
Str4   0.634 -0.136 -0.054  0.105  0.006
Str5  -0.136  0.702 -0.085 -0.051 -0.110
Ang1  -0.054 -0.085  0.628 -0.072 -0.067
Ang2   0.105 -0.051 -0.072  0.425 -0.204
Ang3   0.006 -0.110 -0.067 -0.204  0.617
\end{verbatim}

Their calculated difference (-0.20) is quite close to our hand calculation (-0.25). There are several strategies to evaluate this matrix:

\begin{itemize}
\tightlist
\item
  See how large the residuals are compared to the original correlations.

  \begin{itemize}
  \tightlist
  \item
    The worst possible model would occur if we extracted no components and would be the size of the original correlations.
  \item
    If the correlations were small to start with, we expect small residuals.
  \item
    If the correlations were large to start with, the residuals will be relatively larger (this is not terribly problematic).
  \end{itemize}
\item
  Comparing residuals requires squaring them first (because residuals can be both positive and negative).

  \begin{itemize}
  \tightlist
  \item
    The sum of the squared residuals divided by the sum of the squared correlations is an estimate of model fit.Subtracting this from 1.0 means that it ranges from 0 to 1. Values \textgreater{} .95 are an indication of good fit.
  \end{itemize}
\end{itemize}

Analyzing the residuals means we need to extract only the upper right of the triangle of the matrix into an object. We can do this in steps.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# first extract the residuals}
\NormalTok{pca2\_resids }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{factor.residuals}\NormalTok{(GRMSmatrix, pca2}\SpecialCharTok{$}\NormalTok{loadings)}
\CommentTok{\# the object has the residuals in a single column}
\NormalTok{pca2\_resids }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(pca2\_resids[}\FunctionTok{upper.tri}\NormalTok{(pca2\_resids)])}
\CommentTok{\# display the first 6 rows of the residuals}
\FunctionTok{head}\NormalTok{(pca2\_resids)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            [,1]
[1,] -0.02024211
[2,] -0.09293107
[3,] -0.03803285
[4,] -0.10123779
[5,] -0.14040791
[6,] -0.10510587
\end{verbatim}

One criteria of residual analysis is to see how many residuals there are that are greater than an absolute value of 0.05. The result will be a single column with TRUE if it is \textgreater{} \textbar0.05\textbar{} and false if it is smaller. The sum function will tell us how many TRUE responses are in the matrix. Further, we can write script to obtain the proportion of total number of residuals.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{large.resid }\OtherTok{\textless{}{-}} \FunctionTok{abs}\NormalTok{(pca2\_resids) }\SpecialCharTok{\textgreater{}} \FloatTok{0.05}
\CommentTok{\# large.resid}
\FunctionTok{sum}\NormalTok{(large.resid)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 129
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{sum}\NormalTok{(large.resid)}\SpecialCharTok{/}\FunctionTok{nrow}\NormalTok{(pca2\_resids), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.43
\end{verbatim}

We learn that there are 129 residuals greater than the absolute value of 0.05. This represents 43\% of the total number of residuals.

There are no hard rules about what proportion of residuals can be greater than 0.05. A common practice is to stay below 50\% \citep{field_discovering_2012}.

Another approach to analyzing residuals is to look at their mean. Because of the +/- valences, we need to square them (to eliminate the negative), take the average, then take the square root.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{mean}\NormalTok{(pca2\_resids}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.064
\end{verbatim}

While there are no clear guidelines to interpret these, one recommendation is to consider extracting more components if the value is higher than 0.08 \citep{field_discovering_2012}. Our value of 0.064 is \textless{} 0.08.

Finally, we expect our residuals to be normally distributed. A histogram can help us inspect the distribution.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(pca2\_resids)}
\end{Highlighting}
\end{Shaded}

\includegraphics{08-EFA_PCA_files/figure-latex/unnamed-chunk-24-1.pdf} This looks reasonably normal to me, and I do not see an indication of outliers.

\hypertarget{quick-recap-of-how-to-evaluate-the-of-components-we-extracted}{%
\subsubsection{Quick recap of how to evaluate the \# of components we extracted}\label{quick-recap-of-how-to-evaluate-the-of-components-we-extracted}}

\begin{itemize}
\tightlist
\item
  If fewer than 30 variables, the eigenvalue \textgreater{} 1 (Kaiser's) criteria is fine, so long as communalities are all \textgreater{} .70.
\item
  If sample size \textgreater{} 250 and the average communalities are .6 or greater, this is acceptable.
\item
  When \emph{N} \textgreater{} 200, the scree plot can be used.
\item
  Regarding residuals:

  \begin{itemize}
  \tightlist
  \item
    Fewer than 50\% should have absolute values \textgreater{} 0.05.
  \item
    Model fit should be \textgreater{} 0.90.
  \end{itemize}
\end{itemize}

\hypertarget{component-rotation}{%
\subsection{Component Rotation}\label{component-rotation}}

Below is a snip from the workflow to remind us of where we are in the steps to PCA.

\begin{figure}
\centering
\includegraphics{images/PCA/rotation.png}
\caption{Image of an excerpt from the workflow}
\end{figure}

Rotation improves the interpretation of the components by maximizing the loading on each variable on one of the extracted components while minimizing the loading on all other components. Rotation works by changing the absolute values of the variables while keeping their differential values constant.

There are two big choices, and we need to make them on theoretical grounds:

\begin{itemize}
\tightlist
\item
  Orthogonal rotation if you think that the components are independent/unrelated.

  \begin{itemize}
  \tightlist
  \item
    Varimax is the most common orthogonal rotation.
  \end{itemize}
\item
  Oblique rotation if you think that the components are related correlated.

  \begin{itemize}
  \tightlist
  \item
    Oblimin and promax are common oblique rotations.
  \end{itemize}
\end{itemize}

Which to do?

\begin{itemize}
\tightlist
\item
  Orthogonal is sometimes considered to be ``easier'' because it minimizes cross-loadings, but
\item
  Can you think of a measure where the subscales would \emph{not} be correlated?
\end{itemize}

\hypertarget{orthogonal-rotation}{%
\subsubsection{Orthogonal rotation}\label{orthogonal-rotation}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# pcaORTH \textless{}{-} psych::principal(GRMSmatrix, nfactors = 4, rotate =}
\CommentTok{\# \textquotesingle{}varimax\textquotesingle{})}
\NormalTok{pcaORTH }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{principal}\NormalTok{(dfGRMS, }\AttributeTok{nfactors =} \DecValTok{4}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"varimax"}\NormalTok{)}
\NormalTok{pcaORTH}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = dfGRMS, nfactors = 4, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix
        RC1   RC2   RC3   RC4   h2   u2 com
Obj1   0.57  0.12  0.13  0.08 0.37 0.63 1.2
Obj2   0.58  0.18  0.10  0.09 0.39 0.61 1.3
Obj3   0.53  0.13  0.14  0.07 0.32 0.68 1.3
Obj4   0.60  0.20  0.00  0.01 0.40 0.60 1.2
Obj5   0.53  0.18  0.20 -0.10 0.36 0.64 1.6
Obj6   0.49 -0.04 -0.02  0.43 0.42 0.58 2.0
Obj7   0.46  0.28  0.13  0.15 0.32 0.68 2.1
Obj8   0.57  0.27  0.00 -0.26 0.47 0.53 1.9
Obj9   0.47  0.05  0.21 -0.05 0.27 0.73 1.4
Obj10  0.54  0.02  0.00  0.17 0.33 0.67 1.2
Marg1  0.11  0.78  0.06  0.05 0.63 0.37 1.1
Marg2  0.09  0.61  0.28  0.18 0.50 0.50 1.7
Marg3  0.13  0.60  0.16  0.02 0.40 0.60 1.2
Marg4  0.24  0.56 -0.07  0.11 0.39 0.61 1.5
Marg5  0.20  0.60  0.15  0.04 0.43 0.57 1.4
Marg6  0.37  0.40  0.03  0.18 0.33 0.67 2.4
Marg7  0.10  0.51  0.00  0.12 0.29 0.71 1.2
Str1   0.16  0.19  0.65 -0.12 0.50 0.50 1.4
Str2   0.27  0.04  0.38  0.24 0.28 0.72 2.6
Str3   0.05  0.05  0.66  0.09 0.45 0.55 1.1
Str4   0.14  0.02  0.57 -0.12 0.37 0.63 1.2
Str5   0.10  0.06  0.47  0.25 0.30 0.70 1.7
Ang1  -0.04  0.20  0.44  0.37 0.37 0.63 2.4
Ang2   0.03  0.19  0.01  0.73 0.57 0.43 1.1
Ang3   0.09  0.25  0.12  0.55 0.38 0.62 1.5

                       RC1  RC2  RC3  RC4
SS loadings           3.31 2.92 2.05 1.56
Proportion Var        0.13 0.12 0.08 0.06
Cumulative Var        0.13 0.25 0.33 0.39
Proportion Explained  0.34 0.30 0.21 0.16
Cumulative Proportion 0.34 0.63 0.84 1.00

Mean item complexity =  1.5
Test of the hypothesis that 4 components are sufficient.

The root mean square of the residuals (RMSR) is  0.06 
 with the empirical chi square  639.9  with prob <  0.00000000000000000000000000000000000000000000056 

Fit based upon off diagonal values = 0.89
\end{verbatim}

Essentially, we have the same information as before, except that loadings are calculated after rotation (which adjusts the absolute values of the component loadings while keeping their differential values constant). Our communality and uniqueness values remain the same. The eigenvalues (SS loadings) should even out, but the proportion of variance explained and cumulative variance will remain the same (39\%).

The \emph{print.psych()} function facilitates interpretation and prioritizes the information about which we care most:

\begin{itemize}
\tightlist
\item
  ``cut'' will display loadings above .3

  \begin{itemize}
  \tightlist
  \item
    if some items load on no factors
  \item
    if some items have cross-loadings (and their relative weights)
  \end{itemize}
\item
  ``sort'' will reorder the loadings to make it clearer (to the best of its ability\ldots in the case of ties) to which component/scale it belongs
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca\_table }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{print.psych}\NormalTok{(pcaORTH, }\AttributeTok{cut =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = dfGRMS, nfactors = 4, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix
      item   RC1   RC2   RC3   RC4   h2   u2 com
Obj4     4  0.60                   0.40 0.60 1.2
Obj2     2  0.58                   0.39 0.61 1.3
Obj1     1  0.57                   0.37 0.63 1.2
Obj8     8  0.57                   0.47 0.53 1.9
Obj10   10  0.54                   0.33 0.67 1.2
Obj5     5  0.53                   0.36 0.64 1.6
Obj3     3  0.53                   0.32 0.68 1.3
Obj6     6  0.49              0.43 0.42 0.58 2.0
Obj9     9  0.47                   0.27 0.73 1.4
Obj7     7  0.46                   0.32 0.68 2.1
Marg1   11        0.78             0.63 0.37 1.1
Marg2   12        0.61             0.50 0.50 1.7
Marg5   15        0.60             0.43 0.57 1.4
Marg3   13        0.60             0.40 0.60 1.2
Marg4   14        0.56             0.39 0.61 1.5
Marg7   17        0.51             0.29 0.71 1.2
Marg6   16  0.37  0.40             0.33 0.67 2.4
Str3    20              0.66       0.45 0.55 1.1
Str1    18              0.65       0.50 0.50 1.4
Str4    21              0.57       0.37 0.63 1.2
Str5    22              0.47       0.30 0.70 1.7
Ang1    23              0.44  0.37 0.37 0.63 2.4
Str2    19              0.38       0.28 0.72 2.6
Ang2    24                    0.73 0.57 0.43 1.1
Ang3    25                    0.55 0.38 0.62 1.5

                       RC1  RC2  RC3  RC4
SS loadings           3.31 2.92 2.05 1.56
Proportion Var        0.13 0.12 0.08 0.06
Cumulative Var        0.13 0.25 0.33 0.39
Proportion Explained  0.34 0.30 0.21 0.16
Cumulative Proportion 0.34 0.63 0.84 1.00

Mean item complexity =  1.5
Test of the hypothesis that 4 components are sufficient.

The root mean square of the residuals (RMSR) is  0.06 
 with the empirical chi square  639.9  with prob <  0.00000000000000000000000000000000000000000000056 

Fit based upon off diagonal values = 0.89
\end{verbatim}

In the unrotated solution, most variables loaded on the first component. After rotation, there are four clear components/scales. Further, there is clear (or at least reasonable) component/scale membership for each item. This table lists all factor loadings that are greater than 0.30. When an item has multiple factor loadings listed, we inspect it for ``cross-loading.'' We observe cross-loadings with the following items: Obj6, Marg6, Ang1.

If this were a new scale and we had not yet established ideas for subscales, the next step would be to examine the items, themselves, and try to name the scales/components. If our scale construction included a priori/planned subscales, this is where we hope that the items fall where they were hypothesized to do so. Our simulated data worked pretty well, and with the exception of one item (i.e., Ang1) replicated the four scales that Lewis and Neville \citeyearpar{lewis_construction_2015} reported in the article.

\begin{itemize}
\tightlist
\item
  Assumptions of Beauty and Sexual Objectification
\item
  Silenced and Marginalized
\item
  Strong Woman Stereotype
\item
  Angry Woman Stereotype
\end{itemize}

We can also create a figure of the result.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{fa.diagram}\NormalTok{(pcaORTH)}
\end{Highlighting}
\end{Shaded}

\includegraphics{08-EFA_PCA_files/figure-latex/unnamed-chunk-27-1.pdf}

We can extract the component loadings and write them to a table. This can be useful in preparing an APA style table for a manuscript or presentation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(pcaORTH)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "values"       "rotation"     "n.obs"        "communality"  "loadings"    
 [6] "fit"          "fit.off"      "fn"           "Call"         "uniquenesses"
[11] "complexity"   "valid"        "chi"          "EPVAL"        "R2"          
[16] "objective"    "residual"     "rms"          "factors"      "dof"         
[21] "null.dof"     "null.model"   "criteria"     "STATISTIC"    "PVAL"        
[26] "weights"      "r.scores"     "rot.mat"      "Vaccounted"   "Structure"   
[31] "scores"      
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcaORTH\_table }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(pcaORTH}\SpecialCharTok{$}\NormalTok{loadings, }\DecValTok{3}\NormalTok{)}
\FunctionTok{write.table}\NormalTok{(pcaORTH\_table, }\AttributeTok{file =} \StringTok{"pcaORTH\_table.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{col.names =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{pcaORTH\_table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Loadings:
      RC1    RC2    RC3    RC4   
Obj1   0.574  0.121  0.131       
Obj2   0.581  0.181              
Obj3   0.531  0.125  0.137       
Obj4   0.604  0.200              
Obj5   0.532  0.176  0.202       
Obj6   0.489                0.428
Obj7   0.457  0.278  0.128  0.150
Obj8   0.572  0.273        -0.260
Obj9   0.468         0.210       
Obj10  0.545                0.170
Marg1  0.111  0.784              
Marg2         0.615  0.284  0.185
Marg3  0.134  0.596  0.155       
Marg4  0.237  0.559         0.113
Marg5  0.201  0.601  0.146       
Marg6  0.368  0.403         0.176
Marg7  0.102  0.511         0.121
Str1   0.157  0.192  0.648 -0.117
Str2   0.271         0.381  0.238
Str3                 0.660       
Str4   0.142         0.574 -0.124
Str5   0.104         0.470  0.250
Ang1          0.196  0.444  0.367
Ang2          0.195         0.732
Ang3          0.245  0.118  0.549

                 RC1   RC2   RC3   RC4
SS loadings    3.310 2.916 2.045 1.559
Proportion Var 0.132 0.117 0.082 0.062
Cumulative Var 0.132 0.249 0.331 0.393
\end{verbatim}

\hypertarget{oblique-rotation}{%
\subsubsection{Oblique rotation}\label{oblique-rotation}}

Whereas the orthogonal rotation sought to maximize the independence/unrelatedness of the components, an oblique rotation will allow them to be correlated. Researchers often explore both solutions but then only report one.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# pcaOBL \textless{}{-} psych::principal(GRMSmatrix, nfactors = 4, rotate =}
\CommentTok{\# \textquotesingle{}oblimin\textquotesingle{})}
\NormalTok{pcaOBL }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{principal}\NormalTok{(dfGRMS, }\AttributeTok{nfactors =} \DecValTok{4}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"oblimin"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required namespace: GPArotation
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcaOBL}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = dfGRMS, nfactors = 4, rotate = "oblimin")
Standardized loadings (pattern matrix) based upon correlation matrix
        TC2   TC1   TC3   TC4   h2   u2 com
Obj1   0.57  0.01  0.09  0.03 0.37 0.63 1.1
Obj2   0.57  0.08  0.05  0.04 0.39 0.61 1.1
Obj3   0.53  0.02  0.10  0.02 0.32 0.68 1.1
Obj4   0.60  0.11 -0.05 -0.04 0.40 0.60 1.1
Obj5   0.51  0.09  0.17 -0.15 0.36 0.64 1.5
Obj6   0.53 -0.17 -0.05  0.40 0.42 0.58 2.1
Obj7   0.42  0.19  0.07  0.10 0.32 0.68 1.6
Obj8   0.55  0.22 -0.05 -0.32 0.47 0.53 2.0
Obj9   0.46 -0.04  0.19 -0.09 0.27 0.73 1.4
Obj10  0.58 -0.09 -0.04  0.13 0.33 0.67 1.2
Marg1 -0.03  0.81 -0.03  0.01 0.63 0.37 1.0
Marg2 -0.04  0.60  0.21  0.14 0.50 0.50 1.4
Marg3  0.02  0.60  0.08 -0.02 0.40 0.60 1.0
Marg4  0.16  0.55 -0.16  0.07 0.39 0.61 1.4
Marg5  0.09  0.59  0.07  0.00 0.43 0.57 1.1
Marg6  0.32  0.35 -0.04  0.13 0.33 0.67 2.3
Marg7  0.02  0.52 -0.06  0.09 0.29 0.71 1.1
Str1   0.06  0.13  0.64 -0.15 0.50 0.50 1.2
Str2   0.24 -0.07  0.37  0.21 0.28 0.72 2.5
Str3  -0.02 -0.03  0.67  0.07 0.45 0.55 1.0
Str4   0.09 -0.04  0.59 -0.15 0.37 0.63 1.2
Str5   0.05 -0.02  0.46  0.23 0.30 0.70 1.5
Ang1  -0.12  0.14  0.42  0.36 0.37 0.63 2.4
Ang2   0.01  0.13 -0.04  0.73 0.57 0.43 1.1
Ang3   0.05  0.18  0.07  0.54 0.38 0.62 1.3

                       TC2  TC1  TC3  TC4
SS loadings           3.34 2.90 2.05 1.55
Proportion Var        0.13 0.12 0.08 0.06
Cumulative Var        0.13 0.25 0.33 0.39
Proportion Explained  0.34 0.29 0.21 0.16
Cumulative Proportion 0.34 0.63 0.84 1.00

 With component correlations of 
     TC2  TC1  TC3  TC4
TC2 1.00 0.35 0.20 0.10
TC1 0.35 1.00 0.24 0.16
TC3 0.20 0.24 1.00 0.09
TC4 0.10 0.16 0.09 1.00

Mean item complexity =  1.4
Test of the hypothesis that 4 components are sufficient.

The root mean square of the residuals (RMSR) is  0.06 
 with the empirical chi square  639.9  with prob <  0.00000000000000000000000000000000000000000000056 

Fit based upon off diagonal values = 0.89
\end{verbatim}

We can make it a little easier to interpret by removing all factor loadings below .30.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{print.psych}\NormalTok{(pcaOBL, }\AttributeTok{cut =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = dfGRMS, nfactors = 4, rotate = "oblimin")
Standardized loadings (pattern matrix) based upon correlation matrix
      item   TC2   TC1   TC3   TC4   h2   u2 com
Obj4     4  0.60                   0.40 0.60 1.1
Obj10   10  0.58                   0.33 0.67 1.2
Obj1     1  0.57                   0.37 0.63 1.1
Obj2     2  0.57                   0.39 0.61 1.1
Obj8     8  0.55             -0.32 0.47 0.53 2.0
Obj6     6  0.53              0.40 0.42 0.58 2.1
Obj3     3  0.53                   0.32 0.68 1.1
Obj5     5  0.51                   0.36 0.64 1.5
Obj9     9  0.46                   0.27 0.73 1.4
Obj7     7  0.42                   0.32 0.68 1.6
Marg1   11        0.81             0.63 0.37 1.0
Marg3   13        0.60             0.40 0.60 1.0
Marg2   12        0.60             0.50 0.50 1.4
Marg5   15        0.59             0.43 0.57 1.1
Marg4   14        0.55             0.39 0.61 1.4
Marg7   17        0.52             0.29 0.71 1.1
Marg6   16  0.32  0.35             0.33 0.67 2.3
Str3    20              0.67       0.45 0.55 1.0
Str1    18              0.64       0.50 0.50 1.2
Str4    21              0.59       0.37 0.63 1.2
Str5    22              0.46       0.30 0.70 1.5
Ang1    23              0.42  0.36 0.37 0.63 2.4
Str2    19              0.37       0.28 0.72 2.5
Ang2    24                    0.73 0.57 0.43 1.1
Ang3    25                    0.54 0.38 0.62 1.3

                       TC2  TC1  TC3  TC4
SS loadings           3.34 2.90 2.05 1.55
Proportion Var        0.13 0.12 0.08 0.06
Cumulative Var        0.13 0.25 0.33 0.39
Proportion Explained  0.34 0.29 0.21 0.16
Cumulative Proportion 0.34 0.63 0.84 1.00

 With component correlations of 
     TC2  TC1  TC3  TC4
TC2 1.00 0.35 0.20 0.10
TC1 0.35 1.00 0.24 0.16
TC3 0.20 0.24 1.00 0.09
TC4 0.10 0.16 0.09 1.00

Mean item complexity =  1.4
Test of the hypothesis that 4 components are sufficient.

The root mean square of the residuals (RMSR) is  0.06 
 with the empirical chi square  639.9  with prob <  0.00000000000000000000000000000000000000000000056 

Fit based upon off diagonal values = 0.89
\end{verbatim}

The solution from the oblique rotation was similar to the orthogonal one. Note, though, that because our specification included ``sort=TRUE'' that the relative weights wiggled around and so the items are listed in a little different order.

Let's create a table and write it to a file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcaOBL\_table }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(pcaOBL}\SpecialCharTok{$}\NormalTok{loadings, }\DecValTok{3}\NormalTok{)}
\FunctionTok{write.table}\NormalTok{(pcaOBL\_table, }\AttributeTok{file =} \StringTok{"pcaOBL\_table.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{col.names =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{pcaOBL\_table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Loadings:
      TC2    TC1    TC3    TC4   
Obj1   0.574                     
Obj2   0.574                     
Obj3   0.526                     
Obj4   0.604  0.108              
Obj5   0.509         0.166 -0.149
Obj6   0.533 -0.170         0.401
Obj7   0.424  0.192         0.103
Obj8   0.553  0.219        -0.318
Obj9   0.464         0.190       
Obj10  0.576                0.132
Marg1         0.810              
Marg2         0.600  0.211  0.145
Marg3         0.600              
Marg4  0.163  0.554 -0.155       
Marg5         0.592              
Marg6  0.319  0.349         0.131
Marg7         0.518              
Str1          0.129  0.644 -0.152
Str2   0.245         0.366  0.212
Str3                 0.670       
Str4                 0.589 -0.148
Str5                 0.463  0.234
Ang1  -0.116  0.140  0.423  0.356
Ang2          0.131         0.729
Ang3          0.183         0.535

                 TC2   TC1   TC3   TC4
SS loadings    3.107 2.668 1.930 1.471
Proportion Var 0.124 0.107 0.077 0.059
Cumulative Var 0.124 0.231 0.308 0.367
\end{verbatim}

The same four components/scales have emerged, but they are in different order.

The oblique rotation allows us to see the correlation between the components/scales. This was not available in the orthogonal rotation because the assumption of the orthogonal/varimax rotation is that the scales/components are uncorrelated; hence in the analysis they were fixed to 0.0.

We can see that all the scales have low to moderate (i.e., 0.09 to 0.35) correlations with each other.

Of course, there is always a little complexity. In oblique rotations, there is a distinction between the \emph{pattern} matrix (which reports component loadings and is comparable to the matrix we interpreted for the orthogonal rotation) and the \emph{structure} matrix (takes into account the relationship between the components/scales -- it is a product of the pattern matrix and the matrix containing the correlation coefficients between the components/scales). Most interpret the pattern matrix because it is simpler; however, it could be that values in the pattern matrix are suppressed because of relations between the components. Therefore, the structure matrix can be a useful check and some editors will request it.

Obtaining the structure matrix requires two steps. First, we multiply the factor loadings with the phi matrix.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# names(pcaOBL)}
\NormalTok{pcaOBL}\SpecialCharTok{$}\NormalTok{loadings }\SpecialCharTok{\%*\%}\NormalTok{ pcaOBL}\SpecialCharTok{$}\NormalTok{Phi}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             TC2       TC1        TC3         TC4
Obj1  0.59854109 0.2370854 0.21076193  0.10026056
Obj2  0.61370763 0.2936877 0.18613777  0.11300577
Obj3  0.55625886 0.2340806 0.21133177  0.08838322
Obj4  0.62653372 0.2994708 0.09212284  0.03271529
Obj5  0.55782211 0.2813705 0.27627677 -0.06671857
Obj6  0.50436458 0.0686600 0.04959170  0.42362055
Obj7  0.51583784 0.3746086 0.21323217  0.18475919
Obj8  0.58631883 0.3483430 0.08569629 -0.22925474
Obj9  0.47739175 0.1500764 0.26407686 -0.03124475
Obj10 0.55033661 0.1232499 0.06974955  0.17330360
Marg1 0.25202377 0.7945638 0.16105543  0.13171661
Marg2 0.22669221 0.6607073 0.36285145  0.25811600
Marg3 0.24653097 0.6246027 0.23339896  0.08632620
Marg4 0.33257030 0.5843225 0.01980084  0.16383648
Marg5 0.31382840 0.6413860 0.23355689  0.10932345
Marg6 0.44535049 0.4708617 0.11768477  0.21681782
Marg7 0.19733852 0.5242792 0.07531732  0.17048225
Str1  0.22295271 0.2846102 0.67447904 -0.06303143
Str2  0.31665124 0.1416729 0.41881452  0.26069989
Str3  0.11375939 0.1383416 0.66583221  0.12771942
Str4  0.17266253 0.1046982 0.58122385 -0.09110045
Str5  0.16426964 0.1477531 0.49084330  0.27949111
Ang1  0.05479887 0.2604643 0.46720892  0.40679247
Ang2  0.12294975 0.2431195 0.06259901  0.74708242
Ang3  0.17778840 0.3025083 0.17023961  0.57585722
\end{verbatim}

Next, we can use Field's \citeyearpar{field_discovering_2012} function to produce the matrix.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Field\textquotesingle{}s function to produce the structure matrix}
\NormalTok{factor.structure }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(fa, }\AttributeTok{cut =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{decimals =} \DecValTok{2}\NormalTok{) \{}
\NormalTok{    structure.matrix }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{fa.sort}\NormalTok{(fa}\SpecialCharTok{$}\NormalTok{loadings }\SpecialCharTok{\%*\%}\NormalTok{ fa}\SpecialCharTok{$}\NormalTok{Phi)}
\NormalTok{    structure.matrix }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(}\FunctionTok{abs}\NormalTok{(structure.matrix) }\SpecialCharTok{\textless{}}\NormalTok{ cut,}
        \StringTok{""}\NormalTok{, }\FunctionTok{round}\NormalTok{(structure.matrix, decimals)))}
    \FunctionTok{return}\NormalTok{(structure.matrix)}
\NormalTok{\}}

\FunctionTok{factor.structure}\NormalTok{(pcaOBL, }\AttributeTok{cut =} \FloatTok{0.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       TC2  TC1  TC3  TC4
Obj4  0.63               
Obj2  0.61               
Obj1   0.6               
Obj8  0.59 0.35          
Obj5  0.56               
Obj3  0.56               
Obj10 0.55               
Obj7  0.52 0.37          
Obj6   0.5           0.42
Obj9  0.48               
Marg1      0.79          
Marg2      0.66 0.36     
Marg5 0.31 0.64          
Marg3      0.62          
Marg4 0.33 0.58          
Marg7      0.52          
Marg6 0.45 0.47          
Str1            0.67     
Str3            0.67     
Str4            0.58     
Str5            0.49     
Ang1            0.47 0.41
Str2  0.32      0.42     
Ang2                 0.75
Ang3        0.3      0.58
\end{verbatim}

Although some of the relative values changed, our items were stable regarding their component membership.

\hypertarget{component-scores}{%
\subsection{Component Scores}\label{component-scores}}

Component \emph{scores} (PC scores) can be created for each case (row) on each component (column). These can be used to assess the relative standing of one person on the construct/variable to another. We can also use them in regression (in place of means or sums) when groups of predictors correlate so highly that there is multicollinearity.

Computation involves multiplying an individual's item-level responses by the component loadings we obtained through the PCA process. The results will be one score per component for each row/case.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcaOBL }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{principal}\NormalTok{(dfGRMS, }\AttributeTok{nfactors =} \DecValTok{4}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"oblimin"}\NormalTok{, }\AttributeTok{scores =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{head}\NormalTok{(pcaOBL}\SpecialCharTok{$}\NormalTok{scores, }\DecValTok{10}\NormalTok{)  }\CommentTok{\#shows us only the first 10 (of N = 2571)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             TC2        TC1         TC3         TC4
 [1,] -0.6458500 -1.5519979  0.73544246 -0.76005053
 [2,]  0.5116784 -1.0097579  0.74427743  0.29538913
 [3,]  0.5825763  0.1327654 -0.03596272  1.40125523
 [4,] -1.1296840 -1.0820066  0.46203018 -2.08203840
 [5,] -0.3066491  0.7020903 -1.15731968 -0.08696116
 [6,] -0.6165563  0.5443947 -0.37332546  1.06336132
 [7,]  0.3100935  0.7005788 -1.12949607 -0.99349900
 [8,] -1.7404874 -0.6389406  0.51456794  1.23689663
 [9,] -0.4801252 -1.0772976 -0.71350564  0.77814794
[10,]  0.3986271 -0.5906088 -0.71507356  0.80831256
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dfGRMS }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(dfGRMS, pcaOBL}\SpecialCharTok{$}\NormalTok{scores)  }\CommentTok{\#adds them to our raw dataset}
\end{Highlighting}
\end{Shaded}

To bring this full circle, we can see the correlation of the component scores; the pattern maps onto what we saw previously.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{corr.test}\NormalTok{(dfGRMS[}\FunctionTok{c}\NormalTok{(}\StringTok{"TC1"}\NormalTok{, }\StringTok{"TC4"}\NormalTok{, }\StringTok{"TC3"}\NormalTok{, }\StringTok{"TC2"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:psych::corr.test(x = dfGRMS[c("TC1", "TC4", "TC3", "TC2")])
Correlation matrix 
     TC1  TC4  TC3  TC2
TC1 1.00 0.16 0.24 0.35
TC4 0.16 1.00 0.09 0.10
TC3 0.24 0.09 1.00 0.20
TC2 0.35 0.10 0.20 1.00
Sample Size 
[1] 259
Probability values (Entries above the diagonal are adjusted for multiple tests.) 
     TC1  TC4  TC3  TC2
TC1 0.00 0.03 0.00 0.00
TC4 0.01 0.00 0.19 0.19
TC3 0.00 0.13 0.00 0.00
TC2 0.00 0.10 0.00 0.00

 To see confidence intervals of the correlations, print with the short=FALSE option
\end{verbatim}

And now for a figure of the oblique rotation. Note that figure includes semi-circles between TC1/TC2 and TC1/TC4. These represent significant correlation coefficients between the components that are named. In contrast, the orthogonal rotation required the components to be uncorrelated.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{fa.diagram}\NormalTok{(pcaOBL, }\AttributeTok{error =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{side =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{08-EFA_PCA_files/figure-latex/unnamed-chunk-36-1.pdf}

\hypertarget{apa-style-results}{%
\section{APA Style Results}\label{apa-style-results}}

\textbf{Results}

\begin{quote}
The dimensionality of the 25 items from the Gendered Racial Microaggressions Scale for Black Women was analyzed using principal components analysis. Data screening were conducted to determine the suitability of the data for principal components analyses. The Kaiser-Meyer-Olkin measure of sampling adequacy (KMO; Kaiser, 1970) represents the ratio of the squared correlation between variables to the squared partial correlation between variables. KMO ranges from 0.00 to 1.00; values closer to 1.00 indicate that the patterns of correlations are relatively compact, and that component analysis should yield distinct and reliable components (Field, 2012). In our dataset, the KMO value was .85, indicating acceptable sampling adequacy. The Barlett's Test of Sphericity examines whether the population correlation matrix resembles an identity matrix (Field, 2012). When the \emph{p} value for the Bartlett's test is \textless{} .05, we are fairly certain we have clusters of correlated variables. In our dataset, \(\chi^{2}(300)=1217.508, p < .001\), indicating the correlations between items are sufficiently large enough for principal components analysis. The determinant of the correlation matrix alerts us to any issues of multicollinearity or singularity and should be larger than 0.00001. Our determinant was 0.0075, supporting the suitability of our data for analysis.
\end{quote}

\begin{quote}
Four criteria were used to determine the number of components to extract: a priori theory, the scree test, the eigenvalue-greater-than-one criteria, and the interpretability of the solution. Kaiser's eigenvalue-greater-than-one criteria suggested seven components, and, in combination explained 39\% of the variance. The inflexion in the scree plot suggested retaining between one and four components. Considering the a priori theory obtained from the original psychometric article \citep{lewis_construction_2015}, four components were extracted. We investigated each with orthogonal (varimax) and oblique (oblimin) procedures. Given the low-to-moderate correlations (ranging from 0.09 to 0.35) and the clear component loadings, we determined that an oblique solution was most appropriate.
\end{quote}

\begin{quote}
The rotated solution, as shown in Table 1 and Figure 1, yielded four interpretable components, each listed with the proportion of variance accounted for: assumptions of beauty and sexual objectification (13\%), silenced and marginalized (12\%), strong woman stereotype (8\%), and angry woman stereotype (6\%).
\end{quote}

Regarding the Table 1, I would include a table with all the values, bolding those with component membership. This is easy, though, because it is how the table was exported when we wrote it to a .csv file.

\hypertarget{back-to-the-future-the-relationship-between-pca-and-item-analysis}{%
\section{Back to the FutuRe: The relationship between PCA and item analysis}\label{back-to-the-future-the-relationship-between-pca-and-item-analysis}}

Earlier in the ReCentering Psych Stats OER I included the lesson on \protect\hyperlink{ItemAnalSurvey}{item analysis} because I find it to be a useful stepping stone into principal components and principal factor analyses.

Nearing the end of the lesson on PCA, we can ask, ``How do the results we obtained from PCA compare to those found in item analysis?'' To answer these questions, we will (a) calculate corrected item-total correlation coefficients, (c) calculate correlations between each item and the mean scores from the remaining scales, (c) calculate component loadings for a PCA with an orthogonal rotation, and (d) calculate component loadings for a PCA with an oblique rotation. I will teach the last step -- assembling them into a single table -- in R. The code is complicated, and many might choose to do it in a spreadsheet, outside of the R environment. After assembly, though, we can compare the results.

First, we score the total and subscales using the dataset we simulated above (dfGRMS).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{GRMSVars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{, }\StringTok{"Obj7"}\NormalTok{, }\StringTok{"Obj8"}\NormalTok{,}
    \StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{, }\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{, }\StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{,}
    \StringTok{"Marg7"}\NormalTok{, }\StringTok{"Str1"}\NormalTok{, }\StringTok{"Str2"}\NormalTok{, }\StringTok{"Str3"}\NormalTok{, }\StringTok{"Str4"}\NormalTok{, }\StringTok{"Str5"}\NormalTok{, }\StringTok{"Ang1"}\NormalTok{, }\StringTok{"Ang2"}\NormalTok{, }\StringTok{"Ang3"}\NormalTok{)}
\NormalTok{ObjectifiedVars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{, }\StringTok{"Obj7"}\NormalTok{,}
    \StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{)}
\NormalTok{MarginalizedVars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{, }\StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{,}
    \StringTok{"Marg7"}\NormalTok{)}
\NormalTok{StrongVars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Str1"}\NormalTok{, }\StringTok{"Str2"}\NormalTok{, }\StringTok{"Str3"}\NormalTok{, }\StringTok{"Str4"}\NormalTok{, }\StringTok{"Str5"}\NormalTok{)}
\NormalTok{AngryVars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Ang1"}\NormalTok{, }\StringTok{"Ang2"}\NormalTok{, }\StringTok{"Ang3"}\NormalTok{)}

\NormalTok{dfGRMS}\SpecialCharTok{$}\NormalTok{GRMStot }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(dfGRMS[, GRMSVars], }\FloatTok{0.8}\NormalTok{)  }\CommentTok{\#will create the mean for each individual if 80\% of variables are present }
\NormalTok{dfGRMS}\SpecialCharTok{$}\NormalTok{Objectified }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(dfGRMS[, ObjectifiedVars], }\FloatTok{0.8}\NormalTok{)  }\CommentTok{\#will create the mean for each individual if 80\% of variables are present }
\NormalTok{dfGRMS}\SpecialCharTok{$}\NormalTok{Marginalized }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(dfGRMS[, MarginalizedVars], }\FloatTok{0.8}\NormalTok{)  }\CommentTok{\#will create the mean for each individual if 80\% of variables are present }
\NormalTok{dfGRMS}\SpecialCharTok{$}\NormalTok{Strong }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(dfGRMS[, StrongVars], }\FloatTok{0.8}\NormalTok{)  }\CommentTok{\#will create the mean for each individual if 80\% of variables are present (in this case all variables must be present)}
\NormalTok{dfGRMS}\SpecialCharTok{$}\NormalTok{Angry }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(dfGRMS[, AngryVars], }\FloatTok{0.8}\NormalTok{)  }\CommentTok{\#will create the mean for each individual if 80\% of variables are present (in this case all variables must be present)}
\end{Highlighting}
\end{Shaded}

While we are at it, let's just create tiny dfs with just our variables of interest.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GRMStotal }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(dfGRMS, Obj1}\SpecialCharTok{:}\NormalTok{Ang3)}
\NormalTok{Objectification }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(dfGRMS, Obj1}\SpecialCharTok{:}\NormalTok{Obj10)}
\NormalTok{Marginalization }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(dfGRMS, Marg1}\SpecialCharTok{:}\NormalTok{Marg7)}
\NormalTok{Strong }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(dfGRMS, Str1}\SpecialCharTok{:}\NormalTok{Str5)}
\NormalTok{Angry }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(dfGRMS, Ang1}\SpecialCharTok{:}\NormalTok{Ang3)}
\end{Highlighting}
\end{Shaded}

\hypertarget{calculating-and-extracting-item-total-correlation-coefficients}{%
\subsection{Calculating and Extracting Item-Total Correlation Coefficients}\label{calculating-and-extracting-item-total-correlation-coefficients}}

\hypertarget{corrected-item-total-correlations-from-the-psychalpha}{%
\subsubsection{\texorpdfstring{Corrected item-total correlations from the \emph{psych::alpha()}}{Corrected item-total correlations from the psych::alpha()}}\label{corrected-item-total-correlations-from-the-psychalpha}}

Let's first ask, ``Is there support for this instrument as a unidimensional measure?'' To do that, we get an alpha for the whole scale score.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GRMSalpha }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(GRMStotal)  }\CommentTok{\#creating an object from this analysis so I can extract and manipulate the item statistics (specifically the r.drop)}
\NormalTok{GRMSalpha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = GRMStotal)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
      0.84      0.84    0.86      0.18 5.3 0.014  2.5 0.44     0.18

    95% confidence boundaries 
         lower alpha upper
Feldt     0.81  0.84  0.87
Duhachek  0.81  0.84  0.87

 Reliability if an item is dropped:
      raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r
Obj1       0.83      0.83    0.85      0.17 5.0    0.015 0.0066  0.17
Obj2       0.83      0.83    0.85      0.17 5.0    0.015 0.0066  0.17
Obj3       0.83      0.84    0.85      0.17 5.1    0.015 0.0068  0.17
Obj4       0.83      0.83    0.85      0.17 5.0    0.015 0.0065  0.17
Obj5       0.83      0.83    0.85      0.17 5.0    0.015 0.0066  0.17
Obj6       0.84      0.84    0.85      0.18 5.2    0.015 0.0067  0.18
Obj7       0.83      0.83    0.85      0.17 5.0    0.015 0.0066  0.17
Obj8       0.84      0.84    0.85      0.18 5.1    0.015 0.0063  0.18
Obj9       0.84      0.84    0.85      0.18 5.2    0.015 0.0067  0.18
Obj10      0.84      0.84    0.85      0.18 5.2    0.015 0.0067  0.18
Marg1      0.83      0.83    0.85      0.17 5.0    0.015 0.0059  0.17
Marg2      0.83      0.83    0.85      0.17 5.0    0.015 0.0063  0.17
Marg3      0.83      0.83    0.85      0.17 5.1    0.015 0.0065  0.17
Marg4      0.83      0.84    0.85      0.17 5.1    0.015 0.0064  0.17
Marg5      0.83      0.83    0.85      0.17 5.0    0.015 0.0065  0.17
Marg6      0.83      0.83    0.85      0.17 5.0    0.015 0.0066  0.17
Marg7      0.84      0.84    0.85      0.18 5.2    0.015 0.0066  0.18
Str1       0.84      0.84    0.85      0.18 5.1    0.015 0.0068  0.17
Str2       0.84      0.84    0.85      0.18 5.2    0.015 0.0070  0.18
Str3       0.84      0.84    0.86      0.18 5.2    0.014 0.0066  0.18
Str4       0.84      0.84    0.86      0.18 5.3    0.014 0.0064  0.18
Str5       0.84      0.84    0.86      0.18 5.2    0.014 0.0067  0.18
Ang1       0.84      0.84    0.85      0.18 5.2    0.014 0.0067  0.18
Ang2       0.84      0.84    0.86      0.18 5.3    0.014 0.0064  0.18
Ang3       0.84      0.84    0.85      0.18 5.2    0.015 0.0067  0.18

 Item statistics 
        n raw.r std.r r.cor r.drop mean   sd
Obj1  259  0.50  0.51  0.48   0.44  2.8 0.87
Obj2  259  0.53  0.54  0.51   0.47  2.6 0.91
Obj3  259  0.49  0.49  0.46   0.41  2.6 0.99
Obj4  259  0.51  0.51  0.49   0.44  2.6 1.05
Obj5  259  0.50  0.50  0.47   0.43  2.3 0.94
Obj6  259  0.41  0.41  0.37   0.34  2.4 0.93
Obj7  259  0.55  0.54  0.52   0.47  2.5 1.20
Obj8  259  0.45  0.46  0.43   0.38  2.5 0.90
Obj9  259  0.41  0.41  0.37   0.34  2.7 0.97
Obj10 259  0.41  0.42  0.38   0.35  2.2 0.83
Marg1 259  0.55  0.55  0.54   0.49  2.7 0.91
Marg2 259  0.56  0.56  0.55   0.50  2.8 0.89
Marg3 259  0.50  0.50  0.47   0.42  2.4 1.05
Marg4 259  0.47  0.48  0.45   0.41  2.5 0.87
Marg5 259  0.54  0.54  0.52   0.47  2.3 0.92
Marg6 259  0.52  0.53  0.50   0.46  2.7 0.94
Marg7 259  0.41  0.41  0.36   0.33  2.5 1.04
Str1  259  0.44  0.45  0.41   0.37  2.7 0.93
Str2  259  0.42  0.42  0.38   0.34  2.6 0.91
Str3  259  0.37  0.37  0.32   0.29  2.3 0.98
Str4  259  0.31  0.32  0.27   0.24  2.2 0.80
Str5  259  0.38  0.37  0.32   0.29  2.5 1.05
Ang1  259  0.39  0.38  0.34   0.31  2.6 1.04
Ang2  259  0.36  0.35  0.31   0.28  2.3 1.05
Ang3  259  0.40  0.40  0.36   0.32  2.4 0.91

Non missing response frequency for each item
         0    1    2    3    4    5 miss
Obj1  0.01 0.07 0.27 0.49 0.16 0.01    0
Obj2  0.01 0.07 0.36 0.41 0.12 0.03    0
Obj3  0.02 0.11 0.32 0.39 0.14 0.02    0
Obj4  0.01 0.14 0.32 0.33 0.16 0.04    0
Obj5  0.02 0.16 0.39 0.33 0.09 0.01    0
Obj6  0.01 0.15 0.37 0.36 0.09 0.01    0
Obj7  0.04 0.16 0.31 0.29 0.13 0.06    0
Obj8  0.02 0.11 0.36 0.42 0.09 0.01    0
Obj9  0.01 0.10 0.33 0.37 0.17 0.02    0
Obj10 0.02 0.15 0.47 0.31 0.04 0.00    0
Marg1 0.00 0.07 0.34 0.39 0.17 0.02    0
Marg2 0.00 0.07 0.29 0.44 0.18 0.01    0
Marg3 0.03 0.16 0.32 0.34 0.12 0.02    0
Marg4 0.01 0.11 0.39 0.39 0.10 0.00    0
Marg5 0.01 0.18 0.39 0.33 0.08 0.01    0
Marg6 0.01 0.09 0.31 0.40 0.18 0.02    0
Marg7 0.02 0.16 0.28 0.39 0.13 0.03    0
Str1  0.01 0.06 0.36 0.37 0.17 0.03    0
Str2  0.01 0.08 0.36 0.40 0.14 0.02    0
Str3  0.02 0.17 0.42 0.27 0.12 0.01    0
Str4  0.01 0.14 0.52 0.28 0.05 0.00    0
Str5  0.02 0.15 0.31 0.32 0.20 0.00    0
Ang1  0.02 0.14 0.31 0.36 0.15 0.03    0
Ang2  0.03 0.22 0.32 0.33 0.08 0.02    0
Ang3  0.02 0.13 0.39 0.38 0.07 0.01    0
\end{verbatim}

Alpha for the total scale score is 0.84.

And now each of the subscales:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ObjAlpha }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(Objectification)  }\CommentTok{\#creating an object from this analysis so I can extract and manipulate the item statistics (specifically the r.drop)}
\NormalTok{ObjAlpha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = Objectification)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
      0.76      0.77    0.76      0.25 3.3 0.022  2.5 0.55     0.25

    95% confidence boundaries 
         lower alpha upper
Feldt     0.72  0.76  0.81
Duhachek  0.72  0.76  0.81

 Reliability if an item is dropped:
      raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r
Obj1       0.74      0.74    0.73      0.24 2.9    0.024 0.0030  0.25
Obj2       0.74      0.74    0.73      0.24 2.8    0.024 0.0032  0.24
Obj3       0.75      0.75    0.73      0.25 3.0    0.024 0.0036  0.25
Obj4       0.74      0.74    0.72      0.24 2.9    0.024 0.0032  0.25
Obj5       0.74      0.75    0.73      0.25 2.9    0.024 0.0028  0.25
Obj6       0.76      0.76    0.74      0.26 3.2    0.023 0.0026  0.26
Obj7       0.75      0.75    0.73      0.25 3.0    0.023 0.0032  0.25
Obj8       0.74      0.74    0.73      0.24 2.9    0.024 0.0030  0.25
Obj9       0.75      0.76    0.74      0.26 3.1    0.023 0.0033  0.26
Obj10      0.75      0.75    0.74      0.25 3.1    0.023 0.0032  0.25

 Item statistics 
        n raw.r std.r r.cor r.drop mean   sd
Obj1  259  0.59  0.60  0.54   0.47  2.8 0.87
Obj2  259  0.61  0.62  0.56   0.49  2.6 0.91
Obj3  259  0.57  0.57  0.49   0.43  2.6 0.99
Obj4  259  0.63  0.61  0.56   0.49  2.6 1.05
Obj5  259  0.58  0.58  0.51   0.44  2.3 0.94
Obj6  259  0.49  0.50  0.40   0.35  2.4 0.93
Obj7  259  0.60  0.57  0.50   0.43  2.5 1.20
Obj8  259  0.58  0.59  0.52   0.45  2.5 0.90
Obj9  259  0.52  0.51  0.42   0.37  2.7 0.97
Obj10 259  0.52  0.53  0.44   0.39  2.2 0.83

Non missing response frequency for each item
         0    1    2    3    4    5 miss
Obj1  0.01 0.07 0.27 0.49 0.16 0.01    0
Obj2  0.01 0.07 0.36 0.41 0.12 0.03    0
Obj3  0.02 0.11 0.32 0.39 0.14 0.02    0
Obj4  0.01 0.14 0.32 0.33 0.16 0.04    0
Obj5  0.02 0.16 0.39 0.33 0.09 0.01    0
Obj6  0.01 0.15 0.37 0.36 0.09 0.01    0
Obj7  0.04 0.16 0.31 0.29 0.13 0.06    0
Obj8  0.02 0.11 0.36 0.42 0.09 0.01    0
Obj9  0.01 0.10 0.33 0.37 0.17 0.02    0
Obj10 0.02 0.15 0.47 0.31 0.04 0.00    0
\end{verbatim}

Alpha for the Assumptions of Beauty and Sexual Objectification scale is 0.77.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MargAlpha }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(Marginalization)  }\CommentTok{\#creating an object from this analysis so I can extract and manipulate the item statistics (specifically the r.drop)}
\NormalTok{MargAlpha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = Marginalization)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r
      0.75      0.75    0.73       0.3   3 0.024  2.6 0.6      0.3

    95% confidence boundaries 
         lower alpha upper
Feldt      0.7  0.75  0.79
Duhachek   0.7  0.75  0.80

 Reliability if an item is dropped:
      raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r
Marg1      0.69      0.69    0.66      0.27 2.3    0.030 0.0038  0.27
Marg2      0.71      0.71    0.68      0.29 2.5    0.028 0.0054  0.29
Marg3      0.72      0.73    0.70      0.31 2.6    0.027 0.0059  0.30
Marg4      0.73      0.73    0.70      0.31 2.8    0.026 0.0055  0.31
Marg5      0.71      0.72    0.69      0.30 2.5    0.028 0.0062  0.31
Marg6      0.73      0.74    0.71      0.32 2.8    0.026 0.0064  0.32
Marg7      0.74      0.74    0.72      0.33 2.9    0.025 0.0045  0.32

 Item statistics 
        n raw.r std.r r.cor r.drop mean   sd
Marg1 259  0.73  0.74  0.69   0.60  2.7 0.91
Marg2 259  0.67  0.68  0.60   0.52  2.8 0.89
Marg3 259  0.64  0.63  0.53   0.46  2.4 1.05
Marg4 259  0.58  0.60  0.49   0.42  2.5 0.87
Marg5 259  0.65  0.66  0.58   0.50  2.3 0.92
Marg6 259  0.59  0.59  0.47   0.41  2.7 0.94
Marg7 259  0.58  0.56  0.43   0.37  2.5 1.04

Non missing response frequency for each item
         0    1    2    3    4    5 miss
Marg1 0.00 0.07 0.34 0.39 0.17 0.02    0
Marg2 0.00 0.07 0.29 0.44 0.18 0.01    0
Marg3 0.03 0.16 0.32 0.34 0.12 0.02    0
Marg4 0.01 0.11 0.39 0.39 0.10 0.00    0
Marg5 0.01 0.18 0.39 0.33 0.08 0.01    0
Marg6 0.01 0.09 0.31 0.40 0.18 0.02    0
Marg7 0.02 0.16 0.28 0.39 0.13 0.03    0
\end{verbatim}

Alpha for the Silenced and Marginalized Scale is 0.75.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{StrongAlpha }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(Strong)  }\CommentTok{\#creating an object from this analysis so I can extract and manipulate the item statistics (specifically the r.drop)}
\NormalTok{StrongAlpha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = Strong)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
      0.56      0.57    0.52      0.21 1.3 0.043  2.5 0.56     0.21

    95% confidence boundaries 
         lower alpha upper
Feldt     0.47  0.56  0.64
Duhachek  0.48  0.56  0.64

 Reliability if an item is dropped:
     raw_alpha std.alpha G6(smc) average_r  S/N alpha se  var.r med.r
Str1      0.46      0.47    0.40      0.18 0.89    0.054 0.0032  0.19
Str2      0.53      0.53    0.47      0.22 1.13    0.048 0.0042  0.23
Str3      0.47      0.48    0.41      0.19 0.91    0.053 0.0027  0.21
Str4      0.51      0.51    0.45      0.21 1.05    0.050 0.0035  0.21
Str5      0.55      0.55    0.48      0.24 1.23    0.045 0.0017  0.22

 Item statistics 
       n raw.r std.r r.cor r.drop mean   sd
Str1 259  0.65  0.65  0.52   0.39  2.7 0.93
Str2 259  0.57  0.58  0.38   0.28  2.6 0.91
Str3 259  0.65  0.64  0.51   0.38  2.3 0.98
Str4 259  0.56  0.60  0.43   0.32  2.2 0.80
Str5 259  0.58  0.55  0.33   0.25  2.5 1.05

Non missing response frequency for each item
        0    1    2    3    4    5 miss
Str1 0.01 0.06 0.36 0.37 0.17 0.03    0
Str2 0.01 0.08 0.36 0.40 0.14 0.02    0
Str3 0.02 0.17 0.42 0.27 0.12 0.01    0
Str4 0.01 0.14 0.52 0.28 0.05 0.00    0
Str5 0.02 0.15 0.31 0.32 0.20 0.00    0
\end{verbatim}

Alpha for the Strong Black Woman Stereotype is 0.57.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AngryAlpha }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(Angry)  }\CommentTok{\#creating an object from this analysis so I can extract and manipulate the item statistics (specifically the r.drop)}
\NormalTok{AngryAlpha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = Angry)

  raw_alpha std.alpha G6(smc) average_r  S/N   ase mean  sd median_r
      0.49      0.49    0.39      0.24 0.95 0.055  2.4 0.7     0.24

    95% confidence boundaries 
         lower alpha upper
Feldt     0.37  0.49  0.59
Duhachek  0.38  0.49  0.59

 Reliability if an item is dropped:
     raw_alpha std.alpha G6(smc) average_r  S/N alpha se var.r med.r
Ang1      0.40      0.40    0.25      0.25 0.67    0.074    NA  0.25
Ang2      0.37      0.38    0.23      0.23 0.60    0.077    NA  0.23
Ang3      0.39      0.39    0.24      0.24 0.63    0.076    NA  0.24

 Item statistics 
       n raw.r std.r r.cor r.drop mean   sd
Ang1 259  0.71  0.70  0.43   0.30  2.6 1.04
Ang2 259  0.72  0.71  0.45   0.31  2.3 1.05
Ang3 259  0.67  0.70  0.44   0.31  2.4 0.91

Non missing response frequency for each item
        0    1    2    3    4    5 miss
Ang1 0.02 0.14 0.31 0.36 0.15 0.03    0
Ang2 0.03 0.22 0.32 0.33 0.08 0.02    0
Ang3 0.02 0.13 0.39 0.38 0.07 0.01    0
\end{verbatim}

Alpha for the Angry Black Woman Stereotypes is 0.49.

\hypertarget{correlating-items-with-other-subscale-totals}{%
\subsubsection{Correlating items with other subscale totals}\label{correlating-items-with-other-subscale-totals}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Obj\_othR }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{corr.test}\NormalTok{(dfGRMS[}\FunctionTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{,}
    \StringTok{"Obj6"}\NormalTok{, }\StringTok{"Obj7"}\NormalTok{, }\StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{, }\StringTok{"Marginalized"}\NormalTok{, }\StringTok{"Strong"}\NormalTok{,}
    \StringTok{"Angry"}\NormalTok{)])}
\NormalTok{Obj\_othR}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:psych::corr.test(x = dfGRMS[c("Obj1", "Obj2", "Obj3", "Obj4", 
    "Obj5", "Obj6", "Obj7", "Obj8", "Obj9", "Obj10", "Marginalized", 
    "Strong", "Angry")])
Correlation matrix 
             Obj1 Obj2 Obj3 Obj4 Obj5 Obj6 Obj7 Obj8 Obj9 Obj10 Marginalized
Obj1         1.00 0.35 0.25 0.27 0.28 0.25 0.28 0.35 0.15  0.24         0.28
Obj2         0.35 1.00 0.31 0.25 0.27 0.23 0.31 0.28 0.26  0.24         0.35
Obj3         0.25 0.31 1.00 0.24 0.28 0.28 0.20 0.25 0.21  0.22         0.30
Obj4         0.27 0.25 0.24 1.00 0.39 0.23 0.28 0.30 0.26  0.28         0.33
Obj5         0.28 0.27 0.28 0.39 1.00 0.15 0.18 0.29 0.25  0.20         0.30
Obj6         0.25 0.23 0.28 0.23 0.15 1.00 0.20 0.14 0.21  0.12         0.22
Obj7         0.28 0.31 0.20 0.28 0.18 0.20 1.00 0.31 0.19  0.28         0.36
Obj8         0.35 0.28 0.25 0.30 0.29 0.14 0.31 1.00 0.19  0.23         0.31
Obj9         0.15 0.26 0.21 0.26 0.25 0.21 0.19 0.19 1.00  0.20         0.22
Obj10        0.24 0.24 0.22 0.28 0.20 0.12 0.28 0.23 0.20  1.00         0.22
Marginalized 0.28 0.35 0.30 0.33 0.30 0.22 0.36 0.31 0.22  0.22         1.00
Strong       0.27 0.24 0.25 0.18 0.26 0.17 0.26 0.14 0.20  0.17         0.34
Angry        0.15 0.16 0.13 0.14 0.15 0.18 0.24 0.05 0.11  0.14         0.38
             Strong Angry
Obj1           0.27  0.15
Obj2           0.24  0.16
Obj3           0.25  0.13
Obj4           0.18  0.14
Obj5           0.26  0.15
Obj6           0.17  0.18
Obj7           0.26  0.24
Obj8           0.14  0.05
Obj9           0.20  0.11
Obj10          0.17  0.14
Marginalized   0.34  0.38
Strong         1.00  0.30
Angry          0.30  1.00
Sample Size 
[1] 259
Probability values (Entries above the diagonal are adjusted for multiple tests.) 
             Obj1 Obj2 Obj3 Obj4 Obj5 Obj6 Obj7 Obj8 Obj9 Obj10 Marginalized
Obj1         0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.16  0.00         0.00
Obj2         0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00         0.00
Obj3         0.00 0.00 0.00 0.00 0.00 0.00 0.03 0.00 0.02  0.01         0.00
Obj4         0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00  0.00         0.00
Obj5         0.00 0.00 0.00 0.00 0.00 0.16 0.07 0.00 0.00  0.02         0.00
Obj6         0.00 0.00 0.00 0.00 0.01 0.00 0.03 0.18 0.02  0.18         0.01
Obj7         0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04  0.00         0.00
Obj8         0.00 0.00 0.00 0.00 0.00 0.02 0.00 0.00 0.04  0.01         0.00
Obj9         0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.03         0.01
Obj10        0.00 0.00 0.00 0.00 0.00 0.05 0.00 0.00 0.00  0.00         0.01
Marginalized 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00         0.00
Strong       0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.02 0.00  0.01         0.00
Angry        0.02 0.01 0.04 0.03 0.02 0.00 0.00 0.40 0.08  0.03         0.00
             Strong Angry
Obj1           0.00  0.16
Obj2           0.00  0.12
Obj3           0.00  0.18
Obj4           0.06  0.18
Obj5           0.00  0.17
Obj6           0.09  0.05
Obj7           0.00  0.00
Obj8           0.18  0.40
Obj9           0.03  0.18
Obj10          0.09  0.18
Marginalized   0.00  0.00
Strong         0.00  0.00
Angry          0.00  0.00

 To see confidence intervals of the correlations, print with the short=FALSE option
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Marg\_othR }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{corr.test}\NormalTok{(dfGRMS[}\FunctionTok{c}\NormalTok{(}\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{,}
    \StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{, }\StringTok{"Marg7"}\NormalTok{, }\StringTok{"Objectified"}\NormalTok{, }\StringTok{"Strong"}\NormalTok{, }\StringTok{"Angry"}\NormalTok{)])}
\NormalTok{Marg\_othR}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:psych::corr.test(x = dfGRMS[c("Marg1", "Marg2", "Marg3", "Marg4", 
    "Marg5", "Marg6", "Marg7", "Objectified", "Strong", "Angry")])
Correlation matrix 
            Marg1 Marg2 Marg3 Marg4 Marg5 Marg6 Marg7 Objectified Strong Angry
Marg1        1.00  0.43  0.41  0.38  0.41  0.35  0.31        0.33   0.21  0.28
Marg2        0.43  1.00  0.35  0.23  0.40  0.27  0.33        0.32   0.32  0.33
Marg3        0.41  0.35  1.00  0.32  0.25  0.25  0.20        0.30   0.22  0.23
Marg4        0.38  0.23  0.32  1.00  0.30  0.26  0.16        0.33   0.15  0.24
Marg5        0.41  0.40  0.25  0.30  1.00  0.29  0.28        0.36   0.26  0.22
Marg6        0.35  0.27  0.25  0.26  0.29  1.00  0.20        0.40   0.22  0.21
Marg7        0.31  0.33  0.20  0.16  0.28  0.20  1.00        0.25   0.13  0.19
Objectified  0.33  0.32  0.30  0.33  0.36  0.40  0.25        1.00   0.38  0.26
Strong       0.21  0.32  0.22  0.15  0.26  0.22  0.13        0.38   1.00  0.30
Angry        0.28  0.33  0.23  0.24  0.22  0.21  0.19        0.26   0.30  1.00
Sample Size 
[1] 259
Probability values (Entries above the diagonal are adjusted for multiple tests.) 
            Marg1 Marg2 Marg3 Marg4 Marg5 Marg6 Marg7 Objectified Strong Angry
Marg1           0     0     0  0.00     0     0  0.00           0   0.01  0.00
Marg2           0     0     0  0.00     0     0  0.00           0   0.00  0.00
Marg3           0     0     0  0.00     0     0  0.01           0   0.00  0.00
Marg4           0     0     0  0.00     0     0  0.02           0   0.03  0.00
Marg5           0     0     0  0.00     0     0  0.00           0   0.00  0.00
Marg6           0     0     0  0.00     0     0  0.01           0   0.00  0.01
Marg7           0     0     0  0.01     0     0  0.00           0   0.04  0.01
Objectified     0     0     0  0.00     0     0  0.00           0   0.00  0.00
Strong          0     0     0  0.02     0     0  0.04           0   0.00  0.00
Angry           0     0     0  0.00     0     0  0.00           0   0.00  0.00

 To see confidence intervals of the correlations, print with the short=FALSE option
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Str\_othR }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{corr.test}\NormalTok{(dfGRMS[}\FunctionTok{c}\NormalTok{(}\StringTok{"Str1"}\NormalTok{, }\StringTok{"Str2"}\NormalTok{, }\StringTok{"Str3"}\NormalTok{, }\StringTok{"Str4"}\NormalTok{, }\StringTok{"Str5"}\NormalTok{,}
    \StringTok{"Objectified"}\NormalTok{, }\StringTok{"Marginalized"}\NormalTok{, }\StringTok{"Angry"}\NormalTok{)])}
\NormalTok{Str\_othR}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:psych::corr.test(x = dfGRMS[c("Str1", "Str2", "Str3", "Str4", 
    "Str5", "Objectified", "Marginalized", "Angry")])
Correlation matrix 
             Str1 Str2 Str3 Str4 Str5 Objectified Marginalized Angry
Str1         1.00 0.21 0.30 0.23 0.23        0.28         0.26  0.16
Str2         0.21 1.00 0.20 0.20 0.12        0.28         0.24  0.21
Str3         0.30 0.20 1.00 0.27 0.18        0.18         0.18  0.20
Str4         0.23 0.20 0.27 1.00 0.12        0.18         0.14  0.10
Str5         0.23 0.12 0.18 0.12 1.00        0.21         0.20  0.23
Objectified  0.28 0.28 0.18 0.18 0.21        1.00         0.51  0.26
Marginalized 0.26 0.24 0.18 0.14 0.20        0.51         1.00  0.38
Angry        0.16 0.21 0.20 0.10 0.23        0.26         0.38  1.00
Sample Size 
[1] 259
Probability values (Entries above the diagonal are adjusted for multiple tests.) 
             Str1 Str2 Str3 Str4 Str5 Objectified Marginalized Angry
Str1         0.00 0.01 0.00 0.00 0.00        0.00         0.00  0.05
Str2         0.00 0.00 0.02 0.01 0.15        0.00         0.00  0.01
Str3         0.00 0.00 0.00 0.00 0.03        0.03         0.03  0.02
Str4         0.00 0.00 0.00 0.00 0.15        0.03         0.10  0.15
Str5         0.00 0.05 0.00 0.06 0.00        0.01         0.02  0.00
Objectified  0.00 0.00 0.00 0.00 0.00        0.00         0.00  0.00
Marginalized 0.00 0.00 0.00 0.02 0.00        0.00         0.00  0.00
Angry        0.01 0.00 0.00 0.11 0.00        0.00         0.00  0.00

 To see confidence intervals of the correlations, print with the short=FALSE option
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Ang\_othR }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{corr.test}\NormalTok{(dfGRMS[}\FunctionTok{c}\NormalTok{(}\StringTok{"Ang1"}\NormalTok{, }\StringTok{"Ang2"}\NormalTok{, }\StringTok{"Ang3"}\NormalTok{, }\StringTok{"Objectified"}\NormalTok{,}
    \StringTok{"Marginalized"}\NormalTok{, }\StringTok{"Strong"}\NormalTok{)])}
\NormalTok{Ang\_othR}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:psych::corr.test(x = dfGRMS[c("Ang1", "Ang2", "Ang3", "Objectified", 
    "Marginalized", "Strong")])
Correlation matrix 
             Ang1 Ang2 Ang3 Objectified Marginalized Strong
Ang1         1.00 0.24 0.23        0.16         0.25   0.31
Ang2         0.24 1.00 0.25        0.15         0.28   0.15
Ang3         0.23 0.25 1.00        0.23         0.28   0.18
Objectified  0.16 0.15 0.23        1.00         0.51   0.38
Marginalized 0.25 0.28 0.28        0.51         1.00   0.34
Strong       0.31 0.15 0.18        0.38         0.34   1.00
Sample Size 
[1] 259
Probability values (Entries above the diagonal are adjusted for multiple tests.) 
             Ang1 Ang2 Ang3 Objectified Marginalized Strong
Ang1         0.00 0.00    0        0.02            0   0.00
Ang2         0.00 0.00    0        0.03            0   0.03
Ang3         0.00 0.00    0        0.00            0   0.01
Objectified  0.01 0.01    0        0.00            0   0.00
Marginalized 0.00 0.00    0        0.00            0   0.00
Strong       0.00 0.02    0        0.00            0   0.00

 To see confidence intervals of the correlations, print with the short=FALSE option
\end{verbatim}

\hypertarget{exctracting-values-binding-them-together-and-joining-the-files}{%
\subsubsection{Exctracting values, binding them together, and joining the files}\label{exctracting-values-binding-them-together-and-joining-the-files}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# names(Obj\_other) Extracting the item{-}level statistics from the}
\CommentTok{\# alpha object}
\NormalTok{Obj\_othR }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(Obj\_othR}\SpecialCharTok{$}\NormalTok{r)  }\CommentTok{\#Makes the item{-}total(other) correlation matrix a df}
\CommentTok{\# Adding variable names so we don\textquotesingle{}t get lost}
\NormalTok{Obj\_othR}\SpecialCharTok{$}\NormalTok{Items }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{, }\StringTok{"Obj7"}\NormalTok{,}
    \StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{, }\StringTok{"Marginalized"}\NormalTok{, }\StringTok{"Strong"}\NormalTok{, }\StringTok{"Angry"}\NormalTok{)}
\CommentTok{\# deleting the ROWS with the total scale scores (the columns)}
\NormalTok{Obj\_othR }\OtherTok{\textless{}{-}}\NormalTok{ Obj\_othR[}\SpecialCharTok{!}\NormalTok{Obj\_othR}\SpecialCharTok{$}\NormalTok{Items }\SpecialCharTok{==} \StringTok{"Marginalized"}\NormalTok{, ]}
\NormalTok{Obj\_othR }\OtherTok{\textless{}{-}}\NormalTok{ Obj\_othR[}\SpecialCharTok{!}\NormalTok{Obj\_othR}\SpecialCharTok{$}\NormalTok{Items }\SpecialCharTok{==} \StringTok{"Strong"}\NormalTok{, ]}
\NormalTok{Obj\_othR }\OtherTok{\textless{}{-}}\NormalTok{ Obj\_othR[}\SpecialCharTok{!}\NormalTok{Obj\_othR}\SpecialCharTok{$}\NormalTok{Items }\SpecialCharTok{==} \StringTok{"Angry"}\NormalTok{, ]}
\NormalTok{Obj\_othR[, }\StringTok{"Objectified"}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}  \CommentTok{\#We need a column for this to bind the items, later.}
\NormalTok{Obj\_othR }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Obj\_othR, Items, Objectified, Marginalized, Strong,}
\NormalTok{    Angry)  }\CommentTok{\#Putting items in order}
\CommentTok{\# Item Corrected Total Correlations}
\NormalTok{ObjAlpha }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(ObjAlpha}\SpecialCharTok{$}\NormalTok{item.stats)  }\CommentTok{\#Grabbing the alpha object we created earlier and making it a df }
\NormalTok{ObjAlpha}\SpecialCharTok{$}\NormalTok{Items }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{, }\StringTok{"Obj7"}\NormalTok{,}
    \StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{)}
\CommentTok{\# Joining the two and selecting the vars of interest}
\NormalTok{ObjStats }\OtherTok{\textless{}{-}} \FunctionTok{full\_join}\NormalTok{(ObjAlpha, Obj\_othR, }\AttributeTok{by =} \StringTok{"Items"}\NormalTok{)}
\NormalTok{ObjStats}\SpecialCharTok{$}\NormalTok{Objectified }\OtherTok{\textless{}{-}}\NormalTok{ ObjStats}\SpecialCharTok{$}\NormalTok{r.drop  }\CommentTok{\#Copy the item{-}corrected total (r.drop) into the Objectified variable}
\NormalTok{ObjStats }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(ObjStats, Items, Objectified, Marginalized, Strong,}
\NormalTok{    Angry)}
\CommentTok{\# rm(ObjAlpha, Obj\_othR) \#It\textquotesingle{}s messay, dropping all the}
\CommentTok{\# no{-}longer{-}necessary objects from the Global Environment}


\CommentTok{\# Extracting the item{-}level statistics from the alpha object}
\NormalTok{Marg\_othR }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(Marg\_othR}\SpecialCharTok{$}\NormalTok{r)  }\CommentTok{\#Makes the item{-}total(other) correlation matrix a df}
\CommentTok{\# Adding variable names so we don\textquotesingle{}t get lost}
\NormalTok{Marg\_othR}\SpecialCharTok{$}\NormalTok{Items }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{, }\StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{,}
    \StringTok{"Marg7"}\NormalTok{, }\StringTok{"Objectified"}\NormalTok{, }\StringTok{"Strong"}\NormalTok{, }\StringTok{"Angry"}\NormalTok{)}
\CommentTok{\# deleting the rows with the total scale scores}
\NormalTok{Marg\_othR }\OtherTok{\textless{}{-}}\NormalTok{ Marg\_othR[}\SpecialCharTok{!}\NormalTok{Marg\_othR}\SpecialCharTok{$}\NormalTok{Items }\SpecialCharTok{==} \StringTok{"Objectified"}\NormalTok{, ]}
\NormalTok{Marg\_othR }\OtherTok{\textless{}{-}}\NormalTok{ Marg\_othR[}\SpecialCharTok{!}\NormalTok{Marg\_othR}\SpecialCharTok{$}\NormalTok{Items }\SpecialCharTok{==} \StringTok{"Strong"}\NormalTok{, ]}
\NormalTok{Marg\_othR }\OtherTok{\textless{}{-}}\NormalTok{ Marg\_othR[}\SpecialCharTok{!}\NormalTok{Marg\_othR}\SpecialCharTok{$}\NormalTok{Items }\SpecialCharTok{==} \StringTok{"Angry"}\NormalTok{, ]}
\NormalTok{Marg\_othR[, }\StringTok{"Marginalized"}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}  \CommentTok{\#We need a column for this to bind the items, later.}
\NormalTok{Marg\_othR }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Marg\_othR, Items, Objectified, Marginalized,}
\NormalTok{    Strong, Angry)}
\CommentTok{\# Item Corrected Total Correlations}
\NormalTok{MargAlpha }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(MargAlpha}\SpecialCharTok{$}\NormalTok{item.stats)  }\CommentTok{\#Grabbing the alpha objet we created earlier and making it a df  }
\NormalTok{MargAlpha}\SpecialCharTok{$}\NormalTok{Items }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{, }\StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{,}
    \StringTok{"Marg7"}\NormalTok{)}
\CommentTok{\# Joining the two and selecting the vars of interest}
\NormalTok{MargStats }\OtherTok{\textless{}{-}} \FunctionTok{full\_join}\NormalTok{(MargAlpha, Marg\_othR, }\AttributeTok{by =} \StringTok{"Items"}\NormalTok{)}
\NormalTok{MargStats}\SpecialCharTok{$}\NormalTok{Marginalized }\OtherTok{\textless{}{-}}\NormalTok{ MargStats}\SpecialCharTok{$}\NormalTok{r.drop  }\CommentTok{\#Copy the item{-}corrected total (r.drop) into the Marginalized variable}
\NormalTok{MargStats }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(MargStats, Items, Objectified, Marginalized,}
\NormalTok{    Strong, Angry)}
\CommentTok{\# rm(MargAlpha, Marg\_othR) \#It\textquotesingle{}s messay, dropping all the}
\CommentTok{\# no{-}longer{-}necessary objects from the Global Environment}

\NormalTok{Str\_othR }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(Str\_othR}\SpecialCharTok{$}\NormalTok{r)  }\CommentTok{\#Makes the item{-}total(other) correlation matrix a df}
\CommentTok{\# Adding variable names so we don\textquotesingle{}t get lost}
\NormalTok{Str\_othR}\SpecialCharTok{$}\NormalTok{Items }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Strong1"}\NormalTok{, }\StringTok{"Strong2"}\NormalTok{, }\StringTok{"Strong3"}\NormalTok{, }\StringTok{"Strong4"}\NormalTok{, }\StringTok{"Strong5"}\NormalTok{,}
    \StringTok{"Objectified"}\NormalTok{, }\StringTok{"Marginalized"}\NormalTok{, }\StringTok{"Angry"}\NormalTok{)}
\CommentTok{\# deleting the rows with the total scale scores}
\NormalTok{Str\_othR }\OtherTok{\textless{}{-}}\NormalTok{ Str\_othR[}\SpecialCharTok{!}\NormalTok{Str\_othR}\SpecialCharTok{$}\NormalTok{Items }\SpecialCharTok{==} \StringTok{"Objectified"}\NormalTok{, ]}
\NormalTok{Str\_othR }\OtherTok{\textless{}{-}}\NormalTok{ Str\_othR[}\SpecialCharTok{!}\NormalTok{Str\_othR}\SpecialCharTok{$}\NormalTok{Items }\SpecialCharTok{==} \StringTok{"Marginalized"}\NormalTok{, ]}
\NormalTok{Str\_othR }\OtherTok{\textless{}{-}}\NormalTok{ Str\_othR[}\SpecialCharTok{!}\NormalTok{Str\_othR}\SpecialCharTok{$}\NormalTok{Items }\SpecialCharTok{==} \StringTok{"Angry"}\NormalTok{, ]}
\NormalTok{Str\_othR[, }\StringTok{"Strong"}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\NormalTok{Str\_othR }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Str\_othR, Items, Objectified, Marginalized, Strong,}
\NormalTok{    Angry)}
\CommentTok{\# Item Corrected Total Correlations}
\NormalTok{StrongAlpha }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(StrongAlpha}\SpecialCharTok{$}\NormalTok{item.stats)  }\CommentTok{\#Grabbing the alpha objet we created earlier and making it a df  }
\NormalTok{StrongAlpha}\SpecialCharTok{$}\NormalTok{Items }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Strong1"}\NormalTok{, }\StringTok{"Strong2"}\NormalTok{, }\StringTok{"Strong3"}\NormalTok{, }\StringTok{"Strong4"}\NormalTok{, }\StringTok{"Strong5"}\NormalTok{)}
\CommentTok{\# Joining the two and selecting the vars of interest}
\NormalTok{StrStats }\OtherTok{\textless{}{-}} \FunctionTok{full\_join}\NormalTok{(StrongAlpha, Str\_othR, }\AttributeTok{by =} \StringTok{"Items"}\NormalTok{)}
\NormalTok{StrStats}\SpecialCharTok{$}\NormalTok{Strong }\OtherTok{\textless{}{-}}\NormalTok{ StrStats}\SpecialCharTok{$}\NormalTok{r.drop  }\CommentTok{\#Copy the item{-}corrected total (r.drop) into the Strong variable}
\NormalTok{StrStats }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(StrStats, Items, Objectified, Marginalized, Strong,}
\NormalTok{    Angry)}
\FunctionTok{rm}\NormalTok{(StrongAlpha, Str\_othR)  }\CommentTok{\#It\textquotesingle{}s messay, dropping all the no{-}longer{-}necessary objects from the Global Environment}

\NormalTok{Ang\_othR }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(Ang\_othR}\SpecialCharTok{$}\NormalTok{r)  }\CommentTok{\#Makes the item{-}total(other) correlation matrix a df}
\CommentTok{\# Adding variable names so we don\textquotesingle{}t get lost}
\NormalTok{Ang\_othR}\SpecialCharTok{$}\NormalTok{Items }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Angry1"}\NormalTok{, }\StringTok{"Angry2"}\NormalTok{, }\StringTok{"Angry3"}\NormalTok{, }\StringTok{"Objectified"}\NormalTok{, }\StringTok{"Marginalized"}\NormalTok{,}
    \StringTok{"Strong"}\NormalTok{)}
\CommentTok{\# deleting the rows with the total scale scores}
\NormalTok{Ang\_othR }\OtherTok{\textless{}{-}}\NormalTok{ Ang\_othR[}\SpecialCharTok{!}\NormalTok{Ang\_othR}\SpecialCharTok{$}\NormalTok{Items }\SpecialCharTok{==} \StringTok{"Objectified"}\NormalTok{, ]}
\NormalTok{Ang\_othR }\OtherTok{\textless{}{-}}\NormalTok{ Ang\_othR[}\SpecialCharTok{!}\NormalTok{Ang\_othR}\SpecialCharTok{$}\NormalTok{Items }\SpecialCharTok{==} \StringTok{"Marginalized"}\NormalTok{, ]}
\NormalTok{Ang\_othR }\OtherTok{\textless{}{-}}\NormalTok{ Ang\_othR[}\SpecialCharTok{!}\NormalTok{Ang\_othR}\SpecialCharTok{$}\NormalTok{Items }\SpecialCharTok{==} \StringTok{"Strong"}\NormalTok{, ]}
\NormalTok{Ang\_othR[, }\StringTok{"Angry"}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\NormalTok{Ang\_othR }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Ang\_othR, Items, Objectified, Marginalized, Strong,}
\NormalTok{    Angry)}
\CommentTok{\# Item Corrected Total Correlations}
\NormalTok{AngryAlpha }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(AngryAlpha}\SpecialCharTok{$}\NormalTok{item.stats)  }\CommentTok{\#Grabbing the alpha objet we created earlier and making it a df  }
\NormalTok{AngryAlpha}\SpecialCharTok{$}\NormalTok{Items }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Angry1"}\NormalTok{, }\StringTok{"Angry2"}\NormalTok{, }\StringTok{"Angry3"}\NormalTok{)}
\CommentTok{\# Joining the two and selecting the vars of interest}
\NormalTok{AngStats }\OtherTok{\textless{}{-}} \FunctionTok{full\_join}\NormalTok{(AngryAlpha, Ang\_othR, }\AttributeTok{by =} \StringTok{"Items"}\NormalTok{)}
\NormalTok{AngStats}\SpecialCharTok{$}\NormalTok{Angry }\OtherTok{\textless{}{-}}\NormalTok{ AngStats}\SpecialCharTok{$}\NormalTok{r.drop  }\CommentTok{\#Copy the item{-}corrected total (r.drop) into the Angry variable}
\NormalTok{AngStats }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(AngStats, Items, Objectified, Marginalized, Strong,}
\NormalTok{    Angry)}
\FunctionTok{rm}\NormalTok{(AngryAlpha, Ang\_othR)  }\CommentTok{\#It\textquotesingle{}s messay, dropping all the no{-}longer{-}necessary objects from the Global Environment}

\CommentTok{\# Adding all the variables into a single table}
\NormalTok{ItemAnalysis }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(ObjStats, MargStats, StrStats, AngStats)}

\CommentTok{\# Preparing and adding the r.drop for total scale score}
\NormalTok{TotAlpha }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(GRMSalpha}\SpecialCharTok{$}\NormalTok{item.stats)}
\NormalTok{TotAlpha}\SpecialCharTok{$}\NormalTok{Items }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{, }\StringTok{"Obj7"}\NormalTok{,}
    \StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{, }\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{, }\StringTok{"Marg5"}\NormalTok{,}
    \StringTok{"Marg6"}\NormalTok{, }\StringTok{"Marg7"}\NormalTok{, }\StringTok{"Strong1"}\NormalTok{, }\StringTok{"Strong2"}\NormalTok{, }\StringTok{"Strong3"}\NormalTok{, }\StringTok{"Strong4"}\NormalTok{, }\StringTok{"Strong5"}\NormalTok{,}
    \StringTok{"Angry1"}\NormalTok{, }\StringTok{"Angry2"}\NormalTok{, }\StringTok{"Angry3"}\NormalTok{)}
\NormalTok{TotAlpha }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(TotAlpha, Items, r.drop)  }\CommentTok{\#deleting the rows with the total scale scores}


\CommentTok{\# Adding the r.drop for the total scale score}
\NormalTok{ItemAnalysis }\OtherTok{\textless{}{-}} \FunctionTok{full\_join}\NormalTok{(TotAlpha, ItemAnalysis, }\AttributeTok{by =} \StringTok{"Items"}\NormalTok{)}

\CommentTok{\# Adding the values from the orthogonal rotation I had to add}
\CommentTok{\# \textquotesingle{}unclass\textquotesingle{} to the loadings to render them into a df}
\NormalTok{pcaORTH\_loadings }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{unclass}\NormalTok{(pcaORTH}\SpecialCharTok{$}\NormalTok{loadings))}
\CommentTok{\# Item names for joining (and to make sure we know which variable is}
\CommentTok{\# which)}
\NormalTok{pcaORTH\_loadings}\SpecialCharTok{$}\NormalTok{Items }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{,}
    \StringTok{"Obj7"}\NormalTok{, }\StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{, }\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{,}
    \StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{, }\StringTok{"Marg7"}\NormalTok{, }\StringTok{"Strong1"}\NormalTok{, }\StringTok{"Strong2"}\NormalTok{, }\StringTok{"Strong3"}\NormalTok{, }\StringTok{"Strong4"}\NormalTok{,}
    \StringTok{"Strong5"}\NormalTok{, }\StringTok{"Angry1"}\NormalTok{, }\StringTok{"Angry2"}\NormalTok{, }\StringTok{"Angry3"}\NormalTok{)}

\CommentTok{\# Deleting those lower rows}
\NormalTok{pcaORTH\_loadings }\OtherTok{\textless{}{-}} \FunctionTok{rename}\NormalTok{(pcaORTH\_loadings, }\AttributeTok{objORTH =}\NormalTok{ RC1, }\AttributeTok{margORTH =}\NormalTok{ RC2,}
    \AttributeTok{strORTH =}\NormalTok{ RC3, }\AttributeTok{angORTH2 =}\NormalTok{ RC4)}

\CommentTok{\# Joining with the Item Stats}
\NormalTok{Comparisons }\OtherTok{\textless{}{-}} \FunctionTok{full\_join}\NormalTok{(ItemAnalysis, pcaORTH\_loadings, }\AttributeTok{by =} \StringTok{"Items"}\NormalTok{)  }\CommentTok{\#I had to add \textquotesingle{}unclass\textquotesingle{} to the loadings to render them into a df}

\CommentTok{\# Adding the oblique loadings}
\NormalTok{pcaOBLQ\_loadings }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{unclass}\NormalTok{(pcaOBL}\SpecialCharTok{$}\NormalTok{loadings))  }\CommentTok{\#I had to add \textquotesingle{}unclass\textquotesingle{} to the loadings to render them into a df}
\NormalTok{pcaOBLQ\_loadings}\SpecialCharTok{$}\NormalTok{Items }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{,}
    \StringTok{"Obj7"}\NormalTok{, }\StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{, }\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{,}
    \StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{, }\StringTok{"Marg7"}\NormalTok{, }\StringTok{"Strong1"}\NormalTok{, }\StringTok{"Strong2"}\NormalTok{, }\StringTok{"Strong3"}\NormalTok{, }\StringTok{"Strong4"}\NormalTok{,}
    \StringTok{"Strong5"}\NormalTok{, }\StringTok{"Angry1"}\NormalTok{, }\StringTok{"Angry2"}\NormalTok{, }\StringTok{"Angry3"}\NormalTok{)  }\CommentTok{\#Item names for joining (and to make sure we know which variable is which)}
\CommentTok{\# Deleting those lower rows pcaOBLQ\_loadings \textless{}{-}}
\CommentTok{\# pcaOBLQ\_loadings[!pcaORTH\_loadings$Items == \textquotesingle{}GRMSTot\textquotesingle{},]}
\CommentTok{\# pcaOBLQ\_loadings \textless{}{-} pcaOBLQ\_loadings[!pcaORTH\_loadings$Items ==}
\CommentTok{\# \textquotesingle{}Objectified\textquotesingle{},] pcaOBLQ\_loadings \textless{}{-}}
\CommentTok{\# pcaOBLQ\_loadings[!pcaORTH\_loadings$Items == \textquotesingle{}Marginalized\textquotesingle{},]}
\CommentTok{\# pcaOBLQ\_loadings \textless{}{-} pcaOBLQ\_loadings[!pcaORTH\_loadings$Items ==}
\CommentTok{\# \textquotesingle{}Strong\textquotesingle{},] pcaOBLQ\_loadings \textless{}{-}}
\CommentTok{\# pcaOBLQ\_loadings[!pcaORTH\_loadings$Items == \textquotesingle{}Angry\textquotesingle{},]}
\NormalTok{pcaOBLQ\_loadings }\OtherTok{\textless{}{-}} \FunctionTok{rename}\NormalTok{(pcaOBLQ\_loadings, }\AttributeTok{margOBLQ =}\NormalTok{ TC1, }\AttributeTok{objOBLQ =}\NormalTok{ TC2,}
    \AttributeTok{strOBLQ =}\NormalTok{ TC3, }\AttributeTok{angOBLQ =}\NormalTok{ TC4)}

\CommentTok{\# Joining with the Item Stats}
\NormalTok{Comparisons }\OtherTok{\textless{}{-}} \FunctionTok{full\_join}\NormalTok{(Comparisons, pcaOBLQ\_loadings, }\AttributeTok{by =} \StringTok{"Items"}\NormalTok{)  }\CommentTok{\#I had to add \textquotesingle{}unclass\textquotesingle{} to the loadings to render them into a df}

\FunctionTok{write.csv}\NormalTok{(Comparisons, }\AttributeTok{file =} \StringTok{"GRMS\_Comparisons.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{col.names =} \ConstantTok{TRUE}\NormalTok{)  }\CommentTok{\#Writes the table to a .csv file where you can open it with Excel and format}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in write.csv(Comparisons, file = "GRMS_Comparisons.csv", sep = ",", :
attempt to set 'col.names' ignored
\end{verbatim}

\begin{verbatim}
Warning in write.csv(Comparisons, file = "GRMS_Comparisons.csv", sep = ",", :
attempt to set 'sep' ignored
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{saveRDS}\NormalTok{(Comparisons, }\StringTok{"GRMS\_Comparisons.rds"}\NormalTok{)  }\CommentTok{\#Writes the file as an .rds so that if anything is specially formatted, it is retained}
\end{Highlighting}
\end{Shaded}

\hypertarget{interpreting-the-result}{%
\subsubsection{Interpreting the result}\label{interpreting-the-result}}

The result of this work is a table that includes:

\begin{itemize}
\tightlist
\item
  \textbf{r.drop} Corrected item total (entire GRMS) coefficients
\item
  \textbf{Item-total correlations} of the items correlated with their own subscale (bold; correlation does not include the item being correlated) and the other subscales
\item
  \textbf{PCA: Orthogonal rotation} factor loadings of the four-scales with a rotation that maximizes the independents (uncorrelatedness) of the scales
\item
  \textbf{PCA: Oblique rotation} factor loadings of the four-scales with a rotation that permits correlation between subscales
\end{itemize}

\begin{figure}
\hypertarget{id}{%
\centering
\includegraphics[width=6.25in,height=4.16667in]{images/PCA/ComparisonsTable.png}
\caption{Image of a table of values from the item analysis and PCA solutions with orthogonal and oblique rotations}\label{id}
}
\end{figure}

We expect to see similar results across the item-analysis, PCA orthogonal, and PCA oblique solutions. Our biggest interest is in whether items change scale membership and/or have cross-loadings. Overall, we are looking for items that \emph{load} higher on their own scales than they do on other scales.

\begin{itemize}
\tightlist
\item
  When there are a number of cross-loadings, it means that the item will not discriminate well (think within-in scale discriminant validity).
\item
  If there are a number of cross-loadings, there will likely be stronger correlations between subscales (indicating that an oblique rotation is/was an appropriate choice).
\item
  Low/no cross-loadings, supports the choices of an orthogonal (uncorrelated) solution.
\item
  Within-scale convergent validity is supported when an item has a strong, positive loading on its own scale and low/zero loadings on the other scales..
\end{itemize}

Our simulation from the Lewis and Neville's \citeyearpar{lewis_construction_2015} GRMS produced slightly different results from their original data. Specifically, our ``Angry1'' item cross-loaded on both the Strong Angry subscales with slightly stronger loadings on the Strong (incorrect) subscale. The items behaved much better in the original article.

\hypertarget{practice-problems-6}{%
\section{Practice Problems}\label{practice-problems-6}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. In psychometrics, I strongly recommend that you have started with a dataset that has a minimum of three subscales and use it for all of the assignments in the OER. In any case, please plan to:

\begin{itemize}
\tightlist
\item
  Properly format and prepare the data.
\item
  Conduct diagnostic tests to determine the suitability of the data for PCA.
\item
  Conduct tests to guide the decisions about number of components to extract.
\item
  Conduct orthogonal and oblique rotations (at least two each with different numbers of extracted components).
\item
  Select one solution and preparing an APA style results section (with table and figure).
\end{itemize}

\hypertarget{problem-1-play-around-with-this-simulation.-3}{%
\subsection{Problem \#1: Play around with this simulation.}\label{problem-1-play-around-with-this-simulation.-3}}

Copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results. If PCA is new to you, perhaps you just change the number in ``set.seed(240311)'' from 240311 to something else. Your results should \emph{parallel} those obtained in the lecture, making it easier for you to check your work as you go. Don't be surprised if the factor loadings wiggle around a little. Do try to make sense of them.

\hypertarget{problem-2-conduct-a-pca-with-another-simulated-set-of-data-in-the-oer.}{%
\subsection{Problem \#2: Conduct a PCA with another simulated set of data in the OER.}\label{problem-2-conduct-a-pca-with-another-simulated-set-of-data-in-the-oer.}}

The second option involves utilizing one of the simulated datasets available in this OER. The \protect\hyperlink{sims}{last lesson} in the OER contains three simulations that could be used for all of the statistics-based practice suggestions. Especially if you started with one of these examples in an earlier lesson, I highly recommend you continue with that.

Alternatively, Keum et al.'s Gendered Racial Microaggressions Scale for Asian American Women \citep{keum_gendered_2018} will be used in the lessons on confirmatory factor analysis and Conover et al.'s \citep{conover_development_2017} Ableist Microaggressions Scale is used in the lesson on invariance testing. Both of these would be suitable for the PCA and PAF homework assignments.

\hypertarget{problem-3-try-something-entirely-new.-3}{%
\subsection{Problem \#3: Try something entirely new.}\label{problem-3-try-something-entirely-new.-3}}

Using data for which you have permission and access (e.g., IRB approved data you have collected or from your lab; data you simulate from a published article; data from the ReCentering Psych Stats survey described in the \protect\hyperlink{qualTRIXdata}{Qualtrics lesson}, or data from an open science repository), complete a PCA analysis. The data should allow for at least three factors/subscales.

\hypertarget{grading-rubric-3}{%
\subsection{Grading Rubric}\label{grading-rubric-3}}

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5493}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1972}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Possible
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Earned
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1. Check and, if needed, format data & 5 & \_\_\_\_\_ \\
2. Conduct and interpret the three diagnostic tests to determine if PCA is appropriate as an analysis (KMO, Bartlett's, determinant). & 5 & \_\_\_\_\_ \\
3. Determine how many components to extract (e.g., scree plot, eigenvalues, theory). & 5 & \_\_\_\_\_ \\
4. Conduct an orthogonal rotation with a minimum of two different numbers of component extractions. & 5 & \_\_\_\_\_ \\
5. Conduct an oblique rotation with a minimum of two different numbers of component extractions. & 5 & \_\_\_\_\_ \\
6. Determine which factor solution (e.g., orthogonal or oblique; with which number of components) you will suggest. & 5 & \_\_\_\_\_ \\
7. APA style results section with table and figure of one of the solutions. & 5 & \_\_\_\_\_ \\
8. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 40 & \_\_\_\_\_ \\
\end{longtable}

\hypertarget{homeworked-example-3}{%
\section{Homeworked Example}\label{homeworked-example-3}}

\href{https://youtu.be/LPtAwkICR0w}{Screencast Link}

For more information about the data used in this homeworked example, please refer to the description and codebook located at the end of the \href{https://lhbikos.github.io/ReCenterPsychStats/ReCintro.html\#introduction-to-the-data-set-used-for-homeworked-examples}{introduction} in first volume of ReCentering Psych Stats.

As a brief review, this data is part of an IRB-approved study, with consent to use in teaching demonstrations and to be made available to the general public via the open science framework. Hence, it is appropriate to use in this context. You will notice there are student- and teacher- IDs. These numbers are not actual student and teacher IDs, rather they were further re-identified so that they could not be connected to actual people.

Because this is an actual dataset, if you wish to work the problem along with me, you will need to download the \href{https://github.com/lhbikos/ReC_Psychometrics/blob/main/Worked_Examples/ReC.rds}{ReC.rds} data file from the Worked\_Examples folder in the ReC\_Psychometrics project on the GitHub.

The course evaluation items can be divided into three subscales:

\begin{itemize}
\tightlist
\item
  \textbf{Valued by the student} includes the items: ValObjectives, IncrUnderstanding, IncrInterest
\item
  \textbf{Traditional pedagogy} includes the items: ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation
\item
  \textbf{Socially responsive pedagogy} includes the items: InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration
\end{itemize}

In this homewoRked example I will conduct a principal components analysis. My hope is that the results will support my solution of three dimensions: valued-by-the-student, traditional pedagogy, socially responsive pedagogy.

\hypertarget{check-and-if-needed-format-data-1}{%
\subsection{Check and, if needed, format data}\label{check-and-if-needed-format-data-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{big }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"ReC.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

With the next code I will create an item-level df with only the items used in the three scales.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{items }\OtherTok{\textless{}{-}}\NormalTok{ big }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(ValObjectives, IncrUnderstanding, IncrInterest, ClearResponsibilities,}
\NormalTok{        EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation,}
\NormalTok{        MultPerspectives, InclusvClassrm, DEIintegration, EquitableEval)}
\end{Highlighting}
\end{Shaded}

Some of the analyses require non-missing data in the df.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{items }\OtherTok{\textless{}{-}} \FunctionTok{na.omit}\NormalTok{(items)}
\end{Highlighting}
\end{Shaded}

Let's check the structure of the data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(items)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Classes 'data.table' and 'data.frame':  267 obs. of  12 variables:
 $ ValObjectives        : int  5 5 4 4 5 5 5 4 5 3 ...
 $ IncrUnderstanding    : int  2 3 4 3 4 5 2 4 5 4 ...
 $ IncrInterest         : int  5 3 4 2 4 5 3 2 5 1 ...
 $ ClearResponsibilities: int  5 5 4 4 5 5 4 4 5 3 ...
 $ EffectiveAnswers     : int  5 3 5 3 5 4 3 2 3 3 ...
 $ Feedback             : int  5 3 4 2 5 5 4 4 5 2 ...
 $ ClearOrganization    : int  3 4 3 4 4 5 4 4 5 2 ...
 $ ClearPresentation    : int  4 4 4 2 5 4 4 4 5 2 ...
 $ MultPerspectives     : int  5 5 4 5 5 5 5 5 5 1 ...
 $ InclusvClassrm       : int  5 5 5 5 5 5 5 4 5 3 ...
 $ DEIintegration       : int  5 5 5 5 5 5 5 5 5 2 ...
 $ EquitableEval        : int  5 5 3 5 5 5 5 3 5 3 ...
 - attr(*, ".internal.selfref")=<externalptr> 
 - attr(*, "na.action")= 'omit' Named int [1:43] 6 20 106 109 112 113 114 117 122 128 ...
  ..- attr(*, "names")= chr [1:43] "6" "20" "106" "109" ...
\end{verbatim}

\hypertarget{conduct-and-interpret-the-three-diagnostic-tests-to-determine-if-pca-is-appropriate-as-an-analysis-kmo-bartletts-determinant}{%
\subsection{Conduct and interpret the three diagnostic tests to determine if PCA is appropriate as an analysis (KMO, Bartlett's, determinant)}\label{conduct-and-interpret-the-three-diagnostic-tests-to-determine-if-pca-is-appropriate-as-an-analysis-kmo-bartletts-determinant}}

\hypertarget{kmo}{%
\subsubsection{KMO}\label{kmo}}

The Kaiser-Meyer-Olkin (KMO) index is an index of \emph{sampling adequacy} to let us know if the sample size is sufficient relative to the statistical characteristics of the data.

General criteria (1974, Kaiser):

\begin{itemize}
\tightlist
\item
  bare minimum of .5
\item
  values between .5 and .7 as mediocre
\item
  values between .7 and .8 are good
\item
  values above .9 are superb
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{KMO}\NormalTok{(items)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Kaiser-Meyer-Olkin factor adequacy
Call: psych::KMO(r = items)
Overall MSA =  0.91
MSA for each item = 
        ValObjectives     IncrUnderstanding          IncrInterest 
                 0.94                  0.89                  0.89 
ClearResponsibilities      EffectiveAnswers              Feedback 
                 0.91                  0.93                  0.94 
    ClearOrganization     ClearPresentation      MultPerspectives 
                 0.94                  0.91                  0.93 
       InclusvClassrm        DEIintegration         EquitableEval 
                 0.86                  0.78                  0.95 
\end{verbatim}

With a KMO of 0.91, the data seems appropriate to continue with the PCA.

\hypertarget{bartletts}{%
\subsubsection{Bartlett's}\label{bartletts}}

Barlett's test lets us know if the matrix is an \emph{identity matrix} (i.e., where elements on the off-diagonal would be 0.0 and elements on the diagonal would be 1.0). Stated another way -- items only correlate with ``themselves'' and not other variables.

When \(p < 0.05\) the matrix is not an identity matrix. That is, there are some relationships between variables that can be analyzed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{cortest.bartlett}\NormalTok{(items)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
R was not square, finding R from data
\end{verbatim}

\begin{verbatim}
$chisq
[1] 1897.769

$p.value
[1] 0

$df
[1] 66
\end{verbatim}

The Barlett's test, \(\chi^2(66) = 1897.77, p < 0.001\), indicating that the correlation matrix is not an identity matrix and, on that dimension, is suitable for analysis.

\hypertarget{determinant}{%
\subsubsection{Determinant}\label{determinant}}

Multicollinearity or singularity is diagnosed by the determinant. The determinant should be greater than 0.00001. If smaller, then there may be an issue with multicollinearity (variables that are too highly correlated) or singularity (variables that are perfectly correlated).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{items }\OtherTok{\textless{}{-}} \FunctionTok{na.omit}\NormalTok{(items)}
\FunctionTok{det}\NormalTok{(}\FunctionTok{cor}\NormalTok{(items)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0006985496
\end{verbatim}

The value of the determinant is 0.0007; greater than 0.00001. We are not concerned with multicollinearity or singularity.

Summary from data screening:

\begin{quote}
Data screening were conducted to determine the suitability of the data for this analyses. The Kaiser-Meyer-Olkin measure of sampling adequacy (KMO; Kaiser, 1970) represents the ratio of the squared correlation between variables to the squared partial correlation between variables. KMO ranges from 0.00 to 1.00; values closer to 1.00 indicate that the patterns of correlations are relatively compact, and that component analysis should yield distinct and reliable components (Field, 2012). In our dataset, the KMO value was 0.91, indicating acceptable sampling adequacy. The Barlett's Test of Sphericity examines whether the population correlation matrix resembles an identity matrix (Field, 2012). When the \emph{p} value for the Bartlett's test is \textless{} .05, we are fairly certain we have clusters of correlated variables. In our dataset, \(\chi^2(66) = 1897.77, p < 0.001\) indicating the correlations between items are sufficiently large enough for principal components analysis. The determinant of the correlation matrix alerts us to any issues of multicollinearity or singularity and should be larger than 0.00001. Our determinant was 0.0007 and, again, indicated that our data was suitable for the analysis.
\end{quote}

\hypertarget{determine-how-many-components-to-extract-e.g.-scree-plot-eigenvalues-theory}{%
\subsection{Determine how many components to extract (e.g., scree plot, eigenvalues, theory)}\label{determine-how-many-components-to-extract-e.g.-scree-plot-eigenvalues-theory}}

Step \#1: creating a principal components model with the same number of components as items

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca1 }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{principal}\NormalTok{(items, }\AttributeTok{nfactors =} \FunctionTok{length}\NormalTok{(items), }\AttributeTok{rotate =} \StringTok{"none"}\NormalTok{)  }\CommentTok{\# using raw data and letting the length function automatically calculate the \# factors as a function of how many columns in the raw data}
\NormalTok{pca1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = items, nfactors = length(items), rotate = "none")
Standardized loadings (pattern matrix) based upon correlation matrix
                       PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9
ValObjectives         0.57 -0.13  0.42  0.68 -0.08  0.03  0.03 -0.06 -0.04
IncrUnderstanding     0.68 -0.37  0.39 -0.28  0.08  0.09 -0.12  0.25 -0.04
IncrInterest          0.73 -0.19  0.41 -0.17  0.32 -0.05  0.06 -0.12  0.03
ClearResponsibilities 0.81 -0.08 -0.37  0.04 -0.20  0.02 -0.03  0.09 -0.10
EffectiveAnswers      0.80 -0.16 -0.20 -0.09 -0.03  0.05  0.48 -0.05  0.07
Feedback              0.77  0.06 -0.28  0.11  0.32 -0.29  0.01  0.09 -0.31
ClearOrganization     0.79 -0.27 -0.12  0.04 -0.22 -0.22 -0.20  0.19  0.20
ClearPresentation     0.85 -0.21  0.00 -0.11 -0.24  0.01  0.05 -0.18  0.04
MultPerspectives      0.79  0.26 -0.11 -0.05  0.13 -0.14 -0.23 -0.35  0.17
InclusvClassrm        0.66  0.50  0.24 -0.20 -0.30  0.11 -0.06 -0.07 -0.28
DEIintegration        0.50  0.75  0.19  0.03  0.02 -0.12  0.14  0.25  0.21
EquitableEval         0.73  0.13 -0.27  0.13  0.22  0.52 -0.12  0.07  0.07
                       PC10  PC11  PC12 h2                   u2 com
ValObjectives          0.05  0.02  0.04  1 -0.00000000000000022 2.9
IncrUnderstanding      0.27  0.00  0.07  1  0.00000000000000389 3.7
IncrInterest          -0.28  0.15 -0.09  1  0.00000000000000111 3.0
ClearResponsibilities  0.05  0.36 -0.11  1  0.00000000000000089 2.2
EffectiveAnswers       0.03 -0.01  0.19  1  0.00000000000000078 2.1
Feedback               0.02 -0.14 -0.02  1  0.00000000000000089 2.6
ClearOrganization     -0.23 -0.09  0.11  1  0.00000000000000033 2.5
ClearPresentation      0.08 -0.21 -0.28  1  0.00000000000000122 1.9
MultPerspectives       0.18  0.06  0.12  1  0.00000000000000100 2.4
InclusvClassrm        -0.13 -0.03  0.12  1  0.00000000000000056 3.7
DEIintegration         0.06  0.01 -0.11  1  0.00000000000000033 2.6
EquitableEval         -0.08 -0.09 -0.02  1  0.00000000000000011 2.7

                       PC1  PC2  PC3  PC4  PC5  PC6  PC7  PC8  PC9 PC10 PC11
SS loadings           6.38 1.23 0.95 0.67 0.52 0.47 0.39 0.37 0.32 0.27 0.24
Proportion Var        0.53 0.10 0.08 0.06 0.04 0.04 0.03 0.03 0.03 0.02 0.02
Cumulative Var        0.53 0.63 0.71 0.77 0.81 0.85 0.88 0.91 0.94 0.96 0.98
Proportion Explained  0.53 0.10 0.08 0.06 0.04 0.04 0.03 0.03 0.03 0.02 0.02
Cumulative Proportion 0.53 0.63 0.71 0.77 0.81 0.85 0.88 0.91 0.94 0.96 0.98
                      PC12
SS loadings           0.20
Proportion Var        0.02
Cumulative Var        1.00
Proportion Explained  0.02
Cumulative Proportion 1.00

Mean item complexity =  2.7
Test of the hypothesis that 12 components are sufficient.

The root mean square of the residuals (RMSR) is  0 
 with the empirical chi square  0  with prob <  NA 

Fit based upon off diagonal values = 1
\end{verbatim}

The eigenvalue-greater-than-one criteria suggests 2 factors (but the third component has an SSloading of .95 -- it's close to three).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(pca1}\SpecialCharTok{$}\NormalTok{values, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{08-EFA_PCA_files/figure-latex/unnamed-chunk-109-1.pdf} The scree plot looks like one factor.

Ugh.

\begin{itemize}
\tightlist
\item
  I want 3 factors (we could think of this as a priori theory); would account for 71\% of variance.
\item
  Eigenvalues-greater-than-one criteria suggests two; could account for 63\% of variance.
\item
  Scree plot suggests 1 (would account for 53\% of variance)
\end{itemize}

\emph{Note}: The lecture has more on evaluating communalities and uniquenesses and how this information can also inform the number of components we want to extract. Because it is easy to get lost (very lost) I will skip over this for now. If you were to create a measure and use PCA as an exploratory approach to understanding the dimensionality of an instrument, you would likely want to investigate further and report on these.

\hypertarget{conduct-an-orthogonal-extraction-and-rotation-with-a-minimum-of-two-different-factor-extractions}{%
\subsection{Conduct an orthogonal extraction and rotation with a minimum of two different factor extractions}\label{conduct-an-orthogonal-extraction-and-rotation-with-a-minimum-of-two-different-factor-extractions}}

\textbf{An orthogonal two factor solution}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcaORTH2f }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{principal}\NormalTok{(items, }\AttributeTok{nfactors =} \DecValTok{2}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"varimax"}\NormalTok{)}
\NormalTok{pcaORTH2f}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = items, nfactors = 2, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix
                       RC1  RC2   h2   u2 com
ValObjectives         0.55 0.19 0.34 0.66 1.2
IncrUnderstanding     0.77 0.05 0.59 0.41 1.0
IncrInterest          0.72 0.23 0.57 0.43 1.2
ClearResponsibilities 0.73 0.36 0.66 0.34 1.5
EffectiveAnswers      0.76 0.29 0.67 0.33 1.3
Feedback              0.62 0.46 0.59 0.41 1.8
ClearOrganization     0.81 0.18 0.69 0.31 1.1
ClearPresentation     0.83 0.27 0.76 0.24 1.2
MultPerspectives      0.53 0.64 0.70 0.30 1.9
InclusvClassrm        0.29 0.77 0.68 0.32 1.3
DEIintegration        0.03 0.90 0.80 0.20 1.0
EquitableEval         0.55 0.50 0.55 0.45 2.0

                       RC1  RC2
SS loadings           4.93 2.68
Proportion Var        0.41 0.22
Cumulative Var        0.41 0.63
Proportion Explained  0.65 0.35
Cumulative Proportion 0.65 1.00

Mean item complexity =  1.4
Test of the hypothesis that 2 components are sufficient.

The root mean square of the residuals (RMSR) is  0.07 
 with the empirical chi square  170.34  with prob <  0.000000000000000045 

Fit based upon off diagonal values = 0.98
\end{verbatim}

Sorting the scores into a table can help see the results more clearly. The ``cut = \#'' command will not show the factor scores for factor loading \textless{} .30. I would do this ``to see'', but I would include all the values in an APA style table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca\_tableOR2f }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{print.psych}\NormalTok{(pcaORTH2f, }\AttributeTok{cut =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = items, nfactors = 2, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix
                      item  RC1  RC2   h2   u2 com
ClearPresentation        8 0.83      0.76 0.24 1.2
ClearOrganization        7 0.81      0.69 0.31 1.1
IncrUnderstanding        2 0.77      0.59 0.41 1.0
EffectiveAnswers         5 0.76      0.67 0.33 1.3
ClearResponsibilities    4 0.73 0.36 0.66 0.34 1.5
IncrInterest             3 0.72      0.57 0.43 1.2
Feedback                 6 0.62 0.46 0.59 0.41 1.8
ValObjectives            1 0.55      0.34 0.66 1.2
EquitableEval           12 0.55 0.50 0.55 0.45 2.0
DEIintegration          11      0.90 0.80 0.20 1.0
InclusvClassrm          10      0.77 0.68 0.32 1.3
MultPerspectives         9 0.53 0.64 0.70 0.30 1.9

                       RC1  RC2
SS loadings           4.93 2.68
Proportion Var        0.41 0.22
Cumulative Var        0.41 0.63
Proportion Explained  0.65 0.35
Cumulative Proportion 0.65 1.00

Mean item complexity =  1.4
Test of the hypothesis that 2 components are sufficient.

The root mean square of the residuals (RMSR) is  0.07 
 with the empirical chi square  170.34  with prob <  0.000000000000000045 

Fit based upon off diagonal values = 0.98
\end{verbatim}

F1: Includes everything else. F2: Includes the SCR items (although MultPerspectives cross-loads onto F1; Similarly, EquitableEval is on F1)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{fa.diagram}\NormalTok{(pcaORTH2f)}
\end{Highlighting}
\end{Shaded}

\includegraphics{08-EFA_PCA_files/figure-latex/unnamed-chunk-112-1.pdf} Plotting these figures from the program can facilitate conceptual understanding of what is going on -- and can be a ``check'' to your work.

In the lecture I made a ``biggish deal'' about PCA being \emph{components} (not \emph{factor}) analysis. Although the two approaches can lead to similar results/conclusions, there are some significant differences ``under the hood.'' PCA can be thought of more as regression where the items predict the component. Consequently, the arrows go \emph{from} the item \emph{to} the component. Starting with the next lesson, the arrows will go from the factor to the item -- because the factors (or latent variables) are assumed to predict the scores on the items (i.e., ``depression'' would predict how someone rates items that assess hopelessness, sleep, anhedonia, and so forth).

\textbf{An orthogonal three factor solution}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcaORTH3f }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{principal}\NormalTok{(items, }\AttributeTok{nfactors =} \DecValTok{3}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"varimax"}\NormalTok{)}
\NormalTok{pcaORTH3f}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = items, nfactors = 3, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix
                       RC1  RC3  RC2   h2   u2 com
ValObjectives         0.16 0.67 0.21 0.52 0.48 1.3
IncrUnderstanding     0.29 0.81 0.04 0.75 0.25 1.3
IncrInterest          0.30 0.78 0.23 0.74 0.26 1.5
ClearResponsibilities 0.85 0.22 0.14 0.80 0.20 1.2
EffectiveAnswers      0.75 0.37 0.12 0.71 0.29 1.5
Feedback              0.75 0.19 0.28 0.67 0.33 1.4
ClearOrganization     0.70 0.47 0.03 0.71 0.29 1.8
ClearPresentation     0.65 0.56 0.14 0.76 0.24 2.1
MultPerspectives      0.63 0.24 0.51 0.71 0.29 2.2
InclusvClassrm        0.26 0.30 0.76 0.74 0.26 1.6
DEIintegration        0.15 0.07 0.90 0.84 0.16 1.1
EquitableEval         0.70 0.15 0.33 0.62 0.38 1.5

                       RC1  RC3  RC2
SS loadings           3.93 2.64 1.99
Proportion Var        0.33 0.22 0.17
Cumulative Var        0.33 0.55 0.71
Proportion Explained  0.46 0.31 0.23
Cumulative Proportion 0.46 0.77 1.00

Mean item complexity =  1.5
Test of the hypothesis that 3 components are sufficient.

The root mean square of the residuals (RMSR) is  0.06 
 with the empirical chi square  115.69  with prob <  0.000000000041 

Fit based upon off diagonal values = 0.99
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca\_tableOR3f }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{print.psych}\NormalTok{(pcaORTH3f, }\AttributeTok{cut =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = items, nfactors = 3, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix
                      item  RC1  RC3  RC2   h2   u2 com
ClearResponsibilities    4 0.85           0.80 0.20 1.2
EffectiveAnswers         5 0.75 0.37      0.71 0.29 1.5
Feedback                 6 0.75           0.67 0.33 1.4
EquitableEval           12 0.70      0.33 0.62 0.38 1.5
ClearOrganization        7 0.70 0.47      0.71 0.29 1.8
ClearPresentation        8 0.65 0.56      0.76 0.24 2.1
MultPerspectives         9 0.63      0.51 0.71 0.29 2.2
IncrUnderstanding        2      0.81      0.75 0.25 1.3
IncrInterest             3      0.78      0.74 0.26 1.5
ValObjectives            1      0.67      0.52 0.48 1.3
DEIintegration          11           0.90 0.84 0.16 1.1
InclusvClassrm          10      0.30 0.76 0.74 0.26 1.6

                       RC1  RC3  RC2
SS loadings           3.93 2.64 1.99
Proportion Var        0.33 0.22 0.17
Cumulative Var        0.33 0.55 0.71
Proportion Explained  0.46 0.31 0.23
Cumulative Proportion 0.46 0.77 1.00

Mean item complexity =  1.5
Test of the hypothesis that 3 components are sufficient.

The root mean square of the residuals (RMSR) is  0.06 
 with the empirical chi square  115.69  with prob <  0.000000000041 

Fit based upon off diagonal values = 0.99
\end{verbatim}

F1: Traditional Pedagogy F2: Valued-by-Me F3: SCRPed--except Equitable Eval * MultPerspectives are on TradPed; MultPerspectives cross-load

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{fa.diagram}\NormalTok{(pcaORTH3f)}
\end{Highlighting}
\end{Shaded}

\includegraphics{08-EFA_PCA_files/figure-latex/unnamed-chunk-115-1.pdf} The three-factor solution gets really close to my goals of (a) traditional pedagogy, (b) valued by the student, and (c) socially responsive pedagogy. The trouble is that I would prefer ``multiple perspectives'' to load with the socially responsive pedagogy factor.

\hypertarget{conduct-an-oblique-extraction-and-rotation-with-a-minimum-of-two-different-factor-extractions}{%
\subsection{Conduct an oblique extraction and rotation with a minimum of two different factor extractions}\label{conduct-an-oblique-extraction-and-rotation-with-a-minimum-of-two-different-factor-extractions}}

\textbf{An oblique two factor solution}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcaOBL2f }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{principal}\NormalTok{(items, }\AttributeTok{nfactors =} \DecValTok{2}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"oblimin"}\NormalTok{)}
\NormalTok{pcaOBL2f}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = items, nfactors = 2, rotate = "oblimin")
Standardized loadings (pattern matrix) based upon correlation matrix
                        TC1   TC2   h2   u2 com
ValObjectives          0.58  0.01 0.34 0.66 1.0
IncrUnderstanding      0.84 -0.21 0.59 0.41 1.1
IncrInterest           0.76  0.00 0.57 0.43 1.0
ClearResponsibilities  0.75  0.13 0.66 0.34 1.1
EffectiveAnswers       0.80  0.05 0.67 0.33 1.0
Feedback               0.61  0.27 0.59 0.41 1.4
ClearOrganization      0.86 -0.08 0.69 0.31 1.0
ClearPresentation      0.87  0.00 0.76 0.24 1.0
MultPerspectives       0.50  0.49 0.70 0.30 2.0
InclusvClassrm         0.21  0.71 0.68 0.32 1.2
DEIintegration        -0.10  0.93 0.80 0.20 1.0
EquitableEval          0.53  0.34 0.55 0.45 1.7

                       TC1  TC2
SS loadings           5.50 2.11
Proportion Var        0.46 0.18
Cumulative Var        0.46 0.63
Proportion Explained  0.72 0.28
Cumulative Proportion 0.72 1.00

 With component correlations of 
     TC1  TC2
TC1 1.00 0.43
TC2 0.43 1.00

Mean item complexity =  1.2
Test of the hypothesis that 2 components are sufficient.

The root mean square of the residuals (RMSR) is  0.07 
 with the empirical chi square  170.34  with prob <  0.000000000000000045 

Fit based upon off diagonal values = 0.98
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca\_tableOBL2f }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{print.psych}\NormalTok{(pcaOBL2f, }\AttributeTok{cut =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = items, nfactors = 2, rotate = "oblimin")
Standardized loadings (pattern matrix) based upon correlation matrix
                      item   TC1   TC2   h2   u2 com
ClearPresentation        8  0.87       0.76 0.24 1.0
ClearOrganization        7  0.86       0.69 0.31 1.0
IncrUnderstanding        2  0.84       0.59 0.41 1.1
EffectiveAnswers         5  0.80       0.67 0.33 1.0
IncrInterest             3  0.76       0.57 0.43 1.0
ClearResponsibilities    4  0.75       0.66 0.34 1.1
Feedback                 6  0.61       0.59 0.41 1.4
ValObjectives            1  0.58       0.34 0.66 1.0
EquitableEval           12  0.53  0.34 0.55 0.45 1.7
MultPerspectives         9  0.50  0.49 0.70 0.30 2.0
DEIintegration          11        0.93 0.80 0.20 1.0
InclusvClassrm          10        0.71 0.68 0.32 1.2

                       TC1  TC2
SS loadings           5.50 2.11
Proportion Var        0.46 0.18
Cumulative Var        0.46 0.63
Proportion Explained  0.72 0.28
Cumulative Proportion 0.72 1.00

 With component correlations of 
     TC1  TC2
TC1 1.00 0.43
TC2 0.43 1.00

Mean item complexity =  1.2
Test of the hypothesis that 2 components are sufficient.

The root mean square of the residuals (RMSR) is  0.07 
 with the empirical chi square  170.34  with prob <  0.000000000000000045 

Fit based upon off diagonal values = 0.98
\end{verbatim}

Fairly similar results to the orthogonal variation of this -- with EquitableEval and MultPerspectives cross-loading, with stronger loadings on the TradPed/Valued dimension.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{fa.diagram}\NormalTok{(pcaOBL2f)}
\end{Highlighting}
\end{Shaded}

\includegraphics{08-EFA_PCA_files/figure-latex/unnamed-chunk-118-1.pdf} The curved line and value between TC1 and TC2 illustrates that in the oblique solution the components are allowed to correlate. There was no such path on the orthogonal figures. This is because the rotation required the components to be uncorrelated.

\textbf{An oblique three factor solution}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcaOBL3f }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{principal}\NormalTok{(items, }\AttributeTok{nfactors =} \DecValTok{3}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"oblimin"}\NormalTok{)}
\NormalTok{pcaOBL3f}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = items, nfactors = 3, rotate = "oblimin")
Standardized loadings (pattern matrix) based upon correlation matrix
                        TC1   TC3   TC2   h2   u2 com
ValObjectives         -0.08  0.71  0.15 0.52 0.48 1.1
IncrUnderstanding      0.06  0.84 -0.07 0.75 0.25 1.0
IncrInterest           0.05  0.79  0.13 0.74 0.26 1.1
ClearResponsibilities  0.95 -0.06 -0.05 0.80 0.20 1.0
EffectiveAnswers       0.76  0.15 -0.06 0.71 0.29 1.1
Feedback               0.80 -0.06  0.12 0.67 0.33 1.1
ClearOrganization      0.68  0.29 -0.15 0.71 0.29 1.5
ClearPresentation      0.57  0.41 -0.02 0.76 0.24 1.8
MultPerspectives       0.59  0.03  0.39 0.71 0.29 1.7
InclusvClassrm         0.08  0.22  0.73 0.74 0.26 1.2
DEIintegration         0.00 -0.02  0.92 0.84 0.16 1.0
EquitableEval          0.75 -0.10  0.18 0.62 0.38 1.2

                       TC1  TC3  TC2
SS loadings           4.23 2.50 1.83
Proportion Var        0.35 0.21 0.15
Cumulative Var        0.35 0.56 0.71
Proportion Explained  0.49 0.29 0.21
Cumulative Proportion 0.49 0.79 1.00

 With component correlations of 
     TC1  TC3  TC2
TC1 1.00 0.58 0.39
TC3 0.58 1.00 0.25
TC2 0.39 0.25 1.00

Mean item complexity =  1.2
Test of the hypothesis that 3 components are sufficient.

The root mean square of the residuals (RMSR) is  0.06 
 with the empirical chi square  115.69  with prob <  0.000000000041 

Fit based upon off diagonal values = 0.99
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca\_tableOBL3f }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{print.psych}\NormalTok{(pcaOBL3f, }\AttributeTok{cut =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = items, nfactors = 3, rotate = "oblimin")
Standardized loadings (pattern matrix) based upon correlation matrix
                      item   TC1   TC3   TC2   h2   u2 com
ClearResponsibilities    4  0.95             0.80 0.20 1.0
Feedback                 6  0.80             0.67 0.33 1.1
EffectiveAnswers         5  0.76             0.71 0.29 1.1
EquitableEval           12  0.75             0.62 0.38 1.2
ClearOrganization        7  0.68             0.71 0.29 1.5
MultPerspectives         9  0.59        0.39 0.71 0.29 1.7
ClearPresentation        8  0.57  0.41       0.76 0.24 1.8
IncrUnderstanding        2        0.84       0.75 0.25 1.0
IncrInterest             3        0.79       0.74 0.26 1.1
ValObjectives            1        0.71       0.52 0.48 1.1
DEIintegration          11              0.92 0.84 0.16 1.0
InclusvClassrm          10              0.73 0.74 0.26 1.2

                       TC1  TC3  TC2
SS loadings           4.23 2.50 1.83
Proportion Var        0.35 0.21 0.15
Cumulative Var        0.35 0.56 0.71
Proportion Explained  0.49 0.29 0.21
Cumulative Proportion 0.49 0.79 1.00

 With component correlations of 
     TC1  TC3  TC2
TC1 1.00 0.58 0.39
TC3 0.58 1.00 0.25
TC2 0.39 0.25 1.00

Mean item complexity =  1.2
Test of the hypothesis that 3 components are sufficient.

The root mean square of the residuals (RMSR) is  0.06 
 with the empirical chi square  115.69  with prob <  0.000000000041 

Fit based upon off diagonal values = 0.99
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{fa.diagram}\NormalTok{(pcaOBL3f)}
\end{Highlighting}
\end{Shaded}

\includegraphics{08-EFA_PCA_files/figure-latex/unnamed-chunk-121-1.pdf} The results are quite similar to the orthogonal solution.

\hypertarget{determine-which-factor-solution-e.g.-orthogonal-or-oblique-which-number-of-factors-you-will-suggest}{%
\subsection{Determine which factor solution (e.g., orthogonal or oblique; which number of factors) you will suggest}\label{determine-which-factor-solution-e.g.-orthogonal-or-oblique-which-number-of-factors-you-will-suggest}}

From the oblique output we see that the correlations between the three subscales range from 0.25 to 0.58. These are high. Therefore, I will choose a 3-component, oblique, solution.

\hypertarget{apa-style-results-section-with-table-and-figure-of-one-of-the-solutions}{%
\subsection{APA style results section with table and figure of one of the solutions}\label{apa-style-results-section-with-table-and-figure-of-one-of-the-solutions}}

\begin{quote}
The dimensionality of the 12 course evaluation items was analyzed using principal components analysis. First, data were screened to determine the suitability of the data for this principal components analysis. The Kaiser-Meyer-Olkin measure of sampling adequacy (KMO; Kaiser, 1970) represents the ratio of the squared correlation between variables to the squared partial correlation between variables. KMO ranges from 0.00 to 1.00; values closer to 1.00 indicate that the patterns of correlations are relatively compact, and that component analysis should yield distinct and reliable components (Field, 2012). In our dataset, the KMO value was 0.91, indicating acceptable sampling adequacy. The Barlett's Test of Sphericity examines whether the population correlation matrix resembles an identity matrix (Field, 2012). When the \emph{p} value for the Bartlett's test is \textless{} .05, we are fairly certain we have clusters of correlated variables. In our dataset, \(\chi^2(66) = 1897.77, p < 0.001\) indicating the correlations between items are sufficiently large enough for principal components analysis. The determinant of the correlation matrix alerts us to any issues of multicollinearity or singularity and should be larger than 0.00001. Our determinant was 0.0007 and, again, indicated that our data was suitable for the analysis.
\end{quote}

\begin{quote}
Four criteria were used to determine the number of components to extract: a priori theory, the scree test, the eigenvalue-greater-than-one criteria, and the interpretability of the solution. Kaiser's eigenvalue-greater-than-one criteria suggested two components, and, in combination explained 63\% of the variance. The inflexion in the scree plot justified retaining one component. A priorily, we researchers were expecting three components -- which would explain 71\% of the variance. Correspondingly, we investigated two and three component solutions with orthogonal (varimax) and oblique (oblimin) procedures. Given the significant correlations (ranging from .25 to .58) and the correspondence of items loading on the a priorili hypothesized components, we determined that an oblique, three-component, solution was most appropriate.
\end{quote}

\begin{quote}
The rotated solution, as shown in Table 1 and Figure 1, yielded three interpretable components, each listed with the proportion of variance accounted for: traditional pedagogy (35\%), valued-by-me (21\%), and socially and culturally responsive pedagogy (15\%).
\end{quote}

Regarding the Table 1, I would include a table with ALL the values, bolding those with component membership. This is easy, though, because we can export it to a .csv file and

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcaOBL3fb }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{principal}\NormalTok{(items, }\AttributeTok{nfactors =} \DecValTok{3}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"oblimin"}\NormalTok{)}
\NormalTok{pca\_tableOBL3fb }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{print.psych}\NormalTok{(pcaOBL3fb, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = items, nfactors = 3, rotate = "oblimin")
Standardized loadings (pattern matrix) based upon correlation matrix
                      item   TC1   TC3   TC2   h2   u2 com
ClearResponsibilities    4  0.95 -0.06 -0.05 0.80 0.20 1.0
Feedback                 6  0.80 -0.06  0.12 0.67 0.33 1.1
EffectiveAnswers         5  0.76  0.15 -0.06 0.71 0.29 1.1
EquitableEval           12  0.75 -0.10  0.18 0.62 0.38 1.2
ClearOrganization        7  0.68  0.29 -0.15 0.71 0.29 1.5
MultPerspectives         9  0.59  0.03  0.39 0.71 0.29 1.7
ClearPresentation        8  0.57  0.41 -0.02 0.76 0.24 1.8
IncrUnderstanding        2  0.06  0.84 -0.07 0.75 0.25 1.0
IncrInterest             3  0.05  0.79  0.13 0.74 0.26 1.1
ValObjectives            1 -0.08  0.71  0.15 0.52 0.48 1.1
DEIintegration          11  0.00 -0.02  0.92 0.84 0.16 1.0
InclusvClassrm          10  0.08  0.22  0.73 0.74 0.26 1.2

                       TC1  TC3  TC2
SS loadings           4.23 2.50 1.83
Proportion Var        0.35 0.21 0.15
Cumulative Var        0.35 0.56 0.71
Proportion Explained  0.49 0.29 0.21
Cumulative Proportion 0.49 0.79 1.00

 With component correlations of 
     TC1  TC3  TC2
TC1 1.00 0.58 0.39
TC3 0.58 1.00 0.25
TC2 0.39 0.25 1.00

Mean item complexity =  1.2
Test of the hypothesis that 3 components are sufficient.

The root mean square of the residuals (RMSR) is  0.06 
 with the empirical chi square  115.69  with prob <  0.000000000041 

Fit based upon off diagonal values = 0.99
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcaOBL3fb\_table }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(pcaOBL3fb}\SpecialCharTok{$}\NormalTok{loadings, }\DecValTok{3}\NormalTok{)}
\FunctionTok{write.table}\NormalTok{(pcaOBL3fb\_table, }\AttributeTok{file =} \StringTok{"pcaOBL3f\_table.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{col.names =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{row.names =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{pcaOBL3fb\_table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Loadings:
                      TC1    TC3    TC2   
ValObjectives                 0.712  0.151
IncrUnderstanding             0.844       
IncrInterest                  0.787  0.132
ClearResponsibilities  0.947              
EffectiveAnswers       0.764  0.154       
Feedback               0.800         0.119
ClearOrganization      0.685  0.293 -0.149
ClearPresentation      0.574  0.413       
MultPerspectives       0.593         0.391
InclusvClassrm                0.218  0.730
DEIintegration                       0.921
EquitableEval          0.751         0.184

                 TC1   TC3   TC2
SS loadings    3.854 2.185 1.655
Proportion Var 0.321 0.182 0.138
Cumulative Var 0.321 0.503 0.641
\end{verbatim}

\hypertarget{explanation-to-grader-1}{%
\subsection{Explanation to grader}\label{explanation-to-grader-1}}

\hypertarget{PAF}{%
\chapter{Principal Axis Factoring}\label{PAF}}

\href{https://youtube.com/playlist?list=PLtz5cFLQl4KMdUnDzMY2m-jm7-rAY9R0n\&si=cJP-7pgYDhi_X459}{Screencasted Lecture Link}

This is the second lesson of \emph{exploratory} principal components analysis (PCA) and factor analysis (EFA/PAF). This time the focus is on actual \emph{factor analysis}. There are numerous approaches to this process (e.g., principal components analysis, parallel analyses). In this lesson I will demonstrate principal axis factoring (PAF).

\hypertarget{navigating-this-lesson-7}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-7}}

There is about an hour-and-a-half of lecture. If you work through the materials with me it would be plan for an additional two hours.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-7}{%
\subsection{Learning Objectives}\label{learning-objectives-7}}

Focusing on this lesson's materials, make sure you can:

\begin{itemize}
\tightlist
\item
  Distinguish between PCA and PAF on several levels:

  \begin{itemize}
  \tightlist
  \item
    recognize PCA and PAF from a path diagram
  \item
    define keywords associated with each: factor loadings, linear components, describe v. explain.\\
  \end{itemize}
\item
  Recognize/define an identity matrix -- what test would you use to diagnose it?
\item
  Recognize/define multicollinearity and singularity -- what test would you use to diagnose it?
\item
  Describe the desired pattern of ``loadings'' (i.e., the relative weights of an item on its own scale compared to other scales)
\item
  Compare the results from item analysis, PCA, PAF, and omega.
\end{itemize}

\hypertarget{planning-for-practice-7}{%
\subsection{Planning for Practice}\label{planning-for-practice-7}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. The least complex is to change the random seed in the research and rework the problem demonstrated in the lesson. The results \emph{should} map onto the ones obtained in the lecture.

The second option involves utilizing one of the simulated datasets available in this OER. The \protect\hyperlink{sims}{last lesson} in the OER contains three simulations that could be used for all of the statistics-based practice suggestions. Especially if you started with one of these examples in an earlier lesson, I highly recommend you continue with that.

Alternatively, Keum et al.'s Gendered Racial Microaggressions Scale for Asian American Women \citeyearpar{keum_gendered_2018} will be used in the lessons on confirmatory factor analysis and Conover et al.'s \citeyearpar{conover_development_2017} Ableist Microaggressions Scale is used in the lesson on invariance testing. Both of these would be suitable for the PCA and PAF homework assignments.

As a third option, you are welcome to use data to which you have access and is suitable for PCA. These could include other vignettes from this OER, other simulated data, or your own data (presuming you have permission to use it). In any case, please plan to:

\begin{itemize}
\tightlist
\item
  Properly format and prepare the data.
\item
  Conduct diagnostic tests to determine the suitability of the data for PCA.
\item
  Conducting tests to guide the decisions about number of factors to extract.
\item
  Conducting orthogonal and oblique extractions (at least two each with different numbers of factors).
\item
  Selecting one solution and preparing an APA style results section (with table and figure).
\item
  Compare your results in light of any other psychometrics lessons where you have used this data (especially the \protect\hyperlink{ItemAnalSurvey}{item analysis} and \protect\hyperlink{PCA}{PCA} lessons).
\end{itemize}

\hypertarget{readings-resources-7}{%
\subsection{Readings \& Resources}\label{readings-resources-7}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Revelle, William. (n.d.). Chapter 6: Constructs, components, and factor models. In \emph{An introduction to psychometric theory with applications in R}. Retrieved from \url{https://personality-project.org/r/book/\#chapter6}

  \begin{itemize}
  \tightlist
  \item
    pp.~150 to 167. Stop at ``Non-Simple Structure Solutions: The Simplex and Circumplex.''
  \item
    A simultaneously theoretical review of psychometric theory while working with R and data to understand the concepts.
  \end{itemize}
\item
  Revelle, W. (2019). \emph{How To: Use the psych package for Factor Analysis and data reduction}.

  \begin{itemize}
  \tightlist
  \item
    Treat as reference. Pages 13 through 24 provide technical information about what we are doing.
  \end{itemize}
\end{itemize}

\hypertarget{packages-7}{%
\subsection{Packages}\label{packages-7}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# will install the package if not already installed}
\CommentTok{\# if(!require(psych))\{install.packages(\textquotesingle{}psych\textquotesingle{})\}}
\CommentTok{\# if(!require(tidyverse))\{install.packages(\textquotesingle{}tidyverse\textquotesingle{})\}}
\CommentTok{\# if(!require(MASS))\{install.packages(\textquotesingle{}MASS\textquotesingle{})\}}
\CommentTok{\# if(!require(sjstats))\{install.packages(\textquotesingle{}sjstats\textquotesingle{})\}}
\CommentTok{\# if(!require(apaTables))\{install.packages(\textquotesingle{}apaTables\textquotesingle{})\}}
\CommentTok{\# if(!require(qualtRics))\{install.packages(\textquotesingle{}qualtRics\textquotesingle{})\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{exploratory-factor-analysis-with-a-quick-contrast-to-pca}{%
\section{Exploratory Factor Analysis (with a quick contrast to PCA)}\label{exploratory-factor-analysis-with-a-quick-contrast-to-pca}}

Whereas principal components analysis (PCA) is a regression analysis technique, principal factor analysis is a latent variable model \citep{revelle_william_chapter_nodate}.

Exploratory factor analysis has a rich history. In 1904, Spearman used it for a single factor. In 1947, Thurstone generalized it to multiple factors. Factor analysis is frequently used and controversial.

Factor analysis and principal components are commonly confused:

\textbf{Principal components} is

\begin{itemize}
\tightlist
\item
  linear sums of variables,
\item
  solved with an eigenvalue or singular decomposition,
\item
  represented by an \(n*n\) matrix in terms of the first \emph{k} components and attempts to reproduce all of the \(R\) matrix, and
\item
  paths which point from the items to a total scale score -- all represented as observed/manifest (square) variables.
\end{itemize}

\textbf{Factor analysis} is

\begin{itemize}
\tightlist
\item
  linear sums of unknown factors,
\item
  estimated as best fitting solutions, normally through iterative procedures, and
\item
  controversial. Because:

  \begin{itemize}
  \tightlist
  \item
    At the \emph{structural} level (i.e., covariance or correlation matrix), there are normally more observed variables than parameters to estimate them and the procedure seeks to find the best fitting solution using ordinary least squares, weighted least squares, or maximum likelihood.
  \item
    At the \emph{data} level, although scores can be estimated, the model is indeterminate.
  \item
    This leads some to argue for using principal components; however, fans of factor analysis suggest that it is useful for constructing and evaluating theories.
  \end{itemize}
\item
  an attempt to model only the \emph{common} part of the matrix, which means all of the off-diagonal elements and the common part of the diagonal (the \emph{communalities}); the \emph{uniquenesses} are the non-common (leftover) part

  \begin{itemize}
  \tightlist
  \item
    Stated another way, the factor model partitions the correlation or covariance matrix into

    \begin{itemize}
    \tightlist
    \item
      \emph{common factors}, \(FF'\), and
    \item
      that which is \emph{unique}, \(U^2\) (the diagonal matrix of \emph{uniquenesses})
    \end{itemize}
  \end{itemize}
\item
  paths which point from the latent variable (LV) representing the factor (oval) to the items (squares) illustrating that the factor/LV ``causes'' the item's score
\end{itemize}

\begin{figure}
\centering
\includegraphics{images/PAF/PCAvPAF.png}
\caption{Comparison of path models for PCA and EFA}
\end{figure}

Our focus today is on the PAF approach to scale construction. By utilizing the same research vignette as in the \protect\hyperlink{PCA}{PCA lesson}, we can identify similarities in differences in the approach, results, and interpretation. Let's first take a look at the workflow for PAF.

\hypertarget{paf-workflow}{%
\section{PAF Workflow}\label{paf-workflow}}

Below is a screenshot of the workflow. The original document is located in the \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub site} that hosts the ReCentering Psych Stats: Psychometrics OER. You may find it refreshing that, with the exception of the change from ``components'' to ``factors,'' the workflow for PCA and PAF are quite similar.

\begin{figure}
\centering
\includegraphics{images/PAF/PAFworkflow.png}
\caption{Image of the workflow for PAF}
\end{figure}

Steps in the process include:

\begin{itemize}
\tightlist
\item
  Creating an items only dataframe where all items are scaled in the same direction (i.e., negatively worded items are reverse scored).
\item
  Conducting tests that assess the statistical assumptions of PAF to ensure that the data is appropriate for PAF.
\item
  Determining the number of factors (think ``subscales'') to extract.
\item
  Conducting the factor extraction -- this process will likely occur iteratively,

  \begin{itemize}
  \tightlist
  \item
    exploring orthogonal (uncorrelated/independent) and oblique (correlated) factors, and
  \item
    changing the number of factors to extract
  \end{itemize}
\end{itemize}

Because the intended audience for the ReCentering Psych Stats OER is the scientist-practitioner-advocate, this lesson focuses on the workflow and decisions. As you might guess, the details of PAF can be quite complex. Some important notions to consider that may not be obvious from lesson, are these:

\begin{itemize}
\tightlist
\item
  The values of factor loadings are directly related to the correlation matrix.

  \begin{itemize}
  \tightlist
  \item
    Although I do not explain this in detail, nearly every analytic step attempts to convey this notion by presenting equivalent analytic options using the raw data and correlation matrix.
  \end{itemize}
\item
  PAF (like PCA and related EFA procedures) is about \emph{dimension reduction} -- our goal is fewer factors (think subscales) than there are items.

  \begin{itemize}
  \tightlist
  \item
    In this lesson's vignette there are 25 items on the scale, and we will have 4 subscales.
  \end{itemize}
\item
  As a latent variable procedure, PAF is both \emph{exploratory} and \emph{factor analysis.} This is in contrast to our prior \protect\hyperlink{PCA}{PCA lesson}. Recall that PCA is a regression-based model and therefore not ``factor analysis.''
\item
  Matrix algebra (e.g., using the transpose of a matrix, multiplying matrices together) plays a critical role in the analytic solution.
\end{itemize}

\hypertarget{research-vignette-6}{%
\section{Research Vignette}\label{research-vignette-6}}

This lesson's research vignette emerges from Lewis and Neville's Gendered Racial Microaggressions Scale for Black Women \citeyearpar{lewis_construction_2015}. The article reports on two separate studies that comprised the development, refinement, and psychometric evaluation of two parallel versions (stress appraisal, frequency) of the scale. Below, I simulate data from the final construction of the stress appraisal version as the basis of the lecture. Items were on a 6-point Likert scale ranging from 0 (\emph{not at all stressful}) to 5 (\emph{extremely stressful}).

Lewis and Neville \citeyearpar{lewis_construction_2015} reported support for a total scale score (25 items) and four subscales. Below, I list the four subscales along with the items and their abbreviation. At the outset, let me provide a content advisory. For those who hold this particular identity (or related identities) the content in the items may be upsetting. In other lessons, I often provide a variable name that gives an indication of the primary content of the item. In the case of the GRMS, I will simply provide an abbreviation of the subscale name and its respective item number. This will allow us to easily inspect the alignment of the item with its intended factor, and hopefully minimize discomfort.

If you are not a member of this particular identity, I encourage you to learn about these microaggressions by reading the article in its entirety. Please do not ask members of this group to explain why these microaggressions are harmful or ask if they have encountered them. The four factors, number of items, and sample item are as follows:

\begin{itemize}
\tightlist
\item
  Assumptions of Beauty and Sexual Objectification (10 items)

  \begin{itemize}
  \tightlist
  \item
    Unattractive because of size of butt (Obj1)
  \item
    Negative comments about size of facial features (Obj2)
  \item
    Imitated the way they think Black women speak (Obj3)
  \item
    Someone made me feel unattractive (Obj4)
  \item
    Negative comment about skin tone (Obj5)
  \item
    Someone assumed I speak a certain way (Obj6)
  \item
    Objectified me based on physical features(Obj7)
  \item
    Someone assumed I have a certain body type (Obj8; stress only)
  \item
    Made a sexually inappropriate comment (Obj9)
  \item
    Negative comments about my hair when natural (Obj10)
  \item
    Assumed I was sexually promiscuous (frequency only; not used in this simulation)
  \end{itemize}
\item
  Silenced and Marginalized (7 items)

  \begin{itemize}
  \tightlist
  \item
    I have felt unheard (Marg1)
  \item
    My comments have been ignored (Marg2)
  \item
    Someone challenged my authority (Marg3)
  \item
    I have been disrespected in workplace (Marg4)
  \item
    Someone has tried to ``put me in my place'' (Marg5)
  \item
    Felt excluded from networking opportunities (Marg6)
  \item
    Assumed I did not have much to contribute to the conversation (Marg7)
  \end{itemize}
\item
  Strong Black Woman Stereotype (5 items)

  \begin{itemize}
  \tightlist
  \item
    Someone assumed I was sassy and straightforward (Str1; stress only)
  \item
    I have been told that I am too independent (Str2)
  \item
    Someone made me feel exotic as a Black woman (Str2; stress only)
  \item
    I have been told that I am too assertive
  \item
    Assumed to be a strong Black woman
  \end{itemize}
\item
  Angry Black Woman Stereotype (3 items)

  \begin{itemize}
  \tightlist
  \item
    Someone has told me to calm down (Ang1)
  \item
    Perceived to be ``angry Black woman'' (Ang2)
  \item
    Someone accused me of being angry when speaking calm (Ang3)
  \end{itemize}
\end{itemize}

Three additional scales were reported in the Lewis and Neville article \citeyearpar{lewis_construction_2015}. Because (a) the focus of this lesson is on exploratory factor analytic approaches and, therefore, only requires item-level data for the scale, and (b) the article does not include correlations between the subscales/scales of all involved measures, I only simulated item-level data for the GRMS items.

Below, I walk through the data simulation. This is not an essential portion of the lesson, but I will lecture it in case you are interested. None of the items are negatively worded (relative to the other items), so there is no need to reverse-score any items.

Simulating the data involved using factor loadings, means, standard deviations, and correlations between the scales. Because the simulation will produce ``out-of-bounds'' values, the code below rescales the scores into the range of the Likert-type scaling and rounds them to whole values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Entering the intercorrelations means and standard deviations from}
\CommentTok{\# the journal article}

\NormalTok{LewisGRMS\_generating\_model }\OtherTok{\textless{}{-}} \StringTok{"}
\StringTok{        \#measurement model}
\StringTok{        Objectification =\textasciitilde{} .69*Obj1 + .69*Obj2 + .60*Obj3 + .59*Obj4 + .55*Obj5 + .55*Obj6 + .54*Obj7 + .50*Obj8 + .41*Obj9 + .41*Obj10}
\StringTok{        Marginalized =\textasciitilde{} .93*Marg1 + .81*Marg2 +.69*Marg3 + .67*Marg4 + .61*Marg5 + .58*Marg6 +.54*Marg7}
\StringTok{        }
\StringTok{        Strong =\textasciitilde{} .59*Str1 + .55*Str2 + .54*Str3 + .54*Str4 + .51*Str5}
\StringTok{        Angry =\textasciitilde{} .70*Ang1 + .69*Ang2 + .68*Ang3}
\StringTok{        }
\StringTok{        \#Means}
\StringTok{         Objectification \textasciitilde{} 1.85*1}
\StringTok{         Marginalized \textasciitilde{} 2.67*1}
\StringTok{         Strong \textasciitilde{} 1.61*1}
\StringTok{         Angry \textasciitilde{} 2.29*1}
\StringTok{         }
\StringTok{        \#Correlations}
\StringTok{         Objectification \textasciitilde{}\textasciitilde{} .63*Marginalized}
\StringTok{         Objectification \textasciitilde{}\textasciitilde{} .66*Strong}
\StringTok{         Objectification \textasciitilde{}\textasciitilde{} .51*Angry}
\StringTok{         }
\StringTok{         Marginalized \textasciitilde{}\textasciitilde{} .59*Strong}
\StringTok{         Marginalized \textasciitilde{}\textasciitilde{} .62*Angry}

\StringTok{         Strong \textasciitilde{}\textasciitilde{} .61*Angry}
\StringTok{ }
\StringTok{        "}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{dfGRMS }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{simulateData}\NormalTok{(}\AttributeTok{model =}\NormalTok{ LewisGRMS\_generating\_model, }\AttributeTok{model.type =} \StringTok{"sem"}\NormalTok{,}
    \AttributeTok{meanstructure =}\NormalTok{ T, }\AttributeTok{sample.nobs =} \DecValTok{259}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# used to retrieve column indices used in the rescaling script below}
\NormalTok{col\_index }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(dfGRMS))}

\CommentTok{\# The code below loops through each column of the dataframe and}
\CommentTok{\# assigns the scaling accordingly Rows 1 thru 26 are the GRMS items}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(dfGRMS)) \{}
    \ControlFlowTok{if}\NormalTok{ (i }\SpecialCharTok{\textgreater{}=} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{25}\NormalTok{) \{}
\NormalTok{        dfGRMS[, i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfGRMS[, i], }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{    \}}
\NormalTok{\}}

\CommentTok{\# rounding to integers so that the data resembles that which was}
\CommentTok{\# collected}
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{dfGRMS }\OtherTok{\textless{}{-}}\NormalTok{ dfGRMS }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{round}\NormalTok{(}\DecValTok{0}\NormalTok{)}

\CommentTok{\# quick check psych::describe(dfGRMS)}
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either an .rds object (preserves any formatting you might do) or a .csv file (think ``Excel lite'').

An .rds file preserves all formatting to variables prior to the export and re-import. For the purpose of this chapter, you don't need to do either. That is, you can re-simulate the data each time you work the problem.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(dfGRMS, \textquotesingle{}dfGRMS.rds\textquotesingle{}) bring back the simulated dat}
\CommentTok{\# from an .rds file dfGRMS \textless{}{-} readRDS(\textquotesingle{}dfGRMS.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

If you save the .csv file and bring it back in, you will lose any formatting (e.g., ordered factors will be interpreted as character variables).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(dfGRMS,}
\CommentTok{\# file=\textquotesingle{}dfGRMS.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE) bring}
\CommentTok{\# back the simulated dat from a .csv file dfGRMS \textless{}{-} read.csv}
\CommentTok{\# (\textquotesingle{}dfGRMS.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

Before moving on, I want to acknowledge that (at their first drafting), I try to select research vignettes that have been published within the prior 5 years. With a publication date of 2015, this article clearly falls outside that range. I have continued to include it because (a) the scholarship is superior -- especially as the measure captures an intersectional identity, (b) the article has been a model for research that follows (e.g., Keum et al's \citeyearpar{keum_gendered_2018} Gendered Racial Microaggression Scale for Asian American Women), and (c) there is often a time lag between the initial publication of a psychometric scale and its use. A key reason I have retained the GRMS as a psychometrics research vignette is that in \href{https://lhbikos.github.io/ReC_MultivModel/}{ReCentering Psych Stats: Multivariate Modeling}, GRMS scales are used in a couple of more recently published research vignettes.

\hypertarget{working-the-vignette-1}{%
\section{Working the Vignette}\label{working-the-vignette-1}}

It may be useful to recall how we might understand factors in the psychometric sense:

\begin{itemize}
\tightlist
\item
  clusters of correlated items in an \(R\)-matrix
\item
  statistical entities that can be plotted as classification axes where coordinates of variables along each axis represent the strength of the relationship between that variable to each factor.
\item
  mathematical equations, resembling regression equations, where each variable is represented according to its relative weight
\end{itemize}

\hypertarget{data-prep-1}{%
\subsection{Data Prep}\label{data-prep-1}}

Since the first step is data preparation, let's start by:

\begin{itemize}
\tightlist
\item
  reverse coding any items that are phrased in the opposite direction
\item
  creating a \emph{df} (as an object) that only contains the items in their properly scored direction (i.e., you might need to replace the original item with the reverse-coded item); there should be no other variables (e.g., ID, demographic variables, other scales) in this df

  \begin{itemize}
  \tightlist
  \item
    because the GRMS has no items like this we can skip these two steps
  \end{itemize}
\end{itemize}

Our example today requires no reverse coding and the dataset I simulated only has item-level data (with no ID and no other variables). This means we are ready to start the PAF process.

Let's take a look at (and make an object of) the correlation matrix.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GRMSr }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(dfGRMS)  }\CommentTok{\#correlation matrix (with the negatively scored item already reversed) created and saved as object}
\FunctionTok{round}\NormalTok{(GRMSr, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      Obj1 Obj2 Obj3 Obj4 Obj5 Obj6 Obj7  Obj8 Obj9 Obj10 Marg1 Marg2 Marg3
Obj1  1.00 0.35 0.25 0.27 0.28 0.25 0.28  0.35 0.15  0.24  0.19  0.25  0.17
Obj2  0.35 1.00 0.31 0.25 0.27 0.23 0.31  0.28 0.26  0.24  0.22  0.21  0.25
Obj3  0.25 0.31 1.00 0.24 0.28 0.28 0.20  0.25 0.21  0.22  0.17  0.23  0.17
Obj4  0.27 0.25 0.24 1.00 0.39 0.23 0.28  0.30 0.26  0.28  0.22  0.18  0.14
Obj5  0.28 0.27 0.28 0.39 1.00 0.15 0.18  0.29 0.25  0.20  0.17  0.20  0.23
Obj6  0.25 0.23 0.28 0.23 0.15 1.00 0.20  0.14 0.21  0.12  0.10  0.14  0.05
Obj7  0.28 0.31 0.20 0.28 0.18 0.20 1.00  0.31 0.19  0.28  0.30  0.21  0.20
Obj8  0.35 0.28 0.25 0.30 0.29 0.14 0.31  1.00 0.19  0.23  0.27  0.14  0.14
Obj9  0.15 0.26 0.21 0.26 0.25 0.21 0.19  0.19 1.00  0.20  0.10  0.12  0.21
Obj10 0.24 0.24 0.22 0.28 0.20 0.12 0.28  0.23 0.20  1.00  0.09  0.12  0.17
Marg1 0.19 0.22 0.17 0.22 0.17 0.10 0.30  0.27 0.10  0.09  1.00  0.43  0.41
Marg2 0.25 0.21 0.23 0.18 0.20 0.14 0.21  0.14 0.12  0.12  0.43  1.00  0.35
Marg3 0.17 0.25 0.17 0.14 0.23 0.05 0.20  0.14 0.21  0.17  0.41  0.35  1.00
Marg4 0.19 0.18 0.24 0.26 0.20 0.10 0.25  0.24 0.07  0.12  0.38  0.23  0.32
Marg5 0.17 0.22 0.21 0.27 0.25 0.16 0.23  0.19 0.19  0.11  0.41  0.40  0.25
Marg6 0.18 0.27 0.16 0.23 0.22 0.26 0.28  0.26 0.15  0.26  0.35  0.27  0.25
Marg7 0.13 0.19 0.14 0.19 0.06 0.17 0.16  0.14 0.10  0.11  0.31  0.33  0.20
Str1  0.22 0.18 0.14 0.06 0.23 0.07 0.25  0.17 0.19  0.10  0.19  0.25  0.20
Str2  0.19 0.18 0.19 0.19 0.12 0.15 0.13  0.06 0.18  0.19  0.12  0.18  0.17
Str3  0.10 0.09 0.09 0.08 0.11 0.09 0.19  0.05 0.12  0.10  0.13  0.18  0.10
Str4  0.09 0.14 0.18 0.15 0.12 0.08 0.07  0.13 0.05  0.02  0.08  0.12  0.08
Str5  0.20 0.15 0.15 0.08 0.19 0.11 0.15  0.04 0.07  0.09  0.10  0.23  0.12
Ang1  0.06 0.07 0.07 0.09 0.12 0.04 0.15  0.07 0.17  0.06  0.16  0.23  0.18
Ang2  0.06 0.15 0.08 0.06 0.09 0.20 0.13 -0.03 0.00  0.14  0.17  0.19  0.19
Ang3  0.21 0.13 0.11 0.14 0.11 0.16 0.23  0.07 0.06  0.08  0.28  0.28  0.11
      Marg4 Marg5 Marg6 Marg7 Str1 Str2 Str3 Str4 Str5 Ang1  Ang2 Ang3
Obj1   0.19  0.17  0.18  0.13 0.22 0.19 0.10 0.09 0.20 0.06  0.06 0.21
Obj2   0.18  0.22  0.27  0.19 0.18 0.18 0.09 0.14 0.15 0.07  0.15 0.13
Obj3   0.24  0.21  0.16  0.14 0.14 0.19 0.09 0.18 0.15 0.07  0.08 0.11
Obj4   0.26  0.27  0.23  0.19 0.06 0.19 0.08 0.15 0.08 0.09  0.06 0.14
Obj5   0.20  0.25  0.22  0.06 0.23 0.12 0.11 0.12 0.19 0.12  0.09 0.11
Obj6   0.10  0.16  0.26  0.17 0.07 0.15 0.09 0.08 0.11 0.04  0.20 0.16
Obj7   0.25  0.23  0.28  0.16 0.25 0.13 0.19 0.07 0.15 0.15  0.13 0.23
Obj8   0.24  0.19  0.26  0.14 0.17 0.06 0.05 0.13 0.04 0.07 -0.03 0.07
Obj9   0.07  0.19  0.15  0.10 0.19 0.18 0.12 0.05 0.07 0.17  0.00 0.06
Obj10  0.12  0.11  0.26  0.11 0.10 0.19 0.10 0.02 0.09 0.06  0.14 0.08
Marg1  0.38  0.41  0.35  0.31 0.19 0.12 0.13 0.08 0.10 0.16  0.17 0.28
Marg2  0.23  0.40  0.27  0.33 0.25 0.18 0.18 0.12 0.23 0.23  0.19 0.28
Marg3  0.32  0.25  0.25  0.20 0.20 0.17 0.10 0.08 0.12 0.18  0.19 0.11
Marg4  1.00  0.30  0.26  0.16 0.10 0.21 0.05 0.06 0.03 0.12  0.22 0.17
Marg5  0.30  1.00  0.29  0.28 0.16 0.13 0.16 0.14 0.18 0.12  0.14 0.21
Marg6  0.26  0.29  1.00  0.20 0.13 0.18 0.15 0.13 0.08 0.11  0.21 0.12
Marg7  0.16  0.28  0.20  1.00 0.14 0.05 0.04 0.02 0.12 0.17  0.13 0.09
Str1   0.10  0.16  0.13  0.14 1.00 0.21 0.30 0.23 0.23 0.18  0.05 0.10
Str2   0.21  0.13  0.18  0.05 0.21 1.00 0.20 0.20 0.12 0.16  0.12 0.16
Str3   0.05  0.16  0.15  0.04 0.30 0.20 1.00 0.27 0.18 0.20  0.07 0.15
Str4   0.06  0.14  0.13  0.02 0.23 0.20 0.27 1.00 0.12 0.15  0.03 0.02
Str5   0.03  0.18  0.08  0.12 0.23 0.12 0.18 0.12 1.00 0.22  0.15 0.11
Ang1   0.12  0.12  0.11  0.17 0.18 0.16 0.20 0.15 0.22 1.00  0.24 0.23
Ang2   0.22  0.14  0.21  0.13 0.05 0.12 0.07 0.03 0.15 0.24  1.00 0.25
Ang3   0.17  0.21  0.12  0.09 0.10 0.16 0.15 0.02 0.11 0.23  0.25 1.00
\end{verbatim}

In case you want to examine it in sections (easier to view):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# round(GRMSr[,1:8], 2) round(GRMSr[,9:16], 2) round(GRMSr[,17:25],}
\CommentTok{\# 2)}
\end{Highlighting}
\end{Shaded}

As with PCA, we can analyze the data with either raw data or correlation matrix. I will do both to demonstrate (a) that it's possible and to (b) continue emphasizing that this is a \emph{structural} analysis. That is, we are trying to see if our more parsimonious extraction \emph{reproduces} this original correlation matrix.

\hypertarget{three-diagnostic-tests-to-evaluate-the-appropriateness-of-the-data-for-factor-or-componentanalysis}{%
\subsubsection{Three Diagnostic Tests to Evaluate the Appropriateness of the Data for Factor (or Component))Analysis}\label{three-diagnostic-tests-to-evaluate-the-appropriateness-of-the-data-for-factor-or-componentanalysis}}

Here's a snip of our location in the PAF workflow.

\begin{figure}
\centering
\includegraphics{images/PAF/PAS_assumptions.png}
\caption{Image of our location in the PAF workflow}
\end{figure}

\hypertarget{is-my-sample-adequate-for-paf}{%
\paragraph{Is my sample adequate for PAF?}\label{is-my-sample-adequate-for-paf}}

We return to the \textbf{KMO} (Kaiser-Meyer-Olkin), an index of \emph{sampling adequacy} that can be used with the actual sample to let us know if the sample size is sufficient (or if we should collect more data).

Kaiser's 1974 recommendations were:

\begin{itemize}
\tightlist
\item
  bare minimum of .5
\item
  values between .5 and .7 are mediocre
\item
  values between .7 and .8 are good
\item
  values above .9 are superb
\end{itemize}

We use the \emph{KMO()} function from the \emph{psych} package with either raw or matrix dat.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{KMO}\NormalTok{(dfGRMS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Kaiser-Meyer-Olkin factor adequacy
Call: psych::KMO(r = dfGRMS)
Overall MSA =  0.85
MSA for each item = 
 Obj1  Obj2  Obj3  Obj4  Obj5  Obj6  Obj7  Obj8  Obj9 Obj10 Marg1 Marg2 Marg3 
 0.87  0.91  0.88  0.85  0.85  0.80  0.90  0.85  0.81  0.85  0.86  0.89  0.86 
Marg4 Marg5 Marg6 Marg7  Str1  Str2  Str3  Str4  Str5  Ang1  Ang2  Ang3 
 0.86  0.90  0.89  0.84  0.83  0.85  0.82  0.74  0.84  0.78  0.76  0.81 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# psych::KMO(GRMSr) \#for the KMO function, do not specify sample size}
\CommentTok{\# if using the matrix form of the data}
\end{Highlighting}
\end{Shaded}

We examine the KMO values for both the overall matrix and the individual items.

At the matrix level, our \(KMO = .85\), which falls in between Kaiser's definitions of \emph{good} and \emph{superb}.

At the item level, the KMO should be \textgreater{} .50. Variables with values below .5 should be evaluated for exclusion from the analysis (or run the analysis with and without the variable and compare the difference). Because removing/adding variables impacts the KMO, be sure to re-evaluate.

At the item level, our KMO values range between .74 (Str4) and .91 (Obj2).

Considering both item- and matrix- levels, we conclude that the sample size and the data are adequate for factor (or component) analysis.

\hypertarget{are-the-correlations-among-the-variables-big-enough-to-be-analyzed}{%
\paragraph{Are the correlations among the variables big enough to be analyzed?}\label{are-the-correlations-among-the-variables-big-enough-to-be-analyzed}}

\textbf{Bartlett's} lets us know if a matrix is an \emph{identity matrix.} In an identity matrix all correlation coefficients (everything on the off-diagonal) would be 0.0 (and everything on the diagonal would be 1.0).

A significant Barlett's (i.e., \(p < .05\)) tells that the \(R\)-matrix is not an identity matrix. That is, there are some relationships between variables that can be analyzed.

The \emph{cortest.bartlett()} function in the \emph{psych} package and can be run either from the raw data or R matrix formats.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{cortest.bartlett}\NormalTok{(dfGRMS)  }\CommentTok{\#from the raw data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
R was not square, finding R from data
\end{verbatim}

\begin{verbatim}
$chisq
[1] 1217.508

$p.value
[1] 0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001107085

$df
[1] 300
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# raw data produces the warning \textquotesingle{}R was not square, finding R from}
\CommentTok{\# data.\textquotesingle{} This means nothing other than we fed it raw data and the}
\CommentTok{\# function is creating a matrix from which to do the analysis.}

\CommentTok{\# psych::cortest.bartlett(GRMSr, n = 259) \#if using the matrix, must}
\CommentTok{\# specify sample size}
\end{Highlighting}
\end{Shaded}

Our Bartlett's test is significant: \(\chi^{2}(300)=1217.508, p < .001\). This supports a factor (or component) analytic approach for investigating the data.

\hypertarget{is-there-multicollinearity-or-singularity-in-my-data-1}{%
\paragraph{Is there multicollinearity or singularity in my data?}\label{is-there-multicollinearity-or-singularity-in-my-data-1}}

The \textbf{determinant of the correlation matrix} should be greater than 0.00001 (that would be 4 zeros before the 1). If it is smaller than 0.00001 then we may have an issue with \emph{multicollinearity} (i.e., variables that are too highly correlated) or \emph{singularity} (variables that are perfectly correlated).

The determinant function comes from base R. It is easiest to compute when the correlation matrix is the object. However, it is also possible to specify the command to work with the raw data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# det(GRMSr)}
\FunctionTok{det}\NormalTok{(}\FunctionTok{cor}\NormalTok{(dfGRMS))  }\CommentTok{\#if using the raw data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.007499909
\end{verbatim}

With a value of 0.0075, our determinant is greater than the 0.00001 requirement. If it were not, then we could identify problematic variables (i.e., those correlating too highly with others and those not correlating sufficiently with others) and re-run the diagnostic statistics.

\hypertarget{apa-style-summary-so-far-1}{%
\subsubsection{APA Style Summary So Far}\label{apa-style-summary-so-far-1}}

\begin{quote}
Data screening were conducted to determine the suitability of the data for principal axis factoring. The Kaiser-Meyer-Olkin measure of sampling adequacy (KMO; Kaiser, 1970) represents the ratio of the squared correlation between variables to the squared partial correlation between variables. KMO ranges from 0.00 to 1.00; values closer to 1.00 indicate that the patterns of correlations are relatively compact, and that factor analysis should yield distinct and reliable factors (Field, 2012). In our dataset, the KMO value was .85, indicating acceptable sampling adequacy. The Barlett's Test of Sphericity examines whether the population correlation matrix resembles an identity matrix (Field, 2012). When the \emph{p} value for the Bartlett's test is \textless{} .05, we are fairly certain we have clusters of correlated variables. In our dataset, \(\chi^{2}(300)=1217.508, p < .001\), indicating the correlations between items are sufficiently large enough for principal axis factoring. The determinant of the correlation matrix alerts us to any issues of multicollinearity or singularity and should be larger than 0.00001. Our determinant was 0.0075, supporting the suitability of our data for analysis.
\end{quote}

\emph{Note}: If this looks familiar, it is! The same diagnostics are used in PAF and \protect\hyperlink{PCA}{PCA}.

\hypertarget{principal-axis-factoring-paf}{%
\subsection{Principal Axis Factoring (PAF)}\label{principal-axis-factoring-paf}}

Here's a snip of our location in the PAF workflow.

\begin{figure}
\centering
\includegraphics{images/PAF/PAF_ExtractionNo.png}
\caption{Image of our location in the PAF workflow}
\end{figure}

We can use the \emph{fa()} function, specifying \emph{fm = ``pa''} from the \emph{psych} package with raw or matrix data.

One difference from PCA is that factor analysis will not (cannot) calculate as many factors as there are items. This means that we should select a reasonable number, like 20 (since there are 25 items). However, I received a number of errors/warnings and 13 is the first number that would run. I also received the warning, ``maximum iteration exceeded.'' Therefore, I increased ``max.iter'' to 100.

Our goal is to begin to get an idea of the cumulative variance explained, and number of factors to extract. If we think there are four factors, we simply need to specify more than four factors on the \emph{nfactors = \#\#} command. As long as that number is less than the total number of items, it does not matter what that number is.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# grmsPAF1 \textless{}{-} psych::fa(GRMSr, nfactors=10, fm = \textquotesingle{}pa\textquotesingle{}, max.iter =}
\CommentTok{\# 100, rotate=\textquotesingle{}none\textquotesingle{})\# using the matrix data and specifying the \# of}
\CommentTok{\# factors.}

\NormalTok{grmsPAF1 }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{fa}\NormalTok{(dfGRMS, }\AttributeTok{nfactors =} \DecValTok{13}\NormalTok{, }\AttributeTok{fm =} \StringTok{"pa"}\NormalTok{, }\AttributeTok{max.iter =} \DecValTok{100}\NormalTok{,}
    \AttributeTok{rotate =} \StringTok{"none"}\NormalTok{)  }\CommentTok{\# using raw data and specifying the max number of factors}

\CommentTok{\# I received the warning \textquotesingle{}maximum iteration exceeded\textquotesingle{}. It gave}
\CommentTok{\# output, but it\textquotesingle{}s best if we don\textquotesingle{}t get that warning, so I increased}
\CommentTok{\# it to 100.}

\NormalTok{grmsPAF1  }\CommentTok{\#this object holds a great deal of information }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  pa
Call: psych::fa(r = dfGRMS, nfactors = 13, rotate = "none", max.iter = 100, 
    fm = "pa")
Standardized loadings (pattern matrix) based upon correlation matrix
       PA1   PA2   PA3   PA4   PA5   PA6   PA7   PA8   PA9  PA10  PA11  PA12
Obj1  0.49 -0.20 -0.01  0.13 -0.01  0.11 -0.17 -0.16  0.01 -0.09  0.21  0.04
Obj2  0.51 -0.19 -0.03  0.13  0.07 -0.02 -0.04  0.00  0.06 -0.08  0.09 -0.01
Obj3  0.45 -0.17 -0.01  0.05  0.03  0.09  0.00  0.12 -0.04 -0.04  0.14  0.10
Obj4  0.49 -0.25 -0.11  0.03 -0.06  0.04  0.04  0.03 -0.09  0.18 -0.09  0.19
Obj5  0.54 -0.46  0.06 -0.50 -0.46  0.01 -0.09  0.08  0.04 -0.01 -0.06 -0.07
Obj6  0.38 -0.15  0.00  0.39 -0.08  0.07 -0.06  0.36 -0.08  0.02  0.09 -0.18
Obj7  0.51 -0.04 -0.04  0.16  0.06  0.03 -0.02 -0.22  0.05  0.02 -0.05 -0.12
Obj8  0.46 -0.29 -0.17 -0.02  0.15  0.05 -0.08 -0.25 -0.03  0.24  0.09 -0.11
Obj9  0.40 -0.30  0.16  0.09  0.06 -0.44  0.35  0.01 -0.27 -0.09 -0.03 -0.04
Obj10 0.39 -0.23 -0.04  0.21  0.00 -0.01  0.12 -0.15  0.33 -0.04 -0.19  0.20
Marg1 0.56  0.33 -0.24 -0.12  0.11 -0.12 -0.15 -0.06 -0.07  0.05 -0.11 -0.07
Marg2 0.55  0.30  0.03 -0.07  0.02 -0.14 -0.21  0.06  0.00 -0.07  0.05  0.15
Marg3 0.48  0.18 -0.09 -0.17  0.08 -0.19  0.11  0.01  0.12 -0.17 -0.01  0.00
Marg4 0.51  0.21 -0.47 -0.20  0.03  0.32  0.38  0.00 -0.08 -0.08  0.14 -0.02
Marg5 0.52  0.15 -0.09 -0.10  0.04 -0.10 -0.13  0.14 -0.13  0.03 -0.08  0.09
Marg6 0.51  0.02 -0.13  0.08  0.09  0.00 -0.01  0.12  0.15  0.08 -0.24 -0.16
Marg7 0.37  0.17 -0.10  0.05  0.08 -0.22 -0.12  0.12  0.06  0.13  0.12  0.08
Str1  0.40  0.02  0.34 -0.14  0.18  0.01 -0.07 -0.14  0.02 -0.18  0.07 -0.14
Str2  0.36  0.01  0.17  0.07  0.07  0.14  0.17  0.04 -0.01 -0.14 -0.05  0.11
Str3  0.30  0.10  0.38 -0.02  0.13  0.13 -0.01 -0.03 -0.03 -0.05 -0.17 -0.07
Str4  0.27 -0.03  0.36 -0.13  0.28  0.32  0.00  0.16 -0.10  0.16 -0.09  0.08
Str5  0.30  0.07  0.27 -0.02 -0.07  0.01 -0.11  0.04  0.13 -0.06  0.16  0.04
Ang1  0.34  0.32  0.38 -0.03 -0.17 -0.10  0.28 -0.11  0.09  0.34  0.16 -0.01
Ang2  0.30  0.28  0.00  0.15 -0.24  0.08  0.12  0.16  0.22 -0.02 -0.03 -0.08
Ang3  0.38  0.32  0.06  0.25 -0.39  0.12 -0.10 -0.24 -0.29 -0.07 -0.12  0.03
       PA13   h2     u2 com
Obj1  -0.05 0.42 0.5768 2.8
Obj2  -0.15 0.36 0.6397 1.9
Obj3  -0.07 0.30 0.7008 2.1
Obj4   0.11 0.43 0.5749 2.7
Obj5   0.00 1.00 0.0031 4.1
Obj6   0.05 0.51 0.4897 4.2
Obj7   0.12 0.38 0.6219 1.9
Obj8  -0.07 0.50 0.4962 4.2
Obj9  -0.01 0.69 0.3115 5.1
Obj10  0.05 0.48 0.5243 5.2
Marg1 -0.07 0.58 0.4213 2.9
Marg2  0.01 0.50 0.5037 2.4
Marg3 -0.20 0.44 0.5594 3.2
Marg4  0.12 0.86 0.1379 4.9
Marg5  0.12 0.41 0.5908 2.2
Marg6 -0.03 0.42 0.5816 2.5
Marg7  0.10 0.31 0.6869 4.2
Str1   0.11 0.43 0.5733 4.2
Str2  -0.02 0.25 0.7472 3.3
Str3   0.15 0.34 0.6630 3.7
Str4  -0.17 0.51 0.4923 6.2
Str5   0.10 0.25 0.7537 4.0
Ang1  -0.02 0.64 0.3585 6.3
Ang2  -0.07 0.35 0.6509 5.8
Ang3  -0.09 0.66 0.3388 6.2

                       PA1  PA2  PA3  PA4  PA5  PA6  PA7  PA8  PA9 PA10 PA11
SS loadings           4.86 1.25 1.04 0.76 0.68 0.62 0.58 0.51 0.45 0.39 0.36
Proportion Var        0.19 0.05 0.04 0.03 0.03 0.02 0.02 0.02 0.02 0.02 0.01
Cumulative Var        0.19 0.24 0.29 0.32 0.34 0.37 0.39 0.41 0.43 0.45 0.46
Proportion Explained  0.40 0.10 0.09 0.06 0.06 0.05 0.05 0.04 0.04 0.03 0.03
Cumulative Proportion 0.40 0.51 0.60 0.66 0.72 0.77 0.82 0.86 0.89 0.93 0.96
                      PA12 PA13
SS loadings           0.27 0.24
Proportion Var        0.01 0.01
Cumulative Var        0.47 0.48
Proportion Explained  0.02 0.02
Cumulative Proportion 0.98 1.00

Mean item complexity =  3.8
Test of the hypothesis that 13 factors are sufficient.

df null model =  300  with the objective function =  4.89 with Chi Square =  1217.51
df of  the model are 53  and the objective function was  0.1 

The root mean square of the residuals (RMSR) is  0.01 
The df corrected root mean square of the residuals is  0.03 

The harmonic n.obs is  259 with the empirical chi square  17.64  with prob <  1 
The total n.obs was  259  with Likelihood Chi Square =  24.56  with prob <  1 

Tucker Lewis Index of factoring reliability =  1.184
RMSEA index =  0  and the 90 % confidence intervals are  0 0
BIC =  -269.95
Fit based upon off diagonal values = 1
Measures of factor score adequacy             
                                                   PA1  PA2  PA3  PA4  PA5  PA6
Correlation of (regression) scores with factors   0.96 0.89 0.85 0.87 0.85 0.79
Multiple R square of scores with factors          0.92 0.79 0.72 0.75 0.72 0.62
Minimum correlation of possible factor scores     0.83 0.58 0.44 0.51 0.44 0.25
                                                   PA7  PA8  PA9  PA10  PA11
Correlation of (regression) scores with factors   0.80 0.71 0.71  0.67  0.64
Multiple R square of scores with factors          0.64 0.50 0.50  0.45  0.42
Minimum correlation of possible factor scores     0.28 0.01 0.00 -0.09 -0.17
                                                   PA12  PA13
Correlation of (regression) scores with factors    0.58  0.56
Multiple R square of scores with factors           0.34  0.31
Minimum correlation of possible factor scores     -0.33 -0.38
\end{verbatim}

The total variance for a particular variable will have two factors: some variance will be shared with other variables (common variance) and some variance will be specific to that measure (unique variance). Random variance is also specific to one item, but not reliably so. We can examine this most easily by examining the matrix (second screen).

The columns PA1 thru PA10 are the (uninteresting at this point) unrotated loadings. These are the loading from each factor to each variable. PA stands for ``principal axis.''

Scrolling to the far right we are interested in:

\textbf{Communalities} are represented as \(h^2\). These are the proportions of common variance present in the variables. A variable that has no specific (or random) variance would have a communality of 1.0. If a variable shares none of its variance with any other variable, its communality would be 0.0. As a point of comparison, in PCA these started as 1.0 because we extracted the same number of components as items. In PAF, because we must extract fewer factors than items, these will have unique values.

**Uniquenesses* are represented as \(u2\). These are the amount of unique variance for each variable. They are calculated as \(1 - h^2\) (or 1 minus the communality).

The final column, \emph{com} represents \emph{item complexity.} This is an indication of how well an item reflects a single construct. If it is 1.0 then the item loads only on one factor, if it is 2.0, it loads evenly on two factors, and so forth. For now, we can ignore this. \emph{I mostly wanted to reassure you that ``com'' is not ``communality'' -- h2 is communality}.

Let's switch to the first screen of output.

\textbf{Eigenvalues} are displayed in the row called, \emph{SS loadings} (i.e., the sum of squared loadings). They represent the variance explained by the particular linear factor. PA1 explains 4.86 units of variance (out of a possible 25; the \# of potential factors). As a proportion, this is 4.86/25 = 0.1944 (reported in the \emph{Proportion Var} row). We inspect the eigenvalues to see how many are \textgreater{} 1.0 (Kaiser's eigenvalue \textgreater{} 1 criteria criteria). We see there are three that meet Kaiser's critera and four that meet Joliffe's criteria (eigenvalues \textgreater{} .70).

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{4.86}\SpecialCharTok{/}\DecValTok{25}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1944
\end{verbatim}

\textbf{Cumulative Var} is helpful to determine how many factors we'd like to retain to balance parsimony (few as possible) with the amount of variance we want to explain. Directly related to the eigenvalues, we can see how including each additional factor accounts for a greater proportion of variance.

\textbf{Scree plots} can help us visualize the relationship between the eigenvalues. A rule of thumb is to select the number of factors that is associated with the number of dots before the flattening of the curve. Eigenvalues are stored in the \emph{grmsPAF1} object's variable, ``values''. We can see all the values captured by this object with the \emph{names()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(grmsPAF1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "residual"               "dof"                    "chi"                   
 [4] "nh"                     "rms"                    "EPVAL"                 
 [7] "crms"                   "EBIC"                   "ESABIC"                
[10] "fit"                    "fit.off"                "sd"                    
[13] "factors"                "complexity"             "n.obs"                 
[16] "objective"              "criteria"               "STATISTIC"             
[19] "PVAL"                   "Call"                   "null.model"            
[22] "null.dof"               "null.chisq"             "TLI"                   
[25] "F0"                     "RMSEA"                  "BIC"                   
[28] "SABIC"                  "r.scores"               "R2"                    
[31] "valid"                  "score.cor"              "weights"               
[34] "rotation"               "hyperplane"             "communality"           
[37] "communalities"          "uniquenesses"           "values"                
[40] "e.values"               "loadings"               "model"                 
[43] "fm"                     "Structure"              "communality.iterations"
[46] "method"                 "scores"                 "R2.scores"             
[49] "r"                      "np.obs"                 "fn"                    
[52] "Vaccounted"            
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(grmsPAF1}\SpecialCharTok{$}\NormalTok{values, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{)  }\CommentTok{\#type = \textquotesingle{}b\textquotesingle{} gives us \textquotesingle{}both\textquotesingle{} lines and points;  type = \textquotesingle{}l\textquotesingle{} gives lines and is relatively worthless}
\end{Highlighting}
\end{Shaded}

\includegraphics{09-EFA_PAF_files/figure-latex/unnamed-chunk-14-1.pdf}

We look for the point of \emph{inflexion}. That is, where the baseline levels out into a plateau. I can see inflections after 1, 2, 3, and 4.

\hypertarget{specifying-the-number-of-factors}{%
\subsubsection{Specifying the number of factors}\label{specifying-the-number-of-factors}}

Here's a snip of our location in the PAF workflow.

\begin{figure}
\centering
\includegraphics{images/PAF/PAF_extraction.png}
\caption{Image of our location in the PAF workflow}
\end{figure}

Having determined the number of factors, we must rerun the analysis with this specification. Especially when researchers may not have a clear theoretical structure that guides the process, researchers may do this iteratively with varying numbers of factors. Lewis and Neville \citep{lewis_construction_2015} examined solutions with 2, 3, 4, and 5 factors (they conducted a parallel \emph{factor} analysis; in contrast this lesson demonstrates principal axis factoring).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# grmsPAF2 \textless{}{-} psych::fa(GRMSr, nfactors=4, fm = \textquotesingle{}pa\textquotesingle{}, rotate=\textquotesingle{}none\textquotesingle{})}
\NormalTok{grmsPAF2 }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{fa}\NormalTok{(dfGRMS, }\AttributeTok{nfactors =} \DecValTok{4}\NormalTok{, }\AttributeTok{fm =} \StringTok{"pa"}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"none"}\NormalTok{)  }\CommentTok{\#can copy prior script, but change nfactors and object name}
\NormalTok{grmsPAF2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  pa
Call: psych::fa(r = dfGRMS, nfactors = 4, rotate = "none", fm = "pa")
Standardized loadings (pattern matrix) based upon correlation matrix
       PA1   PA2   PA3   PA4   h2   u2 com
Obj1  0.48 -0.24  0.02  0.02 0.29 0.71 1.5
Obj2  0.52 -0.22 -0.02  0.06 0.32 0.68 1.4
Obj3  0.45 -0.20  0.02  0.05 0.25 0.75 1.4
Obj4  0.49 -0.27 -0.12  0.03 0.33 0.67 1.7
Obj5  0.47 -0.23  0.03 -0.05 0.28 0.72 1.5
Obj6  0.37 -0.15  0.00  0.26 0.22 0.78 2.2
Obj7  0.51 -0.09 -0.02  0.02 0.27 0.73 1.1
Obj8  0.45 -0.31 -0.16 -0.16 0.35 0.65 2.4
Obj9  0.37 -0.21  0.08 -0.02 0.19 0.81 1.7
Obj10 0.38 -0.23 -0.02  0.14 0.22 0.78 2.0
Marg1 0.58  0.36 -0.31 -0.23 0.62 0.38 2.6
Marg2 0.55  0.31  0.00 -0.09 0.41 0.59 1.6
Marg3 0.48  0.19 -0.09 -0.09 0.28 0.72 1.5
Marg4 0.46  0.11 -0.24  0.01 0.28 0.72 1.6
Marg5 0.53  0.16 -0.11 -0.11 0.33 0.67 1.4
Marg6 0.50  0.01 -0.12  0.06 0.27 0.73 1.2
Marg7 0.37  0.16 -0.13 -0.03 0.18 0.82 1.6
Str1  0.40  0.04  0.38 -0.24 0.36 0.64 2.6
Str2  0.36 -0.01  0.21  0.08 0.18 0.82 1.7
Str3  0.30  0.10  0.41 -0.08 0.28 0.72 2.1
Str4  0.26 -0.03  0.29 -0.11 0.17 0.83 2.3
Str5  0.31  0.09  0.27  0.04 0.18 0.82 2.2
Ang1  0.32  0.24  0.24  0.08 0.22 0.78 3.0
Ang2  0.31  0.31  0.01  0.49 0.43 0.57 2.5
Ang3  0.35  0.21  0.03  0.17 0.19 0.81 2.1

                       PA1  PA2  PA3  PA4
SS loadings           4.67 1.03 0.83 0.57
Proportion Var        0.19 0.04 0.03 0.02
Cumulative Var        0.19 0.23 0.26 0.28
Proportion Explained  0.66 0.15 0.12 0.08
Cumulative Proportion 0.66 0.80 0.92 1.00

Mean item complexity =  1.9
Test of the hypothesis that 4 factors are sufficient.

df null model =  300  with the objective function =  4.89 with Chi Square =  1217.51
df of  the model are 206  and the objective function was  0.77 

The root mean square of the residuals (RMSR) is  0.04 
The df corrected root mean square of the residuals is  0.04 

The harmonic n.obs is  259 with the empirical chi square  202.41  with prob <  0.56 
The total n.obs was  259  with Likelihood Chi Square =  189.19  with prob <  0.79 

Tucker Lewis Index of factoring reliability =  1.027
RMSEA index =  0  and the 90 % confidence intervals are  0 0.018
BIC =  -955.52
Fit based upon off diagonal values = 0.97
Measures of factor score adequacy             
                                                   PA1  PA2  PA3   PA4
Correlation of (regression) scores with factors   0.93 0.79 0.74  0.70
Multiple R square of scores with factors          0.87 0.62 0.55  0.48
Minimum correlation of possible factor scores     0.75 0.24 0.11 -0.03
\end{verbatim}

Our eigenvalues/SS loadings wiggle around a bit from the initial run. With four factors, we now, cumulatively, explain 28\% of the variance.

\emph{Communality} is the proportion of common variance within a variable. Changing from 13 to 4 factors changed these values (\(h2\)) as well as their associated \emph{uniquenesses} (\(u2\)), which are calculated as ``1.0 minus the communality.''

Now we see that 29\% of the variance associated with Obj1 is common/shared (the \(h2\) value).

As a reminder of what we are doing, recall that we are looking for a more \emph{parsimonious} explanation than 25 items on the GRMS. By respecifying a smaller number of factors, we lose some information. That is, the retained factors (now 4) cannot explain all of the variance present in the data (as we saw, it explains about 28\%, cumulatively). The amount of variance explained in each variable is represented by the communalities after extraction.

We can also inspect the communalities through the lens of Kaiser's criterion (the eigenvalue \textgreater{} 1 criteria) to see if we think that four was a good number of factors to extract.

Kaiser's criterion is believed to be accurate:

\begin{itemize}
\tightlist
\item
  when there are fewer than 30 variables (we had 25) and, after extraction, the communalities are greater than .70

  \begin{itemize}
  \tightlist
  \item
    looking at our data, none of the communalities is \textgreater{} .70, so, this does not support extracting four factors
  \end{itemize}
\item
  When the sample size is greater than 250 (ours was 259) and the average communality is \textgreater{} .60

  \begin{itemize}
  \tightlist
  \item
    calculated below, ours was .28.
  \end{itemize}
\end{itemize}

Using the \emph{names()} function again, we see that ``communality'' is available for manipulation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(grmsPAF2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "residual"               "dof"                    "chi"                   
 [4] "nh"                     "rms"                    "EPVAL"                 
 [7] "crms"                   "EBIC"                   "ESABIC"                
[10] "fit"                    "fit.off"                "sd"                    
[13] "factors"                "complexity"             "n.obs"                 
[16] "objective"              "criteria"               "STATISTIC"             
[19] "PVAL"                   "Call"                   "null.model"            
[22] "null.dof"               "null.chisq"             "TLI"                   
[25] "F0"                     "RMSEA"                  "BIC"                   
[28] "SABIC"                  "r.scores"               "R2"                    
[31] "valid"                  "score.cor"              "weights"               
[34] "rotation"               "hyperplane"             "communality"           
[37] "communalities"          "uniquenesses"           "values"                
[40] "e.values"               "loadings"               "model"                 
[43] "fm"                     "Structure"              "communality.iterations"
[46] "method"                 "scores"                 "R2.scores"             
[49] "r"                      "np.obs"                 "fn"                    
[52] "Vaccounted"            
\end{verbatim}

We can use this value to calculate their mean.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(grmsPAF2}\SpecialCharTok{$}\NormalTok{communality)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.2840516
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# sum(grmsPAF2$communality) \#}
\end{Highlighting}
\end{Shaded}

We see that our average communality is 0.28. These two criteria suggest that we may not have the best solution. That said (in our defense):

\begin{itemize}
\tightlist
\item
  We used the scree plot as a guide, and it was very clear.
\item
  We have an adequate sample size and that was supported with the KMO.
\item
  Are the number of factors consistent with theory? We have not yet inspected the factor loadings. This will provide us with more information.
\end{itemize}

We could do several things:

\begin{itemize}
\tightlist
\item
  rerun with a different number of factors (recall Lewis and Neville \citeyearpar{lewis_construction_2015} ran models with 2, 3, 4, and 5 factors)
\item
  conduct more diagnostics tests

  \begin{itemize}
  \tightlist
  \item
    reproduced correlation matrix
  \item
    the difference between the reproduced correlation matrix and the correlation matrix from the data
  \end{itemize}
\end{itemize}

The \emph{factor.model()} function in \emph{psych} produces the \emph{reproduced correlation matrix} by using the \emph{loadings} in our extracted object. Conceptually, this matrix is the correlations that should be produced if we did not have the raw data, but we only had the factor loadings.

The questions, though, is: How close did we get? How different is the \emph{reproduced correlation matrix} from \emph{GRMSmatrix} -- the \(R\)-matrix produced from our raw data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(psych}\SpecialCharTok{::}\FunctionTok{factor.model}\NormalTok{(grmsPAF2}\SpecialCharTok{$}\NormalTok{loadings), }\DecValTok{3}\NormalTok{)  }\CommentTok{\#produces the reproduced correlation matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       Obj1  Obj2  Obj3  Obj4  Obj5  Obj6  Obj7   Obj8  Obj9 Obj10 Marg1 Marg2
Obj1  0.293 0.304 0.270 0.300 0.284 0.219 0.270  0.287 0.229 0.242 0.187 0.193
Obj2  0.304 0.319 0.282 0.317 0.291 0.239 0.286  0.294 0.232 0.256 0.215 0.212
Obj3  0.270 0.282 0.250 0.277 0.260 0.209 0.252  0.258 0.209 0.225 0.178 0.185
Obj4  0.300 0.317 0.277 0.326 0.288 0.229 0.279  0.317 0.225 0.255 0.219 0.186
Obj5  0.284 0.291 0.260 0.288 0.280 0.196 0.261  0.287 0.226 0.225 0.193 0.194
Obj6  0.219 0.239 0.209 0.229 0.196 0.225 0.207  0.169 0.161 0.210 0.097 0.131
Obj7  0.270 0.286 0.252 0.279 0.261 0.207 0.273  0.260 0.205 0.218 0.271 0.256
Obj8  0.287 0.294 0.258 0.317 0.287 0.169 0.260  0.353 0.221 0.223 0.242 0.170
Obj9  0.229 0.232 0.209 0.225 0.226 0.161 0.205  0.221 0.186 0.183 0.118 0.140
Obj10 0.242 0.256 0.225 0.255 0.225 0.210 0.218  0.223 0.183 0.217 0.113 0.126
Marg1 0.187 0.215 0.178 0.219 0.193 0.097 0.271  0.242 0.118 0.113 0.619 0.455
Marg2 0.193 0.212 0.185 0.186 0.194 0.131 0.256  0.170 0.140 0.126 0.455 0.411
Marg3 0.182 0.201 0.173 0.191 0.182 0.121 0.228  0.187 0.129 0.126 0.393 0.329
Marg4 0.195 0.221 0.186 0.226 0.185 0.155 0.234  0.212 0.127 0.158 0.378 0.287
Marg5 0.213 0.233 0.201 0.225 0.213 0.140 0.257  0.225 0.153 0.151 0.424 0.350
Marg6 0.239 0.263 0.227 0.259 0.227 0.197 0.261  0.233 0.169 0.199 0.322 0.276
Marg7 0.140 0.159 0.134 0.154 0.136 0.105 0.180  0.145 0.093 0.104 0.320 0.257
Str1  0.188 0.177 0.171 0.136 0.206 0.082 0.191  0.148 0.177 0.103 0.187 0.258
Str2  0.184 0.190 0.174 0.160 0.178 0.155 0.184  0.122 0.151 0.147 0.124 0.189
Str3  0.128 0.119 0.120 0.072 0.137 0.075 0.136  0.053 0.126 0.071 0.105 0.207
Str4  0.135 0.126 0.122 0.097 0.144 0.070 0.126  0.097 0.128 0.082 0.075 0.145
Str5  0.134 0.136 0.128 0.097 0.132 0.111 0.145  0.061 0.116 0.097 0.115 0.193
Ang1  0.101 0.110 0.103 0.066 0.098 0.101 0.138  0.016 0.083 0.070 0.176 0.242
Ang2  0.087 0.123 0.102 0.085 0.052 0.193 0.143 -0.037 0.038 0.114 0.177 0.223
Ang3  0.125 0.146 0.126 0.120 0.111 0.141 0.165  0.063 0.084 0.109 0.231 0.243
      Marg3 Marg4 Marg5 Marg6 Marg7  Str1  Str2  Str3  Str4  Str5  Ang1   Ang2
Obj1  0.182 0.195 0.213 0.239 0.140 0.188 0.184 0.128 0.135 0.134 0.101  0.087
Obj2  0.201 0.221 0.233 0.263 0.159 0.177 0.190 0.119 0.126 0.136 0.110  0.123
Obj3  0.173 0.186 0.201 0.227 0.134 0.171 0.174 0.120 0.122 0.128 0.103  0.102
Obj4  0.191 0.226 0.225 0.259 0.154 0.136 0.160 0.072 0.097 0.097 0.066  0.085
Obj5  0.182 0.185 0.213 0.227 0.136 0.206 0.178 0.137 0.144 0.132 0.098  0.052
Obj6  0.121 0.155 0.140 0.197 0.105 0.082 0.155 0.075 0.070 0.111 0.101  0.193
Obj7  0.228 0.234 0.257 0.261 0.180 0.191 0.184 0.136 0.126 0.145 0.138  0.143
Obj8  0.187 0.212 0.225 0.233 0.145 0.148 0.122 0.053 0.097 0.061 0.016 -0.037
Obj9  0.129 0.127 0.153 0.169 0.093 0.177 0.151 0.126 0.128 0.116 0.083  0.038
Obj10 0.126 0.158 0.151 0.199 0.104 0.103 0.147 0.071 0.082 0.097 0.070  0.114
Marg1 0.393 0.378 0.424 0.322 0.320 0.187 0.124 0.105 0.075 0.115 0.176  0.177
Marg2 0.329 0.287 0.350 0.276 0.257 0.258 0.189 0.207 0.145 0.193 0.242  0.223
Marg3 0.277 0.260 0.300 0.247 0.220 0.185 0.143 0.132 0.100 0.133 0.165  0.161
Marg4 0.260 0.281 0.286 0.264 0.219 0.098 0.117 0.052 0.045 0.087 0.116  0.182
Marg5 0.300 0.286 0.326 0.274 0.238 0.201 0.156 0.137 0.110 0.140 0.168  0.160
Marg6 0.247 0.264 0.274 0.271 0.203 0.142 0.160 0.097 0.085 0.123 0.136  0.189
Marg7 0.220 0.219 0.238 0.203 0.181 0.113 0.103 0.077 0.056 0.091 0.122  0.151
Str1  0.185 0.098 0.201 0.142 0.113 0.363 0.206 0.300 0.241 0.221 0.209  0.025
Str2  0.143 0.117 0.156 0.160 0.103 0.206 0.179 0.186 0.146 0.170 0.167  0.146
Str3  0.132 0.052 0.137 0.097 0.077 0.300 0.186 0.276 0.205 0.210 0.212  0.087
Str4  0.100 0.045 0.110 0.085 0.056 0.241 0.146 0.205 0.167 0.152 0.136  0.018
Str5  0.133 0.087 0.140 0.123 0.091 0.221 0.170 0.210 0.152 0.178 0.187  0.145
Ang1  0.165 0.116 0.168 0.136 0.122 0.209 0.167 0.212 0.136 0.187 0.223  0.215
Ang2  0.161 0.182 0.160 0.189 0.151 0.025 0.146 0.087 0.018 0.145 0.215  0.431
Ang3  0.187 0.180 0.196 0.186 0.155 0.120 0.142 0.123 0.073 0.140 0.181  0.256
       Ang3
Obj1  0.125
Obj2  0.146
Obj3  0.126
Obj4  0.120
Obj5  0.111
Obj6  0.141
Obj7  0.165
Obj8  0.063
Obj9  0.084
Obj10 0.109
Marg1 0.231
Marg2 0.243
Marg3 0.187
Marg4 0.180
Marg5 0.196
Marg6 0.186
Marg7 0.155
Str1  0.120
Str2  0.142
Str3  0.123
Str4  0.073
Str5  0.140
Ang1  0.181
Ang2  0.256
Ang3  0.195
\end{verbatim}

We're not really interested in this matrix. We just need it to compare it to the \emph{GRMSmatrix} to produce the residuals. We do that next.

\textbf{Residuals} are the difference between the reproduced (i.e., those created from our factor loadings) and \(R\)-matrix produced by the raw data.

If we look at the \(r_{_{Obj1Obj2}}\) in our original correlation matrix (theoretically from the raw data {[}although we simulated data{]}), the value is 0.35. The reproduced correlation for this pair is 0.304. The difference is 0.046. The residuals table below shows 0.051 (rounding error).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{.}\DecValTok{35} \SpecialCharTok{{-}}\NormalTok{ .}\DecValTok{304}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.046
\end{verbatim}

By using the \emph{factor.residuals()} function we can calculate the residuals. Here we will see this difference calculated for us, for all the elements in the matrix.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(psych}\SpecialCharTok{::}\FunctionTok{factor.residuals}\NormalTok{(GRMSr, grmsPAF2}\SpecialCharTok{$}\NormalTok{loadings), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        Obj1   Obj2   Obj3   Obj4   Obj5   Obj6   Obj7   Obj8   Obj9  Obj10
Obj1   0.707  0.051 -0.020 -0.030 -0.002  0.031  0.011  0.059 -0.076 -0.001
Obj2   0.051  0.681  0.030 -0.069 -0.023 -0.005  0.026 -0.012  0.030 -0.011
Obj3  -0.020  0.030  0.750 -0.036  0.023  0.070 -0.051 -0.009  0.000 -0.010
Obj4  -0.030 -0.069 -0.036  0.674  0.099  0.003  0.006 -0.020  0.031  0.024
Obj5  -0.002 -0.023  0.023  0.099  0.720 -0.043 -0.084  0.004  0.026 -0.021
Obj6   0.031 -0.005  0.070  0.003 -0.043  0.775 -0.008 -0.028  0.046 -0.089
Obj7   0.011  0.026 -0.051  0.006 -0.084 -0.008  0.727  0.047 -0.012  0.065
Obj8   0.059 -0.012 -0.009 -0.020  0.004 -0.028  0.047  0.647 -0.028  0.007
Obj9  -0.076  0.030  0.000  0.031  0.026  0.046 -0.012 -0.028  0.814  0.016
Obj10 -0.001 -0.011 -0.010  0.024 -0.021 -0.089  0.065  0.007  0.016  0.783
Marg1 -0.002  0.001 -0.009  0.001 -0.027  0.005  0.025  0.025 -0.017 -0.019
Marg2  0.053 -0.005  0.045 -0.009  0.003  0.007 -0.042 -0.034 -0.023 -0.003
Marg3 -0.013  0.045  0.001 -0.054  0.044 -0.069 -0.030 -0.048  0.086  0.039
Marg4 -0.007 -0.041  0.056  0.030  0.013 -0.054  0.017  0.026 -0.060 -0.036
Marg5 -0.042 -0.014  0.011  0.043  0.039  0.021 -0.026 -0.038  0.036 -0.042
Marg6 -0.061  0.006 -0.067 -0.031 -0.002  0.066  0.024  0.027 -0.019  0.057
Marg7 -0.010  0.032  0.008  0.039 -0.079  0.069 -0.024 -0.004  0.011  0.005
Str1   0.027  0.008 -0.029 -0.074  0.025 -0.016  0.055  0.018  0.014 -0.002
Str2   0.004 -0.006  0.015  0.030 -0.061 -0.003 -0.049 -0.059  0.032  0.040
Str3  -0.030 -0.031 -0.029  0.008 -0.028  0.020  0.052 -0.007 -0.010  0.028
Str4  -0.041  0.015  0.061  0.054 -0.023  0.013 -0.056  0.031 -0.075 -0.060
Str5   0.064  0.009  0.024 -0.017  0.058  0.002  0.003 -0.024 -0.051 -0.006
Ang1  -0.038 -0.042 -0.030  0.026  0.019 -0.061  0.013  0.057  0.086 -0.008
Ang2  -0.028  0.022 -0.019 -0.025  0.035  0.003 -0.012  0.010 -0.042  0.024
Ang3   0.081 -0.019 -0.015  0.020 -0.006  0.015  0.061  0.002 -0.022 -0.025
       Marg1  Marg2  Marg3  Marg4  Marg5  Marg6  Marg7   Str1   Str2   Str3
Obj1  -0.002  0.053 -0.013 -0.007 -0.042 -0.061 -0.010  0.027  0.004 -0.030
Obj2   0.001 -0.005  0.045 -0.041 -0.014  0.006  0.032  0.008 -0.006 -0.031
Obj3  -0.009  0.045  0.001  0.056  0.011 -0.067  0.008 -0.029  0.015 -0.029
Obj4   0.001 -0.009 -0.054  0.030  0.043 -0.031  0.039 -0.074  0.030  0.008
Obj5  -0.027  0.003  0.044  0.013  0.039 -0.002 -0.079  0.025 -0.061 -0.028
Obj6   0.005  0.007 -0.069 -0.054  0.021  0.066  0.069 -0.016 -0.003  0.020
Obj7   0.025 -0.042 -0.030  0.017 -0.026  0.024 -0.024  0.055 -0.049  0.052
Obj8   0.025 -0.034 -0.048  0.026 -0.038  0.027 -0.004  0.018 -0.059 -0.007
Obj9  -0.017 -0.023  0.086 -0.060  0.036 -0.019  0.011  0.014  0.032 -0.010
Obj10 -0.019 -0.003  0.039 -0.036 -0.042  0.057  0.005 -0.002  0.040  0.028
Marg1  0.381 -0.030  0.013 -0.002 -0.017  0.026 -0.011  0.003 -0.004  0.026
Marg2 -0.030  0.589  0.019 -0.057  0.053 -0.009  0.069 -0.005 -0.007 -0.027
Marg3  0.013  0.019  0.723  0.063 -0.049  0.007 -0.016  0.018  0.030 -0.036
Marg4 -0.002 -0.057  0.063  0.719  0.013 -0.006 -0.055  0.001  0.094 -0.004
Marg5 -0.017  0.053 -0.049  0.013  0.674  0.016  0.044 -0.043 -0.023  0.020
Marg6  0.026 -0.009  0.007 -0.006  0.016  0.729 -0.005 -0.012  0.024  0.050
Marg7 -0.011  0.069 -0.016 -0.055  0.044 -0.005  0.819  0.027 -0.051 -0.036
Str1   0.003 -0.005  0.018  0.001 -0.043 -0.012  0.027  0.637  0.009  0.001
Str2  -0.004 -0.007  0.030  0.094 -0.023  0.024 -0.051  0.009  0.821  0.012
Str3   0.026 -0.027 -0.036 -0.004  0.020  0.050 -0.036  0.001  0.012  0.724
Str4   0.009 -0.027 -0.021  0.018  0.031  0.048 -0.039 -0.015  0.057  0.063
Str5  -0.015  0.035 -0.010 -0.054  0.037 -0.040  0.032  0.010 -0.048 -0.029
Ang1  -0.014 -0.010  0.016  0.002 -0.053 -0.031  0.049 -0.028 -0.007 -0.013
Ang2  -0.007 -0.038  0.031  0.041 -0.018  0.024 -0.018  0.025 -0.025 -0.012
Ang3   0.048  0.036 -0.077 -0.007  0.012 -0.069 -0.069 -0.018  0.013  0.024
        Str4   Str5   Ang1   Ang2   Ang3
Obj1  -0.041  0.064 -0.038 -0.028  0.081
Obj2   0.015  0.009 -0.042  0.022 -0.019
Obj3   0.061  0.024 -0.030 -0.019 -0.015
Obj4   0.054 -0.017  0.026 -0.025  0.020
Obj5  -0.023  0.058  0.019  0.035 -0.006
Obj6   0.013  0.002 -0.061  0.003  0.015
Obj7  -0.056  0.003  0.013 -0.012  0.061
Obj8   0.031 -0.024  0.057  0.010  0.002
Obj9  -0.075 -0.051  0.086 -0.042 -0.022
Obj10 -0.060 -0.006 -0.008  0.024 -0.025
Marg1  0.009 -0.015 -0.014 -0.007  0.048
Marg2 -0.027  0.035 -0.010 -0.038  0.036
Marg3 -0.021 -0.010  0.016  0.031 -0.077
Marg4  0.018 -0.054  0.002  0.041 -0.007
Marg5  0.031  0.037 -0.053 -0.018  0.012
Marg6  0.048 -0.040 -0.031  0.024 -0.069
Marg7 -0.039  0.032  0.049 -0.018 -0.069
Str1  -0.015  0.010 -0.028  0.025 -0.018
Str2   0.057 -0.048 -0.007 -0.025  0.013
Str3   0.063 -0.029 -0.013 -0.012  0.024
Str4   0.833 -0.034  0.017  0.013 -0.051
Str5  -0.034  0.822  0.036  0.009 -0.034
Ang1   0.017  0.036  0.777  0.025  0.051
Ang2   0.013  0.009  0.025  0.569 -0.006
Ang3  -0.051 -0.034  0.051 -0.006  0.805
\end{verbatim}

There are several strategies to evaluate this matrix.

\begin{itemize}
\tightlist
\item
  Compare the size of the residuals to the original correlations.

  \begin{itemize}
  \tightlist
  \item
    The worst possible model would occur if we extracted no factors, and the residuals are the size of the original correlations.
  \item
    If the correlations were small to start with, we expect small residuals.
  \item
    If the correlations were large to start with, the residuals will be relatively larger (this is not terribly problematic).
  \end{itemize}
\item
  Comparing residuals requires squaring them first (because residuals can be both positive and negative)

  \begin{itemize}
  \tightlist
  \item
    The sum of the squared residuals divided by the sum of the squared correlations is an estimate of model fit.

    \begin{itemize}
    \tightlist
    \item
      Subtracting this from 1.0 means that it ranges from 0 to 1.\\
    \item
      Values \textgreater{} .95 are an indication of good fit.
    \end{itemize}
  \end{itemize}
\end{itemize}

Analyzing the residuals means we need to extract only the upper right of the triangle them into an object. We can do this in steps.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grmsPAF2\_resids }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{factor.residuals}\NormalTok{(GRMSr, grmsPAF2}\SpecialCharTok{$}\NormalTok{loadings)  }\CommentTok{\#first extract the resids}
\NormalTok{grmsPAF2\_resids }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(grmsPAF2\_resids[}\FunctionTok{upper.tri}\NormalTok{(grmsPAF2\_resids)])  }\CommentTok{\#the object has the residuals in a single column}
\FunctionTok{head}\NormalTok{(grmsPAF2\_resids)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            [,1]
[1,]  0.05128890
[2,] -0.01969873
[3,]  0.03041703
[4,] -0.02971380
[5,] -0.06927144
[6,] -0.03556489
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# unhashtag for a view of all the residuals grmsPAF2\_resids}
\end{Highlighting}
\end{Shaded}

One criteria of residual analysis is to see how many residuals there are that are greater than an absolute value of 0.05. The result will be a single column with TRUE if it is \textgreater{} \textbar0.05\textbar{} and false if it is smaller. The sum function will tell us how many TRUE responses are in the matrix. Further, we can write script to obtain the proportion of total number of residuals.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{large.resid }\OtherTok{\textless{}{-}} \FunctionTok{abs}\NormalTok{(grmsPAF2\_resids) }\SpecialCharTok{\textgreater{}} \FloatTok{0.05}
\CommentTok{\# large.resid}
\FunctionTok{sum}\NormalTok{(large.resid)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 57
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{sum}\NormalTok{(large.resid)}\SpecialCharTok{/}\FunctionTok{nrow}\NormalTok{(grmsPAF2\_resids), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.19
\end{verbatim}

We learn that there are 57 residuals greater than the absolute value of 0.05. This represents 19\% of the total number of residuals.

There are no hard rules about what proportion of residuals can be greater than 0.05. Field recommends that it stay below 50\% \citep{field_discovering_2012}.

Another approach to analyzing residuals is to look at their mean. Because of the +/- valences, we need to square them (to eliminate the negative), take the average, then take the square root.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{mean}\NormalTok{(grmsPAF2\_resids}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.036
\end{verbatim}

While there are no clear guidelines to interpret these, one recommendation is to consider extracting more factors if the value is higher than 0.08 \citep{field_discovering_2012}.

Finally, we expect our residuals to be normally distributed. A histogram can help us inspect the distribution.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(grmsPAF2\_resids)}
\end{Highlighting}
\end{Shaded}

\includegraphics{09-EFA_PAF_files/figure-latex/unnamed-chunk-24-1.pdf}

Not bad! It looks reasonably normal. No outliers.

\hypertarget{quick-recap-of-how-to-evaluate-the-of-factors-we-extracted}{%
\subsubsection{Quick recap of how to evaluate the \# of factors we extracted}\label{quick-recap-of-how-to-evaluate-the-of-factors-we-extracted}}

\begin{itemize}
\tightlist
\item
  If fewer than 30 variables, the eigenvalue \textgreater{} 1 (Kaiser's) critera is fine, so long as communalities are all \textgreater{} .70.
\item
  If sample size \textgreater{} 250 and the average communalities are .6 or greater, this is acceptable
\item
  When \emph{N} \textgreater{} 200, the scree plot can be used.
\item
  Regarding residuals:

  \begin{itemize}
  \tightlist
  \item
    Fewer than 50\% should have absolute values \textgreater{} 0.05.
  \item
    Model fit should be \textgreater{} 0.90.
  \end{itemize}
\end{itemize}

\hypertarget{factor-rotation}{%
\subsection{Factor Rotation}\label{factor-rotation}}

Here's a snip of our location in the PAF workflow.

\begin{figure}
\centering
\includegraphics{images/PAF/PAF_rotation.png}
\caption{Image of our location in the PAF workflow}
\end{figure}

The original solution of a principal components or principal axis factor analysis is a set of vectors that best account for the observed covariance or correlation matrix. Each additional component or factor accounts for progressively less and less variance. The solution is efficient (yay) but difficult to interpret (boo).

Thanks to Thurstone's five rules toward a simple structure (circa 1947), interpretation of a matrix is facilitated by \emph{rotation} (multiplying a matrix by a matrix of orthogonal vectors that preserve the communalities of each variable). Both the original matrix and the solution will be orthogonal.

\emph{Parsimony} becomes a statistical consideration (an equation, in fact) and goal and is maximized when each variable has a 1.0 loading on one factor and the rest are zero.

Different rotation strategies emphasize different goals related to parsimony:

\emph{Quartimax} seeks to maximize the notion of variable parsimony (each variable is associated with one factor) and permits the rotation toward a general factor (ignoring smaller factors). \emph{Varimax} maximizes the variance of squared loadings taken over items instead of over factors and \emph{avoids} a general factor.

Rotation improves the interpretation of the factor by maximizing the loading on each variable on one of the extracted factors while minimizing the loading on all other factors. Rotation works by changing the absolute values of the variables while keeping their differential values constant.

There are two big choices (to be made on theoretical grounds):

\begin{itemize}
\tightlist
\item
  Orthogonal rotation if you think that the factors are independent/unrelated.

  \begin{itemize}
  \tightlist
  \item
    varimax is the most common orthogonal rotation
  \end{itemize}
\item
  Oblique rotation if you think that the factors are related/correlated.

  \begin{itemize}
  \tightlist
  \item
    oblimin and promax are common oblique rotations
  \end{itemize}
\end{itemize}

\hypertarget{orthogonal-rotation-1}{%
\subsubsection{Orthogonal rotation}\label{orthogonal-rotation-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# grmsPAF2ORTH \textless{}{-} psych::fa(GRMSr, nfactors = 4, fm = \textquotesingle{}pa\textquotesingle{}, rotate =}
\CommentTok{\# \textquotesingle{}varimax\textquotesingle{})}
\NormalTok{grmsPAF2ORTH }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{fa}\NormalTok{(dfGRMS, }\AttributeTok{nfactors =} \DecValTok{4}\NormalTok{, }\AttributeTok{fm =} \StringTok{"pa"}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"varimax"}\NormalTok{)}
\NormalTok{grmsPAF2ORTH}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  pa
Call: psych::fa(r = dfGRMS, nfactors = 4, rotate = "varimax", fm = "pa")
Standardized loadings (pattern matrix) based upon correlation matrix
       PA1  PA2   PA3   PA4   h2   u2 com
Obj1  0.49 0.14  0.16  0.04 0.29 0.71 1.4
Obj2  0.51 0.18  0.13  0.09 0.32 0.68 1.4
Obj3  0.45 0.14  0.15  0.07 0.25 0.75 1.5
Obj4  0.54 0.19  0.04  0.03 0.33 0.67 1.3
Obj5  0.47 0.15  0.19 -0.02 0.28 0.72 1.6
Obj6  0.39 0.05  0.06  0.26 0.22 0.78 1.8
Obj7  0.41 0.27  0.16  0.10 0.27 0.73 2.2
Obj8  0.52 0.23  0.03 -0.18 0.35 0.65 1.7
Obj9  0.38 0.07  0.19 -0.01 0.19 0.81 1.5
Obj10 0.44 0.06  0.06  0.12 0.22 0.78 1.2
Marg1 0.13 0.77  0.08  0.02 0.62 0.38 1.1
Marg2 0.13 0.53  0.30  0.15 0.41 0.59 1.9
Marg3 0.18 0.46  0.16  0.08 0.28 0.72 1.6
Marg4 0.26 0.45 -0.01  0.13 0.28 0.72 1.8
Marg5 0.23 0.49  0.16  0.07 0.33 0.67 1.7
Marg6 0.35 0.35  0.08  0.16 0.27 0.73 2.5
Marg7 0.14 0.38  0.07  0.10 0.18 0.82 1.5
Str1  0.16 0.16  0.55 -0.08 0.36 0.64 1.4
Str2  0.24 0.09  0.30  0.16 0.18 0.82 2.7
Str3  0.07 0.07  0.51  0.06 0.28 0.72 1.1
Str4  0.14 0.04  0.38 -0.03 0.17 0.83 1.3
Str5  0.11 0.09  0.36  0.16 0.18 0.82 1.8
Ang1  0.02 0.18  0.35  0.25 0.22 0.78 2.4
Ang2  0.05 0.20  0.06  0.62 0.43 0.57 1.2
Ang3  0.10 0.26  0.15  0.31 0.19 0.81 2.7

                       PA1  PA2  PA3  PA4
SS loadings           2.60 2.26 1.41 0.83
Proportion Var        0.10 0.09 0.06 0.03
Cumulative Var        0.10 0.19 0.25 0.28
Proportion Explained  0.37 0.32 0.20 0.12
Cumulative Proportion 0.37 0.68 0.88 1.00

Mean item complexity =  1.7
Test of the hypothesis that 4 factors are sufficient.

df null model =  300  with the objective function =  4.89 with Chi Square =  1217.51
df of  the model are 206  and the objective function was  0.77 

The root mean square of the residuals (RMSR) is  0.04 
The df corrected root mean square of the residuals is  0.04 

The harmonic n.obs is  259 with the empirical chi square  202.41  with prob <  0.56 
The total n.obs was  259  with Likelihood Chi Square =  189.19  with prob <  0.79 

Tucker Lewis Index of factoring reliability =  1.027
RMSEA index =  0  and the 90 % confidence intervals are  0 0.018
BIC =  -955.52
Fit based upon off diagonal values = 0.97
Measures of factor score adequacy             
                                                   PA1  PA2  PA3  PA4
Correlation of (regression) scores with factors   0.84 0.85 0.76 0.72
Multiple R square of scores with factors          0.71 0.72 0.58 0.52
Minimum correlation of possible factor scores     0.42 0.45 0.17 0.03
\end{verbatim}

Essentially, we have the same information as before, except that loadings are calculated after rotation (which adjusts the absolute values of the factor loadings while keeping their differential vales constant). Our communality and uniqueness values remain the same. The eigenvalues (SS loadings) should even out, but the proportion of variance explained and cumulative variance (28\%) will remain the same.

The \emph{print.psych()} function facilitates interpretation and prioritizes the information about which we care most:

\begin{itemize}
\tightlist
\item
  \emph{cut} displays loadings above .3, this allows us to see

  \begin{itemize}
  \tightlist
  \item
    if some items load on no factors
  \item
    if some items have cross-loadings (and their relative weights)
  \end{itemize}
\item
  \emph{sort} reorders the loadings to make it clearer (considering ties, to the best of its ability) to which factor/scale it belongs
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grmsPAF2\_table }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{print.psych}\NormalTok{(grmsPAF2ORTH, }\AttributeTok{cut =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  pa
Call: psych::fa(r = dfGRMS, nfactors = 4, rotate = "varimax", fm = "pa")
Standardized loadings (pattern matrix) based upon correlation matrix
      item  PA1  PA2   PA3   PA4   h2   u2 com
Obj4     4 0.54                  0.33 0.67 1.3
Obj8     8 0.52                  0.35 0.65 1.7
Obj2     2 0.51                  0.32 0.68 1.4
Obj1     1 0.49                  0.29 0.71 1.4
Obj5     5 0.47                  0.28 0.72 1.6
Obj3     3 0.45                  0.25 0.75 1.5
Obj10   10 0.44                  0.22 0.78 1.2
Obj7     7 0.41                  0.27 0.73 2.2
Obj6     6 0.39                  0.22 0.78 1.8
Obj9     9 0.38                  0.19 0.81 1.5
Marg1   11      0.77             0.62 0.38 1.1
Marg2   12      0.53             0.41 0.59 1.9
Marg5   15      0.49             0.33 0.67 1.7
Marg3   13      0.46             0.28 0.72 1.6
Marg4   14      0.45             0.28 0.72 1.8
Marg7   17      0.38             0.18 0.82 1.5
Marg6   16 0.35 0.35             0.27 0.73 2.5
Str1    18            0.55       0.36 0.64 1.4
Str3    20            0.51       0.28 0.72 1.1
Str4    21            0.38       0.17 0.83 1.3
Str5    22            0.36       0.18 0.82 1.8
Ang1    23            0.35       0.22 0.78 2.4
Str2    19            0.30       0.18 0.82 2.7
Ang2    24                  0.62 0.43 0.57 1.2
Ang3    25                  0.31 0.19 0.81 2.7

                       PA1  PA2  PA3  PA4
SS loadings           2.60 2.26 1.41 0.83
Proportion Var        0.10 0.09 0.06 0.03
Cumulative Var        0.10 0.19 0.25 0.28
Proportion Explained  0.37 0.32 0.20 0.12
Cumulative Proportion 0.37 0.68 0.88 1.00

Mean item complexity =  1.7
Test of the hypothesis that 4 factors are sufficient.

df null model =  300  with the objective function =  4.89 with Chi Square =  1217.51
df of  the model are 206  and the objective function was  0.77 

The root mean square of the residuals (RMSR) is  0.04 
The df corrected root mean square of the residuals is  0.04 

The harmonic n.obs is  259 with the empirical chi square  202.41  with prob <  0.56 
The total n.obs was  259  with Likelihood Chi Square =  189.19  with prob <  0.79 

Tucker Lewis Index of factoring reliability =  1.027
RMSEA index =  0  and the 90 % confidence intervals are  0 0.018
BIC =  -955.52
Fit based upon off diagonal values = 0.97
Measures of factor score adequacy             
                                                   PA1  PA2  PA3  PA4
Correlation of (regression) scores with factors   0.84 0.85 0.76 0.72
Multiple R square of scores with factors          0.71 0.72 0.58 0.52
Minimum correlation of possible factor scores     0.42 0.45 0.17 0.03
\end{verbatim}

In the unrotated solution, most variables loaded on the first factor. After rotation, there are four clear factors/scales. Further, there is clear (or at least reasonable) factor/scale membership for each item and few cross-loadings. As with the PAC in the previous lesson, Ang1 is not clearly loading on the Angry scale.

If this were a new scale and we had not yet established ideas for subscales, the next step would be to look back at the items, themselves, and try to name the scales/factors. If our scale construction included a priori/planned subscales, we would hope the items would where they were hypothesized to do so. As we noted with the Ang1 item, our simulated data nearly replicated the item membership onto the four scales that Lewis and Neville \citep{lewis_construction_2015} reported in the article.

\begin{itemize}
\tightlist
\item
  Assumptions of Beauty and Sexual Objectification
\item
  Silenced and Marginalized
\item
  Strong Woman Stereotype
\item
  Angry Woman Stereotype
\end{itemize}

We can also create a figure of the result. Note the direction of the arrows from the factor (latent variable) to the items in PAF -- in PCA the arrows went from item to component.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{fa.diagram}\NormalTok{(grmsPAF2ORTH)}
\end{Highlighting}
\end{Shaded}

\includegraphics{09-EFA_PAF_files/figure-latex/unnamed-chunk-27-1.pdf}

We can extract the factor loadings and write them to a table. This can be useful in preparing an APA style table for a manuscript or presentation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# names(grmsPAF2ORTH)}
\NormalTok{pafORTH\_table }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(grmsPAF2ORTH}\SpecialCharTok{$}\NormalTok{loadings, }\DecValTok{3}\NormalTok{)}
\FunctionTok{write.table}\NormalTok{(pafORTH\_table, }\AttributeTok{file =} \StringTok{"pafORTH\_table.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{col.names =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{pafORTH\_table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Loadings:
      PA1    PA2    PA3    PA4   
Obj1   0.495  0.144  0.160       
Obj2   0.513  0.179  0.131       
Obj3   0.452  0.140  0.147       
Obj4   0.536  0.191              
Obj5   0.469  0.154  0.189       
Obj6   0.391                0.257
Obj7   0.408  0.265  0.164       
Obj8   0.517  0.232        -0.177
Obj9   0.382         0.187       
Obj10  0.442                0.122
Marg1  0.126  0.772              
Marg2  0.126  0.534  0.296  0.151
Marg3  0.175  0.462  0.161       
Marg4  0.256  0.446         0.130
Marg5  0.230  0.493  0.162       
Marg6  0.345  0.348         0.157
Marg7  0.142  0.381         0.104
Str1   0.160  0.161  0.553       
Str2   0.236         0.301  0.160
Str3                 0.512       
Str4   0.141         0.381       
Str5   0.115         0.362  0.161
Ang1          0.182  0.354  0.254
Ang2          0.198         0.621
Ang3   0.104  0.258  0.154  0.307

                 PA1   PA2   PA3   PA4
SS loadings    2.600 2.257 1.414 0.830
Proportion Var 0.104 0.090 0.057 0.033
Cumulative Var 0.104 0.194 0.251 0.284
\end{verbatim}

\hypertarget{oblique-rotation-1}{%
\subsubsection{Oblique rotation}\label{oblique-rotation-1}}

Whereas the orthogonal rotation sought to maximize the independence/unrelatedness of the factors, an oblique rotation will allow them to be correlated. Researchers often explore both solutions but only report one.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# grmsPAF2obl \textless{}{-} psych::fa(GRMSr, nfactors = 4, fm = \textquotesingle{}pa\textquotesingle{}, rotate =}
\CommentTok{\# \textquotesingle{}oblimin\textquotesingle{})}
\NormalTok{grmsPAF2obl }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{fa}\NormalTok{(dfGRMS, }\AttributeTok{nfactors =} \DecValTok{4}\NormalTok{, }\AttributeTok{fm =} \StringTok{"pa"}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"oblimin"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required namespace: GPArotation
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grmsPAF2obl}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  pa
Call: psych::fa(r = dfGRMS, nfactors = 4, rotate = "oblimin", fm = "pa")
Standardized loadings (pattern matrix) based upon correlation matrix
        PA1   PA2   PA3   PA4   h2   u2 com
Obj1   0.51  0.00  0.08  0.00 0.29 0.71 1.1
Obj2   0.53  0.02  0.04  0.05 0.32 0.68 1.0
Obj3   0.46  0.00  0.07  0.04 0.25 0.75 1.1
Obj4   0.56  0.05 -0.05  0.00 0.33 0.67 1.0
Obj5   0.47  0.02  0.12 -0.06 0.28 0.72 1.2
Obj6   0.43 -0.11 -0.02  0.25 0.22 0.78 1.8
Obj7   0.38  0.14  0.08  0.06 0.27 0.73 1.4
Obj8   0.53  0.15 -0.05 -0.22 0.35 0.65 1.5
Obj9   0.39 -0.05  0.14 -0.04 0.19 0.81 1.3
Obj10  0.48 -0.08 -0.02  0.10 0.22 0.78 1.2
Marg1 -0.03  0.81 -0.03 -0.02 0.62 0.38 1.0
Marg2 -0.01  0.49  0.22  0.12 0.41 0.59 1.5
Marg3  0.08  0.43  0.09  0.05 0.28 0.72 1.2
Marg4  0.20  0.40 -0.11  0.11 0.28 0.72 1.8
Marg5  0.14  0.45  0.08  0.03 0.33 0.67 1.3
Marg6  0.31  0.25 -0.02  0.13 0.27 0.73 2.3
Marg7  0.07  0.36  0.00  0.08 0.18 0.82 1.2
Str1   0.07  0.08  0.56 -0.12 0.36 0.64 1.2
Str2   0.21 -0.04  0.27  0.14 0.18 0.82 2.5
Str3  -0.01 -0.02  0.53  0.04 0.28 0.72 1.0
Str4   0.10 -0.04  0.39 -0.06 0.17 0.83 1.2
Str5   0.06 -0.01  0.35  0.14 0.18 0.82 1.4
Ang1  -0.07  0.10  0.33  0.24 0.22 0.78 2.1
Ang2   0.01  0.07 -0.02  0.64 0.43 0.57 1.0
Ang3   0.04  0.18  0.09  0.30 0.19 0.81 1.9

                       PA1  PA2  PA3  PA4
SS loadings           2.79 2.03 1.37 0.91
Proportion Var        0.11 0.08 0.05 0.04
Cumulative Var        0.11 0.19 0.25 0.28
Proportion Explained  0.39 0.29 0.19 0.13
Cumulative Proportion 0.39 0.68 0.87 1.00

 With factor correlations of 
     PA1  PA2  PA3  PA4
PA1 1.00 0.46 0.34 0.17
PA2 0.46 1.00 0.32 0.27
PA3 0.34 0.32 1.00 0.19
PA4 0.17 0.27 0.19 1.00

Mean item complexity =  1.4
Test of the hypothesis that 4 factors are sufficient.

df null model =  300  with the objective function =  4.89 with Chi Square =  1217.51
df of  the model are 206  and the objective function was  0.77 

The root mean square of the residuals (RMSR) is  0.04 
The df corrected root mean square of the residuals is  0.04 

The harmonic n.obs is  259 with the empirical chi square  202.41  with prob <  0.56 
The total n.obs was  259  with Likelihood Chi Square =  189.19  with prob <  0.79 

Tucker Lewis Index of factoring reliability =  1.027
RMSEA index =  0  and the 90 % confidence intervals are  0 0.018
BIC =  -955.52
Fit based upon off diagonal values = 0.97
Measures of factor score adequacy             
                                                   PA1  PA2  PA3  PA4
Correlation of (regression) scores with factors   0.89 0.89 0.80 0.75
Multiple R square of scores with factors          0.79 0.79 0.65 0.57
Minimum correlation of possible factor scores     0.59 0.58 0.30 0.14
\end{verbatim}

We can make it a little easier to interpret by removing all factor loadings below .30.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{print.psych}\NormalTok{(grmsPAF2obl, }\AttributeTok{cut =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  pa
Call: psych::fa(r = dfGRMS, nfactors = 4, rotate = "oblimin", fm = "pa")
Standardized loadings (pattern matrix) based upon correlation matrix
      item   PA1   PA2   PA3   PA4   h2   u2 com
Obj4     4  0.56                   0.33 0.67 1.0
Obj8     8  0.53                   0.35 0.65 1.5
Obj2     2  0.53                   0.32 0.68 1.0
Obj1     1  0.51                   0.29 0.71 1.1
Obj10   10  0.48                   0.22 0.78 1.2
Obj5     5  0.47                   0.28 0.72 1.2
Obj3     3  0.46                   0.25 0.75 1.1
Obj6     6  0.43                   0.22 0.78 1.8
Obj9     9  0.39                   0.19 0.81 1.3
Obj7     7  0.38                   0.27 0.73 1.4
Marg6   16  0.31                   0.27 0.73 2.3
Marg1   11        0.81             0.62 0.38 1.0
Marg2   12        0.49             0.41 0.59 1.5
Marg5   15        0.45             0.33 0.67 1.3
Marg3   13        0.43             0.28 0.72 1.2
Marg4   14        0.40             0.28 0.72 1.8
Marg7   17        0.36             0.18 0.82 1.2
Str1    18              0.56       0.36 0.64 1.2
Str3    20              0.53       0.28 0.72 1.0
Str4    21              0.39       0.17 0.83 1.2
Str5    22              0.35       0.18 0.82 1.4
Ang1    23              0.33       0.22 0.78 2.1
Str2    19                         0.18 0.82 2.5
Ang2    24                    0.64 0.43 0.57 1.0
Ang3    25                         0.19 0.81 1.9

                       PA1  PA2  PA3  PA4
SS loadings           2.79 2.03 1.37 0.91
Proportion Var        0.11 0.08 0.05 0.04
Cumulative Var        0.11 0.19 0.25 0.28
Proportion Explained  0.39 0.29 0.19 0.13
Cumulative Proportion 0.39 0.68 0.87 1.00

 With factor correlations of 
     PA1  PA2  PA3  PA4
PA1 1.00 0.46 0.34 0.17
PA2 0.46 1.00 0.32 0.27
PA3 0.34 0.32 1.00 0.19
PA4 0.17 0.27 0.19 1.00

Mean item complexity =  1.4
Test of the hypothesis that 4 factors are sufficient.

df null model =  300  with the objective function =  4.89 with Chi Square =  1217.51
df of  the model are 206  and the objective function was  0.77 

The root mean square of the residuals (RMSR) is  0.04 
The df corrected root mean square of the residuals is  0.04 

The harmonic n.obs is  259 with the empirical chi square  202.41  with prob <  0.56 
The total n.obs was  259  with Likelihood Chi Square =  189.19  with prob <  0.79 

Tucker Lewis Index of factoring reliability =  1.027
RMSEA index =  0  and the 90 % confidence intervals are  0 0.018
BIC =  -955.52
Fit based upon off diagonal values = 0.97
Measures of factor score adequacy             
                                                   PA1  PA2  PA3  PA4
Correlation of (regression) scores with factors   0.89 0.89 0.80 0.75
Multiple R square of scores with factors          0.79 0.79 0.65 0.57
Minimum correlation of possible factor scores     0.59 0.58 0.30 0.14
\end{verbatim}

In this rotation, the Angry scale falls apart. As before, Ang1 is loading onto Strong. Additionally, Ang3 has factor loadings that fall below .30, and therefore do not appear in this table. Additionally, because our specification included ``sort=TRUE'', the relative weights wiggled around and so the items are listed in a different order than in the orthogonal rotation.

The oblique rotation allows us to see the correlation between the factors/scales. This was not available in the orthogonal rotation because the assumption of the orthogonal/varimax rotation is that the scales/factors are uncorrelated; hence in the analysis they were fixed to 0.0. The correlations from our simulated data range from .19 to .46.

Of course there is always a little complexity. In oblique rotations, there is a distinction between the \emph{pattern} matrix (which reports factor loadings and is comparable to the matrix we interpreted for the orthogonal rotation) and the \emph{structure} matrix (takes into account the relationship between the factors/scales -- it is a product of the pattern matrix and the matrix containing the correlation coefficients between the factors/scales). Most interpret the pattern matrix because it is simpler; however, it could be that values in the pattern matrix are suppressed because of relations between the factors. Therefore, the structure matrix can be a useful check and some editors will request it.

Obtaining the structure matrix requires two steps. First, multiply the factor loadings with the phi matrix.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grmsPAF2obl}\SpecialCharTok{$}\NormalTok{loadings }\SpecialCharTok{\%*\%}\NormalTok{ grmsPAF2obl}\SpecialCharTok{$}\NormalTok{Phi}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            PA1       PA2       PA3         PA4
Obj1  0.5351558 0.2587471 0.2578729  0.10672133
Obj2  0.5596959 0.2939415 0.2397101  0.15920737
Obj3  0.4931574 0.2456837 0.2386994  0.13178696
Obj4  0.5676937 0.2946510 0.1536862  0.09619880
Obj5  0.5143469 0.2639663 0.2797209  0.05320390
Obj6  0.4083011 0.1433380 0.1363673  0.28476763
Obj7  0.4891359 0.3627793 0.2697523  0.18376868
Obj8  0.5462750 0.3181931 0.1337799 -0.09863944
Obj9  0.4098436 0.1673842 0.2524221  0.04306615
Obj10 0.4526040 0.1614974 0.1403757  0.15874466
Marg1 0.3385023 0.7854134 0.2197285  0.18936979
Marg2 0.3164379 0.5904981 0.3994360  0.29125182
Marg3 0.3190254 0.5084002 0.2613340  0.19797897
Marg4 0.3686089 0.4872840 0.1082789  0.22714662
Marg5 0.3780702 0.5477661 0.2747713  0.19177240
Marg6 0.4417130 0.4228182 0.1911285  0.24703507
Marg7 0.2525475 0.4115888 0.1520707  0.19123240
Str1  0.2752766 0.2583504 0.5843472  0.02598261
Str2  0.3031342 0.1787399 0.3515298  0.21686734
Str3  0.1645811 0.1543190 0.5235449  0.13423477
Str4  0.1999438 0.1116093 0.3967457  0.02531912
Str5  0.1976930 0.1656974 0.3929920  0.21880162
Ang1  0.1344340 0.2417716 0.3896315  0.32188168
Ang2  0.1467619 0.2423578 0.1284210  0.65279494
Ang3  0.2088530 0.3073000 0.2225652  0.37115411
\end{verbatim}

Next, use Field's \citeyearpar{field_discovering_2012} function to produce the matrix.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Field\textquotesingle{}s function to produce the structure matrix}
\NormalTok{factor.structure }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(fa, }\AttributeTok{cut =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{decimals =} \DecValTok{2}\NormalTok{) \{}
\NormalTok{    structure.matrix }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{fa.sort}\NormalTok{(fa}\SpecialCharTok{$}\NormalTok{loadings }\SpecialCharTok{\%*\%}\NormalTok{ fa}\SpecialCharTok{$}\NormalTok{Phi)}
\NormalTok{    structure.matrix }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(}\FunctionTok{abs}\NormalTok{(structure.matrix) }\SpecialCharTok{\textless{}}\NormalTok{ cut,}
        \StringTok{""}\NormalTok{, }\FunctionTok{round}\NormalTok{(structure.matrix, decimals)))}
    \FunctionTok{return}\NormalTok{(structure.matrix)}
\NormalTok{\}}

\FunctionTok{factor.structure}\NormalTok{(grmsPAF2obl, }\AttributeTok{cut =} \FloatTok{0.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       PA1  PA2  PA3  PA4
Obj4  0.57               
Obj2  0.56               
Obj8  0.55 0.32          
Obj1  0.54               
Obj5  0.51               
Obj3  0.49               
Obj7  0.49 0.36          
Obj10 0.45               
Marg6 0.44 0.42          
Obj9  0.41               
Obj6  0.41               
Marg1 0.34 0.79          
Marg2 0.32 0.59  0.4     
Marg5 0.38 0.55          
Marg3 0.32 0.51          
Marg4 0.37 0.49          
Marg7      0.41          
Str1            0.58     
Str3            0.52     
Str4             0.4     
Str5            0.39     
Ang1            0.39 0.32
Str2   0.3      0.35     
Ang2                 0.65
Ang3       0.31      0.37
\end{verbatim}

Here we see some instability. Marg6 had cross-loadings with two scales and ``hopped'' membership onto Objectification. All three Ang items are showing factor loadings on their own scale. However, Ang1 is still loading on Strong and Ang3 has some cross-loading.

\hypertarget{factor-scores}{%
\subsection{Factor Scores}\label{factor-scores}}

Factor \emph{scores} (PA scores) can be created for each case (row) on each factor (column). These can be used to assess the relative standing of one person on the construct/variable to another. We can also use them in regression (in place of means or sums) when groups of predictors correlate so highly that there is multicollinearity.

Computation involves multiplying an individual's item-level response by the factor loadings we obtained through the PAF process. The results will be one score per factor for each row/case.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# in all of this, don\textquotesingle{}t forget to be specifying the datset that has}
\CommentTok{\# the reverse{-}coded item replaced}
\NormalTok{grmsPAF2obl }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{fa}\NormalTok{(dfGRMS, }\AttributeTok{nfactors =} \DecValTok{4}\NormalTok{, }\AttributeTok{fm =} \StringTok{"pa"}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"oblimin"}\NormalTok{,}
    \AttributeTok{scores =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{head}\NormalTok{(grmsPAF2obl}\SpecialCharTok{$}\NormalTok{scores, }\DecValTok{10}\NormalTok{)  }\CommentTok{\#shows us only the first 10 (of N = 2571)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             PA1        PA2         PA3        PA4
 [1,] -0.8113951 -1.2471438  0.12524275 -0.5260185
 [2,]  0.2984396 -0.7314827  0.50376631  0.2511270
 [3,]  0.3834222  0.4035418  0.18143350  1.0737292
 [4,] -0.9982416 -1.1583980 -0.01836819 -1.4226140
 [5,] -0.1985534  0.4507445 -0.78705628  0.1813746
 [6,] -0.4233586  0.3917061 -0.17730679  0.7556499
 [7,]  0.2528621  0.4465398 -0.78836577 -0.4751612
 [8,] -1.3823984 -0.5908492  0.36410712  0.5085523
 [9,] -0.5534479 -1.2460939 -0.65935346  0.4168911
[10,]  0.1212895 -0.4771782 -0.60054957  0.1698606
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dfGRMS }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(dfGRMS, grmsPAF2obl}\SpecialCharTok{$}\NormalTok{scores)  }\CommentTok{\#adds them to our raw dataset}
\end{Highlighting}
\end{Shaded}

To bring this full circle, we can see the correlation of the factor scores; the pattern maps onto what we saw previously in the correlations between factors in the oblique rotation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{corr.test}\NormalTok{(dfGRMS[}\FunctionTok{c}\NormalTok{(}\StringTok{"PA1"}\NormalTok{, }\StringTok{"PA2"}\NormalTok{, }\StringTok{"PA3"}\NormalTok{, }\StringTok{"PA4"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:psych::corr.test(x = dfGRMS[c("PA1", "PA2", "PA3", "PA4")])
Correlation matrix 
     PA1  PA2  PA3  PA4
PA1 1.00 0.58 0.48 0.28
PA2 0.58 1.00 0.45 0.41
PA3 0.48 0.45 1.00 0.33
PA4 0.28 0.41 0.33 1.00
Sample Size 
[1] 259
Probability values (Entries above the diagonal are adjusted for multiple tests.) 
    PA1 PA2 PA3 PA4
PA1   0   0   0   0
PA2   0   0   0   0
PA3   0   0   0   0
PA4   0   0   0   0

 To see confidence intervals of the correlations, print with the short=FALSE option
\end{verbatim}

We can extract the factor loadings and write them to a table. This can be useful in preparing an APA style table for a manuscript or presentation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# names(grmsPAF2obl)}
\NormalTok{pafOBL\_table }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(grmsPAF2obl}\SpecialCharTok{$}\NormalTok{loadings, }\DecValTok{3}\NormalTok{)}
\FunctionTok{write.table}\NormalTok{(pafOBL\_table, }\AttributeTok{file =} \StringTok{"pafOBL\_table.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{col.names =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{pafOBL\_table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Loadings:
      PA1    PA2    PA3    PA4   
Obj1   0.508                     
Obj2   0.526                     
Obj3   0.463                     
Obj4   0.563                     
Obj5   0.473         0.123       
Obj6   0.426 -0.114         0.246
Obj7   0.385  0.141              
Obj8   0.535  0.146        -0.220
Obj9   0.391         0.142       
Obj10  0.479                0.102
Marg1         0.811              
Marg2         0.492  0.223  0.117
Marg3         0.429              
Marg4  0.202  0.400 -0.108  0.106
Marg5  0.136  0.451              
Marg6  0.310  0.250         0.130
Marg7         0.356              
Str1                 0.558 -0.116
Str2   0.206         0.266  0.140
Str3                 0.526       
Str4                 0.388       
Str5                 0.348  0.144
Ang1          0.103  0.334  0.241
Ang2                        0.635
Ang3          0.178         0.298

                 PA1   PA2   PA3   PA4
SS loadings    2.523 1.762 1.190 0.794
Proportion Var 0.101 0.070 0.048 0.032
Cumulative Var 0.101 0.171 0.219 0.251
\end{verbatim}

We can also obtain a figure of this PAF with oblique rotation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{fa.diagram}\NormalTok{(grmsPAF2obl)}
\end{Highlighting}
\end{Shaded}

\includegraphics{09-EFA_PAF_files/figure-latex/unnamed-chunk-36-1.pdf}

\hypertarget{apa-style-results-1}{%
\section{APA Style Results}\label{apa-style-results-1}}

\textbf{Results}

\begin{quote}
The dimensionality of the 25 items from the Gendered Racial Microaggressions Scale for Black Women was analyzed using principal axis factoring. First, data screening were conducted to determine the suitability of the data for principal axis factoring. The Kaiser-Meyer-Olkin measure of sampling adequacy (KMO; Kaiser, 1970) represents the ratio of the squared correlation between variables to the squared partial correlation between variables. KMO ranges from 0.00 to 1.00; values closer to 1.00 indicate that the patterns of correlations are relatively compact, and that factor analysis should yield distinct and reliable factors (Field, 2012). In our dataset, the KMO value was .85, indicating acceptable sampling adequacy. The Barlett's Test of Sphericity examines whether the population correlation matrix resembles an identity matrix (Field, 2012). When the \emph{p} value for the Bartlett's test is \textless{} .05, we are fairly certain we have clusters of correlated variables. In our dataset, \(\chi^{2}(300)=1217.508, p < .001\), indicating the correlations between items are sufficiently large enough for principal axis factoring. The determinant of the correlation matrix alerts us to any issues of multicollinearity or singularity and should be larger than 0.00001. Our determinant was 0.0075, supporting the suitability of our data for analysis.
\end{quote}

\begin{quote}
Four criteria were used to determine the number of factors to rotate: a priori theory, the scree test, the Eigenvalue-greater-than-one criteria, and the interpretability of the solution. Kaiser's eigenvalue-greater-than-one criteria suggested two factors, and, in combination explained 28\% of the variance. The scree plot showed an inflexion that justified retaining between one and four factors. A priori theory based on Lewis and Neville's \citeyearpar{lewis_construction_2015} psychometric evaluation, suggested four factors. Based on the convergence of these decisions, four factors were extracted. We investigated each with orthogonal (varimax) and oblique (oblimin) procedures. Given the correspondence of the orthogonal solution with the original research, we selected this as our final model.
\end{quote}

\begin{quote}
The rotated solution, as shown in Table 1 and Figure 1, yielded four interpretable factors, each listed with the proportion of variance accounted for: assumptions of beauty and sexual objectification (10\%), silenced and marginalized (9\%), strong woman stereotype (6\%), and angry woman stereotype (3\%).
\end{quote}

Regarding the Table 1, I would include a table with all the values, bolding those with factor membership. This will be easy because we exported all those values to a .csv file.

\hypertarget{comparing-fa-and-pca}{%
\subsection{Comparing FA and PCA}\label{comparing-fa-and-pca}}

\begin{itemize}
\tightlist
\item
  FA derives a mathematical solution from which factors are estimated.

  \begin{itemize}
  \tightlist
  \item
    Only FA can estimate underlying factors, but it relies on the various assumptions to be met.
  \end{itemize}
\item
  PCA decomposes the original data into a set of linear variates.

  \begin{itemize}
  \tightlist
  \item
    This limits its concern to establishing which linear components exist within the data and how a particular variable might contribute to that component.
  \end{itemize}
\item
  Generally, FA and PCA result in similar solutions.

  \begin{itemize}
  \tightlist
  \item
    When there are 30 or more variables and communalities are \textgreater{} .7 for all variables, different solutions are unlikely (Stevens, 2002).
  \item
    When there are \textless{} 20 variables and low communalities (\textless{} .4) different solutions are likely to emerge.
  \item
    Both are inferential statistics.
  \end{itemize}
\item
  Critics of PCA suggest

  \begin{itemize}
  \tightlist
  \item
    ``At best it is a common factor analysis with some error added and at worst an unrecognizable hodgepodge of things from which nothing can be determined'' (Cliff, 1987, p.~349).
  \item
    PCA should never be described as FA and the resulting components should not be treated as reverently as true, latent variable, \emph{factors}.
  \item
    To most of us (i.e., scientist-practitioners), the difference is largely from the algorithm used to derive the solutions. This is true for Field \citep{field_discovering_2012} also, who uses the terms interchangeably. My take: use whichever you like, just be precise in the language describing what you did.
  \end{itemize}
\end{itemize}

\hypertarget{going-back-to-the-future-what-then-is-omega}{%
\section{Going Back to the Future: What, then, is Omega?}\label{going-back-to-the-future-what-then-is-omega}}

Now that we've had an introduction to factor analysis, let's revisit the \(\omega\) grouping of reliability estimates. In the context of \emph{psychometrics}, it may be useful to think of factors as scales/subscales where \emph{g} refers to the amount of variance in the \emph{general} factor (or total scale score) and subcales to be items that have something in common that is separate from what is \emph{g}.

Model-based estimates examine the correlations or covariances of the items and decompose the test variance into that which is:

\begin{itemize}
\tightlist
\item
  common to all items (\textbf{g}, a general factor),
\item
  specific to some items (\textbf{f}, orthogonal group factors), and
\item
  unique to each item (confounding \textbf{s} specific, and \textbf{e} error variance)
\end{itemize}

In the \emph{psych} package

\begin{itemize}
\tightlist
\item
  \(\omega_{t}\) represents the total reliability of the test (\(\omega_{t}\)).

  \begin{itemize}
  \tightlist
  \item
    In the \emph{psych} package, this is calculated from a bifactor model where there is one general \emph{g} factor (i.e., each item loads on the single general factor), one or more group factors (\emph{f}), and an item-specific factor (\emph{s}).
  \end{itemize}
\item
  \(\omega_{h}\) extracts a higher order factor from the correlation matrix of lower-level factors, then applies the Schmid and Leiman (1957) transformation to find the general loadings on the original items. Stated another way, it is a measure o f the general factor saturation (\emph{g}; the amount of variance attributable to one common factor). The subscript ``h'' acknowledges the hierarchical nature of the approach.

  \begin{itemize}
  \tightlist
  \item
    the \(\omega_{h}\) approach is exploratory and defined if there are three or more group factors (with only two group factors, the default is to assume they are equally important, hence the factor loadings of those subscales will be equal)
  \item
    Najera Catalan \citep{najera_catalan_reliability_2019} suggests that \(\omega_{h}\) is the best measure of reliability when dealing with multiple dimensions.
  \end{itemize}
\item
  \(\omega_{g}\) is an estimate that uses a bifactor solution via the SEM package \emph{lavaan} and tends to be a larger (because it forces all the cross-loadings of lower-level factors to be 0)

  \begin{itemize}
  \tightlist
  \item
    \(\omega_{g}\) is confirmatory, requiring the specification of which variables load on each group factor
  \end{itemize}
\item
  \emph{psych::omegaSem()} reports both EFA and CFA solutions

  \begin{itemize}
  \tightlist
  \item
    We will use the \emph{psych::omegaSem()} function.
  \end{itemize}
\end{itemize}

Note that in our specification, we indicate there are two factors. We do not tell it (anywhere!) what items belong to what factors (think, \emph{subscales}). One test will be to see if the items align with their respective factors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Because we added the factor scores to our df (and now it has more}
\CommentTok{\# variables than just our items), I will estimate omegaSem with the}
\CommentTok{\# correlation matrix; I will need to tell it the n.obs}

\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{omegaSem}\NormalTok{(GRMSr, }\AttributeTok{nfactors =} \DecValTok{4}\NormalTok{, }\AttributeTok{n.obs =} \DecValTok{259}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats, : lavaan WARNING:
    Could not compute standard errors! The information matrix could
    not be inverted. This may be a symptom that the model is not
    identified.
\end{verbatim}

\includegraphics{09-EFA_PAF_files/figure-latex/unnamed-chunk-37-1.pdf} \includegraphics{09-EFA_PAF_files/figure-latex/unnamed-chunk-37-2.pdf}

\begin{verbatim}
 
Call: psych::omegaSem(m = GRMSr, nfactors = 4, n.obs = 259)
Omega 
Call: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, 
    digits = digits, title = title, sl = sl, labels = labels, 
    plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, 
    covar = covar)
Alpha:                 0.84 
G.6:                   0.86 
Omega Hierarchical:    0.57 
Omega H asymptotic:    0.66 
Omega Total            0.86 

Schmid Leiman Factor loadings greater than  0.2 
         g   F1*   F2*   F3*   F4*   h2   u2   p2
Obj1  0.37  0.39                   0.29 0.71 0.46
Obj2  0.40  0.40                   0.32 0.68 0.49
Obj3  0.35  0.35                   0.25 0.75 0.48
Obj4  0.37  0.43                   0.33 0.67 0.42
Obj5  0.36  0.36                   0.28 0.72 0.47
Obj6  0.27  0.32              0.23 0.22 0.78 0.33
Obj7  0.41  0.29                   0.27 0.73 0.62
Obj8  0.35  0.41             -0.20 0.35 0.65 0.34
Obj9  0.28  0.30                   0.19 0.81 0.41
Obj10 0.28  0.37                   0.22 0.78 0.36
Marg1 0.53        0.58             0.62 0.38 0.46
Marg2 0.49        0.35             0.41 0.59 0.59
Marg3 0.42        0.30             0.28 0.72 0.62
Marg4 0.39        0.28             0.28 0.72 0.55
Marg5 0.46        0.32             0.33 0.67 0.64
Marg6 0.41  0.24                   0.27 0.73 0.62
Marg7 0.33        0.25             0.18 0.82 0.59
Str1  0.34              0.48       0.36 0.64 0.31
Str2  0.29              0.23       0.18 0.82 0.46
Str3  0.25              0.46       0.28 0.72 0.23
Str4  0.21              0.34       0.17 0.83 0.25
Str5  0.25              0.30       0.18 0.82 0.36
Ang1  0.28              0.29  0.22 0.22 0.78 0.34
Ang2  0.27                    0.61 0.44 0.56 0.16
Ang3  0.30                    0.28 0.19 0.81 0.47

With Sums of squares  of:
   g  F1*  F2*  F3*  F4* 
3.14 1.47 0.89 0.90 0.71 

general/max  2.14   max/min =   2.07
mean percent general =  0.44    with sd =  0.13 and cv of  0.3 
Explained Common Variance of the general factor =  0.44 

The degrees of freedom are 206  and the fit is  0.77 
The number of observations was  259  with Chi Square =  189.17  with prob <  0.79
The root mean square of the residuals is  0.04 
The df corrected root mean square of the residuals is  0.04
RMSEA index =  0  and the 10 % confidence intervals are  0 0.018
BIC =  -955.53

Compare this with the adequacy of just a general factor and no group factors
The degrees of freedom for just the general factor are 275  and the fit is  1.79 
The number of observations was  259  with Chi Square =  445.32  with prob <  0.00000000034
The root mean square of the residuals is  0.09 
The df corrected root mean square of the residuals is  0.09 

RMSEA index =  0.049  and the 10 % confidence intervals are  0.041 0.057
BIC =  -1082.81 

Measures of factor score adequacy             
                                                 g  F1*   F2*   F3*   F4*
Correlation of scores with factors            0.77 0.72  0.67  0.70  0.70
Multiple R square of scores with factors      0.60 0.52  0.44  0.49  0.49
Minimum correlation of factor score estimates 0.20 0.03 -0.11 -0.02 -0.01

 Total, General and Subset omega for each subset
                                                 g  F1*  F2*  F3*  F4*
Omega total for total scores and subscales    0.86 0.78 0.72 0.59 0.44
Omega general for total scores and subscales  0.57 0.39 0.44 0.22 0.13
Omega group for total scores and subscales    0.19 0.39 0.28 0.37 0.31

 The following analyses were done using the  lavaan  package 

 Omega Hierarchical from a confirmatory model using sem =  0.7
 Omega Total  from a confirmatory model using sem =  0.86 
With loadings of 
         g  F1*  F2*  F3*  F4*   h2   u2   p2
Obj1  0.38 0.39                0.30 0.70 0.48
Obj2  0.42 0.37                0.31 0.69 0.57
Obj3  0.37 0.32                0.24 0.76 0.57
Obj4  0.37 0.42                0.31 0.69 0.44
Obj5  0.36 0.38                0.28 0.72 0.46
Obj6  0.31 0.25                0.16 0.84 0.60
Obj7  0.46 0.26                0.28 0.72 0.76
Obj8  0.31 0.45                0.30 0.70 0.32
Obj9  0.28 0.31                0.17 0.83 0.46
Obj10 0.28 0.35                0.21 0.79 0.37
Marg1 0.54      0.54           0.58 0.42 0.50
Marg2 0.58      0.22           0.39 0.61 0.86
Marg3 0.46      0.28           0.29 0.71 0.73
Marg4 0.45      0.23           0.25 0.75 0.81
Marg5 0.53      0.23           0.33 0.67 0.85
Marg6 0.49                     0.26 0.74 0.92
Marg7 0.37      0.21           0.18 0.82 0.76
Str1  0.36           0.40      0.29 0.71 0.45
Str2  0.34           0.23      0.17 0.83 0.68
Str3  0.27           0.50      0.32 0.68 0.23
Str4  0.20           0.41      0.21 0.79 0.19
Str5  0.31           0.22      0.14 0.86 0.69
Ang1  0.33           0.22      0.16 0.84 0.68
Ang2  0.35                0.69 0.60 0.40 0.20
Ang3  0.39                     0.18 0.82 0.85

With sum of squared loadings of:
   g  F1*  F2*  F3*  F4* 
3.83 1.28 0.56 0.72 0.50 

The degrees of freedom of the confirmatory model are  250  and the fit is  263.6354  with p =  0.264761
general/max  2.98   max/min =   2.55
mean percent general =  0.58    with sd =  0.22 and cv of  0.37 
Explained Common Variance of the general factor =  0.56 

Measures of factor score adequacy             
                                                 g  F1*   F2*   F3*  F4*
Correlation of scores with factors            0.86 0.75  0.65  0.69 0.77
Multiple R square of scores with factors      0.75 0.56  0.42  0.48 0.59
Minimum correlation of factor score estimates 0.49 0.13 -0.16 -0.05 0.19

 Total, General and Subset omega for each subset
                                                 g  F1*  F2*  F3*  F4*
Omega total for total scores and subscales    0.86 0.78 0.74 0.60 0.51
Omega general for total scores and subscales  0.70 0.43 0.56 0.28 0.22
Omega group for total scores and subscales    0.16 0.35 0.19 0.33 0.29

To get the standard sem fit statistics, ask for summary on the fitted object
\end{verbatim}

There's a ton of output! How do we make sense of it?

First, excepting for the Angry scale (noted before), our items aligned reasonably with their respective factors (subscales).

Second, we can interpret our results. Like alpha, the omegas range from 0 to 1, where values closer to 1 represent good reliability \citep{najera_catalan_reliability_2019}. For unidimensional measures, \(\omega_{t}\) values above 0.80 seem to be an indicator of good reliability. For multidimensional measures with well-defined dimensions we strive for \(\omega_{h}\) values above 0.65 (and \(\omega_{t}\) \textgreater{} 0.8). These recommendations are based on a Monte Carlo study that examined a host of reliability indicators and how their values corresponded with accurate predictions of poverty status. With this in mind, let's examine the output related to our simulated research vignette.

Let's examine the output in the lower portion where the values are ``from a confirmatory model using sem.''

Omega is a reliability estimate for factor analysis that represents the proportion of variance in the GRMS scale attributable to common variance (rather than error). The omega for the total reliability of the test (\(\omega_{t}\); which included the general factors and the subscale factors) was .86, meaning that 86\% of the variance in the total scale is due to the factors and 14\% (100\% - 86\%) is attributable to error.

Omega hierarchical (\(\omega_{h}\)) estimates are the proportion of variance in the GRMS score attributable to the general factor, which in effect treats the subscales as error. \(\omega_{h}\) for the the GRMS total scale was .70 A quick calculation with \(\omega_{h}\) (.70) and \(\omega_{t}\) (.86; .70/.86 = .81) lets us know that that 81\% of the reliable variance in the GRMS total scale is attributable to the general factor.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{.}\DecValTok{70}\SpecialCharTok{/}\NormalTok{.}\DecValTok{86}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.8139535
\end{verbatim}

Amongst the output is the Cronbach's alpha coefficient (.84). Lewis and Neville \citeyearpar{lewis_construction_2015} did not report omega results. They reported an alpha of .92 for the version of the GRMS that assessed stress appraisal.

\hypertarget{comparing-pfa-to-item-analysis-and-pca}{%
\section{Comparing PFA to Item Analysis and PCA}\label{comparing-pfa-to-item-analysis-and-pca}}

In the lesson on PCA, we began a table that compared our item analysis (item corrected-total correlations with item-other scale correlations) and PCA results (both orthogonal and oblique). Let's now add our PAF results (both orthogonal and oblique).

In the prior lecture, I saved the file as both .rds and .csv objects. I will bring back in the .rds object and add to it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GRMScomps }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"GRMS\_Comparisons.rds"}\NormalTok{)}
\NormalTok{grmsPAF2ORTH}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  pa
Call: psych::fa(r = dfGRMS, nfactors = 4, rotate = "varimax", fm = "pa")
Standardized loadings (pattern matrix) based upon correlation matrix
       PA1  PA2   PA3   PA4   h2   u2 com
Obj1  0.49 0.14  0.16  0.04 0.29 0.71 1.4
Obj2  0.51 0.18  0.13  0.09 0.32 0.68 1.4
Obj3  0.45 0.14  0.15  0.07 0.25 0.75 1.5
Obj4  0.54 0.19  0.04  0.03 0.33 0.67 1.3
Obj5  0.47 0.15  0.19 -0.02 0.28 0.72 1.6
Obj6  0.39 0.05  0.06  0.26 0.22 0.78 1.8
Obj7  0.41 0.27  0.16  0.10 0.27 0.73 2.2
Obj8  0.52 0.23  0.03 -0.18 0.35 0.65 1.7
Obj9  0.38 0.07  0.19 -0.01 0.19 0.81 1.5
Obj10 0.44 0.06  0.06  0.12 0.22 0.78 1.2
Marg1 0.13 0.77  0.08  0.02 0.62 0.38 1.1
Marg2 0.13 0.53  0.30  0.15 0.41 0.59 1.9
Marg3 0.18 0.46  0.16  0.08 0.28 0.72 1.6
Marg4 0.26 0.45 -0.01  0.13 0.28 0.72 1.8
Marg5 0.23 0.49  0.16  0.07 0.33 0.67 1.7
Marg6 0.35 0.35  0.08  0.16 0.27 0.73 2.5
Marg7 0.14 0.38  0.07  0.10 0.18 0.82 1.5
Str1  0.16 0.16  0.55 -0.08 0.36 0.64 1.4
Str2  0.24 0.09  0.30  0.16 0.18 0.82 2.7
Str3  0.07 0.07  0.51  0.06 0.28 0.72 1.1
Str4  0.14 0.04  0.38 -0.03 0.17 0.83 1.3
Str5  0.11 0.09  0.36  0.16 0.18 0.82 1.8
Ang1  0.02 0.18  0.35  0.25 0.22 0.78 2.4
Ang2  0.05 0.20  0.06  0.62 0.43 0.57 1.2
Ang3  0.10 0.26  0.15  0.31 0.19 0.81 2.7

                       PA1  PA2  PA3  PA4
SS loadings           2.60 2.26 1.41 0.83
Proportion Var        0.10 0.09 0.06 0.03
Cumulative Var        0.10 0.19 0.25 0.28
Proportion Explained  0.37 0.32 0.20 0.12
Cumulative Proportion 0.37 0.68 0.88 1.00

Mean item complexity =  1.7
Test of the hypothesis that 4 factors are sufficient.

df null model =  300  with the objective function =  4.89 with Chi Square =  1217.51
df of  the model are 206  and the objective function was  0.77 

The root mean square of the residuals (RMSR) is  0.04 
The df corrected root mean square of the residuals is  0.04 

The harmonic n.obs is  259 with the empirical chi square  202.41  with prob <  0.56 
The total n.obs was  259  with Likelihood Chi Square =  189.19  with prob <  0.79 

Tucker Lewis Index of factoring reliability =  1.027
RMSEA index =  0  and the 90 % confidence intervals are  0 0.018
BIC =  -955.52
Fit based upon off diagonal values = 0.97
Measures of factor score adequacy             
                                                   PA1  PA2  PA3  PA4
Correlation of (regression) scores with factors   0.84 0.85 0.76 0.72
Multiple R square of scores with factors          0.71 0.72 0.58 0.52
Minimum correlation of possible factor scores     0.42 0.45 0.17 0.03
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# names(grmsPAF2ORTH) I had to add \textquotesingle{}unclass\textquotesingle{} to the loadings to}
\CommentTok{\# render them into a df}
\NormalTok{pafORTH\_loadings }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{unclass}\NormalTok{(grmsPAF2ORTH}\SpecialCharTok{$}\NormalTok{loadings))}
\NormalTok{pafORTH\_loadings}\SpecialCharTok{$}\NormalTok{Items }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{,}
    \StringTok{"Obj7"}\NormalTok{, }\StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{, }\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{,}
    \StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{, }\StringTok{"Marg7"}\NormalTok{, }\StringTok{"Strong1"}\NormalTok{, }\StringTok{"Strong2"}\NormalTok{, }\StringTok{"Strong3"}\NormalTok{, }\StringTok{"Strong4"}\NormalTok{,}
    \StringTok{"Strong5"}\NormalTok{, }\StringTok{"Angry1"}\NormalTok{, }\StringTok{"Angry2"}\NormalTok{, }\StringTok{"Angry3"}\NormalTok{)  }\CommentTok{\#Item names for joining (and to make sure we know which variable is which)}
\NormalTok{pafORTH\_loadings }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{rename}\NormalTok{(pafORTH\_loadings, }\AttributeTok{PAF\_OR\_Obj =}\NormalTok{ PA1, }\AttributeTok{PAF\_OR\_Mar =}\NormalTok{ PA2,}
    \AttributeTok{PAF\_OR\_Str =}\NormalTok{ PA3, }\AttributeTok{PAF\_OR\_Ang =}\NormalTok{ PA4)}
\CommentTok{\# I had to add \textquotesingle{}unclass\textquotesingle{} to the loadings to render them into a df}
\NormalTok{GRMScomps }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{full\_join}\NormalTok{(GRMScomps, pafORTH\_loadings, }\AttributeTok{by =} \StringTok{"Items"}\NormalTok{)}

\CommentTok{\# Now adding the PAF oblique loadings}
\NormalTok{pafOBLQ\_loadings }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{unclass}\NormalTok{(grmsPAF2obl}\SpecialCharTok{$}\NormalTok{loadings))  }\CommentTok{\#I had to add \textquotesingle{}unclass\textquotesingle{} to the loadings to render them into a df}
\NormalTok{pafOBLQ\_loadings}\SpecialCharTok{$}\NormalTok{Items }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{,}
    \StringTok{"Obj7"}\NormalTok{, }\StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{, }\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{,}
    \StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{, }\StringTok{"Marg7"}\NormalTok{, }\StringTok{"Strong1"}\NormalTok{, }\StringTok{"Strong2"}\NormalTok{, }\StringTok{"Strong3"}\NormalTok{, }\StringTok{"Strong4"}\NormalTok{,}
    \StringTok{"Strong5"}\NormalTok{, }\StringTok{"Angry1"}\NormalTok{, }\StringTok{"Angry2"}\NormalTok{, }\StringTok{"Angry3"}\NormalTok{)}

\CommentTok{\# Item names for joining (and to make sure we know which variable is}
\CommentTok{\# which)}
\NormalTok{pafOBLQ\_loadings }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{rename}\NormalTok{(pafOBLQ\_loadings, }\AttributeTok{PAF\_OB\_Obj =}\NormalTok{ PA1, }\AttributeTok{PAF\_OB\_Mar =}\NormalTok{ PA2,}
    \AttributeTok{PAF\_OB\_Str =}\NormalTok{ PA3, }\AttributeTok{PAF\_OB\_Ang =}\NormalTok{ PA4)}

\CommentTok{\# I had to add \textquotesingle{}unclass\textquotesingle{} to the loadings to render them into a df}
\NormalTok{GRMScomps }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{full\_join}\NormalTok{(GRMScomps, pafOBLQ\_loadings, }\AttributeTok{by =} \StringTok{"Items"}\NormalTok{)}

\CommentTok{\# Writes the table to a .csv file where you can open it with Excel}
\CommentTok{\# and format )}
\FunctionTok{write.csv}\NormalTok{(GRMScomps, }\AttributeTok{file =} \StringTok{"GRMS\_Comps.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{col.names =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in write.csv(GRMScomps, file = "GRMS_Comps.csv", sep = ",", row.names =
FALSE, : attempt to set 'col.names' ignored
\end{verbatim}

\begin{verbatim}
Warning in write.csv(GRMScomps, file = "GRMS_Comps.csv", sep = ",", row.names =
FALSE, : attempt to set 'sep' ignored
\end{verbatim}

Below we can see the consistency across item analysis, PCA (orthogonal and oblique), and PAF (orthogonal and oblique) comparisons. The results were consistent across the analyses, pointing only to problems with the Angry scale. Please note that the problems were with my simulated data and not the original data.

\begin{figure}
\centering
\includegraphics{images/PAF/GRMScomps.png}
\caption{Comparison of path models for PCA and EFA}
\end{figure}

\hypertarget{practice-problems-7}{%
\section{Practice Problems}\label{practice-problems-7}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. In psychometrics, I strongly recommend that you have started with a dataset that has a minimum of three subscales and use it for all of the assignments in the OER. In any case, please plan to:

\begin{itemize}
\tightlist
\item
  Properly format and prepare the data.
\item
  Conduct diagnostic tests to determine the suitability of the data for PAF
\item
  Conduct tests to guide the decisions about number of factors to extract.
\item
  Conduct orthogonal and oblique rotations (at least two each with different numbers of factor).
\item
  Select one solution and preparing an APA style results section (with table and figure).
\item
  Compare your results in light of any other psychometrics lessons where you have used this data (especially the \protect\hyperlink{ItemAnalSurvey}{item analysis} and \protect\hyperlink{PCA}{PCA} lessons).
\end{itemize}

\hypertarget{problem-1-play-around-with-this-simulation.-4}{%
\subsection{Problem \#1: Play around with this simulation.}\label{problem-1-play-around-with-this-simulation.-4}}

Copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results. If PAF is new to you, perhaps you just change the number in ``set.seed(240311)'' from 240311 to something else. Your results should \emph{parallel} those obtained in the lecture, making it easier for you to check your work as you go. Don't be surprised if the factor loadings wiggle around a little. Do try to make sense of them.

\hypertarget{problem-2-conduct-a-paf-with-another-simulated-set-of-data-in-the-oer.}{%
\subsection{Problem \#2: Conduct a PAF with another simulated set of data in the OER.}\label{problem-2-conduct-a-paf-with-another-simulated-set-of-data-in-the-oer.}}

The second option involves utilizing one of the simulated datasets available in this OER. The \protect\hyperlink{sims}{last lesson} in the OER contains three simulations that could be used for all of the statistics-based practice suggestions. Especially if you started with one of these examples in an earlier lesson, I highly recommend you continue with that.

Alternatively, Keum et al.'s Gendered Racial Microaggressions Scale for Asian American Women \citep{keum_gendered_2018} will be used in the lessons on confirmatory factor analysis and Conover et al.'s \citep{conover_development_2017} Ableist Microaggressions Scale is used in the lesson on invariance testing. Both of these would be suitable for the PCA and PAF homework assignments.

\hypertarget{problem-3-try-something-entirely-new.-4}{%
\subsection{Problem \#3: Try something entirely new.}\label{problem-3-try-something-entirely-new.-4}}

Using data for which you have permission and access (e.g., IRB approved data you have collected or from your lab; data you simulate from a published article; data from the ReCentering Psych Stats survey described in the \protect\hyperlink{qualTRIXdata}{Qualtrics lesson}, or data from an open science repository), complete a PAF analysis. The data should allow for at least three factors/subscales.

\hypertarget{grading-rubric-4}{%
\subsection{Grading Rubric}\label{grading-rubric-4}}

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5493}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1972}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Possible
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Earned
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1. Check and, if needed, format data & 5 & \_\_\_\_\_ \\
2. Conduct and interpret the three diagnostic tests to determine if PAF is appropriate as an analysis (KMO, Bartlett's, determinant). & 5 & \_\_\_\_\_ \\
3. Determine how many factors to extract (e.g., scree plot, eigenvalues, theory). & 5 & \_\_\_\_\_ \\
4. Conduct an orthogonal rotation with a minimum of two different numbers of factor extractions. & 5 & \_\_\_\_\_ \\
5. Conduct an oblique rotation with a minimum of two different numbers of factor extractions. & 5 & \_\_\_\_\_ \\
6. Determine which factor solution (e.g., orthogonal or oblique; which number of factors) you will suggest. & 5 & \_\_\_\_\_ \\
7. APA style results section with table and figure of one of the solutions. & 5 & \_\_\_\_\_ \\
8. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 40 & \_\_\_\_\_ \\
\end{longtable}

\hypertarget{homeworked-example-4}{%
\section{Homeworked Example}\label{homeworked-example-4}}

\href{https://youtu.be/HIsdbxqbw0o}{Screencast Link}

For more information about the data used in this homeworked example, please refer to the description and codebook located at the end of the \href{https://lhbikos.github.io/ReCenterPsychStats/ReCintro.html\#introduction-to-the-data-set-used-for-homeworked-examples}{introduction} in first volume of ReCentering Psych Stats.

As a brief review, this data is part of an IRB-approved study, with consent to use in teaching demonstrations and to be made available to the general public via the open science framework. Hence, it is appropriate to use in this context. You will notice there are student- and teacher- IDs. These numbers are not actual student and teacher IDs, rather they were further re-identified so that they could not be connected to actual people.

Because this is an actual dataset, if you wish to work the problem along with me, you will need to download the \href{https://github.com/lhbikos/ReC_Psychometrics/blob/main/Worked_Examples/ReC.rds}{ReC.rds} data file from the Worked\_Examples folder in the ReC\_Psychometrics project on the GitHub.

The course evaluation items can be divided into three subscales:

\begin{itemize}
\tightlist
\item
  \textbf{Valued by the student} includes the items: ValObjectives, IncrUnderstanding, IncrInterest
\item
  \textbf{Traditional pedagogy} includes the items: ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation
\item
  \textbf{Socially responsive pedagogy} includes the items: InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration
\end{itemize}

In this homewoRked example I will use principal axis factoring in an exploratory factor analysis. My hope is that the results will support my solution of three dimensions: valued-by-the-student, traditional pedagogy, socially responsive pedagogy.

\hypertarget{check-and-if-needed-format-data-2}{%
\subsection{Check and, if needed, format data}\label{check-and-if-needed-format-data-2}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{big }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"ReC.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

With the next code I will create an item-level df with only the items used in the three scales.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{items }\OtherTok{\textless{}{-}}\NormalTok{ big }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(ValObjectives, IncrUnderstanding, IncrInterest, ClearResponsibilities,}
\NormalTok{        EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation,}
\NormalTok{        MultPerspectives, InclusvClassrm, DEIintegration, EquitableEval)}
\end{Highlighting}
\end{Shaded}

Some of the analyses require non-missing data in the df.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{items }\OtherTok{\textless{}{-}} \FunctionTok{na.omit}\NormalTok{(items)}
\end{Highlighting}
\end{Shaded}

Let's check the structure of the data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(items)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Classes 'data.table' and 'data.frame':  267 obs. of  12 variables:
 $ ValObjectives        : int  5 5 4 4 5 5 5 4 5 3 ...
 $ IncrUnderstanding    : int  2 3 4 3 4 5 2 4 5 4 ...
 $ IncrInterest         : int  5 3 4 2 4 5 3 2 5 1 ...
 $ ClearResponsibilities: int  5 5 4 4 5 5 4 4 5 3 ...
 $ EffectiveAnswers     : int  5 3 5 3 5 4 3 2 3 3 ...
 $ Feedback             : int  5 3 4 2 5 5 4 4 5 2 ...
 $ ClearOrganization    : int  3 4 3 4 4 5 4 4 5 2 ...
 $ ClearPresentation    : int  4 4 4 2 5 4 4 4 5 2 ...
 $ MultPerspectives     : int  5 5 4 5 5 5 5 5 5 1 ...
 $ InclusvClassrm       : int  5 5 5 5 5 5 5 4 5 3 ...
 $ DEIintegration       : int  5 5 5 5 5 5 5 5 5 2 ...
 $ EquitableEval        : int  5 5 3 5 5 5 5 3 5 3 ...
 - attr(*, ".internal.selfref")=<externalptr> 
 - attr(*, "na.action")= 'omit' Named int [1:43] 6 20 106 109 112 113 114 117 122 128 ...
  ..- attr(*, "names")= chr [1:43] "6" "20" "106" "109" ...
\end{verbatim}

\hypertarget{conduct-and-interpret-the-three-diagnostic-tests-to-determine-if-paf-is-appropriate-as-an-analysis-kmo-bartletts-determinant}{%
\subsection{Conduct and interpret the three diagnostic tests to determine if PAF is appropriate as an analysis (KMO, Bartlett's, determinant)}\label{conduct-and-interpret-the-three-diagnostic-tests-to-determine-if-paf-is-appropriate-as-an-analysis-kmo-bartletts-determinant}}

\hypertarget{kmo-1}{%
\subsubsection{KMO}\label{kmo-1}}

The Kaiser-Meyer-Olkin (KMO) index is an index of \emph{sampling adequacy} to let us know if the sample size is sufficient relative to the statistical characteristics of the data.

General criteria (1974, Kaiser):

\begin{itemize}
\tightlist
\item
  bare minimum of .5
\item
  values between .5 and .7 as mediocre
\item
  values above .9 are superb
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{KMO}\NormalTok{(items)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Kaiser-Meyer-Olkin factor adequacy
Call: psych::KMO(r = items)
Overall MSA =  0.91
MSA for each item = 
        ValObjectives     IncrUnderstanding          IncrInterest 
                 0.94                  0.89                  0.89 
ClearResponsibilities      EffectiveAnswers              Feedback 
                 0.91                  0.93                  0.94 
    ClearOrganization     ClearPresentation      MultPerspectives 
                 0.94                  0.91                  0.93 
       InclusvClassrm        DEIintegration         EquitableEval 
                 0.86                  0.78                  0.95 
\end{verbatim}

With a KMO of 0.91, the data seems appropriate to continue with the PCA.

\hypertarget{bartletts-1}{%
\subsubsection{Bartlett's}\label{bartletts-1}}

Barlett's test lets us know if the matrix is an \emph{identity matrix} (i.e., where elements on the off-diagonal would be 0.0 and elements on the diagonal would be 1.0). Stated another way -- items only correlate with ``themselves'' and not other variables.

When \(p < 0.05\) the matrix is not an identity matrix. That is, there are some relationships between variables that can be analyzed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{cortest.bartlett}\NormalTok{(items)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
R was not square, finding R from data
\end{verbatim}

\begin{verbatim}
$chisq
[1] 1897.769

$p.value
[1] 0

$df
[1] 66
\end{verbatim}

The Barlett's test, \(\chi^2(66) = 1897.77, p < 0.001\), indicating that the correlation matrix is not an identity matrix and, on that dimension, is suitable for analysis.

\hypertarget{determinant-1}{%
\subsubsection{Determinant}\label{determinant-1}}

Multicollinearity or singularity is diagnosed by the determinant. The determinant should be greater than 0.00001. If smaller, then there may be an issue with multicollinearity (variables that are too highly correlated) or singularity (variables that are perfectly correlated).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{items }\OtherTok{\textless{}{-}} \FunctionTok{na.omit}\NormalTok{(items)}
\FunctionTok{det}\NormalTok{(}\FunctionTok{cor}\NormalTok{(items))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0006985496
\end{verbatim}

The value of the determinant is 0.0007; greater than 0.00001. We are not concerned with multicollinearity or singularity.

Summary from data screening:

\begin{quote}
Data screening were conducted to determine the suitability of the data for principal axis factoring. The Kaiser-Meyer-Olkin measure of sampling adequacy (KMO; Kaiser, 1970) represents the ratio of the squared correlation between variables to the squared partial correlation between variables. KMO ranges from 0.00 to 1.00; values closer to 1.00 indicate that the patterns of correlations are relatively compact, and that component analysis should yield distinct and reliable components (Field, 2012). In our dataset, the KMO value was 0.91, indicating acceptable sampling adequacy. The Barlett's Test of Sphericity examines whether the population correlation matrix resembles an identity matrix (Field, 2012). When the \emph{p} value for the Bartlett's test is \textless{} .05, we are fairly certain we have clusters of correlated variables. In our dataset, \(\chi^2(66) = 1897.77, p < 0.001\) indicating the correlations between items are sufficiently large enough for principal components analysis. The determinant of the correlation matrix alerts us to any issues of multicollinearity or singularity and should be larger than 0.00001. Our determinant was 0.0007 and, again, indicated that our data was suitable for the analysis.
\end{quote}

\hypertarget{determine-how-many-components-to-extract-e.g.-scree-plot-eigenvalues-theory-1}{%
\subsection{Determine how many components to extract (e.g., scree plot, eigenvalues, theory)}\label{determine-how-many-components-to-extract-e.g.-scree-plot-eigenvalues-theory-1}}

Specify fewer factors than number of items (12). For me it wouldn't run unless it was 6 or fewer.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{paf1 }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{fa}\NormalTok{(items, }\AttributeTok{nfactors =} \DecValTok{6}\NormalTok{, }\AttributeTok{fm =} \StringTok{"pa"}\NormalTok{, }\AttributeTok{max.iter =} \DecValTok{100}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"none"}\NormalTok{)  }\CommentTok{\# using raw data and letting the length function automatically calculate the \# factors as a function of how many columns in the raw data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
maximum iteration exceeded
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{paf1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  pa
Call: psych::fa(r = items, nfactors = 6, rotate = "none", max.iter = 100, 
    fm = "pa")
Standardized loadings (pattern matrix) based upon correlation matrix
                       PA1   PA2   PA3   PA4   PA5   PA6   h2    u2 com
ValObjectives         0.52 -0.07  0.16  0.05  0.05  0.10 0.31 0.688 1.3
IncrUnderstanding     0.65 -0.28  0.31  0.03 -0.04 -0.01 0.60 0.400 1.9
IncrInterest          0.74 -0.18  0.50 -0.18  0.13 -0.02 0.87 0.127 2.1
ClearResponsibilities 0.80 -0.09 -0.34  0.10  0.03 -0.06 0.77 0.226 1.4
EffectiveAnswers      0.78 -0.14 -0.13  0.05  0.09 -0.17 0.68 0.315 1.3
Feedback              0.74  0.05 -0.20 -0.12  0.22  0.05 0.65 0.347 1.4
ClearOrganization     0.78 -0.28 -0.13  0.12 -0.07  0.32 0.83 0.175 1.8
ClearPresentation     0.84 -0.21  0.01  0.12 -0.20 -0.11 0.82 0.182 1.3
MultPerspectives      0.81  0.30 -0.12 -0.40 -0.23  0.03 0.97 0.029 2.0
InclusvClassrm        0.64  0.41  0.18  0.21 -0.14 -0.08 0.67 0.327 2.3
DEIintegration        0.49  0.66  0.13  0.14  0.11  0.10 0.74 0.265 2.2
EquitableEval         0.69  0.09 -0.17 -0.02  0.13 -0.09 0.54 0.462 1.3

                       PA1  PA2  PA3  PA4  PA5  PA6
SS loadings           6.11 0.97 0.65 0.31 0.22 0.19
Proportion Var        0.51 0.08 0.05 0.03 0.02 0.02
Cumulative Var        0.51 0.59 0.64 0.67 0.69 0.70
Proportion Explained  0.72 0.11 0.08 0.04 0.03 0.02
Cumulative Proportion 0.72 0.84 0.91 0.95 0.98 1.00

Mean item complexity =  1.7
Test of the hypothesis that 6 factors are sufficient.

df null model =  66  with the objective function =  7.27 with Chi Square =  1897.77
df of  the model are 9  and the objective function was  0.06 

The root mean square of the residuals (RMSR) is  0.01 
The df corrected root mean square of the residuals is  0.02 

The harmonic n.obs is  267 with the empirical chi square  2.97  with prob <  0.97 
The total n.obs was  267  with Likelihood Chi Square =  14.81  with prob <  0.096 

Tucker Lewis Index of factoring reliability =  0.976
RMSEA index =  0.049  and the 90 % confidence intervals are  0 0.093
BIC =  -35.47
Fit based upon off diagonal values = 1
Measures of factor score adequacy             
                                                   PA1  PA2  PA3  PA4  PA5
Correlation of (regression) scores with factors   0.98 0.90 0.88 0.86 0.73
Multiple R square of scores with factors          0.97 0.81 0.77 0.74 0.54
Minimum correlation of possible factor scores     0.93 0.63 0.54 0.48 0.07
                                                    PA6
Correlation of (regression) scores with factors    0.68
Multiple R square of scores with factors           0.46
Minimum correlation of possible factor scores     -0.08
\end{verbatim}

The eigenvalue-greater-than-one criteria suggests 1 factor (but the second factor has an SSloading of .97).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(paf1}\SpecialCharTok{$}\NormalTok{values, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{09-EFA_PAF_files/figure-latex/unnamed-chunk-91-1.pdf} The scree plot looks like one factor.

Ugh.

\begin{itemize}
\tightlist
\item
  I want 3 factors (we could think of this as a priori theory); would account for 64\% of variance.
\item
  Two could account for 59\% of variance.
\item
  Eigenvalues-greater-than-one criteria and scree plot suggests 1 or 3 (would account for 64\% of variance)
\end{itemize}

\emph{Note}: The lecture has more on evaluating communalities and uniquenesses and how this information can also inform the number of components we want to extract. Because it is easy to get lost (very lost) I will skip over this for now. If you were to create a measure and use PAF as an exploratory approach to understanding the dimensionality of an instrument, you would likely want to investigate further and report on these.

\hypertarget{conduct-an-orthogonal-rotation-with-a-minimum-of-two-different-factor-extractions}{%
\subsection{Conduct an orthogonal rotation with a minimum of two different factor extractions}\label{conduct-an-orthogonal-rotation-with-a-minimum-of-two-different-factor-extractions}}

\textbf{An orthogonal two factor solution}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pafORTH2f }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{fa}\NormalTok{(items, }\AttributeTok{nfactors =} \DecValTok{2}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"varimax"}\NormalTok{)}
\NormalTok{pafORTH2f}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  minres
Call: psych::fa(r = items, nfactors = 2, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix
                       MR1  MR2   h2   u2 com
ValObjectives         0.48 0.21 0.27 0.73 1.4
IncrUnderstanding     0.67 0.12 0.46 0.54 1.1
IncrInterest          0.66 0.25 0.50 0.50 1.3
ClearResponsibilities 0.74 0.29 0.63 0.37 1.3
EffectiveAnswers      0.76 0.26 0.64 0.36 1.2
Feedback              0.63 0.38 0.54 0.46 1.7
ClearOrganization     0.79 0.17 0.65 0.35 1.1
ClearPresentation     0.83 0.23 0.75 0.25 1.2
MultPerspectives      0.58 0.55 0.64 0.36 2.0
InclusvClassrm        0.36 0.64 0.54 0.46 1.6
DEIintegration        0.08 0.86 0.75 0.25 1.0
EquitableEval         0.57 0.40 0.49 0.51 1.8

                       MR1  MR2
SS loadings           4.74 2.12
Proportion Var        0.39 0.18
Cumulative Var        0.39 0.57
Proportion Explained  0.69 0.31
Cumulative Proportion 0.69 1.00

Mean item complexity =  1.4
Test of the hypothesis that 2 factors are sufficient.

df null model =  66  with the objective function =  7.27 with Chi Square =  1897.77
df of  the model are 43  and the objective function was  0.75 

The root mean square of the residuals (RMSR) is  0.05 
The df corrected root mean square of the residuals is  0.06 

The harmonic n.obs is  267 with the empirical chi square  90.7  with prob <  0.000029 
The total n.obs was  267  with Likelihood Chi Square =  194.26  with prob <  0.0000000000000000000041 

Tucker Lewis Index of factoring reliability =  0.873
RMSEA index =  0.115  and the 90 % confidence intervals are  0.099 0.132
BIC =  -46
Fit based upon off diagonal values = 0.99
Measures of factor score adequacy             
                                                   MR1 MR2
Correlation of (regression) scores with factors   0.94 0.9
Multiple R square of scores with factors          0.89 0.8
Minimum correlation of possible factor scores     0.78 0.6
\end{verbatim}

Sorting the scores into a table can help see the results more clearly. The ``cut = \#'' command will not show the factor scores for factor loading \textless{} .30. I would do this ``to see'', but I would include all the values in an APA style table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{paf\_tableOR2f }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{print.psych}\NormalTok{(pafORTH2f, }\AttributeTok{cut =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  minres
Call: psych::fa(r = items, nfactors = 2, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix
                      item  MR1  MR2   h2   u2 com
ClearPresentation        8 0.83      0.75 0.25 1.2
ClearOrganization        7 0.79      0.65 0.35 1.1
EffectiveAnswers         5 0.76      0.64 0.36 1.2
ClearResponsibilities    4 0.74      0.63 0.37 1.3
IncrUnderstanding        2 0.67      0.46 0.54 1.1
IncrInterest             3 0.66      0.50 0.50 1.3
Feedback                 6 0.63 0.38 0.54 0.46 1.7
MultPerspectives         9 0.58 0.55 0.64 0.36 2.0
EquitableEval           12 0.57 0.40 0.49 0.51 1.8
ValObjectives            1 0.48      0.27 0.73 1.4
DEIintegration          11      0.86 0.75 0.25 1.0
InclusvClassrm          10 0.36 0.64 0.54 0.46 1.6

                       MR1  MR2
SS loadings           4.74 2.12
Proportion Var        0.39 0.18
Cumulative Var        0.39 0.57
Proportion Explained  0.69 0.31
Cumulative Proportion 0.69 1.00

Mean item complexity =  1.4
Test of the hypothesis that 2 factors are sufficient.

df null model =  66  with the objective function =  7.27 with Chi Square =  1897.77
df of  the model are 43  and the objective function was  0.75 

The root mean square of the residuals (RMSR) is  0.05 
The df corrected root mean square of the residuals is  0.06 

The harmonic n.obs is  267 with the empirical chi square  90.7  with prob <  0.000029 
The total n.obs was  267  with Likelihood Chi Square =  194.26  with prob <  0.0000000000000000000041 

Tucker Lewis Index of factoring reliability =  0.873
RMSEA index =  0.115  and the 90 % confidence intervals are  0.099 0.132
BIC =  -46
Fit based upon off diagonal values = 0.99
Measures of factor score adequacy             
                                                   MR1 MR2
Correlation of (regression) scores with factors   0.94 0.9
Multiple R square of scores with factors          0.89 0.8
Minimum correlation of possible factor scores     0.78 0.6
\end{verbatim}

F1: Includes everything else. F2: Includes 2 SCR items -- DEIintegration, InclsvClssrm Also: EquitableEval MultPerspectives have high cross-loadings, but end up on the first factor

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{fa.diagram}\NormalTok{(pafORTH2f)}
\end{Highlighting}
\end{Shaded}

\includegraphics{09-EFA_PAF_files/figure-latex/unnamed-chunk-94-1.pdf} Plotting these figures from the program can facilitate conceptual understanding of what is going on -- and can be a ``check'' to your work.

In the lecture I made a ``biggish deal'' about PCA being \emph{components} analysis and PAF being \emph{factor} analysis. Although the two approaches can lead to similar results/conclusions, there are some significant differences ``under the hood.'' PCA can be thought of more as regression where the items predict the component. Consequently, the arrows went \emph{from} the item \emph{to} the component.

In PAF, the arrows will go from the factor to the item -- because the factors (or latent variables) are assumed to predict the scores on the items (i.e., ``depression'' would predict how someone rates items that assess hopelessness, sleep, anhedonia, and so forth).

\textbf{An orthogonal three factor solution}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pafORTH3f }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{fa}\NormalTok{(items, }\AttributeTok{nfactors =} \DecValTok{3}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"varimax"}\NormalTok{)}
\NormalTok{pafORTH3f}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  minres
Call: psych::fa(r = items, nfactors = 3, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix
                       MR1  MR3  MR2   h2   u2 com
ValObjectives         0.27 0.43 0.20 0.30 0.70 2.1
IncrUnderstanding     0.27 0.76 0.09 0.66 0.34 1.3
IncrInterest          0.27 0.75 0.25 0.69 0.31 1.5
ClearResponsibilities 0.84 0.24 0.17 0.79 0.21 1.2
EffectiveAnswers      0.67 0.41 0.18 0.64 0.36 1.8
Feedback              0.65 0.26 0.30 0.58 0.42 1.8
ClearOrganization     0.65 0.47 0.10 0.64 0.36 1.9
ClearPresentation     0.62 0.57 0.18 0.74 0.26 2.2
MultPerspectives      0.57 0.29 0.49 0.65 0.35 2.5
InclusvClassrm        0.28 0.30 0.63 0.56 0.44 1.9
DEIintegration        0.14 0.07 0.85 0.75 0.25 1.1
EquitableEval         0.60 0.23 0.33 0.52 0.48 1.9

                       MR1  MR3  MR2
SS loadings           3.37 2.39 1.77
Proportion Var        0.28 0.20 0.15
Cumulative Var        0.28 0.48 0.63
Proportion Explained  0.45 0.32 0.23
Cumulative Proportion 0.45 0.77 1.00

Mean item complexity =  1.8
Test of the hypothesis that 3 factors are sufficient.

df null model =  66  with the objective function =  7.27 with Chi Square =  1897.77
df of  the model are 33  and the objective function was  0.29 

The root mean square of the residuals (RMSR) is  0.02 
The df corrected root mean square of the residuals is  0.03 

The harmonic n.obs is  267 with the empirical chi square  19.55  with prob <  0.97 
The total n.obs was  267  with Likelihood Chi Square =  75.2  with prob <  0.000039 

Tucker Lewis Index of factoring reliability =  0.954
RMSEA index =  0.069  and the 90 % confidence intervals are  0.049 0.09
BIC =  -109.18
Fit based upon off diagonal values = 1
Measures of factor score adequacy             
                                                   MR1  MR3  MR2
Correlation of (regression) scores with factors   0.90 0.87 0.89
Multiple R square of scores with factors          0.82 0.76 0.79
Minimum correlation of possible factor scores     0.64 0.52 0.58
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{paf\_tableOR3f }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{print.psych}\NormalTok{(pafORTH3f, }\AttributeTok{cut =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  minres
Call: psych::fa(r = items, nfactors = 3, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix
                      item  MR1  MR3  MR2   h2   u2 com
ClearResponsibilities    4 0.84           0.79 0.21 1.2
EffectiveAnswers         5 0.67 0.41      0.64 0.36 1.8
Feedback                 6 0.65      0.30 0.58 0.42 1.8
ClearOrganization        7 0.65 0.47      0.64 0.36 1.9
ClearPresentation        8 0.62 0.57      0.74 0.26 2.2
EquitableEval           12 0.60      0.33 0.52 0.48 1.9
MultPerspectives         9 0.57      0.49 0.65 0.35 2.5
IncrUnderstanding        2      0.76      0.66 0.34 1.3
IncrInterest             3      0.75      0.69 0.31 1.5
ValObjectives            1      0.43      0.30 0.70 2.1
DEIintegration          11           0.85 0.75 0.25 1.1
InclusvClassrm          10           0.63 0.56 0.44 1.9

                       MR1  MR3  MR2
SS loadings           3.37 2.39 1.77
Proportion Var        0.28 0.20 0.15
Cumulative Var        0.28 0.48 0.63
Proportion Explained  0.45 0.32 0.23
Cumulative Proportion 0.45 0.77 1.00

Mean item complexity =  1.8
Test of the hypothesis that 3 factors are sufficient.

df null model =  66  with the objective function =  7.27 with Chi Square =  1897.77
df of  the model are 33  and the objective function was  0.29 

The root mean square of the residuals (RMSR) is  0.02 
The df corrected root mean square of the residuals is  0.03 

The harmonic n.obs is  267 with the empirical chi square  19.55  with prob <  0.97 
The total n.obs was  267  with Likelihood Chi Square =  75.2  with prob <  0.000039 

Tucker Lewis Index of factoring reliability =  0.954
RMSEA index =  0.069  and the 90 % confidence intervals are  0.049 0.09
BIC =  -109.18
Fit based upon off diagonal values = 1
Measures of factor score adequacy             
                                                   MR1  MR3  MR2
Correlation of (regression) scores with factors   0.90 0.87 0.89
Multiple R square of scores with factors          0.82 0.76 0.79
Minimum correlation of possible factor scores     0.64 0.52 0.58
\end{verbatim}

F1: Traditional Pedagogy\ldots+MultPerspectives F2: Valued-by-the-Student F3: SCRPed--the 2 items; Note: EquitableEval and MultPerspectivs have some cross-loading with first factor

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{fa.diagram}\NormalTok{(pafORTH3f)}
\end{Highlighting}
\end{Shaded}

\includegraphics{09-EFA_PAF_files/figure-latex/unnamed-chunk-97-1.pdf}

\hypertarget{conduct-an-oblique-rotation-with-a-minimum-of-two-different-factor-extractions}{%
\subsection{Conduct an oblique rotation with a minimum of two different factor extractions}\label{conduct-an-oblique-rotation-with-a-minimum-of-two-different-factor-extractions}}

\textbf{An oblique two factor solution}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pafOBL2f }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{fa}\NormalTok{(items, }\AttributeTok{nfactors =} \DecValTok{2}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"oblimin"}\NormalTok{)}
\NormalTok{pafOBL2f}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  minres
Call: psych::fa(r = items, nfactors = 2, rotate = "oblimin")
Standardized loadings (pattern matrix) based upon correlation matrix
                        MR1   MR2   h2   u2 com
ValObjectives          0.50  0.05 0.27 0.73 1.0
IncrUnderstanding      0.73 -0.12 0.46 0.54 1.1
IncrInterest           0.69  0.03 0.50 0.50 1.0
ClearResponsibilities  0.77  0.04 0.63 0.37 1.0
EffectiveAnswers       0.80  0.00 0.64 0.36 1.0
Feedback               0.64  0.18 0.54 0.46 1.2
ClearOrganization      0.85 -0.11 0.65 0.35 1.0
ClearPresentation      0.89 -0.05 0.75 0.25 1.0
MultPerspectives       0.55  0.38 0.64 0.36 1.8
InclusvClassrm         0.30  0.55 0.54 0.46 1.5
DEIintegration        -0.05  0.89 0.75 0.25 1.0
EquitableEval          0.57  0.22 0.49 0.51 1.3

                       MR1  MR2
SS loadings           5.32 1.54
Proportion Var        0.44 0.13
Cumulative Var        0.44 0.57
Proportion Explained  0.78 0.22
Cumulative Proportion 0.78 1.00

 With factor correlations of 
     MR1  MR2
MR1 1.00 0.45
MR2 0.45 1.00

Mean item complexity =  1.2
Test of the hypothesis that 2 factors are sufficient.

df null model =  66  with the objective function =  7.27 with Chi Square =  1897.77
df of  the model are 43  and the objective function was  0.75 

The root mean square of the residuals (RMSR) is  0.05 
The df corrected root mean square of the residuals is  0.06 

The harmonic n.obs is  267 with the empirical chi square  90.7  with prob <  0.000029 
The total n.obs was  267  with Likelihood Chi Square =  194.26  with prob <  0.0000000000000000000041 

Tucker Lewis Index of factoring reliability =  0.873
RMSEA index =  0.115  and the 90 % confidence intervals are  0.099 0.132
BIC =  -46
Fit based upon off diagonal values = 0.99
Measures of factor score adequacy             
                                                   MR1  MR2
Correlation of (regression) scores with factors   0.97 0.91
Multiple R square of scores with factors          0.93 0.83
Minimum correlation of possible factor scores     0.86 0.65
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{paf\_tableOBL2f }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{print.psych}\NormalTok{(pafOBL2f, }\AttributeTok{cut =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  minres
Call: psych::fa(r = items, nfactors = 2, rotate = "oblimin")
Standardized loadings (pattern matrix) based upon correlation matrix
                      item   MR1   MR2   h2   u2 com
ClearPresentation        8  0.89       0.75 0.25 1.0
ClearOrganization        7  0.85       0.65 0.35 1.0
EffectiveAnswers         5  0.80       0.64 0.36 1.0
ClearResponsibilities    4  0.77       0.63 0.37 1.0
IncrUnderstanding        2  0.73       0.46 0.54 1.1
IncrInterest             3  0.69       0.50 0.50 1.0
Feedback                 6  0.64       0.54 0.46 1.2
EquitableEval           12  0.57       0.49 0.51 1.3
MultPerspectives         9  0.55  0.38 0.64 0.36 1.8
ValObjectives            1  0.50       0.27 0.73 1.0
DEIintegration          11        0.89 0.75 0.25 1.0
InclusvClassrm          10        0.55 0.54 0.46 1.5

                       MR1  MR2
SS loadings           5.32 1.54
Proportion Var        0.44 0.13
Cumulative Var        0.44 0.57
Proportion Explained  0.78 0.22
Cumulative Proportion 0.78 1.00

 With factor correlations of 
     MR1  MR2
MR1 1.00 0.45
MR2 0.45 1.00

Mean item complexity =  1.2
Test of the hypothesis that 2 factors are sufficient.

df null model =  66  with the objective function =  7.27 with Chi Square =  1897.77
df of  the model are 43  and the objective function was  0.75 

The root mean square of the residuals (RMSR) is  0.05 
The df corrected root mean square of the residuals is  0.06 

The harmonic n.obs is  267 with the empirical chi square  90.7  with prob <  0.000029 
The total n.obs was  267  with Likelihood Chi Square =  194.26  with prob <  0.0000000000000000000041 

Tucker Lewis Index of factoring reliability =  0.873
RMSEA index =  0.115  and the 90 % confidence intervals are  0.099 0.132
BIC =  -46
Fit based upon off diagonal values = 0.99
Measures of factor score adequacy             
                                                   MR1  MR2
Correlation of (regression) scores with factors   0.97 0.91
Multiple R square of scores with factors          0.93 0.83
Minimum correlation of possible factor scores     0.86 0.65
\end{verbatim}

Curiously, there are fewer cross-loadings. F1 has everything except the 2 SCR items which are on F2.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{fa.diagram}\NormalTok{(pafOBL2f)}
\end{Highlighting}
\end{Shaded}

\includegraphics{09-EFA_PAF_files/figure-latex/unnamed-chunk-100-1.pdf} With the curved line and value between MR1 and MR2, this figure helps make ``allowance'' for components to correlate, clear. There was no such path on the orthogonal figures. This is because the rotation required the factors to be uncorrelated.

\textbf{An oblique three factor solution}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pafOBL3f }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{fa}\NormalTok{(items, }\AttributeTok{nfactors =} \DecValTok{3}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"oblimin"}\NormalTok{)}
\NormalTok{pafOBL3f}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  minres
Call: psych::fa(r = items, nfactors = 3, rotate = "oblimin")
Standardized loadings (pattern matrix) based upon correlation matrix
                        MR1   MR3   MR2   h2   u2 com
ValObjectives          0.14  0.40  0.10 0.30 0.70 1.4
IncrUnderstanding      0.03  0.81 -0.05 0.66 0.34 1.0
IncrInterest           0.00  0.78  0.13 0.69 0.31 1.1
ClearResponsibilities  0.97 -0.11 -0.04 0.79 0.21 1.0
EffectiveAnswers       0.68  0.18 -0.01 0.64 0.36 1.1
Feedback               0.69  0.00  0.15 0.58 0.42 1.1
ClearOrganization      0.64  0.27 -0.10 0.64 0.36 1.4
ClearPresentation      0.54  0.41 -0.02 0.74 0.26 1.9
MultPerspectives       0.53  0.07  0.36 0.65 0.35 1.8
InclusvClassrm         0.12  0.21  0.58 0.56 0.44 1.3
DEIintegration        -0.01 -0.02  0.88 0.75 0.25 1.0
EquitableEval          0.63 -0.01  0.19 0.52 0.48 1.2

                       MR1  MR3  MR2
SS loadings           3.80 2.18 1.56
Proportion Var        0.32 0.18 0.13
Cumulative Var        0.32 0.50 0.63
Proportion Explained  0.50 0.29 0.21
Cumulative Proportion 0.50 0.79 1.00

 With factor correlations of 
     MR1  MR3  MR2
MR1 1.00 0.65 0.43
MR3 0.65 1.00 0.31
MR2 0.43 0.31 1.00

Mean item complexity =  1.3
Test of the hypothesis that 3 factors are sufficient.

df null model =  66  with the objective function =  7.27 with Chi Square =  1897.77
df of  the model are 33  and the objective function was  0.29 

The root mean square of the residuals (RMSR) is  0.02 
The df corrected root mean square of the residuals is  0.03 

The harmonic n.obs is  267 with the empirical chi square  19.55  with prob <  0.97 
The total n.obs was  267  with Likelihood Chi Square =  75.2  with prob <  0.000039 

Tucker Lewis Index of factoring reliability =  0.954
RMSEA index =  0.069  and the 90 % confidence intervals are  0.049 0.09
BIC =  -109.18
Fit based upon off diagonal values = 1
Measures of factor score adequacy             
                                                   MR1  MR3  MR2
Correlation of (regression) scores with factors   0.96 0.93 0.91
Multiple R square of scores with factors          0.92 0.86 0.83
Minimum correlation of possible factor scores     0.84 0.71 0.65
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{paf\_tableOBL3f }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{print.psych}\NormalTok{(pafOBL3f, }\AttributeTok{cut =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  minres
Call: psych::fa(r = items, nfactors = 3, rotate = "oblimin")
Standardized loadings (pattern matrix) based upon correlation matrix
                      item   MR1   MR3   MR2   h2   u2 com
ClearResponsibilities    4  0.97             0.79 0.21 1.0
Feedback                 6  0.69             0.58 0.42 1.1
EffectiveAnswers         5  0.68             0.64 0.36 1.1
ClearOrganization        7  0.64             0.64 0.36 1.4
EquitableEval           12  0.63             0.52 0.48 1.2
ClearPresentation        8  0.54  0.41       0.74 0.26 1.9
MultPerspectives         9  0.53        0.36 0.65 0.35 1.8
IncrUnderstanding        2        0.81       0.66 0.34 1.0
IncrInterest             3        0.78       0.69 0.31 1.1
ValObjectives            1        0.40       0.30 0.70 1.4
DEIintegration          11              0.88 0.75 0.25 1.0
InclusvClassrm          10              0.58 0.56 0.44 1.3

                       MR1  MR3  MR2
SS loadings           3.80 2.18 1.56
Proportion Var        0.32 0.18 0.13
Cumulative Var        0.32 0.50 0.63
Proportion Explained  0.50 0.29 0.21
Cumulative Proportion 0.50 0.79 1.00

 With factor correlations of 
     MR1  MR3  MR2
MR1 1.00 0.65 0.43
MR3 0.65 1.00 0.31
MR2 0.43 0.31 1.00

Mean item complexity =  1.3
Test of the hypothesis that 3 factors are sufficient.

df null model =  66  with the objective function =  7.27 with Chi Square =  1897.77
df of  the model are 33  and the objective function was  0.29 

The root mean square of the residuals (RMSR) is  0.02 
The df corrected root mean square of the residuals is  0.03 

The harmonic n.obs is  267 with the empirical chi square  19.55  with prob <  0.97 
The total n.obs was  267  with Likelihood Chi Square =  75.2  with prob <  0.000039 

Tucker Lewis Index of factoring reliability =  0.954
RMSEA index =  0.069  and the 90 % confidence intervals are  0.049 0.09
BIC =  -109.18
Fit based upon off diagonal values = 1
Measures of factor score adequacy             
                                                   MR1  MR3  MR2
Correlation of (regression) scores with factors   0.96 0.93 0.91
Multiple R square of scores with factors          0.92 0.86 0.83
Minimum correlation of possible factor scores     0.84 0.71 0.65
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{fa.diagram}\NormalTok{(pafOBL3f)}
\end{Highlighting}
\end{Shaded}

\includegraphics{09-EFA_PAF_files/figure-latex/unnamed-chunk-103-1.pdf} Again, pretty similar.

\hypertarget{determine-which-factor-solution-e.g.-orthogonal-or-oblique-which-number-of-factors-you-will-suggest-1}{%
\subsection{Determine which factor solution (e.g., orthogonal or oblique; which number of factors) you will suggest}\label{determine-which-factor-solution-e.g.-orthogonal-or-oblique-which-number-of-factors-you-will-suggest-1}}

From the oblique output we see that the correlations between the three subscales range from 0.25 to 0.58. These are high. Therefore, I will choose a 3-factor, oblique, solution.

\hypertarget{apa-style-results-section-with-table-and-figure-of-one-of-the-solutions-1}{%
\subsection{APA style results section with table and figure of one of the solutions}\label{apa-style-results-section-with-table-and-figure-of-one-of-the-solutions-1}}

\begin{quote}
\begin{quote}
The dimensionality of the 12 course evaluation items was analyzed using principal axis factoring (PAF). First, data were screened to determine the suitability of the data for this analyses. Data screening were conducted to determine the suitability of the data for this analyses. The Kaiser-Meyer-Olkin measure of sampling adequacy (KMO; Kaiser, 1970) represents the ratio of the squared correlation between variables to the squared partial correlation between variables. KMO ranges from 0.00 to 1.00; values closer to 1.00 indicate that the patterns of correlations are relatively compact, and that component analysis should yield distinct and reliable components (Field, 2012). In our dataset, the KMO value was 0.91, indicating acceptable sampling adequacy. The Barlett's Test of Sphericity examines whether the population correlation matrix resembles an identity matrix (Field, 2012). When the \emph{p} value for the Bartlett's test is \textless{} .05, we are fairly certain we have clusters of correlated variables. In our dataset, \(\chi^2(66) = 1897.77, p < 0.001\) indicating the correlations between items are sufficiently large enough for principal components analysis. The determinant of the correlation matrix alerts us to any issues of multicollinearity or singularity and should be larger than 0.00001. Our determinant was 0.0007 and, again, indicated that our data was suitable for the analysis.
\end{quote}
\end{quote}

\begin{quote}
\begin{quote}
Four criteria were used to determine the number of components to extract: a priori theory, the scree test, the eigenvalue-greater-than-one criteria, and the interpretability of the solution. Kaiser's eigenvalue-greater-than-one criteria suggested one component and explained 51\% of the variance. The inflexion in the scree plot justified retaining one to three components. A priorili, we expected three factors -- which would explain 63\% of the variance. Correspondingly, we investigated two and three factor solutions with orthogonal (varimax) and oblique (oblimin) procedures. Given the significant correlations (ranging from .31 to .65) and the correspondence of items loading on the a priorili hypothesized components, we determined that an oblique, three-factor, solution was most appropriate.
\end{quote}
\end{quote}

\begin{quote}
\begin{quote}
The rotated solution, as shown in Table 1 and Figure 1, yielded three interpretable factors, each listed with the proportion of variance accounted for: traditional pedagogy (32\%), valued-by-me (18\%), and socially and culturally responsive pedagogy (13\%).
\end{quote}
\end{quote}

Regarding the Table 1, I would include a table with ALL the values, bolding those with component membership. This is easy, though, because we can export it to a .csv file and

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pafOBL3fb }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{fa}\NormalTok{(items, }\AttributeTok{nfactors =} \DecValTok{3}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"oblimin"}\NormalTok{)}
\NormalTok{paf\_tableOBL3fb }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{print.psych}\NormalTok{(pafOBL3fb, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  minres
Call: psych::fa(r = items, nfactors = 3, rotate = "oblimin")
Standardized loadings (pattern matrix) based upon correlation matrix
                      item   MR1   MR3   MR2   h2   u2 com
ClearResponsibilities    4  0.97 -0.11 -0.04 0.79 0.21 1.0
Feedback                 6  0.69  0.00  0.15 0.58 0.42 1.1
EffectiveAnswers         5  0.68  0.18 -0.01 0.64 0.36 1.1
ClearOrganization        7  0.64  0.27 -0.10 0.64 0.36 1.4
EquitableEval           12  0.63 -0.01  0.19 0.52 0.48 1.2
ClearPresentation        8  0.54  0.41 -0.02 0.74 0.26 1.9
MultPerspectives         9  0.53  0.07  0.36 0.65 0.35 1.8
IncrUnderstanding        2  0.03  0.81 -0.05 0.66 0.34 1.0
IncrInterest             3  0.00  0.78  0.13 0.69 0.31 1.1
ValObjectives            1  0.14  0.40  0.10 0.30 0.70 1.4
DEIintegration          11 -0.01 -0.02  0.88 0.75 0.25 1.0
InclusvClassrm          10  0.12  0.21  0.58 0.56 0.44 1.3

                       MR1  MR3  MR2
SS loadings           3.80 2.18 1.56
Proportion Var        0.32 0.18 0.13
Cumulative Var        0.32 0.50 0.63
Proportion Explained  0.50 0.29 0.21
Cumulative Proportion 0.50 0.79 1.00

 With factor correlations of 
     MR1  MR3  MR2
MR1 1.00 0.65 0.43
MR3 0.65 1.00 0.31
MR2 0.43 0.31 1.00

Mean item complexity =  1.3
Test of the hypothesis that 3 factors are sufficient.

df null model =  66  with the objective function =  7.27 with Chi Square =  1897.77
df of  the model are 33  and the objective function was  0.29 

The root mean square of the residuals (RMSR) is  0.02 
The df corrected root mean square of the residuals is  0.03 

The harmonic n.obs is  267 with the empirical chi square  19.55  with prob <  0.97 
The total n.obs was  267  with Likelihood Chi Square =  75.2  with prob <  0.000039 

Tucker Lewis Index of factoring reliability =  0.954
RMSEA index =  0.069  and the 90 % confidence intervals are  0.049 0.09
BIC =  -109.18
Fit based upon off diagonal values = 1
Measures of factor score adequacy             
                                                   MR1  MR3  MR2
Correlation of (regression) scores with factors   0.96 0.93 0.91
Multiple R square of scores with factors          0.92 0.86 0.83
Minimum correlation of possible factor scores     0.84 0.71 0.65
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pafOBL3fb\_table }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(pafOBL3fb}\SpecialCharTok{$}\NormalTok{loadings, }\DecValTok{3}\NormalTok{)}
\FunctionTok{write.table}\NormalTok{(pafOBL3fb\_table, }\AttributeTok{file =} \StringTok{"pafOBL3f\_table.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{col.names =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{pafOBL3fb\_table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Loadings:
                      MR1    MR3    MR2   
ValObjectives          0.140  0.398  0.103
IncrUnderstanding             0.810       
IncrInterest                  0.784  0.128
ClearResponsibilities  0.971 -0.108       
EffectiveAnswers       0.676  0.182       
Feedback               0.686         0.146
ClearOrganization      0.640  0.267       
ClearPresentation      0.543  0.405       
MultPerspectives       0.527         0.363
InclusvClassrm         0.121  0.207  0.580
DEIintegration                       0.880
EquitableEval          0.629         0.185

                 MR1   MR3   MR2
SS loadings    3.283 1.758 1.339
Proportion Var 0.274 0.146 0.112
Cumulative Var 0.274 0.420 0.532
\end{verbatim}

\hypertarget{explanation-to-grader-2}{%
\subsection{Explanation to grader}\label{explanation-to-grader-2}}

\hypertarget{confirmatory-factor-analysis-1}{%
\chapter*{Confirmatory Factor Analysis}\label{confirmatory-factor-analysis-1}}


\hypertarget{CFA1st}{%
\chapter{CFA: First Order Models}\label{CFA1st}}

\href{https://youtube.com/playlist?list=PLtz5cFLQl4KOVn2zYBDex9x_vvEcpXTG7\&si=eiwLumQjLOIuUbEG}{Screencasted Lecture Link}

This is the first in our series on confirmatory factor analysis (CFA).In this lesson we will compare CFA to principal axis factoring (PAF) and principal components analysis (PCA). We will specify, run, and interpret first order models that are unidimensional and multidimensional. We will compare models to each other and identify key issues in model specification.

\hypertarget{navigating-this-lesson-8}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-8}}

This lesson is just over two hours. I would add another two hours to work through and digest the materials.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}.

\hypertarget{learning-objectives-8}{%
\subsection{Learning Objectives}\label{learning-objectives-8}}

Focusing on this week's materials, make sure you can:

\begin{itemize}
\tightlist
\item
  Compare and contrast EFA and CFA.
\item
  Identify the components of item-level variance in CFA.
\item
  Specify CFA measurement models.
\item
  Interpret fit indices (e.g., Chi-square, CFI, RMSEA).
\item
  Interpret statistics used do compare two CFA models.
\end{itemize}

\hypertarget{planning-for-practice-8}{%
\subsection{Planning for Practice}\label{planning-for-practice-8}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. The least complex is to change the random seed in the research and rework the problem demonstrated in the lesson. The results \emph{should} map onto the ones obtained in the lecture.

The second option involves utilizing one of the simulated datasets available in this OER. The \protect\hyperlink{sims}{last lesson} in the OER contains three simulations that could be used for all of the statistics-based practice suggestions. Especially if you started with one of these examples in an earlier lesson, I highly recommend you continue with that.

Alternatively, Lewis and Neville's \citeyearpar{lewis_construction_2015} Gendered Racial Microaggressions Scale for Black Women was used in the lessons for exploratory factor analysis and Conover et al.'s \citeyearpar{conover_development_2017} Ableist Microaggressions Scale is used in the lesson on invariance testing. Both of these would be suitable for the CFA homework assignments.

As a third option, you are welcome to use data to which you have access and is suitable for CFA. This could include other simulated data, data found on an open access repository, data from the ReCentering Psych Stats survey described in the \protect\hyperlink{qualTRIX}{Qualtrics lesson}, or your own data (presuming you have permission to use it).

The suggestion for practice spans this chapter and the \protect\hyperlink{CFA2nd}{next}. From this assignment, you should plan to:

\begin{itemize}
\tightlist
\item
  Prepare the data frame for CFA.
\item
  Specify and run unidimensional and single order (with correlated factors) models.

  \begin{itemize}
  \tightlist
  \item
    In the next chapter, you will add the specification, evaluation, and write-up of second-order and bifactor models.
  \end{itemize}
\item
  Narrate the adequacy of fit with \(\chi ^{2}\), CFI, RMSEA, SRMR

  \begin{itemize}
  \tightlist
  \item
    Write a mini-results section for each
  \end{itemize}
\item
  Compare model fit with \(\chi ^{2}\Delta\), AIC, and BIC.
\item
  Write an APA style results sections with table(s) and figures.
\end{itemize}

\hypertarget{readings-resources-8}{%
\subsection{Readings \& Resources}\label{readings-resources-8}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

Byrne, B. M. (2016). Structural equation modeling with AMOS: Basic concepts, applications, and programming (3rd ed.). Routledge. \url{http://ebookcentral.proquest.com/lib/spu/detail.action?docID=4556523}

\begin{itemize}
\tightlist
\item
  Chapter 1, Structural Equation Modeling: The basics
\item
  Chapter 3, Application 1: Testing the Factorial Validity of a Theoretical Construct (First-Order CFA Model)
\item
  Chapter 4, Application 2: Testing the Factorial Validity of a Measurement Scale (First-Order CFA Model)
\end{itemize}

Dekay, Nicole (2021). Quick Reference Guide: The statistics for psychometrics \url{https://www.humanalysts.com/quick-reference-guide-the-statistics-for-psychometrics}

Kline, R. (2016). Principles and practice of structural equation modeling (Fourth ed., Methodology in the social sciences). New York: The Guilford Press.

\begin{itemize}
\tightlist
\item
  Chapter 9: Specification and Identification of Confirmatory Factor Analysis Models
\item
  Chapter 13: Analysis of Confirmatory Factor Analysis Models
\item
  Chapter 12: Global Fit Testing
\end{itemize}

Rosseel, Y. (2019). The \emph{lavaan} tutorial. Belgium: Department of Data Analysis, Ghent University. \url{http://lavaan.ugent.be/tutorial/tutorial.pdf}

\begin{itemize}
\tightlist
\item
  ``The model syntax'' pp.~3 - 4
\item
  ``A first example: confirmatory factor analysis (CFA)'' pp.~4-8.
\end{itemize}

\hypertarget{packages-8}{%
\subsection{Packages}\label{packages-8}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# will install the package if not already installed}
\CommentTok{\# if(!require(lavaan))\{install.packages(\textquotesingle{}lavaan\textquotesingle{})\}}
\CommentTok{\# if(!require(lavaanPlot))\{install.packages(\textquotesingle{}lavaanPlot\textquotesingle{})\}}
\CommentTok{\# if(!require(psych))\{install.packages(\textquotesingle{}psych\textquotesingle{})\}}
\CommentTok{\# if(!require(semTable))\{install.packages(\textquotesingle{}semTable\textquotesingle{})\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{two-broad-categories-of-factor-analysis-exploratory-and-confirmatory}{%
\section{Two Broad Categories of Factor Analysis: Exploratory and Confirmatory}\label{two-broad-categories-of-factor-analysis-exploratory-and-confirmatory}}

Kline \citeyearpar{kline_principles_2016} described confirmatory factor analysis as ``exactly half that of SEM -- the other half comes from regression analysis'' (p.~189).

\hypertarget{common-to-both-exploratory-and-confirmatory-approaches}{%
\subsection{Common to Both Exploratory and Confirmatory Approaches}\label{common-to-both-exploratory-and-confirmatory-approaches}}

In both exploratory and confirmatory approaches, the variance of each indicator/item is divided into \textbf{common} and \textbf{unique} variance. When we assume that variance is 1.0, the common variance becomes the communality. If we have 8 items, we will have 8 communalities, and this represents the common variance explained by the factors or components.

\begin{itemize}
\tightlist
\item
  \textbf{Common variance} is shared among the indicators and serves as a basis for observed covariances among them that depart, meaningfully, from zero. We generally assume that:

  \begin{itemize}
  \tightlist
  \item
    Common variance is due to the factors.
  \item
    There will be fewer factors than the number of indicators/items. After all, there is no point in retaining as many factors {[}explanatory entities{]} as there are entities to be explained {[}indicators/items{]}.
  \item
    The proportion of total variance that is shared is the \textbf{communality} (estimated by \(h^2\)); if \(h^2\) =.70, then 70\% of the total indicator variance is common and potentially explained by the factors.
  \end{itemize}
\item
  \textbf{Unique variance} consists of

  \begin{itemize}
  \tightlist
  \item
    \textbf{specific variance}

    \begin{itemize}
    \tightlist
    \item
      systematic variance that is not explained by any factor in the model,
    \end{itemize}
  \item
    \textbf{random measurement error},
  \item
    \textbf{method variance}, which is not represented in the figure, but could be another source of unique variance.
  \end{itemize}
\item
  In factor analysis, summing the communalities represents the total common variance (a portion of the total variance), but not the total variance.
\end{itemize}

Factor analysis, then, aligns well with classic test theory and classic approaches to understanding reliability (observed score = true score + error). The inclusion of error is illustrated well in the classic illustrations of CFA and SEM where each item/indicator includes common variance (from the factor) and error variance.

Recall that in principal components analysis (PCA is not factor analysis) one of the key distinctions is that all variance is common variance (there is no unique variance). Total common variance is equal to the total variance explained, which in turn is equal to the total variance.

\begin{figure}
\centering
\includegraphics{images/CFA1st/FactorVariance.png}
\caption{Figure illustrating the unique and common variance associated with a factor}
\end{figure}

\hypertarget{differences-between-efa-and-cfa}{%
\subsection{Differences between EFA and CFA}\label{differences-between-efa-and-cfa}}

Below are contrasts between \emph{exploratory} and \emph{confirmatory} factor analysis.

\begin{itemize}
\tightlist
\item
  \textbf{A priori specification of the number of factors}

  \begin{itemize}
  \tightlist
  \item
    EFA requires no a priori specification; prior to extraction an EFA program will extract as many factors as indicators. Typically, in subsequent analyses, the researchers specify how many factors to extract.
  \item
    CFA requires researchers to specify the exact number of factors.
  \end{itemize}
\item
  \textbf{The degree of ``exact correspondence'' between indicators/items and factors/scales}

  \begin{itemize}
  \tightlist
  \item
    EFA is an \textbf{unrestricted measurement model} That is, indicators/items depend on (theoretically, measure) all factors. The direct effects from factors to indicators are \emph{pattern coefficients}. Kline \citeyearpar{kline_principles_2016} says that most refer to these as \emph{factor loadings} or just \emph{loadings} but because he believes these terms are ambiguous, he refers to the direct effects as \emph{pattern coefficients}. We assign them to factors based on their highest loadings (and hopefully no cross-loadings). Depending on whether we select an orthogonal or oblique relationship, correlations between factors will be permitted or suppressed.
  \item
    CFA is a \textbf{restricted measurement model}. The researcher specifies the factor(s) on which each indicator/item(s) depends (recall, the causal direction in CFA is from factor to indicators/items.)
  \end{itemize}
\item
  \textbf{Identification status}: The \emph{identification} of a model has to do with whether it is theoretically possible for a statistics package to derive a unique set of model parameter estimates. Identification is related to model \emph{degrees of freedom}; we will later explore under-, just-, and over-identified models. For now:

  \begin{itemize}
  \tightlist
  \item
    EFA models with multiple factors are \emph{unidentified} because they will have more free parameters than observations. Thus, there is no unique set of statistical estimates for the multifactor EFA model, consequently this requires the rotation phase in EFA.
  \item
    CFA models must be identified before they can be analyzed so there is only one unique set of parameter estimates. Correspondingly, there is no rotation phase in CFA.
  \end{itemize}
\item
  \textbf{Sharing variances}

  \begin{itemize}
  \tightlist
  \item
    In EFA the specific variance of each indicator is not shared with that of any other indicator.
  \item
    In CFA, the researchers can specify if variance is shared between certain pairs of indicators (i.e., error covariances).
  \end{itemize}
\end{itemize}

\hypertarget{on-the-relationship-between-efa-and-cfa}{%
\subsection{On the relationship between EFA and CFA}\label{on-the-relationship-between-efa-and-cfa}}

Kline \citeyearpar{kline_principles_2016} admonishes us to not overinterpret the labels ``exploratory'' and ``confirmatory''. Why?

\begin{itemize}
\tightlist
\item
  EFA requires no a priori hypotheses about the relationship between indicators/items and factors, but researchers often expect to specify a predetermined number of factors.
\item
  CFA is not strictly confirmatory. After initial runs, many researchers modify models and hypotheses.
\end{itemize}

CFA is not a verification or confirmation of EFA results for the same data and number of factors. Kline \citeyearpar{kline_principles_2016} does not recommend that researchers follow a model retained from EFA. Why?

\begin{itemize}
\tightlist
\item
  It is possible that the CFA model will be rejected. Oftentimes this is because the secondary coefficients (i.e., non-primary pattern coefficients) accounted for a significant proportion of variance in the model. When they are constrained to 0.0 in the CFA model, the model fit will suffer.
\item
  If the CFA model is retained, then it is possible that both EFA and CFA capitalized on chance variation. Thus, if verification via CFA is desired, it should be evaluated through a replication sample.
\end{itemize}

\hypertarget{exploring-a-standard-cfa-model}{%
\section{Exploring a Standard CFA Model}\label{exploring-a-standard-cfa-model}}

The research vignette for today is a fairly standard CFA model.

\begin{figure}
\centering
\includegraphics{images/CFA1st/GRMSAAW_CorrFactors.png}
\caption{Image of the GRMSAAW represented as a standard CFA model}
\end{figure}

The image represents represents the hypothesis that \(AS_1 - AS_9\), \(AF_1 - AF_4\), \(MI_1 - MI_5\), and \(AUA_1 - AUA_4\) measure, respectively, the AS, AF, MI, and AUA factors, which are assumed to covary. Specifically,in this model:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each indicator is continuous with two causes: \(AS\) --\textgreater{} \(AS_1\) \textless-- \(E_1\)

  \begin{itemize}
  \tightlist
  \item
    a single factor that the indicator is supposed to measure, and
  \item
    all unique sources of influence represented by the error term
  \end{itemize}
\item
  The error terms are independent of each other and of the factors
\item
  All associations are linear and the factors covary.

  \begin{itemize}
  \tightlist
  \item
    Hence, the symbol for an unanalyzed association is a solid line.
  \end{itemize}
\item
  Each item has a single \emph{pattern coefficient} (i.e., often more casually termed as a ``factor loading'')

  \begin{itemize}
  \tightlist
  \item
    All other potential pattern coefficients are set to ``0.00.'' These are \emph{hard hypotheses} and are specified by their absence (i.e., not specified in the code or in the diagram).
  \end{itemize}
\item
  \emph{Structure coefficients} are the Pearson correlations between factors and continuous indicators. They reflect any source of association, causal or non-causal. Sometimes the association is an undirected, back-door path. There is no pattern coefficient for \(AS_2\) \textless-\textgreater{} \(AF\), but there is a connection from \(AS_2\) to \(AF\) via the \(AS\) \textless--\textgreater{} \(AF\) covariance.
\item
  \emph{Scaling constants} (aka \emph{unit loading identification {[}ULI{]} constraints}) are necessary to scale the factors in a metric related to that of the explained (common) variance of the corresponding indicator, or \emph{reference (marker) variable}. In the figure these are the dashed-line paths from \(AS\) --\textgreater{} \(AS_1\), \(AF\) --\textgreater{} \(AF1\), \(MI\) --\textgreater{} \(MI1\) and \(AUA\) --\textgreater{} \(AUA1\).
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Selecting the reference marker variable is usually arbitrary and selected by the computer program as the first (or last) variable in the code/path. So long as all the indicator variables of the same factor have equally reliable scores, this works satisfactorily.
\item
  Additional scaling constants are found for each of the errors and indicators.
\end{itemize}

\hypertarget{model-identification-for-cfa}{%
\subsection{Model Identification for CFA}\label{model-identification-for-cfa}}

SEM, in general, requires that all models be \emph{identified.} Measurement models analyzed in CFA share this requirement, but identification is more straightforward than in other models.

Standard CFA models are sufficiently identified when:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A single factor model has at least three indicators.
\item
  In a model with two or more factors, each factor has two or more indicators. There are some caveats and arguments:

  \begin{itemize}
  \tightlist
  \item
    Some recommend at least three to five indicators per factor to prevent technical problems with statistical identification.
  \item
    In a recent SEM workshop, Todd Little indicated that optimal fit will occur when factors are \emph{just-identified} with three items per factor.

    \begin{itemize}
    \tightlist
    \item
      Of course, three factors may be insufficient to represent the construct definition.
    \end{itemize}
  \end{itemize}
\end{enumerate}

Identification becomes much more complicated than this, but for today's models this instruction is sufficient.

\hypertarget{selecting-indicatorsitems-for-a-reflective-measurement}{%
\subsection{Selecting Indicators/Items for a Reflective Measurement}\label{selecting-indicatorsitems-for-a-reflective-measurement}}

\emph{Reflective measurement} is another term to describe the circumstance where latent variables are assumed to cause observed variables. Observed variables in reflective measurement are called \emph{effect (reflective) indicators}.

\begin{itemize}
\tightlist
\item
  At least three for a unidimensional model; at least two per factor for a multidimensional model (but more is safer).
\item
  The items/indicators should have reasonable internal consistency and correlate with each other.
\item
  If the scale is multidimensional (i.e., with subscales) items should correlate more highly with other items in their factors than with items on other factors.
\item
  Negative correlations reduce the reliability of factor measurement, so they should be reverse coded prior to analysis.
\item
  Do not be tempted to specify a factor with indicators that do not measure something. A common mistake is to create a ``background'' factor and include indicators such as gender, ethnicity, and level of education. \emph{Just what is the predicted relationship between gender and ethnicity?}
\end{itemize}

\hypertarget{cfa-workflow}{%
\section{CFA Workflow}\label{cfa-workflow}}

Below is a screenshot of a CFA workflow. The original document is located in the \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub site} that hosts the ReCentering Psych Stats: Psychometrics OER.

\begin{figure}
\centering
\includegraphics{images/CFA1st/CFA_workflow.png}
\caption{Image of a workflow for specifying and evaluating a confirmatory factor analytic model}
\end{figure}

Because the intended audience for the ReCentering Psych Stats OER is the scientist-practitioner-advocate, this lesson focuses on the workflow and decisions in straightforward CFA models. As you might guess, the details of CFA can be quite complex and require more investigation and decision-making in models that pose more complexity or empirical challenges. The following are the general steps in a CFA.

\begin{itemize}
\tightlist
\item
  Creating an items-only dataframe where any items are scaled in the same direction (e.g., negatively worded items are reverse-scored).
\item
  Determining a factor structure that is \emph{identified}.

  \begin{itemize}
  \tightlist
  \item
    A single factor (unidimensional) model has at least three items/indicators.
  \item
    Multidimensional models have at least two items per factor.
  \end{itemize}
\item
  Specify a series of models, these typically include:

  \begin{itemize}
  \tightlist
  \item
    a unidimensional model (all items on a single factor),
  \item
    a single order structure with correlated factors,
  \item
    a second order structure,
  \item
    a bifactor structure.
  \end{itemize}
\item
  Evaluate model fit with a variety of indicators, including:

  \begin{itemize}
  \tightlist
  \item
    factor loadings,
  \item
    fit indices.
  \end{itemize}
\item
  Compare models.
\item
  In the event of poor model fit, investigate modification indices and consider respecification by:

  \begin{itemize}
  \tightlist
  \item
    eliminating items,
  \item
    changing factor membership,
  \item
    allowing errors to covary.
  \end{itemize}
\end{itemize}

\hypertarget{cfa-in-lavaan-requires-fluency-with-the-syntax}{%
\subsection{\texorpdfstring{CFA in \emph{lavaan} Requires Fluency with the Syntax}{CFA in lavaan Requires Fluency with the Syntax}}\label{cfa-in-lavaan-requires-fluency-with-the-syntax}}

\begin{itemize}
\item
  It's really just regression

  \begin{itemize}
  \tightlist
  \item
    tilda (\textasciitilde, \emph{is regressed on}) is regression operator
  \item
    place DV (y) on left of operator
  \item
    place IVs, separate by + on the right
  \end{itemize}
\item
  f is a latent variable (LV)
\item
  Example: y \textasciitilde{} f1 + f2 + x1 + x2
\item
  LVs must be \emph{defined} by their manifest or latent indicators.

  \begin{itemize}
  \tightlist
  \item
    the special operator (=\textasciitilde, \emph{is measured/defined by}) is used for this
  \item
    Example: f1 =\textasciitilde{} y1 + y2 + y3
  \end{itemize}
\item
  Variances and covariances are specified with a double tilde operator (\textasciitilde\textasciitilde, \emph{is correlated with})

  \begin{itemize}
  \tightlist
  \item
    Example of variance: y1 \textasciitilde\textasciitilde{} y1 (the relationship with itself)
  \item
    Example of covariance: y1 \textasciitilde\textasciitilde{} y2 (relationship with another variable)
  \item
    Example of covariance of a factor: f1 \textasciitilde\textasciitilde{} f2
  \end{itemize}
\end{itemize}

*Intercepts (\textasciitilde{} 1) for observed and LVs are simple, intercept-only regression formulas + Example of variable intercept: y1 \textasciitilde{} 1 + Example of factor intercept: f1 \textasciitilde{} 1

A complete lavaan model is a combination of these formula types, enclosed between single quotation models. Readability of model syntax is improved by:

\begin{itemize}
\tightlist
\item
  splitting formulas over multiple lines
\item
  using blank lines within single quote
\item
  labeling with the hashtag
\end{itemize}

myModel \textless- '\# regressions y1 + y2 \textasciitilde{} f1 + f2 + x1 + x2 f1 \textasciitilde{} f2 + f3 f2 \textasciitilde{} f3 + x1 + x2

\begin{verbatim}
        # latent variable definitions
        f1 =~ y1 + y2 + y3
        f2 =~ y4 + y5 + y6
        f3 =~ y7 + y8 + y9 + y10
        
        # variances and covariances
        y1 ~~ y1
        y2 ~~ y2
        f1 ~~ f2
        
        # intercepts
        y1 ~ 1
        fa ~ 1
\end{verbatim}

\hypertarget{differing-factor-structures}{%
\subsection{Differing Factor Structures}\label{differing-factor-structures}}

All models worked in this lesson are \emph{first-order} (or single-order) models; in the next lesson we extend to hierarchical and bifactor models. To provide an advanced cognitive organizer, let's take a look across the models.

\begin{figure}
\centering
\includegraphics{images/CFA1st/quadrant.png}
\caption{Image of first order (uncorrelated and correlated, second order, and bifactor structures)}
\end{figure}

Models A and B are first-order models. Note that all factors are on a single plane.

\begin{itemize}
\tightlist
\item
  Model A is unidimensional, each item is influenced by a single common factor and a term that includes systematic and random error. Note that there is only one \emph{systematic} source of variance for each item, and it is from a single source.
\item
  Model B is often referred to as a ``correlated traits'' model. Here, the larger construct is separated into distinct-yet-correlated elements. The variance of each item is assumed to be a weighted linear function of two or more common factors.\\
\item
  Models C is a second-order factor structure. Rather than merely being correlated, factors are related because they share a common cause. In this model, the second order factor \emph{explains} why three or more traits are correlated. Note that here is no direct relationship between the item and the target construct. Rather, the relationship between the second-order factor and each item is mediated through the primary factor (yes, an indirect effect!).
\item
  Model D is a bifactor structure. Here each item loads on a general factor. This general factor (bottom row) reflects what is common among the items and represents the individual differences on the target dimension that a researcher is most interested in. Group factors (top row) are now specified as \emph{orthogonal}. The group factors represent common factors measured by the items that explain item response variation not accounted for by the general factor. In some research scenarios, the group factors are termed ``nuisance'' dimensions. That is, that which they have in common interferes with measuring the primary target of interest.
\end{itemize}

\hypertarget{research-vignette-7}{%
\section{Research Vignette}\label{research-vignette-7}}

This lesson's research vignette emerges from Keum et al's Gendered Racial Microaggressions Scale for Asian American Women (GRMSAAW; \citep{keum_gendered_2018}). The article reports on two separate studies that comprised the development, refinement, and psychometric evaluation of two, parallel, versions (stress appraisal, frequency) of scale. I simulated data from the final construction of the frequency version as the basis of the lecture. If the scale looks somewhat familiar it is because the authors used the Gendered Racial Microaggressions Scale for Black Women \citep{lewis_construction_2015} as a model.

Keum et al. \citeyearpar{keum_gendered_2018} reported support for a total scale score (22 items) and four subscales. Below, I list the four subscales, their number of items, and a single example item. At the outset, let me provide a content advisory For those who hold this particular identity (or related identities) the content in the items may be upsetting. In other lessons, I often provide a variable name that gives an indication of the primary content of the item. In the case of the GRMSAAW, I will simply provide an abbreviation of the subscale name and its respective item number. This will allow us to easily inspect the alignment of the item with its intended factor, and hopefully minimize discomfort. If you are not a member of this particular identity, I encourage you to learn about these microaggressions by reading the article in its entirety. Please do not ask members of this group to explain why these microaggressions are harmful or ask if they have encountered them.

There are 22 items on the GRMSAAW scale. Using the same item stems, the authors created two scales. One assesses frequency of the event, the second assesses the degree of stressfulness. I simulated data from the stressfulness scale. Its Likert style scaling included: 0 (\emph{not at all stressful}), 1(\emph{slightly stressful}), 2(\emph{somewhat stressful}), 3(\emph{moderately stressful}), 4(\emph{very stressful}), and 5(\emph{extremely stressful}).

The four factors, number of items, and sample item are as follows:

\begin{itemize}
\tightlist
\item
  Ascribed Submissiveness (9 items)

  \begin{itemize}
  \tightlist
  \item
    Others expect me to be submissive. (AS1)
  \item
    Others have been surprised when I disagree with them. (AS2)
  \item
    Others take my silence as a sign of compliance. (AS3)
  \item
    Others have been surprised when I do things independent of my family. (AS4)
  \item
    Others have implied that AAW seem content for being a subordinate. (AS5)
  \item
    Others treat me as if I will always comply with their requests. (AS6)
  \item
    Others expect me to sacrifice my own needs to take care of others (e.g., family, partner) because I am an AAW. (AS7)
  \item
    Others have hinted that AAW are not assertive enough to be leaders. (AS8)
  \item
    Others have hinted that AAW seem to have no desire for leadership. (AS9)
  \end{itemize}
\item
  Asian Fetishism (4 items)

  \begin{itemize}
  \tightlist
  \item
    Others express sexual interest in me because of my Asian appearance. (AF1)
  \item
    Others take sexual interest in AAW to fulfill their fantasy. (AF2)
  \item
    Others take romantic interest in AAW just because they never had sex with an AAW before. (AF3)
  \item
    Others have treated me as if I am always open to sexual advances. (AF4)
  \end{itemize}
\item
  Media Invalidation (5 items)

  \begin{itemize}
  \tightlist
  \item
    I see non-Asian women being casted to play female Asian characters.(MI1)
  \item
    I rarely see AAW playing the lead role in the media. (MI2)
  \item
    I rarely see AAW in the media. (MI3)
  \item
    I see AAW playing the same type of characters (e.g., Kung Fu woman, sidekick, mistress, tiger mom) in the media. (MI4)
  \item
    I see AAW characters being portrayed as emotionally distant (e.g., cold-hearted, lack of empathy) in the media. (MI5)
  \end{itemize}
\item
  Assumptions of Universal Appearance (4 items)

  \begin{itemize}
  \tightlist
  \item
    Others have talked about AAW as if they all have the same facial features (e.g., eye shape, skin tone). (AUA1)
  \item
    Others have suggested that all AAW look alike.(AUA2)
  \item
    Others have talked about AAW as if they all have the same body type (e.g., petite, tiny, small-chested). (AUA3)
  \item
    Others have pointed out physical traits in AAW that do not look `Asian'.
  \end{itemize}
\end{itemize}

Four additional scales were reported in the Keum et al.~article \citep{keum_gendered_2018}. Fortunately, I was able to find factor loadings from the original psychometric article or subsequent publications. For multidimensional scales, I assign assign variable names according to the scale to which the item belongs (e.g., Env42). In contrast, when subscales or short unidimensional scales were used, I assigned variable names based on item content (e.g., ``blue''). In my own work, I prefer item-level names so that I can quickly see (without having to look up the item names) how the items are behaving. The scales, their original citation, and information about how I simulated data for each are listed below.

\begin{itemize}
\tightlist
\item
  \textbf{Racial Microaggressions Scale} (RMAS; \citep{torres-harding_racial_2012}) is a 32-item scale with Likert scaling ranging from 0 (\emph{never}) to 3 (\emph{often/frequent}). Higher scores represent greater frequency of perceived microaggressions. I simulated data at the subscale level. The RMAS has six subscales, but only four (Invisibility, Low-Achieving/Undesirable Culture, Foreigner/Not Belonging, and Environmental Invalidation) were used in the study. Data were simulated using factor loadings (from the four factors) in the source article.
\item
  \textbf{Schedule of Sexist Events} (SSE; \citep{klonoff_schedule_1995}) is a 20-item scale that with Likert scaling ranging from 1 (\emph{the event has never happened to me}) to 6 (\emph{the event happened almost all {[}i.e., more than 70\%{]} of the time}). Higher scores represent greater frequency of everyday sexist events. I simulated data the subscale level. Within two larger scales (recent events, lifetime events), there are three subscales: Sexist Degradation and Its Consequences, Unfair/Sexist Events at Work/School, and Unfair Treatment in Distant and Close Relationships. Data were simulated using factor loadings from the source article.
\item
  \textbf{PHQ-9} \citep{kroenke_phq-9_2001} is a 9-item scale with Likert scaling ranging from 0 (\emph{not at all}) to 3 (\emph{nearly every day}). Higher scores indicate higher levels of depression. I simulated data by estimating factor loadings from Brattmyr et al. \citeyearpar{brattmyr_factor_2022}.
\item
  \textbf{Internalized Racism in Asian American Scale} (IRAAS \citep{choi_development_2017}) is a 14-item scale with Likert scaling ranging from 1 (\emph{strongly disagree}) to 6 (\emph{strongly agree}). Higher scores indicate greater internalized racism. Data were simulated using the factor loadings from the bifactor model in the source article.
\end{itemize}

As you consider homework options, there is sufficient simulated data to use the RMAS, SSE, or IRAAS.

Below, I walk through the data simulation. This is not an essential portion of the lesson, but I will lecture it in case you are interested. None of the items are negatively worded (relative to the other items), so there is no need to reverse-score any items.

Simulating the data involved using factor loadings, means, standard deviations, and correlations between the scales. Because the simulation will produce ``out-of-bounds'' values, the code below rescales the scores into the range of the Likert-type scaling and rounds them to whole values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Entering the intercorrelations, means, and standard deviations from the journal article}

\NormalTok{Keum\_GRMS\_generating\_model }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}}
\StringTok{        \#measurement model}
\StringTok{        General =\textasciitilde{} .50*AS1 + .44*AS2 + .50*AS3 + .33*AS4 + .58*AS5 + .49*AS6 + .51*AS7 + .53*AS8 + .50*AS9 + .53*AF1 + .74*AF2 + .54*AF3 + .52*AF4 + .64*AUA1 + .59*AUA2 + .67*AUA3 + .64*AUA4 + .59*MI1 + .50*MI2 + .52*MI3 + .40*MI4 + .55*MI5}
\StringTok{        AS =\textasciitilde{} .68*AS1 + .65*AS2 + .53*AS3 + .55*AS4 + .54*AS5 + .55*AS6 + .42*AS7 + .47*AS8 + .50*AS9}
\StringTok{        AF =\textasciitilde{} .63*AF1 + .45*AF2 + .56*AF3 + .54*AF4}
\StringTok{        AUA =\textasciitilde{} .55*AUA1 + .55*AUA2 + .31*AUA3 + .31*AUA4}
\StringTok{        MI =\textasciitilde{} .27*MI1 + .53*MI2 + .57*MI3 + .29*MI4 + .09*MI5}
\StringTok{        RMAS\_FOR =\textasciitilde{} .66*FOR1 + .90*FOR2 + .63*FOR4}
\StringTok{        RMAS\_LOW =\textasciitilde{} .64*LOW22 + .54*LOW23 + .49*LOW28 + .63*LOW29 + .58*LOW30 + .67*LOW32 + .67*LOW35 + .76*LOW36 + .72*LOW37}
\StringTok{        RMAS\_INV =\textasciitilde{} .66*INV33 + .70*INV39 + .79*INV40 + .71*INV41 + .71*INV47 + .61*INV49 + .65*INV51 + .70*INV52}
\StringTok{        RMAS\_ENV =\textasciitilde{} .71*ENV42 + .70*ENV43 + .74*ENV44 + .57*ENV45 + .54*ENV46}
\StringTok{        }
\StringTok{        SSEL\_Deg =\textasciitilde{} .77*LDeg18 + .73*LDeg19 + .71*LDeg21 + .71*LDeg15 + .67*LDeg16 + .67*LDeg13 + .62*LDeg14 + .58*LDeg20}
\StringTok{        SSEL\_dRel =\textasciitilde{} .69*LdRel4 + .68*LdRel6 + .64*LdRel7 + .64*LdRel5 + .63*LdRel1 + .49*LdRel3}
\StringTok{        SSEL\_cRel =\textasciitilde{} .73*LcRel11 + .68*LcRel9 + .66*LcRel23}
\StringTok{        SSEL\_Work =\textasciitilde{} .73*LWork17 + .10*LWork10 + .64*LWork2}
\StringTok{        }
\StringTok{        SSER\_Deg =\textasciitilde{} .72*RDeg15 + .71*RDeg21 + .69*RDeg18 + .68*RDeg16 + .68*RDeg13 + .65*RDeg19 + .58*RDeg14 + .47*RDeg20}
\StringTok{        SSER\_dRel =\textasciitilde{} .74*RDeg4 + .67*RDeg6 + .64*RDeg5 + .54*RDeg7 + .51*RDeg1}
\StringTok{        SSER\_cRel =\textasciitilde{} .69*RcRel9 + .59*RcRel11 + .53*RcRel23}
\StringTok{        SSER\_Work =\textasciitilde{} .72*RWork10 + .67*RWork2 + .62*RWork17 + .51*RWork3}
\StringTok{        }
\StringTok{        SSE\_Lifetime =\textasciitilde{} SSEL\_Deg + SSEL\_dRel + SSEL\_cRel + SSEL\_Work}
\StringTok{        SSE\_Recent =\textasciitilde{} SSER\_Deg + SSER\_dRel + SSEL\_cRel + SSER\_Work}
\StringTok{        }
\StringTok{        PHQ9 =\textasciitilde{} .798*anhedonia + .425*down +  .591*sleep +  .913*lo\_energy +  .441*appetite +  .519*selfworth +  .755*concentration +  .454*too\_slowfast + .695*s\_ideation}
\StringTok{        }
\StringTok{        gIRAAS =\textasciitilde{} .51*SN1 + .69*SN2 + .63*SN3 + .65*SN4 + .67*WS5 + .60*WS6 + .74*WS7 + .44*WS8 + .51*WS9 + .79*WS10 + .65*AB11 + .63*AB12 + .68*AB13 + .46*AB14}
\StringTok{        SelfNegativity =\textasciitilde{} .60*SN1 + .50*SN2 + .63*SN3 + .43*SN4}
\StringTok{        WeakStereotypes =\textasciitilde{} .38*WS5 + .22*WS6 + .10*WS7 + .77*WS8 + .34*WS9 + .14*WS10}
\StringTok{        AppearanceBias =\textasciitilde{} .38*AB11 + .28*AB12 + .50*AB13 + .18*AB14}
\StringTok{        }
\StringTok{        }
\StringTok{        \#Means}
\StringTok{        \#Keum et al reported total scale scores, I divided those totals by the number of items per scale for  mean scores}
\StringTok{         AS \textasciitilde{} 3.25*1}
\StringTok{         AF \textasciitilde{} 3.34*1}
\StringTok{         AUA \textasciitilde{} 4.52}
\StringTok{         MI \textasciitilde{} 5.77*1}
\StringTok{         General \textasciitilde{} 3.81*1}
\StringTok{         RMAS\_FOR \textasciitilde{} 3.05*1}
\StringTok{         RMAS\_LOW \textasciitilde{} 2.6*1}
\StringTok{         RMAS\_INV \textasciitilde{} 2.105*1}
\StringTok{         RMAS\_ENV \textasciitilde{} 3.126*1}
\StringTok{         SSEL\_Deg \textasciitilde{} 2.55*1}
\StringTok{         SSEL\_dRel \textasciitilde{} 1.96*1}
\StringTok{         SSEL\_cRel \textasciitilde{} 3.10*1}
\StringTok{         SSEL\_Work \textasciitilde{} 1.66*1}
\StringTok{         SSER\_Deg \textasciitilde{} 2.02*1}
\StringTok{         SSER\_dRel \textasciitilde{} 1.592*1}
\StringTok{         SSER\_cRel \textasciitilde{} 1.777*1}
\StringTok{         SSER\_Work \textasciitilde{} 1.3925*1}
\StringTok{         SSER\_Lifetime \textasciitilde{} 2.8245*1}
\StringTok{         SSER\_Recent \textasciitilde{} 2.4875*1}
\StringTok{         PHQ9 \textasciitilde{} 1.836*1}
\StringTok{         gIRAAS \textasciitilde{} 2.246*1}
\StringTok{         }
\StringTok{        \#Correlations}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .00*AF}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .00*AUA}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .00*MI}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .00*General}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .28*RMAS\_FOR}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .24*RMAS\_LOW}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .46*RMAS\_INV}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .16*RMAS\_ENV}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .40*SSE\_Lifetime}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .28*SSE\_Recent}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .15*PHQ9}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .13*gIRAAS}
\StringTok{         }
\StringTok{         AF \textasciitilde{}\textasciitilde{} .00*AUA}
\StringTok{         AF \textasciitilde{}\textasciitilde{} .00*MI}
\StringTok{         AF \textasciitilde{}\textasciitilde{} .00*General}
\StringTok{         AF \textasciitilde{}\textasciitilde{} .02*RMAS\_FOR}
\StringTok{         AF \textasciitilde{}\textasciitilde{} .05*RMAS\_LOW}
\StringTok{         AF \textasciitilde{}\textasciitilde{} .11*RMAS\_INV}
\StringTok{         AF \textasciitilde{}\textasciitilde{} .07*RMAS\_ENV}
\StringTok{         AF \textasciitilde{}\textasciitilde{} .34*SSE\_Lifetime}
\StringTok{         AF \textasciitilde{}\textasciitilde{} .27*SSE\_Recent}
\StringTok{         AF \textasciitilde{}\textasciitilde{} {-}.04*PHQ9}
\StringTok{         AF \textasciitilde{}\textasciitilde{} .21*gIRAAS}
\StringTok{          }
\StringTok{         AUA \textasciitilde{}\textasciitilde{} .00*MI}
\StringTok{         AUA \textasciitilde{}\textasciitilde{} .00*General}
\StringTok{         AUA \textasciitilde{}\textasciitilde{} .18*RMAS\_FOR}
\StringTok{         AUA \textasciitilde{}\textasciitilde{} .20*RMAS\_LOW}
\StringTok{         AUA \textasciitilde{}\textasciitilde{} .01*RMAS\_INV}
\StringTok{         AUA \textasciitilde{}\textasciitilde{} {-}.04*RMAS\_ENV}
\StringTok{         AUA \textasciitilde{}\textasciitilde{} .02*SSE\_Lifetime}
\StringTok{         AUA \textasciitilde{}\textasciitilde{} .92*SSE\_Recent}
\StringTok{         AUA \textasciitilde{}\textasciitilde{} .02*PHQ9}
\StringTok{         AUA \textasciitilde{}\textasciitilde{} .17*gIRAAS}
\StringTok{         }
\StringTok{        }
\StringTok{         MI \textasciitilde{}\textasciitilde{} .00*General}
\StringTok{         MI \textasciitilde{}\textasciitilde{} {-}.02*RMAS\_FOR}
\StringTok{         MI \textasciitilde{}\textasciitilde{} .08*RMAS\_LOW}
\StringTok{         MI \textasciitilde{}\textasciitilde{} .31*RMAS\_INV}
\StringTok{         MI \textasciitilde{}\textasciitilde{} .36*RMAS\_ENV}
\StringTok{         MI \textasciitilde{}\textasciitilde{} .15*SSE\_Lifetime}
\StringTok{         MI \textasciitilde{}\textasciitilde{} .08*SSE\_Recent}
\StringTok{         MI \textasciitilde{}\textasciitilde{} {-}.05*PHQ9}
\StringTok{         MI \textasciitilde{}\textasciitilde{} {-}.03*gIRAAS}
\StringTok{         }
\StringTok{         General \textasciitilde{}\textasciitilde{} .34*RMAS\_FOR}
\StringTok{         General \textasciitilde{}\textasciitilde{} .63*RMAS\_LOW}
\StringTok{         General \textasciitilde{}\textasciitilde{} .44*RMAS\_INV}
\StringTok{         General \textasciitilde{}\textasciitilde{} .45*RMAS\_ENV}
\StringTok{         General \textasciitilde{}\textasciitilde{} .54*SSE\_Lifetime}
\StringTok{         General \textasciitilde{}\textasciitilde{} .46*SSE\_Recent}
\StringTok{         General \textasciitilde{}\textasciitilde{} .31*PHQ9}
\StringTok{         General \textasciitilde{}\textasciitilde{} {-}.06*gIRAAS}
\StringTok{         }
\StringTok{         RMAS\_FOR \textasciitilde{}\textasciitilde{} .57*RMAS\_LOW}
\StringTok{         RMAS\_FOR \textasciitilde{}\textasciitilde{} .56*RMAS\_INV}
\StringTok{         RMAS\_FOR \textasciitilde{}\textasciitilde{} .37*RMAS\_ENV}
\StringTok{         RMAS\_FOR \textasciitilde{}\textasciitilde{} .33*SSE\_Lifetime}
\StringTok{         RMAS\_FOR \textasciitilde{}\textasciitilde{} .25*SSE\_Recent}
\StringTok{         RMAS\_FOR \textasciitilde{}\textasciitilde{} .10*PHQ9}
\StringTok{         RMAS\_FOR \textasciitilde{}\textasciitilde{} .02*gIRAAS}
\StringTok{         }
\StringTok{         RMAS\_LOW \textasciitilde{}\textasciitilde{} .69*RMAS\_INV}
\StringTok{         RMAS\_LOW \textasciitilde{}\textasciitilde{} .48*RMAS\_ENV}
\StringTok{         RMAS\_LOW \textasciitilde{}\textasciitilde{} .67*SSE\_Lifetime}
\StringTok{         RMAS\_LOW \textasciitilde{}\textasciitilde{} .57*SSE\_Recent}
\StringTok{         RMAS\_LOW \textasciitilde{}\textasciitilde{} .30*PHQ9}
\StringTok{         RMAS\_LOW \textasciitilde{}\textasciitilde{} .16*gIRAAS}
\StringTok{         }
\StringTok{         RMAS\_INV \textasciitilde{}\textasciitilde{} .59*RMAS\_ENV}
\StringTok{         RMAS\_INV \textasciitilde{}\textasciitilde{} .63*SSE\_Lifetime}
\StringTok{         RMAS\_INV \textasciitilde{}\textasciitilde{} .52*SSE\_Recent}
\StringTok{         RMAS\_INV \textasciitilde{}\textasciitilde{} .32*PHQ9}
\StringTok{         RMAS\_INV \textasciitilde{}\textasciitilde{} .23*gIRAAS}
\StringTok{         }
\StringTok{         RMAS\_ENV \textasciitilde{}\textasciitilde{} .46*SSE\_Lifetime}
\StringTok{         RMAS\_ENV \textasciitilde{}\textasciitilde{} .31*SSE\_Recent}
\StringTok{         RMAS\_ENV \textasciitilde{}\textasciitilde{} .11*PHQ9}
\StringTok{         RMAS\_ENV \textasciitilde{}\textasciitilde{} .07*gIRAAS}
\StringTok{         }
\StringTok{         SSE\_Lifetime \textasciitilde{}\textasciitilde{} .83*SSE\_Recent}
\StringTok{         SSE\_Lifetime \textasciitilde{}\textasciitilde{} .30*PHQ9}
\StringTok{         SSE\_Lifetime \textasciitilde{}\textasciitilde{} .14*gIRAAS}
\StringTok{         }
\StringTok{         SSE\_Recent \textasciitilde{}\textasciitilde{} .30*PHQ9}
\StringTok{         SSE\_Recent \textasciitilde{}\textasciitilde{} .20*gIRAAS}
\StringTok{         }
\StringTok{         PHQ9 \textasciitilde{}\textasciitilde{} .18*gIRAAS}
\StringTok{         }
\StringTok{       }
\StringTok{         \#Correlations between SES scales from the Klonoff and Landrine article}
\StringTok{         \#Note that in the article the factor orders were reversed}
\StringTok{         SSEL\_Deg \textasciitilde{}\textasciitilde{} .64*SSEL\_dRel}
\StringTok{         SSEL\_Deg \textasciitilde{}\textasciitilde{} .61*SSEL\_cRel}
\StringTok{         SSEL\_Deg \textasciitilde{}\textasciitilde{} .50*SSEL\_Work}
\StringTok{         SSEL\_dRel \textasciitilde{}\textasciitilde{} .57*SSEL\_cRel}
\StringTok{         SSEL\_dRel \textasciitilde{}\textasciitilde{} .57*SSEL\_Work}
\StringTok{         SSEL\_cRel \textasciitilde{}\textasciitilde{} .47*SSEL\_Work}
\StringTok{         }
\StringTok{         SSER\_Deg \textasciitilde{} .54*SSER\_dRel}
\StringTok{         SSER\_Deg \textasciitilde{} .54*SSER\_Work}
\StringTok{         SSER\_Deg \textasciitilde{} .59*SSER\_cRel}
\StringTok{         SSER\_dRel \textasciitilde{} .56*SSER\_Work}
\StringTok{         SSER\_dRel \textasciitilde{} .46*SSER\_cRel}
\StringTok{         SSER\_Work \textasciitilde{} .43*SSER\_cRel}
\StringTok{         }
\StringTok{         SSE\_Lifetime \textasciitilde{} .75*SSE\_Recent}
\StringTok{        }
\StringTok{        \textquotesingle{}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{dfGRMSAAW }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{simulateData}\NormalTok{(}\AttributeTok{model =}\NormalTok{ Keum\_GRMS\_generating\_model,}
                              \AttributeTok{model.type =} \StringTok{"sem"}\NormalTok{,}
                              \AttributeTok{meanstructure =}\NormalTok{ T,}
                              \AttributeTok{sample.nobs=}\DecValTok{304}\NormalTok{,}
                              \AttributeTok{standardized=}\ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\#used to retrieve column indices used in the rescaling script below}
\NormalTok{col\_index }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(dfGRMSAAW))}

\CommentTok{\#The code below loops through each column of the dataframe and assigns the scaling accordingly}
\CommentTok{\#Rows 1 thru 22 are the GRMS items}
\CommentTok{\#Rows 23 thru 47 are the RMAS}
\CommentTok{\#Rows 48 thru 87 are the SSE}
\CommentTok{\#Rows 88 thru 96 are the PHQ9}
\CommentTok{\#Rows 97 thru 110 are the IRAAS}
\CommentTok{\#Rows 111 thru 112 are scale scores for SSE}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(dfGRMSAAW))\{  }
  \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{22}\NormalTok{)\{   }
\NormalTok{    dfGRMSAAW[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfGRMSAAW[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{  \}}
    \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{23} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{47}\NormalTok{)\{   }
\NormalTok{    dfGRMSAAW[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfGRMSAAW[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{    \}}
  \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{48} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{87}\NormalTok{)\{   }
\NormalTok{    dfGRMSAAW[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfGRMSAAW[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{  \}}
    \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{88} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{96}\NormalTok{)\{   }
\NormalTok{    dfGRMSAAW[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfGRMSAAW[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{    \}}
    \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{97} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{110}\NormalTok{)\{   }
\NormalTok{    dfGRMSAAW[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfGRMSAAW[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{  \}}
\NormalTok{\}}

\CommentTok{\#rounding to integers so that the data resembles that which was collected}
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{dfGRMSAAW }\OtherTok{\textless{}{-}}\NormalTok{ dfGRMSAAW }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{round}\NormalTok{(}\DecValTok{0}\NormalTok{) }

\CommentTok{\#quick check of my work}
\CommentTok{\#psych::describe(dfGRMSAAW) }
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either an .rds object or a .csv file.

An .rds file preserves all formatting to variables prior to the export and re-import. For the purpose of this chapter, you don't need to do either. That is, you can re-simulate the data each time you work the problem.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(dfGRMSAAW, \textquotesingle{}dfGRMSAAW.rds\textquotesingle{}) bring back the simulated}
\CommentTok{\# dat from an .rds file dfGRMSAAW \textless{}{-} readRDS(\textquotesingle{}dfGRMSAAW.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

If you save the .csv file (think ``Excel lite'') and bring it back in, you will lose any formatting (e.g., ordered factors will be interpreted as character variables).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(dfGRMSAAW,}
\CommentTok{\# file=\textquotesingle{}dfGRMSAAW.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE)}
\CommentTok{\# bring back the simulated dat from a .csv file dfGRMSAAW \textless{}{-} read.csv}
\CommentTok{\# (\textquotesingle{}dfGRMSAAW.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

\hypertarget{modeling-the-grmsaaw-as-unidimensional}{%
\subsection{Modeling the GRMSAAW as Unidimensional}\label{modeling-the-grmsaaw-as-unidimensional}}

Let's start simply, taking the GRMSAAW data and seeing about its fit as a unidimensional instrument. In fact, even when measures are presumed to be multi-dimensional, it is common to begin with a unidimensional assessment. Here's why:

\begin{itemize}
\tightlist
\item
  Operationally, it's a check to see that data, script, and so forth. are all working.
\item
  If you can't reject a single-factor model (e.g., if there is a strong support for such), then it makes little sense to evaluate models with more factors \citep{kline_principles_2016}.
\end{itemize}

Considerations for the \emph{lavaan} code include:

\begin{itemize}
\tightlist
\item
  GRMSAAW is a latent variable and can be named anything. We know this because it is followed by: =\textasciitilde{}
\item
  All the items follow and are ``added'' with the plus sign

  \begin{itemize}
  \tightlist
  \item
    Don't let this fool you\ldots the assumption behind SEM/CFA is that the LV \emph{causes} the score on the item/indicator. Recall, item/indicator scores are influenced by the LV and error.
  \end{itemize}
\item
  The entire model is enclosed in tic marks (' and ')
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grmsAAWmod1 }\OtherTok{\textless{}{-}} \StringTok{"GRMSAAW =\textasciitilde{} AS1 + AS2 + AS3 + AS4 + AS5 + AS6 + AS7 + AS8 + AS9 + AF1 + AF2 + AF3 + AF4 + MI1 + MI2 + MI3 + MI4 + MI5 + AUA1 + AUA2 + AUA3 + AUA4"}
\end{Highlighting}
\end{Shaded}

The object representing the model is then included in the \emph{lavaan::cfa()} along with the dataset.

We can ask for a summary of the object representing the results.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{grmsAAW1fit }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(grmsAAWmod1, }\AttributeTok{data =}\NormalTok{ dfGRMSAAW)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(grmsAAW1fit, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{rsquare =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 29 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        44

  Number of observations                           304

Model Test User Model:
                                                      
  Test statistic                               444.451
  Degrees of freedom                               209
  P-value (Chi-square)                           0.000

Model Test Baseline Model:

  Test statistic                              1439.317
  Degrees of freedom                               231
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.805
  Tucker-Lewis Index (TLI)                       0.785

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -8387.014
  Loglikelihood unrestricted model (H1)      -8164.789
                                                      
  Akaike (AIC)                               16862.028
  Bayesian (BIC)                             17025.577
  Sample-size adjusted Bayesian (SABIC)      16886.032

Root Mean Square Error of Approximation:

  RMSEA                                          0.061
  90 Percent confidence interval - lower         0.053
  90 Percent confidence interval - upper         0.069
  P-value H_0: RMSEA <= 0.050                    0.012
  P-value H_0: RMSEA >= 0.080                    0.000

Standardized Root Mean Square Residual:

  SRMR                                           0.067

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  GRMSAAW =~                                                            
    AS1               1.000                               0.491    0.535
    AS2               1.069    0.152    7.012    0.000    0.525    0.520
    AS3               1.024    0.143    7.151    0.000    0.503    0.535
    AS4               0.909    0.137    6.649    0.000    0.446    0.482
    AS5               1.177    0.154    7.634    0.000    0.578    0.590
    AS6               0.721    0.108    6.658    0.000    0.354    0.483
    AS7               0.914    0.137    6.693    0.000    0.449    0.487
    AS8               0.927    0.137    6.765    0.000    0.455    0.494
    AS9               0.735    0.117    6.262    0.000    0.361    0.445
    AF1               0.675    0.125    5.410    0.000    0.332    0.370
    AF2               0.975    0.144    6.755    0.000    0.479    0.493
    AF3               0.555    0.120    4.637    0.000    0.272    0.308
    AF4               0.851    0.141    6.042    0.000    0.418    0.425
    MI1               0.744    0.120    6.182    0.000    0.365    0.438
    MI2               0.641    0.122    5.252    0.000    0.315    0.357
    MI3               0.860    0.146    5.907    0.000    0.422    0.413
    MI4               0.601    0.130    4.614    0.000    0.295    0.307
    MI5               0.655    0.122    5.356    0.000    0.322    0.365
    AUA1              0.825    0.144    5.740    0.000    0.405    0.398
    AUA2              0.878    0.132    6.659    0.000    0.431    0.483
    AUA3              0.714    0.118    6.058    0.000    0.350    0.426
    AUA4              1.060    0.146    7.262    0.000    0.520    0.547

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .AS1               0.600    0.052   11.500    0.000    0.600    0.713
   .AS2               0.744    0.064   11.565    0.000    0.744    0.730
   .AS3               0.631    0.055   11.503    0.000    0.631    0.714
   .AS4               0.657    0.056   11.704    0.000    0.657    0.767
   .AS5               0.624    0.056   11.225    0.000    0.624    0.651
   .AS6               0.412    0.035   11.701    0.000    0.412    0.766
   .AS7               0.648    0.055   11.689    0.000    0.648    0.763
   .AS8               0.641    0.055   11.663    0.000    0.641    0.756
   .AS9               0.528    0.045   11.820    0.000    0.528    0.802
   .AF1               0.693    0.058   12.002    0.000    0.693    0.863
   .AF2               0.714    0.061   11.666    0.000    0.714    0.757
   .AF3               0.707    0.058   12.113    0.000    0.707    0.905
   .AF4               0.794    0.067   11.875    0.000    0.794    0.820
   .MI1               0.564    0.048   11.841    0.000    0.564    0.809
   .MI2               0.678    0.056   12.028    0.000    0.678    0.873
   .MI3               0.870    0.073   11.906    0.000    0.870    0.830
   .MI4               0.839    0.069   12.115    0.000    0.839    0.906
   .MI5               0.672    0.056   12.011    0.000    0.672    0.866
   .AUA1              0.872    0.073   11.941    0.000    0.872    0.842
   .AUA2              0.610    0.052   11.700    0.000    0.610    0.766
   .AUA3              0.553    0.047   11.871    0.000    0.553    0.818
   .AUA4              0.634    0.055   11.448    0.000    0.634    0.701
    GRMSAAW           0.241    0.051    4.694    0.000    1.000    1.000

R-Square:
                   Estimate
    AS1               0.287
    AS2               0.270
    AS3               0.286
    AS4               0.233
    AS5               0.349
    AS6               0.234
    AS7               0.237
    AS8               0.244
    AS9               0.198
    AF1               0.137
    AF2               0.243
    AF3               0.095
    AF4               0.180
    MI1               0.191
    MI2               0.127
    MI3               0.170
    MI4               0.094
    MI5               0.134
    AUA1              0.158
    AUA2              0.234
    AUA3              0.182
    AUA4              0.299
\end{verbatim}

I find it helpful to immediately plot what we did. A quick look alerts me to errors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(grmsAAW1fit, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{10-CFA_1stOrder_files/figure-latex/unnamed-chunk-8-1.pdf} \#\#\#\# Interpreting the Output

With a quick look at the plot, let's work through the results. Rosseel's (2019) \emph{lavaan} tutorial is a useful resource in walking through the output.

The \emph{header} is the first few lines of the information. It contains:

\begin{itemize}
\tightlist
\item
  the \emph{lavaan} version number (0.6-17 that I'm using on 4/12/24),
\item
  maximum likelihood (ML) was used as the estimator,
\item
  confirmation that the specification converged normally after 28 iterations,
\item
  indication that 304 cases were used in this analysis (this would be less if some were skipped because of missing data),
\item
  results including the model user test statistic, df, and corresponding p value: \(\chi ^{2}(209) = 444.451, p < .001\).
\end{itemize}

Model Test User Model:

\textbf{Fit statistics} are included in the second section. They are only shown when the argument ``fit.measures = TRUE'' is in the script. Standardized values are not the default, they require the argument, ``standardized = TRUE''. We'll come back to these shortly\ldots{}

\emph{Parameter estimates} is the last section.

For now, we are interested in the ``Latent Variables'' section.

\begin{itemize}
\tightlist
\item
  \emph{Estimate} contains the estimated or fixed parameter value for each model parameter;
\item
  \emph{Std. err} is the standard error for each estimated parameter;
\item
  \emph{Z-value} is the Wald statistic (the parameter divided by its SE)
\item
  \emph{P(\textgreater\textbar z\textbar)} is the p value for testing the null hypothesis that the parameter equals zero in the population
\item
  \emph{Std.lv} standardizes only the LVs
\item
  \emph{Std.all} both latent and observed variables are standardized; this is considered the ``completely standardized solution''
\end{itemize}

Note that item AS1 might seem incomplete -- there is only a 1.000 and a value for the Std.lv. Recall we used this to scale the single factor by fixing its value to 1.000. Coefficients that are fixed to 1.0 to scale a factor have no standard errors and therefore no significance test.

The SE and associated \(p\) values are associated with the unstandardized estimates. Intuitively, it is easiest for me to understand the relative magnitude of the pattern coefficients by looking at the \emph{Std.all} column. We can see that the items associated with what we will soon define as the AS factor are all strong and positive. The remaining items have variable loadings with many of the being quite low, non-significant, and even negatively valanced.

Let's examine to the middle set metrics which assess \emph{global fit}.

CFA falls into a \emph{modeling} approach to evaluating results. While it provides some flexibility (we get away from the strict, NHST approach of \(p\) \textless{} .05) there is greater interpretive ambiguity.

Fit statistics tend to be clustered together based on their approach to summarizing the \emph{goodness} or \emph{badness} of fit.

\hypertarget{model-test-user-model}{%
\subsubsection{\texorpdfstring{Model Test \emph{User} Model:}{Model Test User Model:}}\label{model-test-user-model}}

The chi-square statistic that evaluates the \emph{exact-fit hypothesis} that there is no difference between the covariances predicted by the model, given the parameter estimates, and the population covariance matrix. Rejecting the hypothesis says that,

\begin{itemize}
\tightlist
\item
  the data contain covariance information that speak against the model, and
\item
  the researcher should explain model-data discrepancies that exceed those expected by sampling error.
\end{itemize}

Traditional interpretation of the chi-square is an \emph{accept-support test} where the null hypothesis represents the researchers' belief that the model is correct. This means that the absence of statistical significance (\(p\) \textgreater{} .05) that supports the model. This is backwards from our usual \emph{reject-support test} approach.

The \(\chi^2\) is frequently criticized:

\begin{itemize}
\tightlist
\item
  \emph{Accept-support test} approaches are logically weaker because the failure to disprove an assertation (the exact-fit hypothesis) does not prove that the assertion is true.
\item
  Too small a sample size (low power) makes it more likely that the model will be retained.
\item
  CFA/SEM requires large samples and so the \(\chi^2\) is frequently statistically significant; this frequently results in rejection of the researchers' model.
\end{itemize}

Kline \citeyearpar{kline_principles_2016} recommends that we treat the \(\chi^2\) like a smoke alarm -- if the alarm sounds, there may or may not be a fire (a serious model-data discrepancy), but we should treat the alarm seriously and further inspect issues of fit.

For our unidimensional GRMSAAW CFA \(\chi ^{2}(209) = 444.451, p < .001\), the significant \emph{p} value is not what we want because it says that our specified model is different than the covariances in the model.

\hypertarget{model-test-baseline-model}{%
\subsubsection{\texorpdfstring{Model Test \emph{Baseline} Model}{Model Test Baseline Model}}\label{model-test-baseline-model}}

This model is the \emph{independence} model. That is, there is complete independence of all variables in the model (i.e., in which all correlations among variables are zero). This is the most restricted model. It is typical for chi-square values to be quite high (as it is in our example: 1439.317). On its own, this model is not useful to us. It is used, though, in comparisons of \emph{incremental fit}.

\hypertarget{incremental-fit-indices-user-versus-baseline-models}{%
\subsubsection{Incremental Fit Indices (User versus Baseline Models)}\label{incremental-fit-indices-user-versus-baseline-models}}

Incremental fit indices ask the question, how much better is the fit of our specified model to the data then the baseline model (where it is assumed no relations between the variables).

The Comparative Fit Index (CFI) and Tucker-Lewis Index (TLI) are \emph{goodness of fit} statistics, ranging from 0 to 1.0 where 1.0 is best.

\textbf{CFI}: compares the amount of departure from close fit for the researcher's model against that of the independence/baseline (null) model.

\[CFI = 1-\frac{\hat{\Delta_{M}}}{\hat{\Delta_{B}}}\]

Where there is no departure from close fit, then CFI will equal 1.0. We interpret the value of the CFI as a percent of how much better the researcher's model is than the baseline model. While 81\% sounds like an improvement -- Hu and Bentler (1999) stated that ``acceptable fit'' is achieved when the \(CFI \geq .95\) and \(SRMR \leq .08\); the \textbf{combination rule}. It is important to note that later simulation studies have not supported those thresholds.

\textbf{TLI}: aka the \textbf{non-normed fit index (NNFI)} controls for \(df_M\) from the researcher's model and \(df_B\) from the baseline model. As such, it imposes a greater relative penalty for model complexity than the CFI. The TLI is a bit unstable in that the values can exceed 1.0.

Because the two measures are so related, only one should be reported (I typically see the CFI).

For our unidimensional GRMSAAW CFA, CFI = .805 and TLI = .785. While these predict around 81\% better than the baseline/independence model, it does not come close to the standard of \(\geq .95\).

\hypertarget{loglikelihood-and-information-criteria}{%
\subsubsection{Loglikelihood and Information Criteria}\label{loglikelihood-and-information-criteria}}

The \textbf{Akaike Information Criterion (AIC)} and the \textbf{Bayesian Information Criterion (BIC)} utilize an information theory approach to data analysis by combing statistical estimation and model selection into a single framework. The BIC augments the AIC by taking sample size into consideration.

The AIC and BIC are usually used to select among competing nonhierarchical models and are only used in comparison with each other. Thus, our current values of 16862.028 (AIC) and 17025.577 (BIC) are meaningless on their own. The model with the smallest values of the predictive fit indices is chosen as the one that is most likely to replicate. It means that this model has relatively better fit and fewer free parameters than competing models.

Later in the lesson we will return to these values to compare a correlated, four-factor solution with this unidimensional model.

\hypertarget{root-mean-square-error-of-approximation}{%
\subsubsection{Root Mean Square Error of Approximation}\label{root-mean-square-error-of-approximation}}

The RMSEA is an absolute fit index scaled as a \emph{badness-of-fit} statistic where a value of 0.00 is the best fit. The RMSEA favors models with more degrees of freedom and larger sample sizes. A unique aspect of the RMSEA is its 90\% confidence interval.

While there is chatter/controversy about what constitutes an acceptable value, there is general consensus that \(RMSEA \geq .10\) points to serious problems. An \(RMSEA\leq .05\) is desired. In evaluating the RMSEA, we need to monitor the upper bound of the confidence interval to see that it isn't sneaking into the danger zone.

For our unidimensional GRMSAAW CFA, RMSEA = 0.061, 90\% CI(0.053, 0.069). This value is within the range of acceptability.

\hypertarget{standardized-root-mean-square-residual}{%
\subsubsection{Standardized Root Mean Square Residual}\label{standardized-root-mean-square-residual}}

The SRMR is an absolute fit index that is a \emph{badness-of-fit} statistic (i.e., perfect model fit is when the value = 0.00 and increasingly higher values indicate the ``badness'').

The SRMR is a standardized version of the \textbf{root mean square residual (RMR)}, which is a measure of the mean absolute covariance residual. Standardizing the value facilitates interpretation.

Poor fit is indicated when \(SRMR \geq .10\).

Recall, Hu and Bentler's \textbf{combination rule} (which is somewhat contested) suggested that the SRMR be interpreted along with the CFI such that: \(CFI \geqslant .95\) and \(SRMR \leq .08\).

For our unidimensional GRMSAAW CFA, SRMR = 0.067.

Inspecting the residuals (we look for relatively large values) may help understand the source of poor fit, so let's do that.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{fitted}\NormalTok{(grmsAAW1fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$cov
       AS1   AS2   AS3   AS4   AS5   AS6   AS7   AS8   AS9   AF1   AF2   AF3
AS1  0.841                                                                  
AS2  0.258 1.020                                                            
AS3  0.247 0.264 0.883                                                      
AS4  0.219 0.234 0.224 0.856                                                
AS5  0.284 0.303 0.290 0.258 0.958                                          
AS6  0.174 0.186 0.178 0.158 0.205 0.537                                    
AS7  0.220 0.235 0.225 0.200 0.259 0.159 0.849                              
AS8  0.223 0.239 0.229 0.203 0.263 0.161 0.204 0.849                        
AS9  0.177 0.189 0.181 0.161 0.209 0.128 0.162 0.164 0.658                  
AF1  0.163 0.174 0.167 0.148 0.192 0.117 0.149 0.151 0.120 0.803            
AF2  0.235 0.251 0.241 0.214 0.277 0.170 0.215 0.218 0.173 0.159 0.943      
AF3  0.134 0.143 0.137 0.122 0.157 0.096 0.122 0.124 0.098 0.090 0.130 0.781
AF4  0.205 0.219 0.210 0.187 0.242 0.148 0.188 0.190 0.151 0.139 0.200 0.114
MI1  0.179 0.192 0.184 0.163 0.211 0.129 0.164 0.166 0.132 0.121 0.175 0.100
MI2  0.154 0.165 0.158 0.140 0.182 0.111 0.141 0.143 0.114 0.104 0.151 0.086
MI3  0.207 0.222 0.212 0.189 0.244 0.150 0.190 0.192 0.152 0.140 0.202 0.115
MI4  0.145 0.155 0.148 0.132 0.170 0.104 0.132 0.134 0.106 0.098 0.141 0.080
MI5  0.158 0.169 0.162 0.144 0.186 0.114 0.144 0.146 0.116 0.107 0.154 0.088
AUA1 0.199 0.213 0.204 0.181 0.234 0.143 0.182 0.184 0.146 0.134 0.194 0.110
AUA2 0.212 0.226 0.217 0.192 0.249 0.153 0.193 0.196 0.156 0.143 0.207 0.118
AUA3 0.172 0.184 0.176 0.156 0.202 0.124 0.157 0.159 0.126 0.116 0.168 0.095
AUA4 0.255 0.273 0.261 0.232 0.301 0.184 0.233 0.237 0.188 0.173 0.249 0.142
       AF4   MI1   MI2   MI3   MI4   MI5  AUA1  AUA2  AUA3  AUA4
AS1                                                             
AS2                                                             
AS3                                                             
AS4                                                             
AS5                                                             
AS6                                                             
AS7                                                             
AS8                                                             
AS9                                                             
AF1                                                             
AF2                                                             
AF3                                                             
AF4  0.968                                                      
MI1  0.153 0.697                                                
MI2  0.132 0.115 0.777                                          
MI3  0.177 0.154 0.133 1.048                                    
MI4  0.123 0.108 0.093 0.125 0.926                              
MI5  0.134 0.118 0.101 0.136 0.095 0.775                        
AUA1 0.169 0.148 0.127 0.171 0.119 0.130 1.036                  
AUA2 0.180 0.158 0.136 0.182 0.127 0.139 0.175 0.796            
AUA3 0.146 0.128 0.110 0.148 0.103 0.113 0.142 0.151 0.676      
AUA4 0.217 0.190 0.164 0.220 0.153 0.167 0.211 0.224 0.182 0.904
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# lavaan::residuals(grmsAAW1fit, type = \textquotesingle{}raw\textquotesingle{})}
\CommentTok{\# lavaan::residuals(grmsAAW1fit, type = \textquotesingle{}standardized\textquotesingle{})}

\CommentTok{\# will hashtag out for knitted file}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{residuals}\NormalTok{(grmsAAW1fit, }\AttributeTok{type =} \StringTok{"cor"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$type
[1] "cor.bollen"

$cov
        AS1    AS2    AS3    AS4    AS5    AS6    AS7    AS8    AS9    AF1
AS1   0.000                                                               
AS2   0.101  0.000                                                        
AS3   0.059  0.096  0.000                                                 
AS4   0.073  0.122  0.062  0.000                                          
AS5   0.137  0.055 -0.021  0.060  0.000                                   
AS6   0.024  0.096  0.000 -0.043  0.047  0.000                            
AS7  -0.022  0.042  0.052  0.057  0.027 -0.036  0.000                     
AS8   0.003  0.091  0.037  0.023  0.086  0.042  0.055  0.000              
AS9   0.080  0.109  0.017  0.067  0.072  0.095  0.013  0.064  0.000       
AF1  -0.053 -0.088  0.062 -0.007 -0.057 -0.078 -0.070 -0.008 -0.050  0.000
AF2  -0.040 -0.134 -0.057  0.022 -0.053 -0.064 -0.050  0.008 -0.006  0.154
AF3  -0.027 -0.117  0.041 -0.088 -0.042 -0.074 -0.038 -0.018  0.015  0.222
AF4  -0.093 -0.059 -0.006  0.007 -0.016 -0.063 -0.015 -0.066 -0.079  0.203
MI1  -0.092 -0.091 -0.074 -0.069 -0.096  0.015 -0.015 -0.066 -0.100  0.082
MI2  -0.117 -0.001 -0.063 -0.049 -0.035 -0.005  0.040 -0.067 -0.089 -0.084
MI3  -0.080 -0.025 -0.030 -0.065 -0.045  0.013 -0.088  0.034  0.007 -0.007
MI4  -0.103 -0.111 -0.018 -0.153 -0.088 -0.014  0.070 -0.068 -0.026 -0.012
MI5   0.006 -0.046 -0.042  0.021 -0.028 -0.023  0.003 -0.046 -0.097 -0.015
AUA1 -0.075 -0.114 -0.017 -0.129 -0.057 -0.008 -0.016 -0.015 -0.119 -0.118
AUA2 -0.025 -0.059 -0.068 -0.101 -0.031  0.015 -0.040 -0.055 -0.074 -0.005
AUA3 -0.003 -0.028 -0.019 -0.060 -0.075  0.031 -0.049 -0.125 -0.022 -0.007
AUA4 -0.006 -0.078 -0.042  0.000 -0.013 -0.061  0.025 -0.065 -0.073  0.031
        AF2    AF3    AF4    MI1    MI2    MI3    MI4    MI5   AUA1   AUA2
AS1                                                                       
AS2                                                                       
AS3                                                                       
AS4                                                                       
AS5                                                                       
AS6                                                                       
AS7                                                                       
AS8                                                                       
AS9                                                                       
AF1                                                                       
AF2   0.000                                                               
AF3   0.056  0.000                                                        
AF4   0.138  0.088  0.000                                                 
MI1   0.133  0.019  0.064  0.000                                          
MI2  -0.070 -0.006  0.063  0.115  0.000                                   
MI3   0.016 -0.002 -0.002  0.130  0.223  0.000                            
MI4   0.043  0.017  0.025  0.141  0.111  0.157  0.000                     
MI5   0.035  0.081  0.012 -0.030  0.041 -0.015  0.119  0.000              
AUA1  0.025  0.024 -0.024  0.084  0.113  0.059  0.045  0.022  0.000       
AUA2  0.006 -0.006  0.022  0.029  0.055 -0.013  0.003  0.071  0.184  0.000
AUA3  0.029  0.011 -0.064 -0.008  0.030 -0.016  0.078  0.057  0.141  0.115
AUA4  0.005  0.005  0.017  0.072 -0.016 -0.029  0.017  0.002  0.100  0.098
       AUA3   AUA4
AS1               
AS2               
AS3               
AS4               
AS5               
AS6               
AS7               
AS8               
AS9               
AF1               
AF2               
AF3               
AF4               
MI1               
MI2               
MI3               
MI4               
MI5               
AUA1              
AUA2              
AUA3  0.000       
AUA4  0.092  0.000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{modindices}\NormalTok{(grmsAAW1fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     lhs op  rhs     mi    epc sepc.lv sepc.all sepc.nox
46   AS1 ~~  AS2  6.859  0.108   0.108    0.162    0.162
47   AS1 ~~  AS3  2.401  0.059   0.059    0.096    0.096
48   AS1 ~~  AS4  3.353  0.070   0.070    0.112    0.112
49   AS1 ~~  AS5 14.871  0.148   0.148    0.243    0.243
50   AS1 ~~  AS6  0.381  0.019   0.019    0.038    0.038
51   AS1 ~~  AS7  0.306 -0.021  -0.021   -0.034   -0.034
52   AS1 ~~  AS8  0.005  0.003   0.003    0.004    0.004
53   AS1 ~~  AS9  3.823  0.067   0.067    0.119    0.119
54   AS1 ~~  AF1  1.511 -0.048  -0.048   -0.074   -0.074
55   AS1 ~~  AF2  1.035 -0.041  -0.041   -0.062   -0.062
56   AS1 ~~  AF3  0.374 -0.024  -0.024   -0.037   -0.037
57   AS1 ~~  AF4  5.081 -0.094  -0.094   -0.137   -0.137
58   AS1 ~~  MI1  5.049 -0.080  -0.080   -0.137   -0.137
59   AS1 ~~  MI2  7.368 -0.104  -0.104   -0.164   -0.164
60   AS1 ~~  MI3  3.666 -0.084  -0.084   -0.116   -0.116
61   AS1 ~~  MI4  5.429 -0.099  -0.099   -0.140   -0.140
62   AS1 ~~  MI5  0.018  0.005   0.005    0.008    0.008
63   AS1 ~~ AUA1  3.179 -0.078  -0.078   -0.108   -0.108
64   AS1 ~~ AUA2  0.407 -0.024  -0.024   -0.039   -0.039
65   AS1 ~~ AUA3  0.004 -0.002  -0.002   -0.004   -0.004
66   AS1 ~~ AUA4  0.027 -0.006  -0.006   -0.010   -0.010
67   AS2 ~~  AS3  6.186  0.105   0.105    0.154    0.154
68   AS2 ~~  AS4  9.103  0.129   0.129    0.184    0.184
69   AS2 ~~  AS5  2.292  0.065   0.065    0.095    0.095
70   AS2 ~~  AS6  5.695  0.081   0.081    0.146    0.146
71   AS2 ~~  AS7  1.072  0.044   0.044    0.063    0.063
72   AS2 ~~  AS8  5.239  0.097   0.097    0.140    0.140
73   AS2 ~~  AS9  6.981  0.101   0.101    0.160    0.160
74   AS2 ~~  AF1  4.097 -0.088  -0.088   -0.122   -0.122
75   AS2 ~~  AF2 11.242 -0.150  -0.150   -0.205   -0.205
76   AS2 ~~  AF3  6.878 -0.114  -0.114   -0.157   -0.157
77   AS2 ~~  AF4  1.941 -0.065  -0.065   -0.084   -0.084
78   AS2 ~~  MI1  4.814 -0.086  -0.086   -0.133   -0.133
79   AS2 ~~  MI2  0.001 -0.001  -0.001   -0.002   -0.002
80   AS2 ~~  MI3  0.335 -0.028  -0.028   -0.035   -0.035
81   AS2 ~~  MI4  6.183 -0.118  -0.118   -0.149   -0.149
82   AS2 ~~  MI5  1.123 -0.045  -0.045   -0.064   -0.064
83   AS2 ~~ AUA1  7.163 -0.130  -0.130   -0.162   -0.162
84   AS2 ~~ AUA2  2.155 -0.060  -0.060   -0.090   -0.090
85   AS2 ~~ AUA3  0.445 -0.026  -0.026   -0.040   -0.040
86   AS2 ~~ AUA4  4.164 -0.087  -0.087   -0.126   -0.126
87   AS3 ~~  AS4  2.403  0.061   0.061    0.095    0.095
88   AS3 ~~  AS5  0.360 -0.024  -0.024   -0.038   -0.038
89   AS3 ~~  AS6  0.000  0.000   0.000    0.000    0.000
90   AS3 ~~  AS7  1.752  0.052   0.052    0.081    0.081
91   AS3 ~~  AS8  0.886  0.037   0.037    0.058    0.058
92   AS3 ~~  AS9  0.164  0.014   0.014    0.025    0.025
93   AS3 ~~  AF1  2.082  0.058   0.058    0.087    0.087
94   AS3 ~~  AF2  2.114 -0.060  -0.060   -0.089   -0.089
95   AS3 ~~  AF3  0.850  0.037   0.037    0.055    0.055
96   AS3 ~~  AF4  0.024 -0.007  -0.007   -0.009   -0.009
97   AS3 ~~  MI1  3.213 -0.065  -0.065   -0.109   -0.109
98   AS3 ~~  MI2  2.144 -0.058  -0.058   -0.088   -0.088
99   AS3 ~~  MI3  0.511 -0.032  -0.032   -0.043   -0.043
100  AS3 ~~  MI4  0.158 -0.017  -0.017   -0.024   -0.024
101  AS3 ~~  MI5  0.978 -0.039  -0.039   -0.060   -0.060
102  AS3 ~~ AUA1  0.165 -0.018  -0.018   -0.025   -0.025
103  AS3 ~~ AUA2  2.950 -0.065  -0.065   -0.105   -0.105
104  AS3 ~~ AUA3  0.212 -0.017  -0.017   -0.028   -0.028
105  AS3 ~~ AUA4  1.248 -0.044  -0.044   -0.069   -0.069
106  AS4 ~~  AS5  2.576  0.064   0.064    0.100    0.100
107  AS4 ~~  AS6  1.089 -0.033  -0.033   -0.063   -0.063
108  AS4 ~~  AS7  1.916  0.055   0.055    0.084    0.084
109  AS4 ~~  AS8  0.319  0.022   0.022    0.034    0.034
110  AS4 ~~  AS9  2.469  0.056   0.056    0.095    0.095
111  AS4 ~~  AF1  0.024 -0.006  -0.006   -0.009   -0.009
112  AS4 ~~  AF2  0.287  0.022   0.022    0.033    0.033
113  AS4 ~~  AF3  3.687 -0.078  -0.078   -0.114   -0.114
114  AS4 ~~  AF4  0.026  0.007   0.007    0.010    0.010
115  AS4 ~~  MI1  2.555 -0.059  -0.059   -0.096   -0.096
116  AS4 ~~  MI2  1.178 -0.043  -0.043   -0.065   -0.065
117  AS4 ~~  MI3  2.225 -0.068  -0.068   -0.090   -0.090
118  AS4 ~~  MI4 10.991 -0.146  -0.146   -0.197   -0.197
119  AS4 ~~  MI5  0.225  0.019   0.019    0.028    0.028
120  AS4 ~~ AUA1  8.523 -0.133  -0.133   -0.175   -0.175
121  AS4 ~~ AUA2  5.898 -0.093  -0.093   -0.147   -0.147
122  AS4 ~~ AUA3  1.901 -0.050  -0.050   -0.083   -0.083
123  AS4 ~~ AUA4  0.000  0.000   0.000   -0.001   -0.001
124  AS5 ~~  AS6  1.613  0.040   0.040    0.079    0.079
125  AS5 ~~  AS7  0.511  0.028   0.028    0.045    0.045
126  AS5 ~~  AS8  5.348  0.091   0.091    0.144    0.144
127  AS5 ~~  AS9  3.532  0.067   0.067    0.116    0.116
128  AS5 ~~  AF1  2.026 -0.057  -0.057   -0.087   -0.087
129  AS5 ~~  AF2  2.047 -0.060  -0.060   -0.089   -0.089
130  AS5 ~~  AF3  1.001 -0.040  -0.040   -0.061   -0.061
131  AS5 ~~  AF4  0.164 -0.018  -0.018   -0.025   -0.025
132  AS5 ~~  MI1  6.141 -0.091  -0.091   -0.153   -0.153
133  AS5 ~~  MI2  0.728 -0.034  -0.034   -0.052   -0.052
134  AS5 ~~  MI3  1.311 -0.052  -0.052   -0.070   -0.070
135  AS5 ~~  MI4  4.499 -0.093  -0.093   -0.129   -0.129
136  AS5 ~~  MI5  0.496 -0.028  -0.028   -0.043   -0.043
137  AS5 ~~ AUA1  2.084 -0.065  -0.065   -0.089   -0.089
138  AS5 ~~ AUA2  0.674 -0.032  -0.032   -0.051   -0.051
139  AS5 ~~ AUA3  3.657 -0.069  -0.069   -0.118   -0.118
140  AS5 ~~ AUA4  0.130 -0.014  -0.014   -0.023   -0.023
141  AS6 ~~  AS7  0.738 -0.027  -0.027   -0.052   -0.052
142  AS6 ~~  AS8  1.052  0.032   0.032    0.062    0.062
143  AS6 ~~  AS9  4.961  0.063   0.063    0.134    0.134
144  AS6 ~~  AF1  3.044 -0.056  -0.056   -0.104   -0.104
145  AS6 ~~  AF2  2.437 -0.051  -0.051   -0.095   -0.095
146  AS6 ~~  AF3  2.553 -0.051  -0.051   -0.095   -0.095
147  AS6 ~~  AF4  2.090 -0.050  -0.050   -0.087   -0.087
148  AS6 ~~  MI1  0.127  0.010   0.010    0.022    0.022
149  AS6 ~~  MI2  0.011 -0.003  -0.003   -0.006   -0.006
150  AS6 ~~  MI3  0.095  0.011   0.011    0.019    0.019
151  AS6 ~~  MI4  0.092 -0.011  -0.011   -0.018   -0.018
152  AS6 ~~  MI5  0.272 -0.016  -0.016   -0.031   -0.031
153  AS6 ~~ AUA1  0.035 -0.007  -0.007   -0.011   -0.011
154  AS6 ~~ AUA2  0.133  0.011   0.011    0.022    0.022
155  AS6 ~~ AUA3  0.506  0.020   0.020    0.043    0.043
156  AS6 ~~ AUA4  2.410 -0.049  -0.049   -0.095   -0.095
157  AS7 ~~  AS8  1.769  0.052   0.052    0.081    0.081
158  AS7 ~~  AS9  0.095  0.011   0.011    0.019    0.019
159  AS7 ~~  AF1  2.433 -0.063  -0.063   -0.093   -0.093
160  AS7 ~~  AF2  1.471 -0.050  -0.050   -0.074   -0.074
161  AS7 ~~  AF3  0.701 -0.034  -0.034   -0.050   -0.050
162  AS7 ~~  AF4  0.125 -0.015  -0.015   -0.021   -0.021
163  AS7 ~~  MI1  0.126 -0.013  -0.013   -0.021   -0.021
164  AS7 ~~  MI2  0.806  0.036   0.036    0.054    0.054
165  AS7 ~~  MI3  4.071 -0.091  -0.091   -0.121   -0.121
166  AS7 ~~  MI4  2.347  0.067   0.067    0.091    0.091
167  AS7 ~~  MI5  0.003  0.002   0.002    0.003    0.003
168  AS7 ~~ AUA1  0.138 -0.017  -0.017   -0.022   -0.022
169  AS7 ~~ AUA2  0.942 -0.037  -0.037   -0.059   -0.059
170  AS7 ~~ AUA3  1.301 -0.041  -0.041   -0.069   -0.069
171  AS7 ~~ AUA4  0.400  0.025   0.025    0.039    0.039
172  AS8 ~~  AS9  2.294  0.053   0.053    0.092    0.092
173  AS8 ~~  AF1  0.035 -0.007  -0.007   -0.011   -0.011
174  AS8 ~~  AF2  0.039  0.008   0.008    0.012    0.012
175  AS8 ~~  AF3  0.147 -0.015  -0.015   -0.023   -0.023
176  AS8 ~~  AF4  2.355 -0.066  -0.066   -0.093   -0.093
177  AS8 ~~  MI1  2.370 -0.056  -0.056   -0.093   -0.093
178  AS8 ~~  MI2  2.233 -0.059  -0.059   -0.089   -0.089
179  AS8 ~~  MI3  0.615  0.035   0.035    0.047    0.047
180  AS8 ~~  MI4  2.210 -0.065  -0.065   -0.089   -0.089
181  AS8 ~~  MI5  1.053 -0.040  -0.040   -0.061   -0.061
182  AS8 ~~ AUA1  0.115 -0.015  -0.015   -0.020   -0.020
183  AS8 ~~ AUA2  1.772 -0.051  -0.051   -0.081   -0.081
184  AS8 ~~ AUA3  8.486 -0.105  -0.105   -0.176   -0.176
185  AS8 ~~ AUA4  2.816 -0.066  -0.066   -0.103   -0.103
186  AS9 ~~  AF1  1.186 -0.039  -0.039   -0.065   -0.065
187  AS9 ~~  AF2  0.018 -0.005  -0.005   -0.008   -0.008
188  AS9 ~~  AF3  0.106  0.012   0.012    0.019    0.019
189  AS9 ~~  AF4  3.138 -0.069  -0.069   -0.106   -0.106
190  AS9 ~~  MI1  5.119 -0.074  -0.074   -0.136   -0.136
191  AS9 ~~  MI2  3.713 -0.068  -0.068   -0.114   -0.114
192  AS9 ~~  MI3  0.027  0.007   0.007    0.010    0.010
193  AS9 ~~  MI4  0.304 -0.022  -0.022   -0.033   -0.033
194  AS9 ~~  MI5  4.392 -0.074  -0.074   -0.125   -0.125
195  AS9 ~~ AUA1  6.887 -0.106  -0.106   -0.157   -0.157
196  AS9 ~~ AUA2  3.012 -0.059  -0.059   -0.105   -0.105
197  AS9 ~~ AUA3  0.239 -0.016  -0.016   -0.029   -0.029
198  AS9 ~~ AUA4  3.230 -0.063  -0.063   -0.110   -0.110
199  AF1 ~~  AF2 12.031  0.146   0.146    0.208    0.208
200  AF1 ~~  AF3 20.113  0.184   0.184    0.263    0.263
201  AF1 ~~  AF4 18.999  0.192   0.192    0.259    0.259
202  AF1 ~~  MI1  3.127  0.066   0.066    0.105    0.105
203  AF1 ~~  MI2  2.997 -0.070  -0.070   -0.102   -0.102
204  AF1 ~~  MI3  0.024 -0.007  -0.007   -0.009   -0.009
205  AF1 ~~  MI4  0.057 -0.011  -0.011   -0.014   -0.014
206  AF1 ~~  MI5  0.092 -0.012  -0.012   -0.018   -0.018
207  AF1 ~~ AUA1  6.189 -0.114  -0.114   -0.147   -0.147
208  AF1 ~~ AUA2  0.014 -0.005  -0.005   -0.007   -0.007
209  AF1 ~~ AUA3  0.023 -0.006  -0.006   -0.009   -0.009
210  AF1 ~~ AUA4  0.540  0.029   0.029    0.044    0.044
211  AF2 ~~  AF3  1.501  0.052   0.052    0.073    0.073
212  AF2 ~~  AF4 10.383  0.146   0.146    0.194    0.194
213  AF2 ~~  MI1  9.780  0.120   0.120    0.189    0.189
214  AF2 ~~  MI2  2.427 -0.065  -0.065   -0.093   -0.093
215  AF2 ~~  MI3  0.143  0.018   0.018    0.023    0.023
216  AF2 ~~  MI4  0.879  0.043   0.043    0.056    0.056
217  AF2 ~~  MI5  0.604  0.032   0.032    0.047    0.047
218  AF2 ~~ AUA1  0.339  0.028   0.028    0.035    0.035
219  AF2 ~~ AUA2  0.020  0.006   0.006    0.009    0.009
220  AF2 ~~ AUA3  0.453  0.026   0.026    0.041    0.041
221  AF2 ~~ AUA4  0.019  0.006   0.006    0.009    0.009
222  AF3 ~~  AF4  3.396  0.081   0.081    0.109    0.109
223  AF3 ~~  MI1  0.156  0.015   0.015    0.023    0.023
224  AF3 ~~  MI2  0.016 -0.005  -0.005   -0.007   -0.007
225  AF3 ~~  MI3  0.002 -0.002  -0.002   -0.003   -0.003
226  AF3 ~~  MI4  0.109  0.015   0.015    0.019    0.019
227  AF3 ~~  MI5  2.635  0.066   0.066    0.095    0.095
228  AF3 ~~ AUA1  0.238  0.023   0.023    0.029    0.029
229  AF3 ~~ AUA2  0.018 -0.005  -0.005   -0.008   -0.008
230  AF3 ~~ AUA3  0.049  0.008   0.008    0.013    0.013
231  AF3 ~~ AUA4  0.012  0.004   0.004    0.007    0.007
232  AF4 ~~  MI1  2.068  0.058   0.058    0.086    0.086
233  AF4 ~~  MI2  1.819  0.059   0.059    0.080    0.080
234  AF4 ~~  MI3  0.002 -0.002  -0.002   -0.003   -0.003
235  AF4 ~~  MI4  0.261  0.025   0.025    0.030    0.030
236  AF4 ~~  MI5  0.061  0.011   0.011    0.015    0.015
237  AF4 ~~ AUA1  0.275 -0.026  -0.026   -0.031   -0.031
238  AF4 ~~ AUA2  0.267  0.022   0.022    0.031    0.031
239  AF4 ~~ AUA3  2.030 -0.056  -0.056   -0.085   -0.085
240  AF4 ~~ AUA4  0.164  0.018   0.018    0.025    0.025
241  MI1 ~~  MI2  6.068  0.090   0.090    0.146    0.146
242  MI1 ~~  MI3  8.331  0.121   0.121    0.172    0.172
243  MI1 ~~  MI4  8.702  0.120   0.120    0.174    0.174
244  MI1 ~~  MI5  0.432 -0.024  -0.024   -0.039   -0.039
245  MI1 ~~ AUA1  3.380  0.077   0.077    0.110    0.110
246  MI1 ~~ AUA2  0.457  0.024   0.024    0.041    0.041
247  MI1 ~~ AUA3  0.033 -0.006  -0.006   -0.011   -0.011
248  MI1 ~~ AUA4  3.144  0.065   0.065    0.108    0.108
249  MI2 ~~  MI3 22.148  0.214   0.214    0.278    0.278
250  MI2 ~~  MI4  4.983  0.099   0.099    0.131    0.131
251  MI2 ~~  MI5  0.711  0.034   0.034    0.050    0.050
252  MI2 ~~ AUA1  5.645  0.108   0.108    0.140    0.140
253  MI2 ~~ AUA2  1.484  0.047   0.047    0.073    0.073
254  MI2 ~~ AUA3  0.417  0.023   0.023    0.038    0.038
255  MI2 ~~ AUA4  0.141 -0.015  -0.015   -0.023   -0.023
256  MI3 ~~  MI4 10.514  0.163   0.163    0.191    0.191
257  MI3 ~~  MI5  0.095 -0.014  -0.014   -0.018   -0.018
258  MI3 ~~ AUA1  1.624  0.066   0.066    0.076    0.076
259  MI3 ~~ AUA2  0.088 -0.013  -0.013   -0.018   -0.018
260  MI3 ~~ AUA3  0.119 -0.014  -0.014   -0.021   -0.021
261  MI3 ~~ AUA4  0.506 -0.032  -0.032   -0.043   -0.043
262  MI4 ~~  MI5  5.730  0.105   0.105    0.140    0.140
263  MI4 ~~ AUA1  0.866  0.047   0.047    0.055    0.055
264  MI4 ~~ AUA2  0.003  0.002   0.002    0.003    0.003
265  MI4 ~~ AUA3  2.620  0.065   0.065    0.096    0.096
266  MI4 ~~ AUA4  0.156  0.017   0.017    0.024    0.024
267  MI5 ~~ AUA1  0.207  0.021   0.021    0.027    0.027
268  MI5 ~~ AUA2  2.520  0.061   0.061    0.095    0.095
269  MI5 ~~ AUA3  1.464  0.044   0.044    0.072    0.072
270  MI5 ~~ AUA4  0.002  0.002   0.002    0.003    0.003
271 AUA1 ~~ AUA2 17.432  0.183   0.183    0.250    0.250
272 AUA1 ~~ AUA3  9.421  0.127   0.127    0.183    0.183
273 AUA1 ~~ AUA4  5.782  0.109   0.109    0.146    0.146
274 AUA2 ~~ AUA3  7.002  0.092   0.092    0.159    0.159
275 AUA2 ~~ AUA4  6.210  0.095   0.095    0.153    0.153
276 AUA3 ~~ AUA4  5.069  0.081   0.081    0.137    0.137
\end{verbatim}

Kline recommends evaluating the ``cor'' residuals. In our output, these seem to be the ``cor.bollen'' and are near the bottom. He recommends that residuals \textgreater{} .10 may be possible sources for misfit. He also indicated that patterns may be helpful (is there an item that has consistently high residuals).

Kline also cautions that there is no dependable or trustworthy connection between the size of the residual and the type or degree of model misspecification.

My first read of our results is that the items in the AS\# factor were well-defined. I suspect that a multi-factor solution will improve the fit.

The \emph{tidySEM} package has some useful tools to export the results to .csv files. This first set of code exports the fit indices.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{UniDFitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(grmsAAW1fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Registered S3 method overwritten by 'tidySEM':
  method          from  
  predict.MxModel OpenMx
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{UniDFitStats}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         Name Parameters      fmin    chisq  df pvalue baseline.chisq
1 grmsAAW1fit         44 0.7310043 444.4506 209      0       1439.317
  baseline.df baseline.pvalue       cfi       tli      nnfi       rfi       nfi
1         231               0 0.8051418 0.7846304 0.7846304 0.6587029 0.6912074
       pnfi       ifi       rni        LL unrestricted.logl      aic      bic
1 0.6253781 0.8086262 0.8051418 -8387.014         -8164.789 16862.03 17025.58
    n     bic2      rmsea rmsea.ci.lower rmsea.ci.upper rmsea.ci.level
1 304 16886.03 0.06087514     0.05302463     0.06871732            0.9
  rmsea.pvalue rmsea.close.h0 rmsea.notclose.pvalue rmsea.notclose.h0
1   0.01213836           0.05         0.00001944378              0.08
         rmr rmr_nomean       srmr srmr_bentler srmr_bentler_nomean       crmr
1 0.05740799 0.05740799 0.06699187   0.06699187          0.06699187 0.07010942
  crmr_nomean srmr_mplus srmr_mplus_nomean    cn_05    cn_01       gfi     agfi
1  0.07010942 0.06699187        0.06699187 167.7071 178.4826 0.8602203 0.830793
       pgfi       mfi     ecvi
1 0.7106168 0.6789184 1.751482
\end{verbatim}

The second set of code exports the parameter estimates.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{UniD\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(grmsAAW1fit, }\AttributeTok{digits=}\DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{UniD\_paramEsts}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       lhs op     rhs   est    se  pval        confint  est_sig est_std se_std
1  GRMSAAW =~     AS1 1.000 0.000  <NA> [1.000, 1.000]    1.000   0.535  0.046
2  GRMSAAW =~     AS2 1.069 0.152 0.000 [0.770, 1.368] 1.069***   0.520  0.047
3  GRMSAAW =~     AS3 1.024 0.143 0.000 [0.743, 1.304] 1.024***   0.535  0.046
4  GRMSAAW =~     AS4 0.909 0.137 0.000 [0.641, 1.177] 0.909***   0.482  0.049
5  GRMSAAW =~     AS5 1.177 0.154 0.000 [0.875, 1.479] 1.177***   0.590  0.043
6  GRMSAAW =~     AS6 0.721 0.108 0.000 [0.509, 0.934] 0.721***   0.483  0.049
7  GRMSAAW =~     AS7 0.914 0.137 0.000 [0.646, 1.181] 0.914***   0.487  0.049
8  GRMSAAW =~     AS8 0.927 0.137 0.000 [0.658, 1.196] 0.927***   0.494  0.048
9  GRMSAAW =~     AS9 0.735 0.117 0.000 [0.505, 0.965] 0.735***   0.445  0.051
10 GRMSAAW =~     AF1 0.675 0.125 0.000 [0.431, 0.920] 0.675***   0.370  0.054
11 GRMSAAW =~     AF2 0.975 0.144 0.000 [0.692, 1.258] 0.975***   0.493  0.048
12 GRMSAAW =~     AF3 0.555 0.120 0.000 [0.320, 0.790] 0.555***   0.308  0.057
13 GRMSAAW =~     AF4 0.851 0.141 0.000 [0.575, 1.128] 0.851***   0.425  0.052
14 GRMSAAW =~     MI1 0.744 0.120 0.000 [0.508, 0.980] 0.744***   0.438  0.051
15 GRMSAAW =~     MI2 0.641 0.122 0.000 [0.402, 0.880] 0.641***   0.357  0.055
16 GRMSAAW =~     MI3 0.860 0.146 0.000 [0.575, 1.146] 0.860***   0.413  0.052
17 GRMSAAW =~     MI4 0.601 0.130 0.000 [0.346, 0.856] 0.601***   0.307  0.057
18 GRMSAAW =~     MI5 0.655 0.122 0.000 [0.415, 0.895] 0.655***   0.365  0.054
19 GRMSAAW =~    AUA1 0.825 0.144 0.000 [0.543, 1.107] 0.825***   0.398  0.053
20 GRMSAAW =~    AUA2 0.878 0.132 0.000 [0.620, 1.137] 0.878***   0.483  0.049
21 GRMSAAW =~    AUA3 0.714 0.118 0.000 [0.483, 0.944] 0.714***   0.426  0.052
22 GRMSAAW =~    AUA4 1.060 0.146 0.000 [0.774, 1.346] 1.060***   0.547  0.045
23     AS1 ~~     AS1 0.600 0.052 0.000 [0.497, 0.702] 0.600***   0.713  0.049
24     AS2 ~~     AS2 0.744 0.064 0.000 [0.618, 0.870] 0.744***   0.730  0.049
25     AS3 ~~     AS3 0.631 0.055 0.000 [0.523, 0.738] 0.631***   0.714  0.049
26     AS4 ~~     AS4 0.657 0.056 0.000 [0.547, 0.767] 0.657***   0.767  0.047
27     AS5 ~~     AS5 0.624 0.056 0.000 [0.515, 0.733] 0.624***   0.651  0.050
28     AS6 ~~     AS6 0.412 0.035 0.000 [0.343, 0.480] 0.412***   0.766  0.047
29     AS7 ~~     AS7 0.648 0.055 0.000 [0.539, 0.757] 0.648***   0.763  0.047
30     AS8 ~~     AS8 0.641 0.055 0.000 [0.534, 0.749] 0.641***   0.756  0.048
31     AS9 ~~     AS9 0.528 0.045 0.000 [0.440, 0.615] 0.528***   0.802  0.045
32     AF1 ~~     AF1 0.693 0.058 0.000 [0.580, 0.807] 0.693***   0.863  0.040
33     AF2 ~~     AF2 0.714 0.061 0.000 [0.594, 0.834] 0.714***   0.757  0.048
34     AF3 ~~     AF3 0.707 0.058 0.000 [0.593, 0.821] 0.707***   0.905  0.035
35     AF4 ~~     AF4 0.794 0.067 0.000 [0.663, 0.925] 0.794***   0.820  0.044
36     MI1 ~~     MI1 0.564 0.048 0.000 [0.470, 0.657] 0.564***   0.809  0.045
37     MI2 ~~     MI2 0.678 0.056 0.000 [0.568, 0.789] 0.678***   0.873  0.039
38     MI3 ~~     MI3 0.870 0.073 0.000 [0.727, 1.013] 0.870***   0.830  0.043
39     MI4 ~~     MI4 0.839 0.069 0.000 [0.703, 0.975] 0.839***   0.906  0.035
40     MI5 ~~     MI5 0.672 0.056 0.000 [0.562, 0.781] 0.672***   0.866  0.040
41    AUA1 ~~    AUA1 0.872 0.073 0.000 [0.729, 1.015] 0.872***   0.842  0.042
42    AUA2 ~~    AUA2 0.610 0.052 0.000 [0.508, 0.712] 0.610***   0.766  0.047
43    AUA3 ~~    AUA3 0.553 0.047 0.000 [0.462, 0.644] 0.553***   0.818  0.044
44    AUA4 ~~    AUA4 0.634 0.055 0.000 [0.525, 0.742] 0.634***   0.701  0.049
45 GRMSAAW ~~ GRMSAAW 0.241 0.051 0.000 [0.140, 0.342] 0.241***   1.000  0.000
   pval_std    confint_std est_sig_std             label
1     0.000 [0.445, 0.626]    0.535***    GRMSAAW.BY.AS1
2     0.000 [0.428, 0.612]    0.520***    GRMSAAW.BY.AS2
3     0.000 [0.445, 0.625]    0.535***    GRMSAAW.BY.AS3
4     0.000 [0.387, 0.578]    0.482***    GRMSAAW.BY.AS4
5     0.000 [0.507, 0.674]    0.590***    GRMSAAW.BY.AS5
6     0.000 [0.387, 0.579]    0.483***    GRMSAAW.BY.AS6
7     0.000 [0.391, 0.582]    0.487***    GRMSAAW.BY.AS7
8     0.000 [0.399, 0.589]    0.494***    GRMSAAW.BY.AS8
9     0.000 [0.345, 0.545]    0.445***    GRMSAAW.BY.AS9
10    0.000 [0.264, 0.476]    0.370***    GRMSAAW.BY.AF1
11    0.000 [0.398, 0.588]    0.493***    GRMSAAW.BY.AF2
12    0.000 [0.197, 0.419]    0.308***    GRMSAAW.BY.AF3
13    0.000 [0.323, 0.526]    0.425***    GRMSAAW.BY.AF4
14    0.000 [0.337, 0.538]    0.438***    GRMSAAW.BY.MI1
15    0.000 [0.249, 0.464]    0.357***    GRMSAAW.BY.MI2
16    0.000 [0.310, 0.515]    0.413***    GRMSAAW.BY.MI3
17    0.000 [0.195, 0.418]    0.307***    GRMSAAW.BY.MI4
18    0.000 [0.259, 0.472]    0.365***    GRMSAAW.BY.MI5
19    0.000 [0.294, 0.502]    0.398***   GRMSAAW.BY.AUA1
20    0.000 [0.388, 0.579]    0.483***   GRMSAAW.BY.AUA2
21    0.000 [0.325, 0.528]    0.426***   GRMSAAW.BY.AUA3
22    0.000 [0.458, 0.636]    0.547***   GRMSAAW.BY.AUA4
23    0.000 [0.617, 0.810]    0.713***     Variances.AS1
24    0.000 [0.634, 0.825]    0.730***     Variances.AS2
25    0.000 [0.618, 0.810]    0.714***     Variances.AS3
26    0.000 [0.675, 0.860]    0.767***     Variances.AS4
27    0.000 [0.553, 0.750]    0.651***     Variances.AS5
28    0.000 [0.674, 0.859]    0.766***     Variances.AS6
29    0.000 [0.670, 0.856]    0.763***     Variances.AS7
30    0.000 [0.662, 0.849]    0.756***     Variances.AS8
31    0.000 [0.713, 0.891]    0.802***     Variances.AS9
32    0.000 [0.784, 0.942]    0.863***     Variances.AF1
33    0.000 [0.663, 0.850]    0.757***     Variances.AF2
34    0.000 [0.837, 0.973]    0.905***     Variances.AF3
35    0.000 [0.733, 0.906]    0.820***     Variances.AF4
36    0.000 [0.721, 0.896]    0.809***     Variances.MI1
37    0.000 [0.796, 0.949]    0.873***     Variances.MI2
38    0.000 [0.745, 0.914]    0.830***     Variances.MI3
39    0.000 [0.838, 0.974]    0.906***     Variances.MI4
40    0.000 [0.788, 0.944]    0.866***     Variances.MI5
41    0.000 [0.759, 0.924]    0.842***    Variances.AUA1
42    0.000 [0.674, 0.859]    0.766***    Variances.AUA2
43    0.000 [0.732, 0.905]    0.818***    Variances.AUA3
44    0.000 [0.604, 0.798]    0.701***    Variances.AUA4
45     <NA> [1.000, 1.000]       1.000 Variances.GRMSAAW
\end{verbatim}

We can write each of these to a .csv file that will be stored in the same folder as your .rmd file.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(UniDFitStats, }\AttributeTok{file =} \StringTok{"UnidimensionalFitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(UniD\_paramEsts, }\AttributeTok{file =} \StringTok{"UnidimensionalParamEsts.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\emph{Troubleshooting} If, while working with this function you get the error, ``Error in file(file, ifelse(append,''a'', ``w'')) : cannot open the connection'' it is because the .csv file that received your table is still open. R is just trying to write over it. A similar error happens when knitting or updating any spreadsheet or word document.

\hypertarget{apa-style-results-from-the-unidimensional-model}{%
\subsubsection{APA Style Results from the Unidimensional model}\label{apa-style-results-from-the-unidimensional-model}}

Writing up an APA style results section for a CFA involves describing the statistics that are being used and then presenting the results.

\begin{quote}
\textbf{Model testing}. To evaluate the models we, we used confirmatory factor analysis (CFA) in the R package, \emph{lavaan} (v.0-6.9) with maximum likelihood estimation. Our sample size was 304. We selected fit criteria for their capacity to assess different aspects of the statistical analysis. As is common among SEM researchers, we reported the Chi-square goodness of fit (\(\chi^2\)). This evaluates the discrepancy between the unrestricted sample matrix and the restricted covariance matrix. Although the associated \(p\) value indicates adequate fit when the value is non-significant, it is widely recognized that large sample size can result in a statistically significant p value \citep{byrne_structural_2016}. The comparative fit index (CFI) is an incremental index, comparing the hypothesized model at least .90 and perhaps higher than .95 \citep{kline_principles_2016}. The root mean square error of approximation (RMSEA) takes into account the error of approximation in the population and expresses it per degree of freedom. As such, the fit indicator considers the complexity of the model. Ideal values are equal to or less than .05, values less than .08 represent reasonable fit, and values between .08 and .10 represent mediocre fit. The standardized root mean residual (SRMR) is a standardized measure of the mean absolute covariance residual -- the overall difference between the observed and predicted correlations. Values greater than .10 may indicate poor fit and inspection of residuals is then advised. Kline \citeyearpar{kline_principles_2016} advised researchers to be cautious when using these criteria as strict cut-offs. Elements such as sample size and model complexity should be considered when evaluating fit.
\end{quote}

\begin{quote}
Our first model was unidimensional where each of the 24 items loaded onto a single factor representing overall, gendered racial microaggressions towards Asian American women. The Chi-square index was statistically significant (\(\chi ^{2}(209) = 444.451, p < .001\)) indicating likely misfit. The CFI value of .81 indicated poor fit. In contrast, the RMSEA = 0.061, 90\% CI(0.053, 0.069) and SRMR = 0.067 both fell within the ranges of acceptability. The AIC and BIC values were 16862.028 and 17025.577, respectively, and will become useful in comparing subsequent models.
\end{quote}

\hypertarget{modeling-the-grmsaaw-as-a-first-order-4-factor-model}{%
\subsection{Modeling the GRMSAAW as a First-Order, 4-factor model}\label{modeling-the-grmsaaw-as-a-first-order-4-factor-model}}

\hypertarget{specifying-and-running-the-model}{%
\subsubsection{Specifying and Running the Model}\label{specifying-and-running-the-model}}

As we know from the article, the GRMSAAW has four subscales. Therefore, let's respecify it as a first-order, four-factor model, allowing the factors to correlate.

\textbf{Model identification} is always a consideration. In a multi-dimensional model, each factor requires a minimum of two items/indicators. Our shortest scales are the AF and AUA scales, each with 4 items, so we should be identified.

We will be using the \emph{cfa()} function in lavaan. When we do this, it does three things by default:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The factor loading of the first indicator of a latent variable is fixed to 1.0; this fixes the scale of the LV
\item
  Residual variances are added automatically.
\item
  All exogenous LVs are correlated.\\
\end{enumerate}

\begin{itemize}
\tightlist
\item
  If you are specifying an orthogonal model you will want to switch off the default behavior by including the statement: auto.cov.lv.x=FALSE
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grmsAAW4mod }\OtherTok{\textless{}{-}} \StringTok{"AS =\textasciitilde{} AS1 + AS2 + AS3 + AS4 + AS5 + AS6 + AS7 + AS8 + AS9}
\StringTok{             AF =\textasciitilde{} AF1 + AF2 + AF3 + AF4 }
\StringTok{             MI =\textasciitilde{} MI1 + MI2 + MI3 + MI4 + MI5}
\StringTok{             AUA =\textasciitilde{} AUA1 + AUA2 + AUA3 + AUA4"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This code is identical to the one we ran above {-}{-} in this code}
\CommentTok{\# below, we are just clearly specifying the covariances {-}{-} but the}
\CommentTok{\# default of lavaan is to correlate latent variables when the cfa()}
\CommentTok{\# function is used.}

\NormalTok{grmsAAW4mod }\OtherTok{\textless{}{-}} \StringTok{"AS =\textasciitilde{} AS1 + AS2 + AS3 + AS4 + AS5 + AS6 + AS7 + AS8 + AS9}
\StringTok{             AF =\textasciitilde{} AF1 + AF2 + AF3 + AF4 }
\StringTok{             MI =\textasciitilde{} MI1 + MI2 + MI3 + MI4 + MI5}
\StringTok{             AUA =\textasciitilde{} AUA1 + AUA2 + AUA3 + AUA4}
\StringTok{             }
\StringTok{\#covariances in our oblique model}
\StringTok{  AS \textasciitilde{}\textasciitilde{} AF}
\StringTok{  AS \textasciitilde{}\textasciitilde{} MI}
\StringTok{  AS \textasciitilde{}\textasciitilde{} AUA}
\StringTok{  AF \textasciitilde{}\textasciitilde{} MI}
\StringTok{  AF \textasciitilde{}\textasciitilde{} AUA}
\StringTok{  MI \textasciitilde{}\textasciitilde{} AUA}
\StringTok{  "}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{grmsAAW4fit }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(grmsAAW4mod, }\AttributeTok{data =}\NormalTok{ dfGRMSAAW)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(grmsAAW4fit, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{rsquare =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 42 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        50

  Number of observations                           304

Model Test User Model:
                                                      
  Test statistic                               232.453
  Degrees of freedom                               203
  P-value (Chi-square)                           0.076

Model Test Baseline Model:

  Test statistic                              1439.317
  Degrees of freedom                               231
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.976
  Tucker-Lewis Index (TLI)                       0.972

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -8281.015
  Loglikelihood unrestricted model (H1)      -8164.789
                                                      
  Akaike (AIC)                               16662.030
  Bayesian (BIC)                             16847.882
  Sample-size adjusted Bayesian (SABIC)      16689.307

Root Mean Square Error of Approximation:

  RMSEA                                          0.022
  90 Percent confidence interval - lower         0.000
  90 Percent confidence interval - upper         0.034
  P-value H_0: RMSEA <= 0.050                    1.000
  P-value H_0: RMSEA >= 0.080                    0.000

Standardized Root Mean Square Residual:

  SRMR                                           0.047

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  AS =~                                                                 
    AS1               1.000                               0.550    0.600
    AS2               1.132    0.136    8.330    0.000    0.623    0.617
    AS3               0.958    0.123    7.769    0.000    0.527    0.561
    AS4               0.901    0.120    7.504    0.000    0.496    0.536
    AS5               1.152    0.134    8.620    0.000    0.634    0.647
    AS6               0.669    0.094    7.133    0.000    0.368    0.503
    AS7               0.829    0.118    7.043    0.000    0.456    0.495
    AS8               0.905    0.120    7.551    0.000    0.498    0.540
    AS9               0.757    0.104    7.256    0.000    0.417    0.514
  AF =~                                                                 
    AF1               1.000                               0.505    0.563
    AF2               1.195    0.174    6.862    0.000    0.603    0.621
    AF3               0.738    0.137    5.395    0.000    0.373    0.422
    AF4               1.138    0.171    6.665    0.000    0.575    0.584
  MI =~                                                                 
    MI1               1.000                               0.482    0.577
    MI2               0.917    0.148    6.216    0.000    0.442    0.501
    MI3               1.169    0.177    6.602    0.000    0.563    0.550
    MI4               0.921    0.157    5.865    0.000    0.444    0.461
    MI5               0.688    0.137    5.018    0.000    0.332    0.377
  AUA =~                                                                
    AUA1              1.000                               0.553    0.543
    AUA2              0.981    0.140    7.016    0.000    0.543    0.608
    AUA3              0.785    0.122    6.457    0.000    0.434    0.528
    AUA4              1.083    0.152    7.140    0.000    0.599    0.630

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  AS ~~                                                                 
    AF                0.148    0.030    4.951    0.000    0.533    0.533
    MI                0.136    0.028    4.889    0.000    0.513    0.513
    AUA               0.181    0.034    5.257    0.000    0.595    0.595
  AF ~~                                                                 
    MI                0.154    0.031    5.010    0.000    0.632    0.632
    AUA               0.164    0.034    4.805    0.000    0.588    0.588
  MI ~~                                                                 
    AUA               0.189    0.036    5.303    0.000    0.709    0.709

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .AS1               0.538    0.050   10.833    0.000    0.538    0.640
   .AS2               0.632    0.059   10.699    0.000    0.632    0.620
   .AS3               0.605    0.054   11.111    0.000    0.605    0.685
   .AS4               0.610    0.054   11.260    0.000    0.610    0.713
   .AS5               0.557    0.053   10.408    0.000    0.557    0.581
   .AS6               0.401    0.035   11.433    0.000    0.401    0.747
   .AS7               0.641    0.056   11.470    0.000    0.641    0.755
   .AS8               0.601    0.053   11.235    0.000    0.601    0.708
   .AS9               0.484    0.043   11.379    0.000    0.484    0.736
   .AF1               0.548    0.055    9.928    0.000    0.548    0.683
   .AF2               0.579    0.064    9.062    0.000    0.579    0.614
   .AF3               0.642    0.057   11.230    0.000    0.642    0.822
   .AF4               0.638    0.066    9.651    0.000    0.638    0.659
   .MI1               0.465    0.047    9.823    0.000    0.465    0.667
   .MI2               0.582    0.055   10.664    0.000    0.582    0.749
   .MI3               0.731    0.072   10.158    0.000    0.731    0.697
   .MI4               0.729    0.066   10.994    0.000    0.729    0.787
   .MI5               0.665    0.058   11.519    0.000    0.665    0.858
   .AUA1              0.730    0.069   10.535    0.000    0.730    0.705
   .AUA2              0.501    0.051    9.787    0.000    0.501    0.630
   .AUA3              0.487    0.046   10.675    0.000    0.487    0.721
   .AUA4              0.546    0.058    9.475    0.000    0.546    0.603
    AS                0.303    0.058    5.264    0.000    1.000    1.000
    AF                0.255    0.058    4.412    0.000    1.000    1.000
    MI                0.232    0.051    4.559    0.000    1.000    1.000
    AUA               0.306    0.070    4.391    0.000    1.000    1.000

R-Square:
                   Estimate
    AS1               0.360
    AS2               0.380
    AS3               0.315
    AS4               0.287
    AS5               0.419
    AS6               0.253
    AS7               0.245
    AS8               0.292
    AS9               0.264
    AF1               0.317
    AF2               0.386
    AF3               0.178
    AF4               0.341
    MI1               0.333
    MI2               0.251
    MI3               0.303
    MI4               0.213
    MI5               0.142
    AUA1              0.295
    AUA2              0.370
    AUA3              0.279
    AUA4              0.397
\end{verbatim}

I'm inclined to immediately create a figure. This permits me to inspect for modeling errors ``at a glance.''

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(grmsAAW4fit, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{10-CFA_1stOrder_files/figure-latex/unnamed-chunk-16-1.pdf}

Among my first steps are also to write the code to export the results. Because this involved correlated factors, I will produce an additional table. First, I create the objects that hold the results.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Coor4FitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(grmsAAW4fit)}
\NormalTok{Coor4\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(grmsAAW4fit, }\AttributeTok{digits=}\DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{Coor4Corrs }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_cors}\NormalTok{(grmsAAW4fit, }\AttributeTok{digits=}\DecValTok{3}\NormalTok{)}
\CommentTok{\#to see each of the tables, remove the hashtab}
\CommentTok{\#Coor4FitStats}
\CommentTok{\#Coor4\_paramEsts}
\CommentTok{\#Coor4Corrs}
\end{Highlighting}
\end{Shaded}

Next, I export them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(Coor4FitStats, }\AttributeTok{file =} \StringTok{"Coor4FitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(Coor4\_paramEsts, }\AttributeTok{file =} \StringTok{"Coor4\_paramEstsParamEsts.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(Coor4Corrs, }\AttributeTok{file =} \StringTok{"Coor4Corrs.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{interpretation}{%
\subsubsection{Interpretation}\label{interpretation}}

Let's interpret the results.

Our model converged, normally, with 42 iterations. The estimator was the lavaan default, maximum likelihood (ML). All 304 cases were used in the analysis.

I mapped our pattern coefficients into the GRMSAAW tables. Most pattern coefficients are strong, significant, and stably connected to their respective factor. The lowest factor loading was 0.332 (MI5).

A multidimensional factor structure also includes correlations/covariances between factors. We can see that the correlation (look at the Std.all column) shows the following correlations (all are statistically significant):

AS \& AF: 0.533 AS \& MI: 0.513 AS \& AUA: 0.595

AF \& MI: 0.632 AF \& AUA: 0.588 MI \& AUA: 0.709

For our multi-dimensional GRMSAAW4 CFA \(\chi ^{2}(203)=232.453, p = 0.076\), this non-significant \emph{p} value is exactly what we want because it says that our specified model is not statistically significantly different than the covariances in the model. That is, our more parsimonious model is a reasonable explanation (simplification).

The CFI and TLI compare user (the 4-dimensional model we specified) and baseline (where no relations would exist between variables) models. These values will always be close together because the only difference is that the TLI imposes a penalty for any model complexity. The CFI seems to be more commonly reported and its value is 0.976. This means our model performed 98\% better than a model with no relations. It well-exceeds the traditional cutoffs of .90 and the stricter cutoff of .95. The TLI imposes a greater relative penalty for model complexity, consequently it is a smidge lower at 0.972.

The RMSEA one of the \emph{badness of fit}, absolute fit index, statistics where a value of 0.00 is the best fit. Our RMSEA = 0.022 (90\%CI{[}.000, 0.034{]}). As a quick reminder, there is general consensus that \(RMSEA\leq .05\) is desired and an \(RMSEA \geq .10\) points to serious problems. We watch the upper bound of the confidence interval to see that it isn't sneaking into the danger zone.

The SRMR is another absolute, \emph{badness of fit} index (i.e., perfect model fit is when the value = 0.00 and increasingly higher values indicate the ``badness''). The SRMR is a measure of the mean absolute covariance residual. Standardizing the value facilitates interpretation. Poor fit is indicated when \(SRMR \geq .10\). The GRMSAAW SRMR = 0.047.

Recall, Hu and Bentler's \textbf{combination rule} (which is somewhat contested) suggested that the SRMR be interpreted along with the CFI such that: \(CFI \geq .95\) and \(SRMR \leq .08\).

For our unidimensional GRMSAAW CFA, the CFI = 0.976 and the SRMR = 0.047. Our results fell within that acceptable range!

The AIC and BIC utilize an information theory approach to data analysis by combing statistical estimation and model selection into a single framework. The BIC augments the AIC by taking sample size into consideration. We can compare the values from our current model to the former one. The model with the smallest value of the predictive fit index is chosen as the one that is most likely to replicate. It means that this model has relatively better fit and fewer free parameters than competing models. We will do that in the next section.

Before moving to model comparison, it is a good practice for locating sources of misfit (we look for relatively large values) is to inspect the residuals (in the ``cor.bollen'' section), so let's do that.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{fitted}\NormalTok{(grmsAAW4fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$cov
       AS1   AS2   AS3   AS4   AS5   AS6   AS7   AS8   AS9   AF1   AF2   AF3
AS1  0.841                                                                  
AS2  0.343 1.020                                                            
AS3  0.290 0.328 0.883                                                      
AS4  0.273 0.309 0.261 0.856                                                
AS5  0.349 0.395 0.334 0.314 0.958                                          
AS6  0.203 0.229 0.194 0.183 0.233 0.537                                    
AS7  0.251 0.284 0.240 0.226 0.289 0.168 0.849                              
AS8  0.274 0.310 0.262 0.247 0.315 0.183 0.227 0.849                        
AS9  0.229 0.259 0.220 0.207 0.264 0.153 0.190 0.207 0.658                  
AF1  0.148 0.168 0.142 0.133 0.171 0.099 0.123 0.134 0.112 0.803            
AF2  0.177 0.200 0.169 0.159 0.204 0.118 0.147 0.160 0.134 0.305 0.943      
AF3  0.109 0.124 0.105 0.098 0.126 0.073 0.091 0.099 0.083 0.188 0.225 0.781
AF4  0.168 0.191 0.161 0.152 0.194 0.113 0.140 0.152 0.127 0.290 0.347 0.214
MI1  0.136 0.154 0.130 0.123 0.157 0.091 0.113 0.123 0.103 0.154 0.184 0.114
MI2  0.125 0.141 0.120 0.112 0.144 0.084 0.103 0.113 0.094 0.141 0.169 0.104
MI3  0.159 0.180 0.152 0.143 0.183 0.106 0.132 0.144 0.120 0.180 0.215 0.133
MI4  0.125 0.142 0.120 0.113 0.144 0.084 0.104 0.113 0.095 0.142 0.169 0.105
MI5  0.094 0.106 0.090 0.084 0.108 0.063 0.078 0.085 0.071 0.106 0.126 0.078
AUA1 0.181 0.205 0.174 0.163 0.209 0.121 0.150 0.164 0.137 0.164 0.196 0.121
AUA2 0.178 0.201 0.170 0.160 0.205 0.119 0.147 0.161 0.135 0.161 0.193 0.119
AUA3 0.142 0.161 0.136 0.128 0.164 0.095 0.118 0.129 0.108 0.129 0.154 0.095
AUA4 0.196 0.222 0.188 0.177 0.226 0.131 0.163 0.178 0.149 0.178 0.213 0.131
       AF4   MI1   MI2   MI3   MI4   MI5  AUA1  AUA2  AUA3  AUA4
AS1                                                             
AS2                                                             
AS3                                                             
AS4                                                             
AS5                                                             
AS6                                                             
AS7                                                             
AS8                                                             
AS9                                                             
AF1                                                             
AF2                                                             
AF3                                                             
AF4  0.968                                                      
MI1  0.175 0.697                                                
MI2  0.160 0.213 0.777                                          
MI3  0.205 0.271 0.249 1.048                                    
MI4  0.161 0.214 0.196 0.250 0.926                              
MI5  0.120 0.160 0.147 0.187 0.147 0.775                        
AUA1 0.187 0.189 0.173 0.221 0.174 0.130 1.036                  
AUA2 0.183 0.185 0.170 0.217 0.171 0.128 0.300 0.796            
AUA3 0.147 0.148 0.136 0.173 0.137 0.102 0.240 0.236 0.676      
AUA4 0.202 0.205 0.188 0.239 0.189 0.141 0.331 0.325 0.260 0.904
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# lavaan::residuals(grmsAAW4fit, type = \textquotesingle{}raw\textquotesingle{})}
\CommentTok{\# lavaan::residuals(grmsAAW4fit, type = \textquotesingle{}standardized\textquotesingle{})}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{residuals}\NormalTok{(grmsAAW4fit, }\AttributeTok{type =} \StringTok{"cor"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$type
[1] "cor.bollen"

$cov
        AS1    AS2    AS3    AS4    AS5    AS6    AS7    AS8    AS9    AF1
AS1   0.000                                                               
AS2   0.009  0.000                                                        
AS3   0.009  0.028  0.000                                                 
AS4   0.009  0.042  0.019  0.000                                          
AS5   0.065 -0.038 -0.069 -0.002  0.000                                   
AS6  -0.018  0.037 -0.024 -0.080  0.007  0.000                            
AS7  -0.058 -0.010  0.035  0.027 -0.006 -0.049  0.000                     
AS8  -0.057  0.015 -0.002 -0.028  0.027  0.009  0.028  0.000              
AS9   0.010  0.024 -0.033  0.007  0.003  0.052 -0.024  0.007  0.000       
AF1  -0.035 -0.081  0.091  0.011 -0.033 -0.050 -0.038  0.012 -0.040  0.000
AF2   0.025 -0.082  0.021  0.083  0.024  0.008  0.026  0.073  0.044 -0.014
AF3   0.003 -0.095  0.079 -0.060 -0.005 -0.038  0.000  0.013  0.037  0.099
AF4  -0.052 -0.029  0.046  0.045  0.034 -0.014  0.038 -0.024 -0.050  0.031
MI1  -0.036 -0.046 -0.006 -0.016 -0.029  0.078  0.051 -0.009 -0.057  0.038
MI2  -0.080  0.025 -0.016 -0.015  0.010  0.038  0.087 -0.029 -0.063 -0.130
MI3  -0.028  0.016  0.032 -0.017  0.016  0.071 -0.027  0.085  0.046 -0.051
MI4  -0.081 -0.098  0.014 -0.132 -0.060  0.015  0.103 -0.044 -0.011 -0.063
MI5   0.086  0.025  0.045  0.094  0.062  0.056  0.085  0.031 -0.033 -0.014
AUA1 -0.056 -0.107  0.014 -0.110 -0.032  0.021  0.017  0.007 -0.108 -0.151
AUA2  0.016 -0.031 -0.013 -0.062  0.020  0.067  0.016 -0.012 -0.045 -0.028
AUA3  0.037  0.000  0.033 -0.023 -0.027  0.079  0.003 -0.084  0.006 -0.024
AUA4  0.062 -0.024  0.040  0.063  0.067  0.015  0.105  0.002 -0.022  0.025
        AF2    AF3    AF4    MI1    MI2    MI3    MI4    MI5   AUA1   AUA2
AS1                                                                       
AS2                                                                       
AS3                                                                       
AS4                                                                       
AS5                                                                       
AS6                                                                       
AS7                                                                       
AS8                                                                       
AS9                                                                       
AF1                                                                       
AF2   0.000                                                               
AF3  -0.054  0.000                                                        
AF4  -0.015 -0.027  0.000                                                 
MI1   0.122  0.000  0.037  0.000                                          
MI2  -0.090 -0.030  0.030 -0.018  0.000                                   
MI3   0.004 -0.022 -0.030 -0.007  0.094  0.000                            
MI4   0.013 -0.012 -0.016  0.008 -0.010  0.030  0.000                     
MI5   0.067  0.093  0.028 -0.088 -0.017 -0.071  0.057  0.000              
AUA1  0.023  0.012 -0.042  0.036  0.062  0.011 -0.010  0.022  0.000       
AUA2  0.022 -0.008  0.019 -0.008  0.011 -0.051 -0.048  0.085  0.046  0.000
AUA3  0.046  0.011 -0.065 -0.038 -0.005 -0.046  0.036  0.071  0.023 -0.001
AUA4  0.045  0.017  0.033  0.054 -0.045 -0.049 -0.021  0.034 -0.024 -0.021
       AUA3   AUA4
AS1               
AS2               
AS3               
AS4               
AS5               
AS6               
AS7               
AS8               
AS9               
AF1               
AF2               
AF3               
AF4               
MI1               
MI2               
MI3               
MI4               
MI5               
AUA1              
AUA2              
AUA3  0.000       
AUA4 -0.007  0.000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# lavaan::modindices(grmsAAW4fit)}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-comparison}{%
\section{Model Comparison}\label{model-comparison}}

We evaluated two models (i.e., a unidimensional model and four-factor correlated model), which one is better? While, we have the narrative comparison (and would create a table with the comparisons) where the correlated traits/factor fit values (CFI = 0.976, RMSEA = 0.022 (90\%CI{[}.000, 0.034{]}), and SRMR = .058) outperformed the unidimensional ones (CFI = 0.81, RMSEA = 0.061 (90\%CI{[}0.053, 0.069{]}), and SRMR = 0.067). We can formally compare them with statistical comparisons.

Easy are AIC and BIC comparisons where ``smaller value wins.''

AIC GRMSAAW1: 16862.028 AIC GRMSAAW4: 16662.030

BIC GRMSAAW1: 17025.577 BIC GRMSAAW4: 16847.882

In both cases, the smaller values are for the more complex, 4-dimensional model. The interpretation is that the model with the smaller AIC/BIC values is most likely to replicate.

Additionally, the \textbf{chi-square difference test}, \(\chi_{D}^{2}\) can be used to compare nested models. Single-factor CFA models are nested under any other CFA model with two or more factors \emph{for the same indicators}. This is because a one-factor model is a restricted version of any model with multiple factors. Our unidimensional GRMSAAW was nested under the 4-factor GRMSAAW model.

To calculate the chi-square difference test, we first grab the chi-square test values:

GRMSAAW1: \(\chi ^{2}(209) = 444.451, p < .001\) GRMSAAW4: \(\chi ^{2}(203) = 232.453, p = 0.076\)

Given both sets of results we calculate: \(\chi ^{2}(6)= 211.998, p < .05\) and determine that the two models are statistically significantly different. Given that the fit statistics are better for the single-order, correlated, four-factor model, we prefer that one.

How did I do that?

\begin{itemize}
\tightlist
\item
  Subtract the df
\item
  Subtract the chi-square values
\item
  Use a chi-square difference table to look up the chi-square critical value for a 6 df test

  \begin{itemize}
  \tightlist
  \item
    \url{https://www.itl.nist.gov/div898/handbook/eda/section3/eda3674.htm}, or
  \item
    use this code to look it up \emph{qchisq(p, df, lower.tail=FALSE)}
  \item
    the critical value for our test is 12.592
  \end{itemize}
\item
  We conclude that the two models are statistically significantly different; our 4-factor model is preferred.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{209} \SpecialCharTok{{-}} \DecValTok{203}  \CommentTok{\#subtract df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{444.451} \SpecialCharTok{{-}} \FloatTok{232.453}  \CommentTok{\#subtract chi{-}square values}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 211.998
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qchisq}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\DecValTok{6}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 12.59159
\end{verbatim}

Of course, there is a function for something this easy:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{lavTestLRT}\NormalTok{(grmsAAW1fit, grmsAAW4fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Chi-Squared Difference Test

             Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff
grmsAAW4fit 203 16662 16848 232.45                           
grmsAAW1fit 209 16862 17026 444.45        212 0.33606       6
                       Pr(>Chisq)    
grmsAAW4fit                          
grmsAAW1fit < 0.00000000000000022 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

And we get the same result: \(\chi ^{2}(6)= 212, p < .001\)

\hypertarget{apa-results-section-so-far}{%
\subsection{APA Results Section (so far\ldots)}\label{apa-results-section-so-far}}

\begin{quote}
\textbf{Model testing}. To evaluate the models we, we used confirmatory factor analysis (CFA) in the R package, \emph{lavaan} (v.0-6.9) with maximum likelihood estimation. Our sample size was 304. We selected fit criteria for their capacity to assess different aspects of the statistical analysis. As is common among SEM researchers, we reported the Chi-square goodness of fit (\(\chi^2\)). This evaluates the discrepancy between the unrestricted sample matrix and the restricted covariance matrix. Although the associated \(p\) value indicates adequate fit when the value is non-significant, it is widely recognized that large sample size can result in a statistically significant p value \citep{byrne_structural_2016}. The comparative fit index (CFI) is an incremental index, comparing the hypothesized model to the independent/baseline model. Adequate fit is determined when CFI values are at least .90 and perhaps higher than .95 \citep{kline_principles_2016}. The root mean square error of approximation (RMSEA) takes into account the error of approximation in the population and expresses it per degree of freedom. As such, the fit indicator considers the complexity of the model. Ideal values are equal to or less than .05, values less than .08 represent reasonable fit, and values between .08 and .10 represent mediocre fit. The standardized root mean residual (SRMR) is a standardized measure of the mean absolute covariance residual -- the overall difference between the observed and predicted correlations. Values greater than .10 may indicate poor fit and inspection of residuals is then advised. Kline \citeyearpar{kline_principles_2016} advised researchers to be cautious when using these criteria as strict cut-offs. Elements such as sample size and model complexity should be considered when evaluating fit. Table 1 provides a side-by-side comparison of the resulting parameter estimates and fit statistics; Figures 1 and 2 provide a graphic representation of the models tested.
\end{quote}

\begin{quote}
Our first model was unidimensional where each of the 24 items loaded onto a single factor representing overall, gendered racial microaggressions towards Asian American women. The Chi-square index was statistically significant (\(\chi ^{2}(209) = 444.451, p < .001\)) indicating likely misfit. The CFI value of .81 indicated poor fit. In contrast, the RMSEA = 0.061, 90\% CI(0.053, 0.069) and SRMR = 0.067 both fell within the ranges of acceptability. The AIC and BIC values were 16862.028 and 17025.577, respectively, and will become useful in comparing subsequent models.
\end{quote}

\begin{quote}
Our second model was a single-order, correlated traits model where each of the 22 items loaded onto one of four factors. Standardized pattern coefficients ranged between .37 and .60 on the AF factor, between .37 and .63 on the AS factor, between .33 and .56 on the MI factor, and between .43 and .60 on the AUA factor. The Chi-square index was statistically significant (\(\chi ^{2}(203)=232.453, p = 0.076\)) indicating reasonable fit. The CFI value of 0.972 exceeded the recommendation of .95. The RMSEA = MSEA = 0.022 (90\%CI{[}.000, 0.034{]}) was satisfactory. The SRMR value of 0.047 remained below the warning criteria of .10. The AIC and BIC values were 16662.030 and 16847.882, respectively.
\end{quote}

\begin{quote}
The Chi-square difference test (\(\chi ^{2}(6)= 211.998, p < .001\)) was statistically significant and AIC and BIC values of the multidimensional value were lowest. Thus, we conclude the multidimensional model (i.e., the first-order, correlated factors model) is superior and acceptable for use in preliminary research and evaluation.
\end{quote}

\emph{We will continue to create, evaluate, and compare models in the next lesson.}

\hypertarget{a-concluding-thought}{%
\section{A concluding thought}\label{a-concluding-thought}}

Much like the children's game \emph{Don't Break the Ice} we start with a full, saturated, matrix of sample data where every indicator/item is allowed to correlate/covary with every other.

As researchers, we specify a more parsimonious model where we fix some relations to zero and allow others to relate. In our GRMSAAW example, we allowed

\begin{itemize}
\tightlist
\item
  the AF items to relate via their relationship to the AF factor;
\item
  the AS items to relate via their relationship to the AS factor;
\item
  the MI items to relate via their relationship to the MI factor; and
\item
  the AUA items to relate via their relationship to the AUA factor.
\item
  we did not allow any of the items on any given factor to relate to the items on any other factor; these are \emph{hard hypotheses} where we fix the relation to zero.
\end{itemize}

Our goal (especially via the chi-square test) is that we account for as much variance as possible through the specified relations that remain. Harkening to the \emph{Don't Break the Ice} metaphor, we want the ice matrix to remain stable with as many ice cubes deleted as possible.

\includegraphics{images/CFA1st/breakice.jpg} Source: \url{https://www.flickr.com/photos/arfsb/4407495674}

\hypertarget{practice-problems-8}{%
\section{Practice Problems}\label{practice-problems-8}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. In psychometrics, I strongly recommend that you have started with a dataset that has a minimum of three subscales and use it for all of the assignments in the OER. In any case, please plan to:

\begin{itemize}
\tightlist
\item
  Prepare the data frame for CFA.
\item
  Specify and run unidimensional and single order (with correlated factors) models.

  \begin{itemize}
  \tightlist
  \item
    In the next chapter, you will add the specification, evaluation, and write-up of second-order and bifactor models.
  \end{itemize}
\item
  Narrate the adequacy of fit with \(\chi ^{2}\), CFI, RMSEA, SRMR

  \begin{itemize}
  \tightlist
  \item
    Write a mini-results section for each
  \end{itemize}
\item
  Compare model fit with \(\chi ^{2}\Delta\), AIC, and BIC.
\item
  Write an APA style results sections with table(s) and figures.
\end{itemize}

\hypertarget{problem-1-play-around-with-this-simulation.-5}{%
\subsection{Problem \#1: Play around with this simulation.}\label{problem-1-play-around-with-this-simulation.-5}}

The least complex is to change the random seed in the research and rework the problem demonstrated in the lesson. The results \emph{should} map onto the ones obtained in the lecture.

\hypertarget{problem-2-use-simulated-data-from-other-lessons.}{%
\subsection{Problem \#2: Use simulated data from other lessons.}\label{problem-2-use-simulated-data-from-other-lessons.}}

The second option involves utilizing one of the simulated datasets available in this OER. The \protect\hyperlink{sims}{last lesson} in the OER contains three simulations that could be used for all of the statistics-based practice suggestions. Especially if you started with one of these examples in an earlier lesson, I highly recommend you continue with that.

Alternatively, Lewis and Neville's \citeyearpar{lewis_construction_2015} Gendered Racial Microaggressions Scale for Black Women was used in the lessons for exploratory factor analysis and Conover et al.'s \citeyearpar{conover_development_2017} Ableist Microaggressions Scale is used in the lesson on invariance testing. Both of these would be suitable for the CFA homework assignments.

\hypertarget{problem-3-try-something-entirely-new.-5}{%
\subsection{Problem \#3: Try something entirely new.}\label{problem-3-try-something-entirely-new.-5}}

As a third option, you are welcome to use data to which you have access and is suitable for CFA. This could include other simulated data, data found on an open access repository, data from the ReCentering Psych Stats survey described in the \protect\hyperlink{qualTRIX}{Qualtrics lesson}, or your own data (presuming you have permission to use it).

\hypertarget{grading-rubric-5}{%
\subsection{Grading Rubric}\label{grading-rubric-5}}

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5493}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1972}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Possible
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Earned
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1. Prepare data for CFA (items only df, reverse-scored) & 5 & \_\_\_\_\_ \\
2. Specify and run a unidimensional model & 5 & \_\_\_\_\_ \\
3. Narrate adequacy of fit with \(\chi ^{2}\), CFI, RMSEA, SRMR (write a mini-results section) & 5 & \_\_\_\_\_ \\
4. Specify and run a single-order model with correlated factors & 5 & \_\_\_\_\_ \\
5. Narrate adequacy of fit with \(\chi ^{2}\), CFI, RMSEA, SRMR (write a mini-results section) & 5 & \_\_\_\_\_ \\
6. Compare model fit with \(\chi ^{2}\Delta\), AIC, BIC & 5 & \_\_\_\_\_ \\
7. APA style results with table(s) and figure & 5 & \_\_\_\_\_ \\
8. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 40 & \_\_\_\_\_ \\
\end{longtable}

\hypertarget{homeworked-example-5}{%
\section{Homeworked Example}\label{homeworked-example-5}}

\href{https://youtu.be/hjtSGGzbrRM}{Screencast Link}

For more information about the data used in this homeworked example, please refer to the description and codebook located at the end of the \href{https://lhbikos.github.io/ReCenterPsychStats/ReCintro.html\#introduction-to-the-data-set-used-for-homeworked-examples}{introduction} in first volume of ReCentering Psych Stats.

As a brief review, this data is part of an IRB-approved study, with consent to use in teaching demonstrations and to be made available to the general public via the open science framework. Hence, it is appropriate to use in this context. You will notice there are student- and teacher- IDs. These numbers are not actual student and teacher IDs, rather they were further re-identified so that they could not be connected to actual people.

Because this is an actual dataset, if you wish to work the problem along with me, you will need to download the \href{https://github.com/lhbikos/ReC_Psychometrics/blob/main/Worked_Examples/ReC.rds}{ReC.rds} data file from the Worked\_Examples folder in the ReC\_Psychometrics project on the GitHub.

The course evaluation items can be divided into three subscales:

\begin{itemize}
\tightlist
\item
  \textbf{Valued by the student} includes the items: ValObjectives, IncrUnderstanding, IncrInterest
\item
  \textbf{Traditional pedagogy} includes the items: ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation
\item
  \textbf{Socially responsive pedagogy} includes the items: InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration
\end{itemize}

In this Homeworked Example I will conduct and compare single order, unidimensional and correlated traits CFA models. My hope is that the results will support my solution of three dimensions: valued-by-the-student, traditional pedagogy, socially responsive pedagogy. This is the first part of a larger two-part suggestion for practice. These steps will be repeated in the next lesson's homeworked example. While somewhat redundant, I am hopeful that the second set will provide a fairly complete set of code for someone who is analyzing their own data from the beginning.

\hypertarget{prepare-data-for-cfa-items-only-df-reverse-scored}{%
\subsection{Prepare data for CFA (items only df, reverse-scored)}\label{prepare-data-for-cfa-items-only-df-reverse-scored}}

We can upload the data from the .rds file. The file should be in the same folder as the .rmd file. I've named the df object that holds the data ``big.''

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{big }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"ReC.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For the demonstration of CFA first order models, I will just pull in the items that I believe go onto the three factors.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{items }\OtherTok{\textless{}{-}}\NormalTok{ big }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(ValObjectives, IncrUnderstanding, IncrInterest, ClearResponsibilities,}
\NormalTok{        EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation,}
\NormalTok{        MultPerspectives, InclusvClassrm, DEIintegration, EquitableEval)}
\end{Highlighting}
\end{Shaded}

Let's quickly check the structure. The variables should be numeric or integer.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(items)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Classes 'data.table' and 'data.frame':  310 obs. of  12 variables:
 $ ValObjectives        : int  5 5 4 4 5 5 5 5 4 5 ...
 $ IncrUnderstanding    : int  2 3 4 3 4 4 5 2 4 5 ...
 $ IncrInterest         : int  5 3 4 2 4 3 5 3 2 5 ...
 $ ClearResponsibilities: int  5 5 4 4 5 4 5 4 4 5 ...
 $ EffectiveAnswers     : int  5 3 5 3 5 3 4 3 2 3 ...
 $ Feedback             : int  5 3 4 2 5 NA 5 4 4 5 ...
 $ ClearOrganization    : int  3 4 3 4 4 4 5 4 4 5 ...
 $ ClearPresentation    : int  4 4 4 2 5 3 4 4 4 5 ...
 $ MultPerspectives     : int  5 5 4 5 5 4 5 5 5 5 ...
 $ InclusvClassrm       : int  5 5 5 5 5 4 5 5 4 5 ...
 $ DEIintegration       : int  5 5 5 5 5 4 5 5 5 5 ...
 $ EquitableEval        : int  5 5 3 5 5 3 5 5 3 5 ...
 - attr(*, ".internal.selfref")=<externalptr> 
\end{verbatim}

\hypertarget{specify-and-run-a-unidimensional-model}{%
\subsection{Specify and run a unidimensional model}\label{specify-and-run-a-unidimensional-model}}

First we map the relations we want to analyze.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{uniD }\OtherTok{\textless{}{-}} \StringTok{"CourseEvals =\textasciitilde{} ValObjectives + IncrUnderstanding + IncrInterest + ClearResponsibilities + EffectiveAnswers + Feedback + ClearOrganization + ClearPresentation + MultPerspectives + InclusvClassrm + DEIintegration + EquitableEval"}
\end{Highlighting}
\end{Shaded}

We analyze the relations by naming that object in our \emph{lavaan} code.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{uniDfit }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(uniD, }\AttributeTok{data =}\NormalTok{ items)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(uniDfit, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rsquare =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 32 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        24

                                                  Used       Total
  Number of observations                           267         310

Model Test User Model:
                                                      
  Test statistic                               344.973
  Degrees of freedom                                54
  P-value (Chi-square)                           0.000

Model Test Baseline Model:

  Test statistic                              1940.157
  Degrees of freedom                                66
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.845
  Tucker-Lewis Index (TLI)                       0.810

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -3038.064
  Loglikelihood unrestricted model (H1)      -2865.578
                                                      
  Akaike (AIC)                                6124.129
  Bayesian (BIC)                              6210.223
  Sample-size adjusted Bayesian (SABIC)       6134.129

Root Mean Square Error of Approximation:

  RMSEA                                          0.142
  90 Percent confidence interval - lower         0.128
  90 Percent confidence interval - upper         0.157
  P-value H_0: RMSEA <= 0.050                    0.000
  P-value H_0: RMSEA >= 0.080                    1.000

Standardized Root Mean Square Residual:

  SRMR                                           0.074

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  CourseEvals =~                                                        
    ValObjectives     1.000                               0.309    0.515
    IncrUndrstndng    1.715    0.223    7.684    0.000    0.530    0.642
    IncrInterest      2.142    0.269    7.971    0.000    0.662    0.685
    ClearRspnsblts    2.065    0.239    8.652    0.000    0.638    0.808
    EffectivAnswrs    2.105    0.244    8.617    0.000    0.650    0.800
    Feedback          2.143    0.259    8.285    0.000    0.662    0.738
    ClearOrganiztn    2.678    0.314    8.516    0.000    0.828    0.780
    ClearPresenttn    2.521    0.285    8.832    0.000    0.779    0.846
    MultPerspectvs    2.067    0.246    8.392    0.000    0.639    0.757
    InclusvClassrm    1.246    0.170    7.324    0.000    0.385    0.592
    DEIintegration    1.015    0.174    5.820    0.000    0.314    0.424
    EquitableEval     1.435    0.179    8.027    0.000    0.443    0.694

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ValObjectives     0.265    0.024   11.254    0.000    0.265    0.735
   .IncrUndrstndng    0.401    0.037   10.970    0.000    0.401    0.588
   .IncrInterest      0.494    0.046   10.815    0.000    0.494    0.530
   .ClearRspnsblts    0.217    0.022    9.983    0.000    0.217    0.348
   .EffectivAnswrs    0.237    0.024   10.060    0.000    0.237    0.359
   .Feedback          0.367    0.035   10.555    0.000    0.367    0.455
   .ClearOrganiztn    0.439    0.043   10.250    0.000    0.439    0.391
   .ClearPresenttn    0.242    0.026    9.446    0.000    0.242    0.285
   .MultPerspectvs    0.304    0.029   10.431    0.000    0.304    0.427
   .InclusvClassrm    0.275    0.025   11.104    0.000    0.275    0.649
   .DEIintegration    0.449    0.040   11.372    0.000    0.449    0.820
   .EquitableEval     0.211    0.020   10.777    0.000    0.211    0.518
    CourseEvals       0.096    0.022    4.381    0.000    1.000    1.000

R-Square:
                   Estimate
    ValObjectives     0.265
    IncrUndrstndng    0.412
    IncrInterest      0.470
    ClearRspnsblts    0.652
    EffectivAnswrs    0.641
    Feedback          0.545
    ClearOrganiztn    0.609
    ClearPresenttn    0.715
    MultPerspectvs    0.573
    InclusvClassrm    0.351
    DEIintegration    0.180
    EquitableEval     0.482
\end{verbatim}

Let's plot the results to see if the figure resembles what we intended to specify.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(uniDfit, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{10-CFA_1stOrder_files/figure-latex/unnamed-chunk-52-1.pdf} \#\#\# Narrate adequacy of fit with \(\chi ^{2}\), CFI, RMSEA, SRMR (write a mini-results section)

\begin{quote}
\textbf{Model testing}. To evaluate the models we, we used confirmatory factor analysis (CFA) in the R package, lavaan (v.0-6.17) with maximum likelihood estimation. Our sample size was 267. We selected fit criteria for their capacity to assess different aspects of the statistical analysis. As is common among SEM researchers, we reported the Chi-square goodness of fit (χ2). This evaluates the discrepancy between the unrestricted sample matrix and the restricted covariance matrix. Although the associated \emph{p}-value indicates adequate fit when the value is non-significant, it is widely recognized that a large sample size can result in a statistically significant \emph{p} value (Byrne, 2016b). The comparative fit index (CFI) is an incremental index, comparing the hypothesized model to an independent/baseline model. Acceptable fit occurs when values are at least .90 and perhaps higher than .95 (Kline, 2016). The root mean square error of approximation (RMSEA) takes into account the error of approximation in the population and expresses it per degree of freedom. As such, the fit indicator considers the complexity of the model. Ideal values are equal to or less than .05, values less than .08 represent reasonable fit, and values between .08 and .10 represent mediocre fit. The standardized root mean residual (SRMR) is a standardized measure of the mean absolute covariance residual -- the overall difference between the observed and predicted correlations. Values greater than .10 may indicate poor fit and inspection of residuals is then advised. Kline (2016) advised researchers to be cautious when using these criteria as strict cut-offs. Elements such as sample size and model complexity should be considered when evaluating fit.
\end{quote}

\begin{quote}
Our first model was unidimensional where each of the 12 items loaded onto a single factor representing overall course evaluations. The Chi-square index was statistically significant \((\chi^2(54)=344.97, p<.001)\) indicating likely misfit. The CFI value of .85 indicated poor fit. The RMSEA = .14 (90\% CI {[}.13, .16{]}) suggested serious problems. The SRMR value of .07 was below below the warning criteria of .10. The AIC and BIC values were 6124.13 and 6134.13, respectively, and will become useful in comparing subsequent models.
\end{quote}

The \emph{tidySEM} package has some useful tools to export the results to .csv files. This first set of code exports the fit indices.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{uniDfitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(uniDfit)}
\NormalTok{uniDfit\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(uniDfit, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\CommentTok{\# uniDfitStats uniDfit\_paramEsts}
\end{Highlighting}
\end{Shaded}

We can write each of these to a .csv file that will be stored in the same folder as your .rmd file.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(uniDfitStats, }\AttributeTok{file =} \StringTok{"uniDfitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(uniDfit\_paramEsts, }\AttributeTok{file =} \StringTok{"uniDfit\_paramEsts.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{specify-and-run-a-single-order-model-with-correlated-factors}{%
\subsection{Specify and run a single-order model with correlated factors}\label{specify-and-run-a-single-order-model-with-correlated-factors}}

First we map the relations we want to analyze.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corrF  }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}TradPed =\textasciitilde{} ClearResponsibilities + EffectiveAnswers + Feedback + ClearOrganization + ClearPresentation  }
\StringTok{             Valued =\textasciitilde{} ValObjectives + IncrUnderstanding + IncrInterest }
\StringTok{             SCRPed =\textasciitilde{} MultPerspectives + InclusvClassrm + DEIintegration + EquitableEval}

\StringTok{  TradPed\textasciitilde{}\textasciitilde{}Valued}
\StringTok{  TradPed\textasciitilde{}\textasciitilde{}SCRPed}
\StringTok{  Valued\textasciitilde{}\textasciitilde{}SCRPed}
\StringTok{\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

Next we run the analysis.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{corrF\_fit }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(corrF, }\AttributeTok{data =}\NormalTok{ items)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(corrF\_fit, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rsquare =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 42 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        27

                                                  Used       Total
  Number of observations                           267         310

Model Test User Model:
                                                      
  Test statistic                               224.795
  Degrees of freedom                                51
  P-value (Chi-square)                           0.000

Model Test Baseline Model:

  Test statistic                              1940.157
  Degrees of freedom                                66
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.907
  Tucker-Lewis Index (TLI)                       0.880

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -2977.975
  Loglikelihood unrestricted model (H1)      -2865.578
                                                      
  Akaike (AIC)                                6009.951
  Bayesian (BIC)                              6106.807
  Sample-size adjusted Bayesian (SABIC)       6021.201

Root Mean Square Error of Approximation:

  RMSEA                                          0.113
  90 Percent confidence interval - lower         0.098
  90 Percent confidence interval - upper         0.128
  P-value H_0: RMSEA <= 0.050                    0.000
  P-value H_0: RMSEA >= 0.080                    1.000

Standardized Root Mean Square Residual:

  SRMR                                           0.061

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed =~                                                            
    ClearRspnsblts    1.000                               0.652    0.826
    EffectivAnswrs    1.015    0.065   15.606    0.000    0.662    0.815
    Feedback          1.010    0.075   13.481    0.000    0.659    0.735
    ClearOrganiztn    1.295    0.086   15.106    0.000    0.845    0.797
    ClearPresenttn    1.204    0.072   16.680    0.000    0.785    0.853
  Valued =~                                                             
    ValObjectives     1.000                               0.334    0.557
    IncrUndrstndng    1.942    0.223    8.717    0.000    0.649    0.786
    IncrInterest      2.438    0.273    8.932    0.000    0.815    0.844
  SCRPed =~                                                             
    MultPerspectvs    1.000                               0.713    0.846
    InclusvClassrm    0.622    0.053   11.672    0.000    0.444    0.682
    DEIintegration    0.589    0.063    9.365    0.000    0.420    0.567
    EquitableEval     0.642    0.052   12.410    0.000    0.458    0.717

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed ~~                                                            
    Valued            0.171    0.026    6.640    0.000    0.785    0.785
    SCRPed            0.391    0.045    8.677    0.000    0.841    0.841
  Valued ~~                                                             
    SCRPed            0.164    0.026    6.254    0.000    0.688    0.688

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClearRspnsblts    0.199    0.021    9.456    0.000    0.199    0.319
   .EffectivAnswrs    0.222    0.023    9.618    0.000    0.222    0.336
   .Feedback          0.371    0.036   10.415    0.000    0.371    0.460
   .ClearOrganiztn    0.410    0.042    9.855    0.000    0.410    0.365
   .ClearPresenttn    0.232    0.026    8.939    0.000    0.232    0.273
   .ValObjectives     0.248    0.023   10.650    0.000    0.248    0.690
   .IncrUndrstndng    0.260    0.032    8.041    0.000    0.260    0.382
   .IncrInterest      0.268    0.043    6.308    0.000    0.268    0.288
   .MultPerspectvs    0.203    0.029    7.052    0.000    0.203    0.285
   .InclusvClassrm    0.226    0.023   10.028    0.000    0.226    0.534
   .DEIintegration    0.371    0.035   10.734    0.000    0.371    0.678
   .EquitableEval     0.198    0.020    9.685    0.000    0.198    0.486
    TradPed           0.426    0.053    8.085    0.000    1.000    1.000
    Valued            0.112    0.024    4.595    0.000    1.000    1.000
    SCRPed            0.509    0.063    8.039    0.000    1.000    1.000

R-Square:
                   Estimate
    ClearRspnsblts    0.681
    EffectivAnswrs    0.664
    Feedback          0.540
    ClearOrganiztn    0.635
    ClearPresenttn    0.727
    ValObjectives     0.310
    IncrUndrstndng    0.618
    IncrInterest      0.712
    MultPerspectvs    0.715
    InclusvClassrm    0.466
    DEIintegration    0.322
    EquitableEval     0.514
\end{verbatim}

Plotting the results. Does it look like what we intended to specify?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(corrF\_fit, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{10-CFA_1stOrder_files/figure-latex/unnamed-chunk-57-1.pdf}

\hypertarget{narrate-adequacy-of-fit-with-chi-2-cfi-rmsea-srmr-write-a-mini-results-section}{%
\subsection{\texorpdfstring{Narrate adequacy of fit with \(\chi ^{2}\), CFI, RMSEA, SRMR (write a mini-results section)}{Narrate adequacy of fit with \textbackslash chi \^{}\{2\}, CFI, RMSEA, SRMR (write a mini-results section)}}\label{narrate-adequacy-of-fit-with-chi-2-cfi-rmsea-srmr-write-a-mini-results-section}}

\begin{quote}
Our second model was a single-order, correlated traits model where each of the 12 items loaded onto one of four factors. Standardized pattern coefficients ranged between .74 and .85 on the TradPed factor, between .56 and .84 on the Valued factor, and between .57 and .85 on the SCRPed factor. The Chi-square index was statistically significant \((\chi^2(51) = 224.795, p < 0.001\) indicating some degree of misfit. The CFI value of .91 fell below the recommendation of .95. The RMSEA = .113 (90\% CI {[}.098, .128{]}) was higher than recommended. The SRMR value of .061 remained below the warning criteria of .10. The AIC and BIC values were 6009.95 and 6021.20, respectively.
\end{quote}

Code for saving the results as a .csv file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corrFitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(corrF\_fit)}
\NormalTok{corrF\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(corrF\_fit, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{corrFCorrs }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_cors}\NormalTok{(corrF\_fit, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\CommentTok{\# to see each of the tables, remove the hashtab corrFitStats}
\CommentTok{\# corrF\_paramEsts corrFCorrs}
\end{Highlighting}
\end{Shaded}

Next, I export them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(corrFitStats, }\AttributeTok{file =} \StringTok{"corrFitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(corrF\_paramEsts, }\AttributeTok{file =} \StringTok{"corrF\_paramEsts.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(corrFCorrs, }\AttributeTok{file =} \StringTok{"corrFCorrs.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{compare-model-fit-with-chi-2delta-aic-bic}{%
\subsection{\texorpdfstring{Compare model fit with \(\chi ^{2}\Delta\), AIC, BIC}{Compare model fit with \textbackslash chi \^{}\{2\}\textbackslash Delta, AIC, BIC}}\label{compare-model-fit-with-chi-2delta-aic-bic}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{lavTestLRT}\NormalTok{(uniDfit, corrF\_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Chi-Squared Difference Test

          Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff
corrF_fit 51 6010.0 6106.8 224.79                           
uniDfit   54 6124.1 6210.2 344.97     120.18 0.38248       3
                     Pr(>Chisq)    
corrF_fit                          
uniDfit   < 0.00000000000000022 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{quote}
The Chi-square difference test \((\chi^2(3) = 120.18, p < 0.001\) was statistically significant indicating that the two models were statistically significantly different. The AIC and BIC values of the the correlated factors model were the lowest. Thus, we conclude the first-order, correlated factors model is superior and acceptable for use in preliminary research and evaluation.
\end{quote}

\hypertarget{apa-style-results-with-tables-and-figure}{%
\subsection{APA style results with table(s) and figure}\label{apa-style-results-with-tables-and-figure}}

Because we have written mini-results throughout, we can assemble them into a full results section. Keep in mind that most CFA models will continue testing multidimensional models. Thus, the entire analysis continues in the next lesson and associated practice problem.

\begin{quote}
\textbf{Model testing}. To evaluate the models we, we used confirmatory factor analysis (CFA) in the R package, lavaan (v.0-6.17) with maximum likelihood estimation. Our sample size was 267. We selected fit criteria for their capacity to assess different aspects of the statistical analysis. As is common among SEM researchers, we reported the Chi-square goodness of fit (χ2). This evaluates the discrepancy between the unrestricted sample matrix and the restricted covariance matrix. Although the associated \emph{p}-value indicates adequate fit when the value is non-significant, it is widely recognized that a large sample size can result in a statistically significant \emph{p} value (Byrne, 2016b). The comparative fit index (CFI) is an incremental index, comparing the hypothesized model to an independent/baseline model. Acceptable fit occurs when values are at least .90 and perhaps higher than .95 (Kline, 2016). The root mean square error of approximation (RMSEA) takes into account the error of approximation in the population and expresses it per degree of freedom. As such, the fit indicator considers the complexity of the model. Ideal values are equal to or less than .05, values less than .08 represent reasonable fit, and values between .08 and .10 represent mediocre fit. The standardized root mean residual (SRMR) is a standardized measure of the mean absolute covariance residual -- the overall difference between the observed and predicted correlations. Values greater than .10 may indicate poor fit and inspection of residuals is then advised. Kline (2016) advised researchers to be cautious when using these criteria as strict cut-offs. Elements such as sample size and model complexity should be considered when evaluating fit.
\end{quote}

\begin{quote}
Our first model was unidimensional where each of the 12 items loaded onto a single factor representing overall course evaluations. The Chi-square index was statistically significant \((\chi^2(54)=344.97, p<.001)\) indicating likely misfit. The CFI value of .85 indicated poor fit. The RMSEA = .14 (90\% CI {[}.13, .16{]}) suggested serious problems. The SRMR value of .07 was below below the warning criteria of .10. The AIC and BIC values were 6124.13 and 6134.13, respectively, and will become useful in comparing subsequent models.
\end{quote}

\begin{quote}
Our second model was a single-order, correlated traits model where each of the 12 items loaded onto one of four factors. Standardized pattern coefficients ranged between .74 and .85 on the TradPed factor, between .56 and .84 on the Valued factor, and between .57 and .85 on the SCRPed factor. The Chi-square index was statistically significant \((\chi^2(51) = 224.795, p < 0.001\) indicating some degree of misfit. The CFI value of .91 fell below the recommendation of .95. The RMSEA = .113 (90\% CI {[}.098, .128{]}) was higher than recommended. The SRMR value of .061 remained below the warning criteria of .10. The AIC and BIC values were 6009.95 and 6021.20, respectively.
\end{quote}

\begin{quote}
The Chi-square difference test \((\chi^2(3) = 120.18, p < 0.001\) was statistically significant indicating that the two models were statistically significantly different. The AIC and BIC values of the the correlated factors model were the lowest. Thus, we conclude the first-order, correlated factors model is superior and acceptable for use in preliminary research and evaluation.
\end{quote}

\hypertarget{explanation-to-grader-3}{%
\subsection{Explanation to grader}\label{explanation-to-grader-3}}

\hypertarget{CFA2nd}{%
\chapter{CFA: Hierarchical and Nested Models}\label{CFA2nd}}

\href{https://www.youtube.com/playlist?list=PLtz5cFLQl4KPiMveC6qIqzdnZdMHsSn7o}{Screencasted Lecture Link}

This is the second lecture in our series on confirmatory factor analysis (CFA). In this lesson we will compare first-order structures (with correlated uncorrelated factors) to second-order and bifactor structures. Modification indices will allow us to tweak each model's fit. We will also determine and track the identification status of models, including nested/nesting models and examining issues of equivalent models.

\hypertarget{navigating-this-lesson-9}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-9}}

The lecture is just under two hours. I would add another two-to-three hours to work through and digest the materials.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}.

\hypertarget{learning-objectives-9}{%
\subsection{Learning Objectives}\label{learning-objectives-9}}

Focusing on this week's materials, make sure you can:

\begin{itemize}
\tightlist
\item
  Specify single order (correlated and uncorrelated), second order, and bifactor models.
\item
  Interpret model adequacy and fit.
\item
  Compare models on the basis of statistical criteria.
\item
  Determine which (among models) is the nested model.
\item
  Memorize which model (nested or nesting) will have better fit (without looking at the results).
\item
  Determine whether or not models (or alterations to their specification) remain statistically identified.
\end{itemize}

\hypertarget{planning-for-practice-9}{%
\subsection{Planning for Practice}\label{planning-for-practice-9}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. The least complex is to change the random seed and rework the problem demonstrated in the lesson. The results \emph{should} map onto the ones obtained in the lecture.

The second option involves utilizing one of the simulated datasets available in this OER. The \protect\hyperlink{sims}{last lesson} in the OER contains three simulations that could be used for all of the statistics-based practice suggestions. Especially if you started with one of these examples in an earlier lesson, I highly recommend you continue with that.

Alternatively, Lewis and Neville's \citeyearpar{lewis_construction_2015} Gendered Racial Microaggressions Scale for Black Women was used in the lessons for exploratory factor analysis and Conover et al.'s \citeyearpar{conover_development_2017} Ableist Microaggressions Scale is used in the lesson on invariance testing. Both of these would be suitable for the CFA homework assignments.

As a third option, you are welcome to use data to which you have access and is suitable for CFA. This could include other simulated data, data found on an open access repository, data from the ReCentering Psych Stats survey described in the \protect\hyperlink{qualTRIX}{Qualtrics lesson}, or your own data (presuming you have permission to use it).

The suggestion for practice spans the \protect\hyperlink{CFA1st}{prior chapter} and this one . For this combination assignment, you should plan to:

\begin{itemize}
\tightlist
\item
  Prepare the data frame for CFA.
\item
  Specify and run unidimensional, single order (with correlated factors), second-order, and bifactor models.
\item
  Narrate the adequacy of fit with \(\chi ^{2}\), CFI, RMSEA, SRMR.

  \begin{itemize}
  \tightlist
  \item
    Write a mini-results section for each.
  \end{itemize}
\item
  Compare model fit with \(\chi ^{2}\Delta\), AIC, and BIC.
\item
  Write an APA style results sections with table(s) and figures.
\end{itemize}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\item
  Byrne, B. M. (2016). Application 3: Testing the Factorial Validity of Scores from a Measurement Scale (Second-Order CFA model). Chapter 5. In Structural Equation Modeling with AMOS: Basic Concepts, Applications, and Programming, Third Edition. Taylor \& Francis Group. \url{http://ebookcentral.proquest.com/lib/spu/detail.action?docID=4556523}
\item
  Dekay, Nicole (2021). Quick Reference Guide: The statistics for psychometrics \url{https://www.humanalysts.com/quick-reference-guide-the-statistics-for-psychometrics}
\item
  Flora, D. B. (2020). Your Coefficient Alpha Is Probably Wrong, but Which Coefficient Omega Is Right? A Tutorial on Using R to Obtain Better Reliability Estimates. Advances in Methods and Practices in Psychological Science, 3(4), 484--501. \url{https://doi.org/10.1177/2515245920951747}
\item
  Kline, R. (2016). Principles and practice of structural equation modeling (Fourth ed., Methodology in the social sciences). New York: The Guilford Press.

  \begin{itemize}
  \tightlist
  \item
    Chapter 6: Specification of Observed-Variable (Path Models)
  \item
    Chapter 7: Identification of Observed-Variable (Path) Models *
  \item
    Chapter 9: Specification and Identification of Confirmatory Factor Analysis Models
  \item
    Chapter 13: Analysis of Confirmatory Factor Analysis Models
  \end{itemize}
\item
  Rosseel, Y. (2019). The \emph{lavaan} tutorial. Belgium: Department of Data Analysis, Ghent University. \url{http://lavaan.ugent.be/tutorial/tutorial.pdf}
\end{itemize}

\hypertarget{packages-9}{%
\subsection{Packages}\label{packages-9}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#will install the package if not already installed}
\CommentTok{\#if(!require(lavaan))\{install.packages("lavaan")\}}
\CommentTok{\#if(!require(semPlot))\{install.packages("semPlot")\}}
\CommentTok{\#if(!require(psych))\{install.packages("psych")\}}
\CommentTok{\#if(!require(semTable))\{install.packages("semTable")\}}
\CommentTok{\#if(!require(semTools))\{install.packages("semTools")\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{cfa-workflow-1}{%
\section{CFA Workflow}\label{cfa-workflow-1}}

Below is a screenshot of a CFA workflow. The original document is in the \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub site} that hosts the ReCentering Psych Stats: Psychometrics OER.

\begin{figure}
\centering
\includegraphics{images/CFA1st/CFA_workflow.png}
\caption{Image of a workflow for specifying and evaluating a confirmatory factor analytic model}
\end{figure}

Because the intended audience for the ReCentering Psych Stats OER is the scientist-practitioner-advocate, this lesson focuses on the typical workflow and associated decisions. As you might guess, the details of CFA can be quite complex and require more investigation and decision-making in models that pose more complexity or empirical challenges. The following are the general steps in a CFA.

\begin{itemize}
\tightlist
\item
  Creating an items only dataframe where any items are scaled in the same direction (e.g., negatively worded items are reverse-scored).
\item
  Determining a factor structure that is \emph{identified}, that is

  \begin{itemize}
  \tightlist
  \item
    a single factor (unidimensional) model has at least three items/indicators
  \item
    a multidimensional model with at least two items per factor
  \end{itemize}
\item
  Specify a series of models, these typically include

  \begin{itemize}
  \tightlist
  \item
    a unidimensional model (all items on a single factor)
  \item
    a single order structure with correlated factors
  \item
    a second order structure
  \item
    a bifactor structure
  \end{itemize}
\item
  Evaluate model fit with a variety of indicators

  \begin{itemize}
  \tightlist
  \item
    factor loadings
  \item
    fit indices
  \end{itemize}
\item
  Compare models
\item
  In the event of poor model fit, investigate modification indices and consider respecification

  \begin{itemize}
  \tightlist
  \item
    eliminating items
  \item
    changing factor membership
  \item
    allowing errors to covary
  \end{itemize}
\end{itemize}

\hypertarget{another-look-at-varying-factor-structures}{%
\section{Another Look at Varying Factor Structures}\label{another-look-at-varying-factor-structures}}

In this lecture we move into second-order and bifactor models, let's look again factor structures, considering unidimensional, first-order, and second-order variations.

\begin{figure}
\centering
\includegraphics{images/CFA1st/quadrant.png}
\caption{Image of first-order, second-order, and bifactor factor structures}
\end{figure}

Models A and B are \textbf{first-order models}. Note that all factors are on a single plane.

\begin{itemize}
\item
  Model A is unidimensional. Each item is influenced by a single common factor, and defined by a single term that includes systematic and random error. Note that there is only one \emph{systematic} source of variance for each item AND it is from a single source: F1.
\item
  Model B is often referred to as a ``correlated traits'' model. Here, the larger construct is separated into distinct-yet-correlated elements. The variance of each item is assumed to be a weighted linear function of two or more common factors.
\item
  Model C is a second-order factor structure. Rather than merely being correlated, factors are related because they share a common cause. In this model, the second-order factor \emph{explains} why three or more traits are correlated. Note that there is no direct relationship between the item and the target construct. Rather, the relationship between the second-order factor and each item is mediated through the primary factor (yes, an indirect effect!).
\item
  Model D is a bifactor structure. Here, each item loads on a general factor. This general factor (bottom row) reflects what is common among the items and represents the individual differences on the target dimension that a researcher is most interested in. Group factors (top row) are now specified as \emph{orthogonal}. The group factors represent common factors measured by the items that explain item response variation not accounted for by the general factor. In some research scenarios, the group factors are termed ``nuisance'' dimensions. That is, that which they have in common interferes with measuring the primary target of interest.
\end{itemize}

\hypertarget{revisiting-model-identification}{%
\section{Revisiting Model Identification}\label{revisiting-model-identification}}

Model identification means it is \emph{theoretically possible} for a statistical analysis to derive a unique estimate of every model parameter.

\emph{Theoretical} is emphasizes that identification is a property of the model and not the data; that is, it doesn't matter if the sample size is 100 or 10,000.

CFA has the same general requirements for identification as other forms of SEM:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Every latent variable (including errors) must be scaled; and
\item
  Model degrees of freedom must be at least zero \((df_{M}\leq 0)\) (aka ``the counting rule''; this means that there must be at least as many observations as there are free parameters)
\end{enumerate}

\hypertarget{identification-status}{%
\subsection{Identification Status}\label{identification-status}}

\textbf{Underidentified} (or underdetermined) models violate the counting rule because there are more free parameters than observations.

For example, solve this equation:

\(a + b = 6\).

There are an infinite number of solutions: 4 + 2, 3 + 3, 2.5 + 3.5\ldots and so on to \(\infty\). When the computing algorithm tries to solve this problem, it will fail to converge.

The parallel scenario in an SEM/CFA model with more free parameters than observations would have \emph{negative df}.

\textbf{Just-identified} (or just-determined) models have a single unique solution,

\((df_{M} = 0)\)

For example, for this set of equations:

\(a + b = 6\)\\
\(2a + b = 10\)

The only answer is: \(a = 4, b = 2\)

\textbf{Overidentified} (or overdetermined) models have more observations than free parameters. That is:

\(df_{M} > 0\)

For example, solve for this set of equations:

\(a + b = 6\)\\
\(2a + b = 10\)\\
\(3a + b = 12\)

There is no single solution that satisfies all three formulas, but there is a way to find a unique solution. We can impose a statistical criterion that leads to the \emph{overidentified/overdetermined} circumstance with more observations than free parameters. For example, we could impose the \emph{least squares criterion} (from regression, but with no intercept/constant in the prediction equation). The constraint (instruction) would read:

\emph{Find values of a and b that yield total scores such that the sum of squared differences between the observations (6, 10, 12) and these total scores is as small as possible (and also unique).}

In this case, answers are \(a = 3.00, b = 3.33\) and the solutions are 6.44, 9.33, 12.33. While the solution doesn't perfectly reproduce the data, it facilitates model testing.

\textbf{The bad news} is that SEM/CFA computer tools are generally not helpful in determining whether a model is identified or not. Why? Computers are great a numerical processing, but not symbolic processing (needed for determining identification status). This means that we, the researchers, must learn the \emph{identification heuristics} to determine the model's degree of identification.

\emph{Need a break already?} My \href{https://www.youtube.com/watch?v=_C25CwNlVjA}{favorite scene} during \textbf{The Imitation Game} parallels issues of identification, iterations, and convergence. The Turing machine runs and runs until its users can feed it proper start values so that it finally converges on a solution.

Kenny \citep{kenny_sem_2012} provides some helpful guidelines in determining model identification with the calculation of \emph{knowns} and \emph{unknowns}. In in a standard CFA/SEM specification, \emph{knowns} are the number of covariances between all the variables in the model, \((k(k+1))/2\), where \(k\) is the number of variables in the model. \emph{Unknowns} are the \emph{free parameters} that must be calculated. These include paths; covariances between exogenous variables, between disturbances (error terms), and between exogenous variables and disturbances (error terms); variances of the exogenous variables; and disturbances (error terms) of the endogenous variables (minus the number of linear constraints).

\begin{verbatim}
* If $knowns \lt unknowns$ then the model is *under-identified*
* If $knowns = unknowns$ then the model is *just-identified*
* If $knowns \gt unknowns$ then the model is *overidentified*
\end{verbatim}

\hypertarget{identification-in-practice}{%
\subsection{Identification in Practice}\label{identification-in-practice}}

It is essential that every latent variable (including errors) must be scaled such that the degrees of freedom for the model be greater than or equal to zero (\(df_{M} \geq 0\)).

Operationally, in a standard CFA model:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A single factor model needs at least 3 indicators for the single factor.
\item
  Factor models with more than one factor require at least two or more indicators per factor.

  \begin{itemize}
  \tightlist
  \item
    For purposes of identification, more is better with 3-5 being recommended.
  \end{itemize}
\end{enumerate}

(Among other things) \emph{nonstandard} models occur when:

\begin{itemize}
\tightlist
\item
  errors are allowed to correlate/covary
\item
  \emph{complex indicators} are defined by more than one factor
\end{itemize}

We will return to these as we encounter them later in today's lecture. Essentially, we will need to ``subtract'' 1 df for every parameter we ``free'' to covary. This is because we then need to estimate it and it becomes ``unknown.''

\textbf{Empirical underidentification} is also a threat. This means, the model fails to converge because of the characteristics of the data. For example, perhaps we specified model on the cusp of identification: 2 factors, correlated, with 2 indicators each. If in fact, the data did not support the correlation between the two factors\ldots because of the ``just barely'' identified circumstance, you may receive an ``empirically underidentified'' solution.

Today we are going to specify second-order and bifactor models. As we do each, we will address these issues of model identification.

\hypertarget{research-vignette-8}{%
\section{Research Vignette}\label{research-vignette-8}}

This lesson's research vignette emerges from Keum et al's Gendered Racial Microaggressions Scale for Asian American Women (GRMSAAW; \citep{keum_gendered_2018}). The article reports on two separate studies that comprised the development, refinement, and psychometric evaluation of two, parallel, versions (stress appraisal, frequency) of scale. I simulated data from the final construction of the frequency version as the basis of the lecture. If the scale looks somewhat familiar it is because the authors used the Gendered Racial Microaggressions Scale for Black Women \citep{lewis_construction_2015} as a model.

Keum et al. \citeyearpar{keum_gendered_2018} reported support for a total scale score (22 items) and four subscales. Below, I list the four subscales, their number of items, and a single example item. At the outset, let me provide a content advisory For those who hold this particular identity (or related identities) the content in the items may be upsetting. In other lessons, I often provide a variable name that gives an indication of the primary content of the item. In the case of the GRMSAAW, I will simply provide an abbreviation of the subscale name and its respective item number. This will allow us to easily inspect the alignment of the item with its intended factor, and hopefully minimize discomfort. If you are not a member of this particular identity, I encourage you to learn about these microaggressions by reading the article in its entirety. Please do not ask members of this group to explain why these microaggressions are harmful or ask if they have encountered them.

There are 22 items on the GRMSAAW scale. Using the same item stems, the authors created two scales. One assesses frequency of the event, the second assesses the degree of stressfulness. I simulated data from the stressfulness scale. Its Likert style scaling included: 0 (\emph{not at all stressful}), 1(\emph{slightly stressful}), 2(\emph{somewhat stressful}), 3(\emph{moderately stressful}), 4(\emph{very stressful}), and 5(\emph{extremely stressful}).

The four factors, number of items, and sample item are as follows:

\begin{itemize}
\tightlist
\item
  Ascribed Submissiveness (9 items)

  \begin{itemize}
  \tightlist
  \item
    Others expect me to be submissive. (AS1)
  \item
    Others have been surprised when I disagree with them. (AS2)
  \item
    Others take my silence as a sign of compliance. (AS3)
  \item
    Others have been surprised when I do things independent of my family. (AS4)
  \item
    Others have implied that AAW seem content for being a subordinate. (AS5)
  \item
    Others treat me as if I will always comply with their requests. (AS6)
  \item
    Others expect me to sacrifice my own needs to take care of others (e.g., family, partner) because I am an AAW. (AS7)
  \item
    Others have hinted that AAW are not assertive enough to be leaders. (AS8)
  \item
    Others have hinted that AAW seem to have no desire for leadership. (AS9)
  \end{itemize}
\item
  Asian Fetishism (4 items)

  \begin{itemize}
  \tightlist
  \item
    Others express sexual interest in me because of my Asian appearance. (AF1)
  \item
    Others take sexual interest in AAW to fulfill their fantasy. (AF2)
  \item
    Others take romantic interest in AAW just because they never had sex with an AAW before. (AF3)
  \item
    Others have treated me as if I am always open to sexual advances. (AF4)
  \end{itemize}
\item
  Media Invalidation (5 items)

  \begin{itemize}
  \tightlist
  \item
    I see non-Asian women being casted to play female Asian characters.(MI1)
  \item
    I rarely see AAW playing the lead role in the media. (MI2)
  \item
    I rarely see AAW in the media. (MI3)
  \item
    I see AAW playing the same type of characters (e.g., Kung Fu woman, sidekick, mistress, tiger mom) in the media. (MI4)
  \item
    I see AAW characters being portrayed as emotionally distant (e.g., cold-hearted, lack of empathy) in the media. (MI5)
  \end{itemize}
\item
  Assumptions of Universal Appearance (4 items)

  \begin{itemize}
  \tightlist
  \item
    Others have talked about AAW as if they all have the same facial features (e.g., eye shape, skin tone). (AUA1)
  \item
    Others have suggested that all AAW look alike.(AUA2)
  \item
    Others have talked about AAW as if they all have the same body type (e.g., petite, tiny, small-chested). (AUA3)
  \item
    Others have pointed out physical traits in AAW that do not look `Asian'.
  \end{itemize}
\end{itemize}

Four additional scales were reported in the Keum et al.~article \citep{keum_gendered_2018}. Fortunately, I was able to find factor loadings from the original psychometric article or subsequent publications. For multidimensional scales, I assign variable names according to the scale to which the item belongs (e.g., Env42). In contrast, when subscales or short unidimensional scales were used, I assigned variable names based on item content (e.g., ``blue''). In my own work, I prefer item-level names so that I can quickly see (without having to look up the item names) how the items are behaving. The scales, their original citation, and information about how I simulated data for each are listed below.

\begin{itemize}
\tightlist
\item
  \textbf{Racial Microaggressions Scale} (RMAS; \citep{torres-harding_racial_2012}) is a 32-item scale with Likert scaling ranging from 0 (\emph{never}) to 3 (\emph{often/frequent}). Higher scores represent greater frequency of perceived microaggressions. I simulated data at the subscale level. The RMAS has six subscales, but only four (Invisibility, Low-Achieving/Undesirable Culture, Foreigner/Not Belonging,and Environmental Invalidation) were used in the study. Data were simulated using factor loadings (from the four factors) in the source article.
\item
  \textbf{Schedule of Sexist Events} (SSE; \citep{klonoff_schedule_1995}) is a 20-item scale that with Likert scaling ranging from 1 (\emph{the event has never happened to me}) to 6 (\emph{the event happened almost all {[}i.e., more than 70\%{]} of the time}). Higher scores represent greater frequency of everyday sexist events. I simulated data the subscale level. Within two larger scales (recent events, lifetime events), there are three subscales: Sexist Degradation and Its Consequences, Unfair/Sexist Events at Work/School, and Unfair Treatment in Distant and Close Relationships. Data were simulated using factor loadings from the source article.
\item
  \textbf{PHQ-9} \citep{kroenke_phq-9_2001} is a 9-item scale with Likert scaling ranging from 0 (\emph{not at all}) to 3 (\emph{nearly every day}). Higher scores indicate higher levels of depression. I simulated data by estimating factor loadings from Brattmyr et al. \citeyearpar{brattmyr_factor_2022}.
\item
  \textbf{Internalized Racism in Asian American Scale} (IRAAS \citep{choi_development_2017}) is a 14-item scale with Likert scaling ranging from 1 (\emph{strongly disagree}) to 6 (\emph{strongly agree}). Higher scores indicate greater internalized racism. Data were simulated using the factor loadings from the bifactor model in the source article.
\end{itemize}

As you consider homework options, there is sufficient simulated data to use the RMAS, SSE, or IRAAS.

Below, I walk through the data simulation. This is not an essential portion of the lesson, but I will lecture it in case you are interested. None of the items are negatively worded (relative to the other items), so there is no need to reverse-score any items.

Simulating the data involved using factor loadings, means, standard deviations, and correlations between the scales. Because the simulation will produce ``out-of-bounds'' values, the code below rescales the scores into the range of the Likert-type scaling and rounds them to whole values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Entering the intercorrelations, means, and standard deviations from the journal article}

\NormalTok{Keum\_GRMS\_generating\_model }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}}
\StringTok{        \#measurement model}
\StringTok{        General =\textasciitilde{} .50*AS1 + .44*AS2 + .50*AS3 + .33*AS4 + .58*AS5 + .49*AS6 + .51*AS7 + .53*AS8 + .50*AS9 + .53*AF1 + .74*AF2 + .54*AF3 + .52*AF4 + .64*AUA1 + .59*AUA2 + .67*AUA3 + .64*AUA4 + .59*MI1 + .50*MI2 + .52*MI3 + .40*MI4 + .55*MI5}
\StringTok{        AS =\textasciitilde{} .68*AS1 + .65*AS2 + .53*AS3 + .55*AS4 + .54*AS5 + .55*AS6 + .42*AS7 + .47*AS8 + .50*AS9}
\StringTok{        AF =\textasciitilde{} .63*AF1 + .45*AF2 + .56*AF3 + .54*AF4}
\StringTok{        AUA =\textasciitilde{} .55*AUA1 + .55*AUA2 + .31*AUA3 + .31*AUA4}
\StringTok{        MI =\textasciitilde{} .27*MI1 + .53*MI2 + .57*MI3 + .29*MI4 + .09*MI5}
\StringTok{        RMAS\_FOR =\textasciitilde{} .66*FOR1 + .90*FOR2 + .63*FOR4}
\StringTok{        RMAS\_LOW =\textasciitilde{} .64*LOW22 + .54*LOW23 + .49*LOW28 + .63*LOW29 + .58*LOW30 + .67*LOW32 + .67*LOW35 + .76*LOW36 + .72*LOW37}
\StringTok{        RMAS\_INV =\textasciitilde{} .66*INV33 + .70*INV39 + .79*INV40 + .71*INV41 + .71*INV47 + .61*INV49 + .65*INV51 + .70*INV52}
\StringTok{        RMAS\_ENV =\textasciitilde{} .71*ENV42 + .70*ENV43 + .74*ENV44 + .57*ENV45 + .54*ENV46}
\StringTok{        }
\StringTok{        SSEL\_Deg =\textasciitilde{} .77*LDeg18 + .73*LDeg19 + .71*LDeg21 + .71*LDeg15 + .67*LDeg16 + .67*LDeg13 + .62*LDeg14 + .58*LDeg20}
\StringTok{        SSEL\_dRel =\textasciitilde{} .69*LdRel4 + .68*LdRel6 + .64*LdRel7 + .64*LdRel5 + .63*LdRel1 + .49*LdRel3}
\StringTok{        SSEL\_cRel =\textasciitilde{} .73*LcRel11 + .68*LcRel9 + .66*LcRel23}
\StringTok{        SSEL\_Work =\textasciitilde{} .73*LWork17 + .10*LWork10 + .64*LWork2}
\StringTok{        }
\StringTok{        SSER\_Deg =\textasciitilde{} .72*RDeg15 + .71*RDeg21 + .69*RDeg18 + .68*RDeg16 + .68*RDeg13 + .65*RDeg19 + .58*RDeg14 + .47*RDeg20}
\StringTok{        SSER\_dRel =\textasciitilde{} .74*RDeg4 + .67*RDeg6 + .64*RDeg5 + .54*RDeg7 + .51*RDeg1}
\StringTok{        SSER\_cRel =\textasciitilde{} .69*RcRel9 + .59*RcRel11 + .53*RcRel23}
\StringTok{        SSER\_Work =\textasciitilde{} .72*RWork10 + .67*RWork2 + .62*RWork17 + .51*RWork3}
\StringTok{        }
\StringTok{        SSE\_Lifetime =\textasciitilde{} SSEL\_Deg + SSEL\_dRel + SSEL\_cRel + SSEL\_Work}
\StringTok{        SSE\_Recent =\textasciitilde{} SSER\_Deg + SSER\_dRel + SSEL\_cRel + SSER\_Work}
\StringTok{        }
\StringTok{        PHQ9 =\textasciitilde{} .798*anhedonia + .425*down +  .591*sleep +  .913*lo\_energy +  .441*appetite +  .519*selfworth +  .755*concentration +  .454*too\_slowfast + .695*s\_ideation}
\StringTok{        }
\StringTok{        gIRAAS =\textasciitilde{} .51*SN1 + .69*SN2 + .63*SN3 + .65*SN4 + .67*WS5 + .60*WS6 + .74*WS7 + .44*WS8 + .51*WS9 + .79*WS10 + .65*AB11 + .63*AB12 + .68*AB13 + .46*AB14}
\StringTok{        SelfNegativity =\textasciitilde{} .60*SN1 + .50*SN2 + .63*SN3 + .43*SN4}
\StringTok{        WeakStereotypes =\textasciitilde{} .38*WS5 + .22*WS6 + .10*WS7 + .77*WS8 + .34*WS9 + .14*WS10}
\StringTok{        AppearanceBias =\textasciitilde{} .38*AB11 + .28*AB12 + .50*AB13 + .18*AB14}
\StringTok{        }
\StringTok{        }
\StringTok{        \#Means}
\StringTok{        \#Keum et al reported total scale scores, I divided those totals by the number of items per scale for  mean scores}
\StringTok{         AS \textasciitilde{} 3.25*1}
\StringTok{         AF \textasciitilde{} 3.34*1}
\StringTok{         AUA \textasciitilde{} 4.52}
\StringTok{         MI \textasciitilde{} 5.77*1}
\StringTok{         General \textasciitilde{} 3.81*1}
\StringTok{         RMAS\_FOR \textasciitilde{} 3.05*1}
\StringTok{         RMAS\_LOW \textasciitilde{} 2.6*1}
\StringTok{         RMAS\_INV \textasciitilde{} 2.105*1}
\StringTok{         RMAS\_ENV \textasciitilde{} 3.126*1}
\StringTok{         SSEL\_Deg \textasciitilde{} 2.55*1}
\StringTok{         SSEL\_dRel \textasciitilde{} 1.96*1}
\StringTok{         SSEL\_cRel \textasciitilde{} 3.10*1}
\StringTok{         SSEL\_Work \textasciitilde{} 1.66*1}
\StringTok{         SSER\_Deg \textasciitilde{} 2.02*1}
\StringTok{         SSER\_dRel \textasciitilde{} 1.592*1}
\StringTok{         SSER\_cRel \textasciitilde{} 1.777*1}
\StringTok{         SSER\_Work \textasciitilde{} 1.3925*1}
\StringTok{         SSER\_Lifetime \textasciitilde{} 2.8245*1}
\StringTok{         SSER\_Recent \textasciitilde{} 2.4875*1}
\StringTok{         PHQ9 \textasciitilde{} 1.836*1}
\StringTok{         gIRAAS \textasciitilde{} 2.246*1}
\StringTok{         }
\StringTok{        \#Correlations}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .00*AF}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .00*AUA}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .00*MI}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .00*General}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .28*RMAS\_FOR}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .24*RMAS\_LOW}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .46*RMAS\_INV}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .16*RMAS\_ENV}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .40*SSE\_Lifetime}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .28*SSE\_Recent}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .15*PHQ9}
\StringTok{         AS \textasciitilde{}\textasciitilde{} .13*gIRAAS}
\StringTok{         }
\StringTok{         AF \textasciitilde{}\textasciitilde{} .00*AUA}
\StringTok{         AF \textasciitilde{}\textasciitilde{} .00*MI}
\StringTok{         AF \textasciitilde{}\textasciitilde{} .00*General}
\StringTok{         AF \textasciitilde{}\textasciitilde{} .02*RMAS\_FOR}
\StringTok{         AF \textasciitilde{}\textasciitilde{} .05*RMAS\_LOW}
\StringTok{         AF \textasciitilde{}\textasciitilde{} .11*RMAS\_INV}
\StringTok{         AF \textasciitilde{}\textasciitilde{} .07*RMAS\_ENV}
\StringTok{         AF \textasciitilde{}\textasciitilde{} .34*SSE\_Lifetime}
\StringTok{         AF \textasciitilde{}\textasciitilde{} .27*SSE\_Recent}
\StringTok{         AF \textasciitilde{}\textasciitilde{} {-}.04*PHQ9}
\StringTok{         AF \textasciitilde{}\textasciitilde{} .21*gIRAAS}
\StringTok{          }
\StringTok{         AUA \textasciitilde{}\textasciitilde{} .00*MI}
\StringTok{         AUA \textasciitilde{}\textasciitilde{} .00*General}
\StringTok{         AUA \textasciitilde{}\textasciitilde{} .18*RMAS\_FOR}
\StringTok{         AUA \textasciitilde{}\textasciitilde{} .20*RMAS\_LOW}
\StringTok{         AUA \textasciitilde{}\textasciitilde{} .01*RMAS\_INV}
\StringTok{         AUA \textasciitilde{}\textasciitilde{} {-}.04*RMAS\_ENV}
\StringTok{         AUA \textasciitilde{}\textasciitilde{} .02*SSE\_Lifetime}
\StringTok{         AUA \textasciitilde{}\textasciitilde{} .92*SSE\_Recent}
\StringTok{         AUA \textasciitilde{}\textasciitilde{} .02*PHQ9}
\StringTok{         AUA \textasciitilde{}\textasciitilde{} .17*gIRAAS}
\StringTok{         }
\StringTok{        }
\StringTok{         MI \textasciitilde{}\textasciitilde{} .00*General}
\StringTok{         MI \textasciitilde{}\textasciitilde{} {-}.02*RMAS\_FOR}
\StringTok{         MI \textasciitilde{}\textasciitilde{} .08*RMAS\_LOW}
\StringTok{         MI \textasciitilde{}\textasciitilde{} .31*RMAS\_INV}
\StringTok{         MI \textasciitilde{}\textasciitilde{} .36*RMAS\_ENV}
\StringTok{         MI \textasciitilde{}\textasciitilde{} .15*SSE\_Lifetime}
\StringTok{         MI \textasciitilde{}\textasciitilde{} .08*SSE\_Recent}
\StringTok{         MI \textasciitilde{}\textasciitilde{} {-}.05*PHQ9}
\StringTok{         MI \textasciitilde{}\textasciitilde{} {-}.03*gIRAAS}
\StringTok{         }
\StringTok{         General \textasciitilde{}\textasciitilde{} .34*RMAS\_FOR}
\StringTok{         General \textasciitilde{}\textasciitilde{} .63*RMAS\_LOW}
\StringTok{         General \textasciitilde{}\textasciitilde{} .44*RMAS\_INV}
\StringTok{         General \textasciitilde{}\textasciitilde{} .45*RMAS\_ENV}
\StringTok{         General \textasciitilde{}\textasciitilde{} .54*SSE\_Lifetime}
\StringTok{         General \textasciitilde{}\textasciitilde{} .46*SSE\_Recent}
\StringTok{         General \textasciitilde{}\textasciitilde{} .31*PHQ9}
\StringTok{         General \textasciitilde{}\textasciitilde{} {-}.06*gIRAAS}
\StringTok{         }
\StringTok{         RMAS\_FOR \textasciitilde{}\textasciitilde{} .57*RMAS\_LOW}
\StringTok{         RMAS\_FOR \textasciitilde{}\textasciitilde{} .56*RMAS\_INV}
\StringTok{         RMAS\_FOR \textasciitilde{}\textasciitilde{} .37*RMAS\_ENV}
\StringTok{         RMAS\_FOR \textasciitilde{}\textasciitilde{} .33*SSE\_Lifetime}
\StringTok{         RMAS\_FOR \textasciitilde{}\textasciitilde{} .25*SSE\_Recent}
\StringTok{         RMAS\_FOR \textasciitilde{}\textasciitilde{} .10*PHQ9}
\StringTok{         RMAS\_FOR \textasciitilde{}\textasciitilde{} .02*gIRAAS}
\StringTok{         }
\StringTok{         RMAS\_LOW \textasciitilde{}\textasciitilde{} .69*RMAS\_INV}
\StringTok{         RMAS\_LOW \textasciitilde{}\textasciitilde{} .48*RMAS\_ENV}
\StringTok{         RMAS\_LOW \textasciitilde{}\textasciitilde{} .67*SSE\_Lifetime}
\StringTok{         RMAS\_LOW \textasciitilde{}\textasciitilde{} .57*SSE\_Recent}
\StringTok{         RMAS\_LOW \textasciitilde{}\textasciitilde{} .30*PHQ9}
\StringTok{         RMAS\_LOW \textasciitilde{}\textasciitilde{} .16*gIRAAS}
\StringTok{         }
\StringTok{         RMAS\_INV \textasciitilde{}\textasciitilde{} .59*RMAS\_ENV}
\StringTok{         RMAS\_INV \textasciitilde{}\textasciitilde{} .63*SSE\_Lifetime}
\StringTok{         RMAS\_INV \textasciitilde{}\textasciitilde{} .52*SSE\_Recent}
\StringTok{         RMAS\_INV \textasciitilde{}\textasciitilde{} .32*PHQ9}
\StringTok{         RMAS\_INV \textasciitilde{}\textasciitilde{} .23*gIRAAS}
\StringTok{         }
\StringTok{         RMAS\_ENV \textasciitilde{}\textasciitilde{} .46*SSE\_Lifetime}
\StringTok{         RMAS\_ENV \textasciitilde{}\textasciitilde{} .31*SSE\_Recent}
\StringTok{         RMAS\_ENV \textasciitilde{}\textasciitilde{} .11*PHQ9}
\StringTok{         RMAS\_ENV \textasciitilde{}\textasciitilde{} .07*gIRAAS}
\StringTok{         }
\StringTok{         SSE\_Lifetime \textasciitilde{}\textasciitilde{} .83*SSE\_Recent}
\StringTok{         SSE\_Lifetime \textasciitilde{}\textasciitilde{} .30*PHQ9}
\StringTok{         SSE\_Lifetime \textasciitilde{}\textasciitilde{} .14*gIRAAS}
\StringTok{         }
\StringTok{         SSE\_Recent \textasciitilde{}\textasciitilde{} .30*PHQ9}
\StringTok{         SSE\_Recent \textasciitilde{}\textasciitilde{} .20*gIRAAS}
\StringTok{         }
\StringTok{         PHQ9 \textasciitilde{}\textasciitilde{} .18*gIRAAS}
\StringTok{         }
\StringTok{       }
\StringTok{         \#Correlations between SES scales from the Klonoff and Landrine article}
\StringTok{         \#Note that in the article the factor orders were reversed}
\StringTok{         SSEL\_Deg \textasciitilde{}\textasciitilde{} .64*SSEL\_dRel}
\StringTok{         SSEL\_Deg \textasciitilde{}\textasciitilde{} .61*SSEL\_cRel}
\StringTok{         SSEL\_Deg \textasciitilde{}\textasciitilde{} .50*SSEL\_Work}
\StringTok{         SSEL\_dRel \textasciitilde{}\textasciitilde{} .57*SSEL\_cRel}
\StringTok{         SSEL\_dRel \textasciitilde{}\textasciitilde{} .57*SSEL\_Work}
\StringTok{         SSEL\_cRel \textasciitilde{}\textasciitilde{} .47*SSEL\_Work}
\StringTok{         }
\StringTok{         SSER\_Deg \textasciitilde{} .54*SSER\_dRel}
\StringTok{         SSER\_Deg \textasciitilde{} .54*SSER\_Work}
\StringTok{         SSER\_Deg \textasciitilde{} .59*SSER\_cRel}
\StringTok{         SSER\_dRel \textasciitilde{} .56*SSER\_Work}
\StringTok{         SSER\_dRel \textasciitilde{} .46*SSER\_cRel}
\StringTok{         SSER\_Work \textasciitilde{} .43*SSER\_cRel}
\StringTok{         }
\StringTok{         SSE\_Lifetime \textasciitilde{} .75*SSE\_Recent}
\StringTok{        }
\StringTok{        \textquotesingle{}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{dfGRMSAAW }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{simulateData}\NormalTok{(}\AttributeTok{model =}\NormalTok{ Keum\_GRMS\_generating\_model,}
                              \AttributeTok{model.type =} \StringTok{"sem"}\NormalTok{,}
                              \AttributeTok{meanstructure =}\NormalTok{ T,}
                              \AttributeTok{sample.nobs=}\DecValTok{304}\NormalTok{,}
                              \AttributeTok{standardized=}\ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\#used to retrieve column indices used in the rescaling script below}
\NormalTok{col\_index }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(dfGRMSAAW))}

\CommentTok{\#The code below loops through each column of the dataframe and assigns the scaling accordingly}
\CommentTok{\#Rows 1 thru 22 are the GRMS items}
\CommentTok{\#Rows 23 thru 47 are the RMAS}
\CommentTok{\#Rows 48 thru 87 are the SSE}
\CommentTok{\#Rows 88 thru 96 are the PHQ9}
\CommentTok{\#Rows 97 thru 110 are the IRAAS}
\CommentTok{\#Rows 111 thru 112 are scale scores for SSE}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(dfGRMSAAW))\{  }
  \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{22}\NormalTok{)\{   }
\NormalTok{    dfGRMSAAW[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfGRMSAAW[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{  \}}
    \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{23} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{47}\NormalTok{)\{   }
\NormalTok{    dfGRMSAAW[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfGRMSAAW[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{    \}}
  \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{48} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{87}\NormalTok{)\{   }
\NormalTok{    dfGRMSAAW[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfGRMSAAW[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{  \}}
    \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{88} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{96}\NormalTok{)\{   }
\NormalTok{    dfGRMSAAW[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfGRMSAAW[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{    \}}
    \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{97} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{110}\NormalTok{)\{   }
\NormalTok{    dfGRMSAAW[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfGRMSAAW[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{  \}}
\NormalTok{\}}

\CommentTok{\#rounding to integers so that the data resembles that which was collected}
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{dfGRMSAAW }\OtherTok{\textless{}{-}}\NormalTok{ dfGRMSAAW }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{round}\NormalTok{(}\DecValTok{0}\NormalTok{) }

\CommentTok{\#quick check of my work}
\CommentTok{\#psych::describe(dfGRMSAAW) }
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either an .rds object or a .csv file.

An .rds file preserves all formatting to variables prior to the export and re-import. For the purpose of this chapter, you don't need to do either. That is, you can re-simulate the data each time you work the problem.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(dfGRMSAAW, \textquotesingle{}dfGRMSAAW.rds\textquotesingle{}) bring back the simulated}
\CommentTok{\# dat from an .rds file dfGRMSAAW \textless{}{-} readRDS(\textquotesingle{}dfGRMSAAW.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

If you save the .csv file (think ``Excel lite'') and bring it back in, you will lose any formatting (e.g., ordered factors will be interpreted as character variables).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(dfGRMSAAW,}
\CommentTok{\# file=\textquotesingle{}dfGRMSAAW.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE)}
\CommentTok{\# bring back the simulated dat from a .csv file dfGRMSAAW \textless{}{-} read.csv}
\CommentTok{\# (\textquotesingle{}dfGRMSAAW.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

\hypertarget{a-quick-lavaan-syntax-recap}{%
\section{\texorpdfstring{A Quick \emph{lavaan} Syntax Recap}{A Quick lavaan Syntax Recap}}\label{a-quick-lavaan-syntax-recap}}

\begin{itemize}
\item
  It's really just regression

  \begin{itemize}
  \tightlist
  \item
    tilda (\textasciitilde, \emph{is regressed on}) is the regression operator
  \item
    place DV (y) on left side of the regression operator
  \item
    place IVs, separated by +, on the right of the regression operator
  \end{itemize}
\item
  f is a latent variable (LV)
\item
  Example: y \textasciitilde{} f1 + f2 + x1 + x2
\item
  LVs must be \emph{defined} by their manifest or latent indicators.

  \begin{itemize}
  \tightlist
  \item
    the special operator (=\textasciitilde, \emph{is measured/defined by}) is used for this
  \item
    Example: f1 =\textasciitilde{} y1 + y2 + y3
  \end{itemize}
\item
  Variances and covariances are specified with a double tilde operator (\textasciitilde\textasciitilde, \emph{is correlated with})

  \begin{itemize}
  \tightlist
  \item
    Example of variance: y1 \textasciitilde\textasciitilde{} y1 (variable's relationship with itself)
  \item
    Example of covariance: y1 \textasciitilde\textasciitilde{} y2 (relationship with another variable)
  \item
    Example of covariance of a factor: f1 \textasciitilde\textasciitilde{} f2
  \end{itemize}
\item
  Intercepts (\textasciitilde{} 1) for observed variables and LVs are simple, intercept-only regression formulas.

  \begin{itemize}
  \tightlist
  \item
    Example of variable intercept: y1 \textasciitilde{} 1
  \item
    Example of factor intercept: f1 \textasciitilde{} 1
  \end{itemize}
\item
  A complete lavaan model is a combination of these formula types, enclosed between single quotation marks. Readability of model syntax is improved by:

  \begin{itemize}
  \tightlist
  \item
    splitting formulas over multiple lines
  \item
    using blank lines within single quote
  \item
    labeling with the hashtag
  \end{itemize}
\end{itemize}

CFAmodel \textless- '\\
f1 =\textasciitilde{} y1 + y2 + y3\\
f2 =\textasciitilde{} y4 + y5 + y6\\
f3 =\textasciitilde{} y7 + y8 + y9 + y10\\
'\\
Behind the scenes the \emph{cfa()} function:

\begin{itemize}
\tightlist
\item
  fixes the factor loading of the first indicator of an LV to 1 (setting the scale)
\item
  automatically adds residual variances (required)
\item
  correlates all exogenous LVs; to turn these off add the following statement to the \emph{cfa()} function statement: \emph{orthogonal = TRUE}
\end{itemize}

\hypertarget{comparing-and-tweaking-multidimensional-first-order-models}{%
\section{Comparing and Tweaking Multidimensional First-Order Models}\label{comparing-and-tweaking-multidimensional-first-order-models}}

In the prior lesson we examined unidimensional and multidimensional variants of the GRMSAAW. Our work determined that the first-order structure that included four correlated factors was superior to a unidimensional measure. Starting with the multidimensional model (four factors), let's specify both correlated and uncorrelated options and compare them. We'll choose the best and see if we can further ``tweak'''' it into acceptable fit.

\hypertarget{an-uncorrelated-factors-model}{%
\section{An Uncorrelated Factors Model}\label{an-uncorrelated-factors-model}}

\hypertarget{specifying-the-model}{%
\subsection{Specifying the Model}\label{specifying-the-model}}

In the absence of a more complex (e.g., second-order) structure, \emph{lavaan's} \emph{cfa()} function automatically correlates first-order factors. However, the more parsimonious model is one with uncorrelated factors. We'll run it first. To do so, we need to turn off the default so that factors will be uncorrelated. This is accomplished in the \emph{cfa()} function script with \emph{orthogonal = TRUE}.

In the first step we specify the equations in our model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grmsAAW4mod }\OtherTok{\textless{}{-}} \StringTok{"AS =\textasciitilde{} AS1 + AS2 + AS3 + AS4 + AS5 + AS6 + AS7 + AS8 + AS9}
\StringTok{             AF =\textasciitilde{} AF1 + AF2 + AF3 + AF4 }
\StringTok{             MI =\textasciitilde{} MI1 + MI2 + MI3 + MI4 + MI5}
\StringTok{             AUA =\textasciitilde{} AUA1 + AUA2 + AUA3 + AUA4"}
\NormalTok{grmsAAW4mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "AS =~ AS1 + AS2 + AS3 + AS4 + AS5 + AS6 + AS7 + AS8 + AS9\n             AF =~ AF1 + AF2 + AF3 + AF4 \n             MI =~ MI1 + MI2 + MI3 + MI4 + MI5\n             AUA =~ AUA1 + AUA2 + AUA3 + AUA4"
\end{verbatim}

The next code will run the model. This is where we insert \emph{orthogonal = TRUE}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# next, use the cfa function to apply the model to the data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{uncorrF }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(grmsAAW4mod, }\AttributeTok{data =}\NormalTok{ dfGRMSAAW, }\AttributeTok{orthogonal =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(uncorrF, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rsquare =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 44 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        44

  Number of observations                           304

Model Test User Model:
                                                      
  Test statistic                               461.102
  Degrees of freedom                               209
  P-value (Chi-square)                           0.000

Model Test Baseline Model:

  Test statistic                              1439.317
  Degrees of freedom                               231
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.791
  Tucker-Lewis Index (TLI)                       0.769

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -8395.340
  Loglikelihood unrestricted model (H1)      -8164.789
                                                      
  Akaike (AIC)                               16878.679
  Bayesian (BIC)                             17042.229
  Sample-size adjusted Bayesian (SABIC)      16902.683

Root Mean Square Error of Approximation:

  RMSEA                                          0.063
  90 Percent confidence interval - lower         0.055
  90 Percent confidence interval - upper         0.071
  P-value H_0: RMSEA <= 0.050                    0.003
  P-value H_0: RMSEA >= 0.080                    0.000

Standardized Root Mean Square Residual:

  SRMR                                           0.151

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  AS =~                                                                 
    AS1               1.000                               0.552    0.603
    AS2               1.169    0.139    8.425    0.000    0.646    0.639
    AS3               0.931    0.124    7.534    0.000    0.515    0.548
    AS4               0.904    0.121    7.451    0.000    0.499    0.540
    AS5               1.138    0.135    8.453    0.000    0.629    0.642
    AS6               0.653    0.094    6.935    0.000    0.361    0.493
    AS7               0.800    0.118    6.785    0.000    0.442    0.479
    AS8               0.899    0.121    7.443    0.000    0.496    0.539
    AS9               0.770    0.106    7.291    0.000    0.426    0.525
  AF =~                                                                 
    AF1               1.000                               0.591    0.659
    AF2               0.881    0.151    5.851    0.000    0.521    0.536
    AF3               0.665    0.126    5.287    0.000    0.393    0.445
    AF4               0.943    0.159    5.950    0.000    0.557    0.566
  MI =~                                                                 
    MI1               1.000                               0.426    0.511
    MI2               1.136    0.209    5.447    0.000    0.484    0.549
    MI3               1.475    0.264    5.589    0.000    0.629    0.614
    MI4               1.089    0.212    5.139    0.000    0.464    0.483
    MI5               0.614    0.166    3.703    0.000    0.262    0.297
  AUA =~                                                                
    AUA1              1.000                               0.590    0.579
    AUA2              0.950    0.148    6.401    0.000    0.560    0.628
    AUA3              0.745    0.124    6.027    0.000    0.439    0.535
    AUA4              0.928    0.149    6.243    0.000    0.547    0.576

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  AS ~~                                                                 
    AF                0.000                               0.000    0.000
    MI                0.000                               0.000    0.000
    AUA               0.000                               0.000    0.000
  AF ~~                                                                 
    MI                0.000                               0.000    0.000
    AUA               0.000                               0.000    0.000
  MI ~~                                                                 
    AUA               0.000                               0.000    0.000

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .AS1               0.535    0.050   10.643    0.000    0.535    0.637
   .AS2               0.603    0.059   10.283    0.000    0.603    0.591
   .AS3               0.618    0.056   11.065    0.000    0.618    0.700
   .AS4               0.607    0.055   11.117    0.000    0.607    0.709
   .AS5               0.563    0.055   10.248    0.000    0.563    0.587
   .AS6               0.407    0.036   11.386    0.000    0.407    0.757
   .AS7               0.654    0.057   11.451    0.000    0.654    0.770
   .AS8               0.602    0.054   11.122    0.000    0.602    0.710
   .AS9               0.477    0.043   11.209    0.000    0.477    0.725
   .AF1               0.454    0.065    6.992    0.000    0.454    0.565
   .AF2               0.672    0.071    9.516    0.000    0.672    0.713
   .AF3               0.627    0.059   10.659    0.000    0.627    0.802
   .AF4               0.658    0.073    8.991    0.000    0.658    0.679
   .MI1               0.516    0.053    9.801    0.000    0.516    0.739
   .MI2               0.543    0.059    9.205    0.000    0.543    0.698
   .MI3               0.652    0.082    7.959    0.000    0.652    0.622
   .MI4               0.710    0.070   10.164    0.000    0.710    0.767
   .MI5               0.706    0.061   11.654    0.000    0.706    0.912
   .AUA1              0.689    0.075    9.140    0.000    0.689    0.664
   .AUA2              0.482    0.059    8.205    0.000    0.482    0.606
   .AUA3              0.483    0.049    9.827    0.000    0.483    0.714
   .AUA4              0.605    0.066    9.201    0.000    0.605    0.669
    AS                0.305    0.058    5.229    0.000    1.000    1.000
    AF                0.349    0.076    4.605    0.000    1.000    1.000
    MI                0.182    0.050    3.659    0.000    1.000    1.000
    AUA               0.348    0.081    4.312    0.000    1.000    1.000

R-Square:
                   Estimate
    AS1               0.363
    AS2               0.409
    AS3               0.300
    AS4               0.291
    AS5               0.413
    AS6               0.243
    AS7               0.230
    AS8               0.290
    AS9               0.275
    AF1               0.435
    AF2               0.287
    AF3               0.198
    AF4               0.321
    MI1               0.261
    MI2               0.302
    MI3               0.378
    MI4               0.233
    MI5               0.088
    AUA1              0.336
    AUA2              0.394
    AUA3              0.286
    AUA4              0.331
\end{verbatim}

Producing a figure can be useful to represent what we did to others as well as checking our own work. That is, ``Did we think we did what we intended?'' When the \emph{what = ``col'', whatLabels = ``stand''} combination is shown, paths that are ``fixed'' are represented by dashed lines. Below, we expect to see each of the four factors predicting only the items associated with their factor. One item for each factor (the first on the left) should be specified as the indicator variable (and represented with a dashed line). Additionally, the factors/latent variables should not be freed to covary (i.e., an uncorrelated traits or orthogonal model). Because they are ``fixed'' to be 0.00, they will be represented with dashed (not solid) curves with double-headed arrows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(uncorrF, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{11-CFA_2ndOrder_files/figure-latex/unnamed-chunk-8-1.pdf}

Among my first steps are also to write the code to export the results. The \emph{tidySEM} package has useful functions to export the fit statistics, parameter estimates, and correlations among the latent variables (i.e., factors).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{UncorrFitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(uncorrF)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Registered S3 method overwritten by 'tidySEM':
  method          from  
  predict.MxModel OpenMx
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Uncorr\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(uncorrF, }\AttributeTok{digits=}\DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{UncorrCorrs }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_cors}\NormalTok{(uncorrF, }\AttributeTok{digits=}\DecValTok{3}\NormalTok{)}
\CommentTok{\#to see each of the tables, remove the hashtab}
\CommentTok{\#Uncorr\_FitStats}
\CommentTok{\#Uncorr\_paramEsts}
\CommentTok{\#UncorrCorrs}
\end{Highlighting}
\end{Shaded}

Next, I export them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(UncorrFitStats, }\AttributeTok{file =} \StringTok{"UncorrFitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(Uncorr\_paramEsts, }\AttributeTok{file =} \StringTok{"Uncorr\_paramEsts.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(UncorrCorrs, }\AttributeTok{file =} \StringTok{"UncorrCorrs.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{interpreting-the-output}{%
\subsection{Interpreting the Output}\label{interpreting-the-output}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3780}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4512}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1707}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Our Results
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Criteria met?
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Factor loadings significant, strong, proper valence & AS: .59 to .80; AF: .64 to .82; MI: .35 to .62; AUA: .49 to .82 & Yes \\
Non-significant chi-square & \(\chi ^{2}(209) = 461.102, p < 0.001\) & No \\
\(CFI\geq .90\) or \(.95\) & CFI = 0.791 & No \\
\(RMSEA\leq .05\) (but definitely \textless{} .10) & RMSEA = 0.063, 90\%CI(0.055, 0.071) & Yes (with caution) \\
\(SRMR\leq .08\) (but definitely \textless{} .10) & SRMS = 0.151 & No \\
Combination rule: \(CFI \geq .95\) and \(SRMR \leq .08\) & CFI = 0.791, SRS = 0.151 & No \\
\end{longtable}

\hypertarget{partial-write-up}{%
\subsection{Partial Write-up}\label{partial-write-up}}

\begin{quote}
\textbf{Uncorrelated factors model}. Our model where factors were fixed to remain uncorrelated demonstrated less than adequate fit to the data: \(\chi ^{2}(209) = 461.102, p < 0.001\), CFI = 0.791, RMSEA = 0.063, 90\%CI(0.055, 0.071), SRMR = 0.151. Factor loadings ranged from .59 to .80 for the AS scale, .64 to .82 for the AF scale, .35 to .62 for the MI scale, and .49 to .82 for the fear of AUA scale.
\end{quote}

Our fit is not satisfactory. We can expect the correlated factors model should have a better fit. Instead of ``tweaking'' this one, let's move onto the correlated factors model.

\hypertarget{a-correlated-factors-model}{%
\section{A Correlated Factors Model}\label{a-correlated-factors-model}}

Let's revisit the statement I just made: \emph{\ldots the correlated factors model should have a better fit.} Why did I make this statement? It's all about degrees of freedom and whether the model is the nested or nesting model.

\hypertarget{nested-models}{%
\subsection{Nested Models}\label{nested-models}}

When we specify (i.e., draw) models in SEM/CFA, we often think that the paths (single headed arrows/paths, double-headed arrows/covariances) between the parameters are our hypotheses. They are, but they are \emph{soft hypotheses} in that we are \emph{freeing} the elements to covary. The \emph{hard hypotheses} (i.e., no paths, no covariances) states that the parameters are unrelated. We are trying to explain the covariance matrix (where all parameters are freed to covary) with the fewest paths possible. That is, we are freeing the relations between our hypothesized parameters and restricting all others to be zero.

Two models are \textbf{nested} (aka \textbf{hierarchical}) if one is a proper subset of the other. The \textbf{nesting} model is the one with the most parameters freed to covary. That is, it has more paths/covariances drawn on it. Almost always, the \textbf{nesting model} (i.e., most sticks, fewer degrees of freedom) will have better fit than the \textbf{nested} model (i.e., fewer sticks, more degrees of freedom).

In our example, \emph{uncorrF} has four uncorrelated factors and its degrees of freedom was 209.

Our new model will \emph{add covariances} (making it the \emph{nesting model} with presumed better fit) to all possible combinations of the four factors (we end up with 6 covariance paths). Freeing these additional factors to covary in the corrF model (recall they were fixed to 0.0 in the uncorrF model) leads to a model with 203 degrees of freedom. The degrees of freedom are lower because the algorithm now needs to estimate 6 additional covariances/parameters (i.e., \(209 - 6 = 203\)).

Model fit (generally) improves when paths/covariances are added (and degrees of freedom decreases). The model with the \emph{most paths} (I think of ``sticks'' in a nest) and the \emph{fewest df} is the \emph{nesting model} and it (almost) always has superior fit.

Let's try. We continue to use the model of equations we specified for the orthogonal, uncorrelated traits, model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# in our 4{-}factor models we can use the same baseM, the difference}
\CommentTok{\# here is that we deleted \textquotesingle{}orthogonal = TRUE\textquotesingle{} uncorrF \textless{}{-}}
\CommentTok{\# lavaan::cfa(grmsAAW4mod, data = dfGRMSAAW, orthogonal = TRUE) \#for}
\CommentTok{\# comparison, this was the uncorrelated model}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{corrF }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(grmsAAW4mod, }\AttributeTok{data =}\NormalTok{ dfGRMSAAW)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(corrF, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rsquare =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 42 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        50

  Number of observations                           304

Model Test User Model:
                                                      
  Test statistic                               232.453
  Degrees of freedom                               203
  P-value (Chi-square)                           0.076

Model Test Baseline Model:

  Test statistic                              1439.317
  Degrees of freedom                               231
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.976
  Tucker-Lewis Index (TLI)                       0.972

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -8281.015
  Loglikelihood unrestricted model (H1)      -8164.789
                                                      
  Akaike (AIC)                               16662.030
  Bayesian (BIC)                             16847.882
  Sample-size adjusted Bayesian (SABIC)      16689.307

Root Mean Square Error of Approximation:

  RMSEA                                          0.022
  90 Percent confidence interval - lower         0.000
  90 Percent confidence interval - upper         0.034
  P-value H_0: RMSEA <= 0.050                    1.000
  P-value H_0: RMSEA >= 0.080                    0.000

Standardized Root Mean Square Residual:

  SRMR                                           0.047

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  AS =~                                                                 
    AS1               1.000                               0.550    0.600
    AS2               1.132    0.136    8.330    0.000    0.623    0.617
    AS3               0.958    0.123    7.769    0.000    0.527    0.561
    AS4               0.901    0.120    7.504    0.000    0.496    0.536
    AS5               1.152    0.134    8.620    0.000    0.634    0.647
    AS6               0.669    0.094    7.133    0.000    0.368    0.503
    AS7               0.829    0.118    7.043    0.000    0.456    0.495
    AS8               0.905    0.120    7.551    0.000    0.498    0.540
    AS9               0.757    0.104    7.256    0.000    0.417    0.514
  AF =~                                                                 
    AF1               1.000                               0.505    0.563
    AF2               1.195    0.174    6.862    0.000    0.603    0.621
    AF3               0.738    0.137    5.395    0.000    0.373    0.422
    AF4               1.138    0.171    6.665    0.000    0.575    0.584
  MI =~                                                                 
    MI1               1.000                               0.482    0.577
    MI2               0.917    0.148    6.216    0.000    0.442    0.501
    MI3               1.169    0.177    6.602    0.000    0.563    0.550
    MI4               0.921    0.157    5.865    0.000    0.444    0.461
    MI5               0.688    0.137    5.018    0.000    0.332    0.377
  AUA =~                                                                
    AUA1              1.000                               0.553    0.543
    AUA2              0.981    0.140    7.016    0.000    0.543    0.608
    AUA3              0.785    0.122    6.457    0.000    0.434    0.528
    AUA4              1.083    0.152    7.140    0.000    0.599    0.630

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  AS ~~                                                                 
    AF                0.148    0.030    4.951    0.000    0.533    0.533
    MI                0.136    0.028    4.889    0.000    0.513    0.513
    AUA               0.181    0.034    5.257    0.000    0.595    0.595
  AF ~~                                                                 
    MI                0.154    0.031    5.010    0.000    0.632    0.632
    AUA               0.164    0.034    4.805    0.000    0.588    0.588
  MI ~~                                                                 
    AUA               0.189    0.036    5.303    0.000    0.709    0.709

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .AS1               0.538    0.050   10.833    0.000    0.538    0.640
   .AS2               0.632    0.059   10.699    0.000    0.632    0.620
   .AS3               0.605    0.054   11.111    0.000    0.605    0.685
   .AS4               0.610    0.054   11.260    0.000    0.610    0.713
   .AS5               0.557    0.053   10.408    0.000    0.557    0.581
   .AS6               0.401    0.035   11.433    0.000    0.401    0.747
   .AS7               0.641    0.056   11.470    0.000    0.641    0.755
   .AS8               0.601    0.053   11.235    0.000    0.601    0.708
   .AS9               0.484    0.043   11.379    0.000    0.484    0.736
   .AF1               0.548    0.055    9.928    0.000    0.548    0.683
   .AF2               0.579    0.064    9.062    0.000    0.579    0.614
   .AF3               0.642    0.057   11.230    0.000    0.642    0.822
   .AF4               0.638    0.066    9.651    0.000    0.638    0.659
   .MI1               0.465    0.047    9.823    0.000    0.465    0.667
   .MI2               0.582    0.055   10.664    0.000    0.582    0.749
   .MI3               0.731    0.072   10.158    0.000    0.731    0.697
   .MI4               0.729    0.066   10.994    0.000    0.729    0.787
   .MI5               0.665    0.058   11.519    0.000    0.665    0.858
   .AUA1              0.730    0.069   10.535    0.000    0.730    0.705
   .AUA2              0.501    0.051    9.787    0.000    0.501    0.630
   .AUA3              0.487    0.046   10.675    0.000    0.487    0.721
   .AUA4              0.546    0.058    9.475    0.000    0.546    0.603
    AS                0.303    0.058    5.264    0.000    1.000    1.000
    AF                0.255    0.058    4.412    0.000    1.000    1.000
    MI                0.232    0.051    4.559    0.000    1.000    1.000
    AUA               0.306    0.070    4.391    0.000    1.000    1.000

R-Square:
                   Estimate
    AS1               0.360
    AS2               0.380
    AS3               0.315
    AS4               0.287
    AS5               0.419
    AS6               0.253
    AS7               0.245
    AS8               0.292
    AS9               0.264
    AF1               0.317
    AF2               0.386
    AF3               0.178
    AF4               0.341
    MI1               0.333
    MI2               0.251
    MI3               0.303
    MI4               0.213
    MI5               0.142
    AUA1              0.295
    AUA2              0.370
    AUA3              0.279
    AUA4              0.397
\end{verbatim}

As we plot this model we expect to see each of the four factors predicting only the items associated with their factor, one item for each factor (the first on the left) specified as the indicator variable, and double-headed arrows between the factors/latent variables, indicating that they are free to covary (i.e., a correlated traits model).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(corrF, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{11-CFA_2ndOrder_files/figure-latex/unnamed-chunk-12-1.pdf}

Among my first steps are also to write the code to export the results. The \emph{tidySEM} package has useful functions to export the fit statistics, parameter estimates, and correlations among the latent variables (i.e., factors).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CorrFitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(corrF)}
\NormalTok{Corr\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(corrF, }\AttributeTok{digits=}\DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{CorrCorrs }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_cors}\NormalTok{(corrF, }\AttributeTok{digits=}\DecValTok{3}\NormalTok{)}
\CommentTok{\#to see each of the tables, remove the hashtab}
\CommentTok{\#CorrFitStats}
\CommentTok{\#Corr\_paramEsts}
\CommentTok{\#CorrCorrs}
\end{Highlighting}
\end{Shaded}

Next, I export them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(CorrFitStats, }\AttributeTok{file =} \StringTok{"CorrFitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(Corr\_paramEsts, }\AttributeTok{file =} \StringTok{"Corr\_paramEsts.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(CorrCorrs, }\AttributeTok{file =} \StringTok{"CorrCorrs.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{interpreting-the-output-1}{%
\subsection{Interpreting the Output}\label{interpreting-the-output-1}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3780}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4512}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1707}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Our Results
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Criteria met?
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Factor loadings significant, strong, proper valence & AS: .50 to .65; AF: .42 to .62; MI: .46 to .58; AUA: .54 to .63 & Yes \\
Non-significant chi-square & \(\chi ^{2}(203) = 232.453, p = 0.076\) & Yes \\
\(CFI\geq .95\) & CFI = 0.976 & Yes \\
\(RMSEA\leq .05\) (but definitely \textless{} .10) & RMSEA = 0.022, 90\%CI(0.000, 0.034) & Yes \\
\(SRMR\leq .08\) (but definitely \textless{} .10) & SRMR = 0.047 & Yes \\
Combination rule: \(CFI \geq .95\) and \(SRMR \leq .08\) & CFI = 0.976, SRS = 0.047 & Yes \\
\end{longtable}

\hypertarget{partial-write-up-1}{%
\subsection{Partial Write-up}\label{partial-write-up-1}}

\begin{quote}
\textbf{Correlated factors model}. We evaluated a single-order, correlated factors model where each of the 22 items loaded onto one of four factors and the factors were free to correlate. Standardized pattern coefficients ranged between .37 and .60 on the AF factor, between .37 and .63 on the AS factor, between .33 and .56 on the MI factor, and between .43 and .60 on the AUA factor. The Chi-square index was not statistically significant (\(\chi ^{2}(203)=232.453, p = 0.076\)) indicating reasonable fit. The CFI value of 0.972 exceeded the recommendation of .95. The RMSEA = 0.022 (90\%CI{[}.000, 0.034{]}) was satisfactory. The SRMR value of 0.047 remained below the warning criteria of .10. The AIC and BIC values were 16662.030 and 16847.882, respectively.
\end{quote}

Recall that we can formally compare these models with the \(\chi_{D}^{2}\), AIC, and BIC.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{lavTestLRT}\NormalTok{(uncorrF, corrF)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Chi-Squared Difference Test

         Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff            Pr(>Chisq)
corrF   203 16662 16848 232.45                                                 
uncorrF 209 16879 17042 461.10     228.65 0.34938       6 < 0.00000000000000022
           
corrF      
uncorrF ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The AIC and BIC are flexible to compare nested and non-nested models. Models with the lower values are superior. Consistent with what we expect (i.e., the nesting model {[}the model with the most parameters and fewest degrees of freedom{]} should be superior), the AIC and BIC favor the correlated factors model.

The \(\chi_{D}^{2}\) can only be used for nested models (where items/indicators are identical -- the only difference is the presence/absence of parameters). If it is statistically significant, the better model is the one with the lower chi-square value (and better fit indices). In this particular comparison there is a statistically significant difference favoring the correlated factors model (\(\chi ^{2}(6)=228.65, p < 0.001\)).

To recap the highlights of nesting, the \emph{nesting} model will usually have the best fit. The \emph{nesting} model has:

\begin{itemize}
\tightlist
\item
  the most free parameters

  \begin{itemize}
  \tightlist
  \item
    (the most ``sticks'')
  \end{itemize}
\item
  the fewest degrees of freedom
\end{itemize}

\begin{figure}
\centering
\includegraphics{images/CFA2nd/corr_uncorr_semPlot.png}
\caption{Side by side comparison of uncorrelated and correlated models}
\end{figure}

Examining the two models we compared side-by-side (uncorrelated on left; correlated on right), we note the additional ``sticks'' (i.e., the covariances that were freed) in the correlated factors model (i.e., the nesting model) and can guess (without analyzing the data) that it because it has (a) more sticks and (b) fewer degrees of freedom, it will have (c) better fit.

How to keep them straight: ``the nested is within (or `sits in' or `fits in') the nesting model.'' I also think, ``The nested sits within but the nesting will win.'' Just keep saying these until they ``stick'' (bad pun intended).

\hypertarget{model-respecification}{%
\section{Model Respecification}\label{model-respecification}}

Our correlated factors model has excellent fit, but this is not always the case. One way to improve model fit is to add parameters to simpler models -- this is called \textbf{model building}. This can only occur for models that are \textbf{overidentified} (i.e., they have positive degrees of freedom).

In the CFA/psychometric case, a just-identified model is one that has at least 3 items per scale for a unidimensional factor structure and at least 2 items per scale in a multidimensional factor structure.

As we ``free'' each parameter (i.e., add paths or covariances), we correspondingly decrease the df. So, we must be diligent when engaging in model building.

In the CFA/psychometric case, \emph{freeing parameters} usually means one of two things.

\begin{itemize}
\tightlist
\item
  Allowing cross-loadings.

  \begin{itemize}
  \tightlist
  \item
    This would mean that an item belongs to two factors/scales. While this might be theoretically defensible, items that belong to more than one scale cause scoring difficulties when the scale is put into practice.
  \end{itemize}
\item
  Allowing the error variances of indicators to correlate.

  \begin{itemize}
  \tightlist
  \item
    This would mean that there is something in common about the two items that is not explained/caused by the items' relationship(s) with their respective factor(s). There are a variety of reasons this could occur, perhaps they have a content element that is in common, but different than the factor to which they belong. Methods factors (e.g., reverse scored items) can also contribute to items being correlated.
  \end{itemize}
\end{itemize}

We use \textbf{modification indices} as a guide to determine if an error covariance is worth freeing. Modification indices tell you the degree to which your chi-square value will drop if the relationship between the two parameters is freed to relate (either a path or a covariance). Generally, a 1 degree of freedom change in a model will be a statistically significant difference if the chi-square value drops by 4 points. This is purely a statistical test that you have to then discern:

\begin{itemize}
\tightlist
\item
  if allowing the two elements to relate is theoretically defensible; and/or
\item
  if there is truly something reasonably in common between the elements that is different from the theorized relations with the factors
\end{itemize}

Although many psychometricians frown on this, I think it, minimally, makes good diagnostic sense to take a look. The code below extracts the modification indices (MIs) from the object (\emph{corrF}) that holds the \emph{lavaan} output. Only MIs with a value greater than 4.0 are shown and they are sorted in descending order. We only ask for MIs greater than 4.0 because a 1 degree-of-freedom Chi-square difference test requires a difference of 3.841 (rounds to 4.0) to be statistically significant at \(p < 0.05\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{modindices}\NormalTok{(corrF, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{minimum.value =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    lhs op  rhs    mi    epc sepc.lv sepc.all sepc.nox
282 AF1 ~~ AUA1 8.812 -0.124  -0.124   -0.196   -0.196
324 MI2 ~~  MI3 8.166  0.135   0.135    0.206    0.206
275 AF1 ~~  AF3 7.785  0.115   0.115    0.195    0.195
69   AF =~  AS2 7.738 -0.435  -0.220   -0.218   -0.218
77   AF =~  MI1 7.221  0.517   0.261    0.313    0.313
120 AUA =~  MI5 6.740  0.537   0.297    0.337    0.337
95   MI =~  AF1 6.221 -0.539  -0.260   -0.290   -0.290
241 AS7 ~~  MI4 5.912  0.102   0.102    0.150    0.150
319 MI1 ~~  MI5 5.849 -0.092  -0.092   -0.166   -0.166
288 AF2 ~~  MI1 5.755  0.088   0.088    0.169    0.169
78   AF =~  MI2 5.748 -0.473  -0.239   -0.271   -0.271
193 AS4 ~~  MI4 5.479 -0.097  -0.097   -0.145   -0.145
289 AF2 ~~  MI2 5.191 -0.090  -0.090   -0.155   -0.155
163 AS3 ~~  AS5 5.059 -0.089  -0.089   -0.153   -0.153
124 AS1 ~~  AS5 5.050  0.085   0.085    0.156    0.156
168 AS3 ~~  AF1 4.910  0.082   0.082    0.143    0.143
112 AUA =~  AF1 4.850 -0.372  -0.206   -0.230   -0.230
113 AUA =~  AF2 4.735  0.416   0.230    0.237    0.237
278 AF1 ~~  MI2 4.601 -0.080  -0.080   -0.141   -0.141
63   AS =~  MI5 4.516  0.277   0.152    0.173    0.173
64   AS =~ AUA1 4.492 -0.349  -0.192   -0.188   -0.188
67   AS =~ AUA4 4.463  0.334   0.184    0.193    0.193
259 AS8 ~~ AUA3 4.441 -0.072  -0.072   -0.133   -0.133
182 AS4 ~~  AS6 4.380 -0.065  -0.065   -0.132   -0.132
92   MI =~  AS7 4.127  0.312   0.151    0.163    0.163
96   MI =~  AF2 4.072  0.494   0.238    0.245    0.245
\end{verbatim}

\hypertarget{respecifying-with-correlated-errors}{%
\subsection{Respecifying with Correlated Errors}\label{respecifying-with-correlated-errors}}

When we inspect the modification indices output, we are:

\begin{itemize}
\tightlist
\item
  inspecting (and perhaps acting on) at the highest \emph{mi} value, one at a time,
\item
  seeing if that value seems a substantially higher than the next highest value.
\end{itemize}

In CFA models, freeing the errors of the items to covary means that there is something in common between the items that is not explained by their relationship to the factor (or, factors, if they are assigned to different factors). It is important to consider (theoretically, rationally) what might be shared between the items. It could be content; it could be a methods factor (e.g., reverse-scored items).

In our dataset, allowing the AF1 error to correlate with the AUA1 error will reduce the \(\chi ^{2}\) by 8.812 points. Generally, a 1 degree of freedom change in a model will be a statistically significant difference if the chi-square value drops by 4 points, so we can expect this to make a statistically significant difference.

Next, we must inspect the relationship to see if we could justify connecting them through a path or covariance. The items in question are:

\begin{itemize}
\tightlist
\item
  Others express sexual interest in me because of my Asian appearance. (AF1)
\item
  Others have talked about AAW as if they all have the same facial features (e.g., eye shape, skin tone). (AUA1)
\end{itemize}

The MI value greater than 4.0 suggests that there \emph{may} be something in common between these two items that is not explained by their relationship with their respective factors (which are allowed to correlate). To allow these variables to covary we simply update the object which holds our map/model of equations and specify a covariance between the variables (``AF1 \textasciitilde\textasciitilde{} AUA1)''=.'' What's actually happening is that we are allowing the \emph{errors} of these variables to correlate, hence this is frequently referred to as ``correlated errors'' or ``error covariances.''

Here's where the research team has the opportunity to reexamine these variables, their hypothesized relationship with the factor, and consider alternatives. In the specific case of these variables, the two factors are Asian Fetishism (AF) and Assumption of Universal Appearance (AUA). One of my first thoughts is that, like the items on the AUA factor, AF1 references appearance or physical features. Thus, I can see why the items would be correlated. I do note that an MI of 8 is relatively low. The typical approach is to look at the sorted MIs and start with the ``outrageously high'' ones. Once the MIs even out, we stop respecifying.

Were this my data, I would be happy with these results and not respecify the model. However, because this is a teaching lesson, I will demonstrate the respecification and evaluation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ModInd\_M1 }\OtherTok{\textless{}{-}} \StringTok{"AS =\textasciitilde{} AS1 + AS2 + AS3 + AS4 + AS5 + AS6 + AS7 + AS8 + AS9}
\StringTok{             AF =\textasciitilde{} AF1 + AF2 + AF3 + AF4 }
\StringTok{             MI =\textasciitilde{} MI1 + MI2 + MI3 + MI4 + MI5}
\StringTok{             AUA =\textasciitilde{} AUA1 + AUA2 + AUA3 + AUA4}

\StringTok{             \#adding the correlated errors}
\StringTok{             AF1 \textasciitilde{}\textasciitilde{} AUA1}
\StringTok{"}
\end{Highlighting}
\end{Shaded}

We'll give our respecified model a new object name and run it. Because we have added a path (allowing the cross-loading), this becomes the nesting model (it has the most paths and the fewest degrees of freedom).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{ModInd\_M1f }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(ModInd\_M1, }\AttributeTok{data =}\NormalTok{ dfGRMSAAW)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(ModInd\_M1f, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 42 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        51

  Number of observations                           304

Model Test User Model:
                                                      
  Test statistic                               223.332
  Degrees of freedom                               202
  P-value (Chi-square)                           0.145

Model Test Baseline Model:

  Test statistic                              1439.317
  Degrees of freedom                               231
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.982
  Tucker-Lewis Index (TLI)                       0.980

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -8276.455
  Loglikelihood unrestricted model (H1)      -8164.789
                                                      
  Akaike (AIC)                               16654.910
  Bayesian (BIC)                             16844.478
  Sample-size adjusted Bayesian (SABIC)      16682.732

Root Mean Square Error of Approximation:

  RMSEA                                          0.019
  90 Percent confidence interval - lower         0.000
  90 Percent confidence interval - upper         0.032
  P-value H_0: RMSEA <= 0.050                    1.000
  P-value H_0: RMSEA >= 0.080                    0.000

Standardized Root Mean Square Residual:

  SRMR                                           0.046

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  AS =~                                                                 
    AS1               1.000                               0.550    0.600
    AS2               1.133    0.136    8.327    0.000    0.623    0.617
    AS3               0.959    0.123    7.771    0.000    0.528    0.561
    AS4               0.901    0.120    7.493    0.000    0.495    0.535
    AS5               1.152    0.134    8.611    0.000    0.634    0.647
    AS6               0.670    0.094    7.132    0.000    0.368    0.503
    AS7               0.829    0.118    7.038    0.000    0.456    0.495
    AS8               0.906    0.120    7.553    0.000    0.498    0.541
    AS9               0.757    0.104    7.248    0.000    0.416    0.513
  AF =~                                                                 
    AF1               1.000                               0.508    0.568
    AF2               1.183    0.171    6.915    0.000    0.601    0.619
    AF3               0.740    0.135    5.469    0.000    0.376    0.425
    AF4               1.118    0.167    6.686    0.000    0.568    0.577
  MI =~                                                                 
    MI1               1.000                               0.483    0.578
    MI2               0.915    0.147    6.217    0.000    0.442    0.501
    MI3               1.170    0.177    6.617    0.000    0.565    0.552
    MI4               0.918    0.157    5.860    0.000    0.443    0.460
    MI5               0.683    0.137    5.000    0.000    0.330    0.375
  AUA =~                                                                
    AUA1              1.000                               0.551    0.543
    AUA2              0.982    0.140    7.023    0.000    0.541    0.606
    AUA3              0.785    0.121    6.459    0.000    0.432    0.526
    AUA4              1.087    0.152    7.161    0.000    0.599    0.630

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
 .AF1 ~~                                                                
   .AUA1             -0.126    0.042   -2.996    0.003   -0.126   -0.200
  AS ~~                                                                 
    AF                0.147    0.030    4.933    0.000    0.526    0.526
    MI                0.136    0.028    4.891    0.000    0.513    0.513
    AUA               0.180    0.034    5.251    0.000    0.593    0.593
  AF ~~                                                                 
    MI                0.156    0.031    5.047    0.000    0.635    0.635
    AUA               0.173    0.033    5.217    0.000    0.618    0.618
  MI ~~                                                                 
    AUA               0.187    0.035    5.295    0.000    0.704    0.704

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .AS1               0.538    0.050   10.832    0.000    0.538    0.640
   .AS2               0.632    0.059   10.693    0.000    0.632    0.620
   .AS3               0.605    0.054   11.105    0.000    0.605    0.685
   .AS4               0.611    0.054   11.261    0.000    0.611    0.713
   .AS5               0.557    0.054   10.408    0.000    0.557    0.581
   .AS6               0.401    0.035   11.430    0.000    0.401    0.747
   .AS7               0.642    0.056   11.469    0.000    0.642    0.755
   .AS8               0.600    0.053   11.229    0.000    0.600    0.707
   .AS9               0.485    0.043   11.379    0.000    0.485    0.737
   .AF1               0.543    0.055    9.879    0.000    0.543    0.678
   .AF2               0.582    0.063    9.180    0.000    0.582    0.617
   .AF3               0.640    0.057   11.237    0.000    0.640    0.819
   .AF4               0.646    0.066    9.808    0.000    0.646    0.667
   .MI1               0.464    0.047    9.807    0.000    0.464    0.666
   .MI2               0.582    0.055   10.667    0.000    0.582    0.749
   .MI3               0.729    0.072   10.140    0.000    0.729    0.696
   .MI4               0.730    0.066   11.000    0.000    0.730    0.788
   .MI5               0.666    0.058   11.528    0.000    0.666    0.860
   .AUA1              0.726    0.069   10.534    0.000    0.726    0.705
   .AUA2              0.503    0.051    9.864    0.000    0.503    0.632
   .AUA3              0.489    0.046   10.727    0.000    0.489    0.723
   .AUA4              0.545    0.057    9.527    0.000    0.545    0.603
    AS                0.302    0.057    5.260    0.000    1.000    1.000
    AF                0.258    0.058    4.466    0.000    1.000    1.000
    MI                0.233    0.051    4.567    0.000    1.000    1.000
    AUA               0.304    0.069    4.402    0.000    1.000    1.000
\end{verbatim}

\hypertarget{interpreting-the-output-2}{%
\subsubsection{Interpreting the Output}\label{interpreting-the-output-2}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3780}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4512}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1707}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Our Results
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Criteria met?
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Factor loadings significant, strong, proper valence & AS: AS: .50 to .65; AF: .43 to .62; MI: .38 to .58; AUA: .54 to .63 & The cross-loaded item is really low (-0.20) \\
Non-significant chi-square & \(\chi ^{2} (202) = 223.332, p = 0.145\) & Yes \\
\(CFI\geq .95\) & CFI = 0.982 & Yes \\
\(RMSEA\leq .05\) (but definitely \textless{} .10) & RMSEA = 0.019, 90\%CI(0.000, 0.032) & Yes \\
\(SRMR\leq .08\) (but definitely \textless{} .10) & SRMR = 0.047 & Yes \\
Combination rule: \(CFI \geq .95\) and \(SRMR \leq .08\) & CFI = 0.982, SRMR = 0.047 & \\
Yes & & \\
\end{longtable}

We can formally test the difference in models:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{lavTestLRT}\NormalTok{(corrF, ModInd\_M1f)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Chi-Squared Difference Test

            Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff Pr(>Chisq)   
ModInd_M1f 202 16655 16845 223.33                                         
corrF      203 16662 16848 232.45     9.1206 0.16344       1   0.002527 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We see that the difference between ModInd\_M1f and corrF is statistically significant and that the AIC and BIC are very slightly lower (more favorable) for the respecified model. Because our fit indices were already strong, the correlated error is low, and it ``makes a mess'' of scoring and interpretation, we will not retain this model and I will not write it up. However, we can learn some things from it:

\begin{itemize}
\tightlist
\item
  The correlated error is statistically significant and negative.
\item
  As predicted, freeing one parameter improved model fit. The respecified model with the additional parameter is the \emph{nesting model.}
\item
  Just because there is statistical support for freeing a parameter, there must be strong rationale for doing so.
\end{itemize}

The plot below shows the added covariance between the errors for AF1 and AUA1.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(ModInd\_M1f, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{11-CFA_2ndOrder_files/figure-latex/unnamed-chunk-20-1.pdf}

\hypertarget{respecifying-with-crossloadings}{%
\subsection{Respecifying with Crossloadings}\label{respecifying-with-crossloadings}}

Another route to improving model fit is to allow items to load on more than one factor (i.e., crossload). Let's return to those original modification indices from the corrF specification.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{modindices}\NormalTok{(corrF, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{minimum.value =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    lhs op  rhs    mi    epc sepc.lv sepc.all sepc.nox
282 AF1 ~~ AUA1 8.812 -0.124  -0.124   -0.196   -0.196
324 MI2 ~~  MI3 8.166  0.135   0.135    0.206    0.206
275 AF1 ~~  AF3 7.785  0.115   0.115    0.195    0.195
69   AF =~  AS2 7.738 -0.435  -0.220   -0.218   -0.218
77   AF =~  MI1 7.221  0.517   0.261    0.313    0.313
120 AUA =~  MI5 6.740  0.537   0.297    0.337    0.337
95   MI =~  AF1 6.221 -0.539  -0.260   -0.290   -0.290
241 AS7 ~~  MI4 5.912  0.102   0.102    0.150    0.150
319 MI1 ~~  MI5 5.849 -0.092  -0.092   -0.166   -0.166
288 AF2 ~~  MI1 5.755  0.088   0.088    0.169    0.169
78   AF =~  MI2 5.748 -0.473  -0.239   -0.271   -0.271
193 AS4 ~~  MI4 5.479 -0.097  -0.097   -0.145   -0.145
289 AF2 ~~  MI2 5.191 -0.090  -0.090   -0.155   -0.155
163 AS3 ~~  AS5 5.059 -0.089  -0.089   -0.153   -0.153
124 AS1 ~~  AS5 5.050  0.085   0.085    0.156    0.156
168 AS3 ~~  AF1 4.910  0.082   0.082    0.143    0.143
112 AUA =~  AF1 4.850 -0.372  -0.206   -0.230   -0.230
113 AUA =~  AF2 4.735  0.416   0.230    0.237    0.237
278 AF1 ~~  MI2 4.601 -0.080  -0.080   -0.141   -0.141
63   AS =~  MI5 4.516  0.277   0.152    0.173    0.173
64   AS =~ AUA1 4.492 -0.349  -0.192   -0.188   -0.188
67   AS =~ AUA4 4.463  0.334   0.184    0.193    0.193
259 AS8 ~~ AUA3 4.441 -0.072  -0.072   -0.133   -0.133
182 AS4 ~~  AS6 4.380 -0.065  -0.065   -0.132   -0.132
92   MI =~  AS7 4.127  0.312   0.151    0.163    0.163
96   MI =~  AF2 4.072  0.494   0.238    0.245    0.245
\end{verbatim}

The highest ``factor to item'' (as opposed to ``item to item'') modification index is AF =\textasciitilde{} AS2 . If we draw a path from AF to AS2, the overall chi-square will be reduced by 7.738 points. Recall, a 1 degree of freedom change in a model will be a statistically significant difference if the chi-square value drops by 4 points, so we can expect this to make a statistically significant difference.

In CFA models, allowing an item to load on more than one factor suggests that the item has something in common with both factors. Theoretically, this could make a great deal of sense. on, though, this creates confusion about scoring measures and interpreting them.

The item in question, AS2 reads, ``Others have been surprised when I disagree with them.'' Presently, it is assigned to the Ascribed Submissiveness factor. The MI is suggesting that it also be assigned to the Asian Fetishism scale.

In the context of this instrument whose CFA properties are already strong, I find it difficult to justify allowing these errors to covary, but I want to demonstrate the technique. We respecify it by adding AF1 to the MI factor.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ModInd\_M2 }\OtherTok{\textless{}{-}} \StringTok{"AS =\textasciitilde{} AS1 + AS2 + AS3 + AS4 + AS5 + AS6 + AS7 + AS8 + AS9}
\StringTok{             AF =\textasciitilde{} AF1 + AF2 + AF3 + AF4 + AS2}
\StringTok{             MI =\textasciitilde{} MI1 + MI2 + MI3 + MI4 + MI5}
\StringTok{             AUA =\textasciitilde{} AUA1 + AUA2 + AUA3 + AUA4 }
\StringTok{            "}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{ModInd\_M2f }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(ModInd\_M2, }\AttributeTok{data =}\NormalTok{ dfGRMSAAW)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(ModInd\_M2f, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 45 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        51

  Number of observations                           304

Model Test User Model:
                                                      
  Test statistic                               224.190
  Degrees of freedom                               202
  P-value (Chi-square)                           0.136

Model Test Baseline Model:

  Test statistic                              1439.317
  Degrees of freedom                               231
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.982
  Tucker-Lewis Index (TLI)                       0.979

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -8276.884
  Loglikelihood unrestricted model (H1)      -8164.789
                                                      
  Akaike (AIC)                               16655.768
  Bayesian (BIC)                             16845.336
  Sample-size adjusted Bayesian (SABIC)      16683.590

Root Mean Square Error of Approximation:

  RMSEA                                          0.019
  90 Percent confidence interval - lower         0.000
  90 Percent confidence interval - upper         0.032
  P-value H_0: RMSEA <= 0.050                    1.000
  P-value H_0: RMSEA >= 0.080                    0.000

Standardized Root Mean Square Residual:

  SRMR                                           0.045

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  AS =~                                                                 
    AS1               1.000                               0.544    0.593
    AS2               1.437    0.194    7.424    0.000    0.781    0.774
    AS3               0.974    0.125    7.806    0.000    0.530    0.564
    AS4               0.913    0.121    7.514    0.000    0.496    0.536
    AS5               1.149    0.135    8.536    0.000    0.625    0.638
    AS6               0.675    0.095    7.124    0.000    0.367    0.501
    AS7               0.835    0.119    7.032    0.000    0.454    0.493
    AS8               0.913    0.121    7.542    0.000    0.496    0.539
    AS9               0.760    0.105    7.221    0.000    0.413    0.510
  AF =~                                                                 
    AF1               1.000                               0.501    0.559
    AF2               1.222    0.176    6.943    0.000    0.613    0.631
    AF3               0.752    0.138    5.456    0.000    0.377    0.426
    AF4               1.123    0.170    6.622    0.000    0.563    0.572
    AS2              -0.480    0.181   -2.648    0.008   -0.240   -0.238
  MI =~                                                                 
    MI1               1.000                               0.481    0.577
    MI2               0.916    0.148    6.200    0.000    0.441    0.500
    MI3               1.171    0.177    6.597    0.000    0.564    0.551
    MI4               0.924    0.157    5.872    0.000    0.445    0.463
    MI5               0.690    0.137    5.021    0.000    0.332    0.377
  AUA =~                                                                
    AUA1              1.000                               0.553    0.543
    AUA2              0.981    0.140    7.018    0.000    0.543    0.608
    AUA3              0.784    0.121    6.459    0.000    0.434    0.528
    AUA4              1.083    0.152    7.146    0.000    0.599    0.630

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  AS ~~                                                                 
    AF                0.156    0.031    5.092    0.000    0.572    0.572
    MI                0.140    0.028    4.997    0.000    0.534    0.534
    AUA               0.182    0.034    5.302    0.000    0.607    0.607
  AF ~~                                                                 
    MI                0.152    0.030    5.003    0.000    0.629    0.629
    AUA               0.165    0.034    4.847    0.000    0.597    0.597
  MI ~~                                                                 
    AUA               0.189    0.036    5.301    0.000    0.709    0.709

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .AS1               0.545    0.050   10.974    0.000    0.545    0.648
   .AS2               0.566    0.063    9.029    0.000    0.566    0.555
   .AS3               0.602    0.054   11.167    0.000    0.602    0.682
   .AS4               0.610    0.054   11.324    0.000    0.610    0.712
   .AS5               0.568    0.054   10.607    0.000    0.568    0.593
   .AS6               0.402    0.035   11.496    0.000    0.402    0.749
   .AS7               0.643    0.056   11.531    0.000    0.643    0.757
   .AS8               0.602    0.053   11.310    0.000    0.602    0.710
   .AS9               0.487    0.043   11.456    0.000    0.487    0.740
   .AF1               0.552    0.055   10.047    0.000    0.552    0.687
   .AF2               0.568    0.063    8.987    0.000    0.568    0.602
   .AF3               0.639    0.057   11.232    0.000    0.639    0.818
   .AF4               0.652    0.066    9.885    0.000    0.652    0.673
   .MI1               0.466    0.047    9.826    0.000    0.466    0.668
   .MI2               0.583    0.055   10.673    0.000    0.583    0.750
   .MI3               0.730    0.072   10.152    0.000    0.730    0.697
   .MI4               0.728    0.066   10.982    0.000    0.728    0.786
   .MI5               0.665    0.058   11.515    0.000    0.665    0.858
   .AUA1              0.730    0.069   10.536    0.000    0.730    0.705
   .AUA2              0.502    0.051    9.795    0.000    0.502    0.630
   .AUA3              0.487    0.046   10.679    0.000    0.487    0.721
   .AUA4              0.545    0.058    9.475    0.000    0.545    0.603
    AS                0.296    0.057    5.220    0.000    1.000    1.000
    AF                0.251    0.057    4.403    0.000    1.000    1.000
    MI                0.232    0.051    4.553    0.000    1.000    1.000
    AUA               0.306    0.070    4.394    0.000    1.000    1.000
\end{verbatim}

\hypertarget{interpreting-the-output-3}{%
\subsubsection{Interpreting the Output}\label{interpreting-the-output-3}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3780}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4512}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1707}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Our Results
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Criteria met?
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Factor loadings significant, strong, proper valence & AS:49. to .77; AF: -.24 to .63; MI: .38 to .58; AUA: .53 to .63 & No! When added to AF, AS1 is in the wrong direction \\
Non-significant chi-square & \(\chi ^{2}(202) = 224.190, p = 0.136\) & Yes \\
\(CFI\geq 0.982\) & CFI = 0.980 & Yes \\
\(RMSEA\leq .05\) (but definitely \textless{} .10) & RMSEA = 0.019, 90\%CI(0.000, 0.032) & Yes \\
\(SRMR\leq .08\) (but definitely \textless{} .10) & SRMR = 0.045 & Yes \\
Combination rule: \(CFI \geq .95\) and \(SRMR \leq .08\) & CFI = 0.982, SRMR = 0.045 & Yes \\
\end{longtable}

We can formally test the difference in models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{lavTestLRT}\NormalTok{(corrF, ModInd\_M2f)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Chi-Squared Difference Test

            Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff Pr(>Chisq)   
ModInd_M2f 202 16656 16845 224.19                                         
corrF      203 16662 16848 232.45     8.2622 0.15456       1   0.004048 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

AIC and BIC are able to compare nested and non-nested models. Models with the lower values are superior. They both favor the model that allows AF1 to crossload on MI.

The \(\chi_{D}^{2}\) can only be used for nested models (where items/indicators are identical -- the only difference is the presence/absence of parameters). If it is statistically significant, the better model is the one with the lower chi-square value. This, too, favors the correlated factors model, \(\chi ^{2}(1) = 8.262, p = 0.004\) .

Diagramming this model helps further clarify how we have specified this crossloading. It's maybe tough to see, but AF1 now has arrows pointing from the AF and MI factors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(ModInd\_M2f, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{11-CFA_2ndOrder_files/figure-latex/unnamed-chunk-25-1.pdf} After each step, we should look again for modification indices.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{modindices}\NormalTok{(ModInd\_M2f, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{minimum.value =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    lhs op  rhs    mi    epc sepc.lv sepc.all sepc.nox
282 AF1 ~~ AUA1 9.276 -0.127  -0.127   -0.200   -0.200
324 MI2 ~~  MI3 8.250  0.135   0.135    0.207    0.207
275 AF1 ~~  AF3 7.525  0.113   0.113    0.190    0.190
77   AF =~  MI1 7.359  0.520   0.261    0.312    0.312
120 AUA =~  MI5 6.739  0.538   0.298    0.338    0.338
78   AF =~  MI2 6.647 -0.506  -0.254   -0.288   -0.288
124 AS1 ~~  AS5 6.220  0.094   0.094    0.169    0.169
319 MI1 ~~  MI5 5.868 -0.092  -0.092   -0.166   -0.166
241 AS7 ~~  MI4 5.828  0.101   0.101    0.148    0.148
288 AF2 ~~  MI1 5.707  0.087   0.087    0.169    0.169
193 AS4 ~~  MI4 5.463 -0.096  -0.096   -0.145   -0.145
95   MI =~  AF1 5.441 -0.492  -0.237   -0.264   -0.264
65   AS =~ AUA1 4.882 -0.372  -0.202   -0.199   -0.199
289 AF2 ~~  MI2 4.780 -0.086  -0.086   -0.149   -0.149
112 AUA =~  AF1 4.739 -0.367  -0.203   -0.226   -0.226
64   AS =~  MI5 4.561  0.286   0.156    0.177    0.177
259 AS8 ~~ AUA3 4.480 -0.072  -0.072   -0.133   -0.133
168 AS3 ~~  AF1 4.472  0.079   0.079    0.136    0.136
68   AS =~ AUA4 4.463  0.342   0.186    0.195    0.195
163 AS3 ~~  AS5 4.382 -0.082  -0.082   -0.140   -0.140
182 AS4 ~~  AS6 4.229 -0.064  -0.064   -0.129   -0.129
278 AF1 ~~  MI2 4.045 -0.075  -0.075   -0.132   -0.132
\end{verbatim}

Not surprisingly, these values continue to be quite low, and I would not propose that we make any of the modifications (not even the ones I have just demonstrated).

\begin{figure}
\centering
\includegraphics{images/CFA2nd/FourFigs.png}
\caption{Side by side comparison of correlated, uncorrelated models, error covarying, and cross-loading models}
\end{figure}

Looking at the models side-by-side, we can continue to think about the nested-to-nesting continuum. The \emph{uncorrF} (upper left) model is \emph{nested} (fewer specified parameters, higher degrees of freedom) in the \emph{corrF} model (upper right). Our initial comparison was of these two models. We expected \emph{corrF} to have superior fit, and it did!

We then compared the \emph{corrF} model to the two models below. In these comparisons \emph{corrF} was nested in each of the lower models which had one parameter freed (the error covariance on the lower left; the cross-loading on the lower right). As is common, each of these nesting models (more parameters, fewer degrees of freedom) had better fit. However, because the additions were not theoretically justifiable (and the fit for \emph{corrF} was satisfatory), we did not retain these respecifications.

Think back to the don't break the ice analogy -- freeing all those parameters gets closer to the just-identified circumstance where all the relations in the sample covariance matrix are allowed to relate to each other (none are set to 0.0 or knocked out of the ice frame).

\includegraphics{images/CFA1st/breakice.jpg} Source: \url{https://www.flickr.com/photos/arfsb/4407495674}

\hypertarget{modeling-the-grmsaaw-as-a-second-order-structure}{%
\section{Modeling the GRMSAAW as a Second-Order Structure}\label{modeling-the-grmsaaw-as-a-second-order-structure}}

Another approach to model building is to explore alternative factor structures. Let's investigate a second-order model.

A \textbf{second-order model} represents the hypothesis that a second-order factor, \emph{g}, causes each of the identified \textbf{first-order factors}. Note that:

\begin{itemize}
\tightlist
\item
  The first-order factors have indicators, but the general factor has none; that is, the second-order factor is measured only indirectly through the indicators of the first-order factors.
\item
  The specification of \emph{g} as a common cause of the lower order factors implies that any additional association between the first-order factors is spurious.
\item
  There must be at least three first-order factors or their disturbance variances may be underidentified;

  \begin{itemize}
  \tightlist
  \item
    each first-order factor should have at least two indicators; more is better
  \end{itemize}
\item
  There are two options for scaling \emph{g}:

  \begin{itemize}
  \tightlist
  \item
    fixing the direct of effect of \emph{g} on one factor (usually the first or last) to 1.0; or
  \item
    fixing the variance of \emph{g} to 1.0 (standardizing it); this leaves all direct effects of \emph{g} on the first-order factors as free parameters.
  \end{itemize}
\end{itemize}

In our second-order model, we will add an the overall GRMS factor as our \emph{g} below the four existing factors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{secondM }\OtherTok{\textless{}{-}} \StringTok{"AS =\textasciitilde{} AS1 + AS2 + AS3 + AS4 + AS5 + AS6 + AS7 + AS8 + AS9}
\StringTok{             AF =\textasciitilde{} AF1 + AF2 + AF3 + AF4 }
\StringTok{             MI =\textasciitilde{} MI1 + MI2 + MI3 + MI4 + MI5}
\StringTok{             AUA =\textasciitilde{} AUA1 + AUA2 + AUA3 + AUA4}
\StringTok{             GRMS =\textasciitilde{} AS + AF + MI + AUA"}
\end{Highlighting}
\end{Shaded}

Next, we extract the results from the \emph{secondM} object with the \emph{lavaan::cfa()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{secondF }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(secondM, }\AttributeTok{data =}\NormalTok{ dfGRMSAAW)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(secondF, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rsquare =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 41 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        48

  Number of observations                           304

Model Test User Model:
                                                      
  Test statistic                               234.741
  Degrees of freedom                               205
  P-value (Chi-square)                           0.076

Model Test Baseline Model:

  Test statistic                              1439.317
  Degrees of freedom                               231
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.975
  Tucker-Lewis Index (TLI)                       0.972

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -8282.159
  Loglikelihood unrestricted model (H1)      -8164.789
                                                      
  Akaike (AIC)                               16660.319
  Bayesian (BIC)                             16838.736
  Sample-size adjusted Bayesian (SABIC)      16686.504

Root Mean Square Error of Approximation:

  RMSEA                                          0.022
  90 Percent confidence interval - lower         0.000
  90 Percent confidence interval - upper         0.034
  P-value H_0: RMSEA <= 0.050                    1.000
  P-value H_0: RMSEA >= 0.080                    0.000

Standardized Root Mean Square Residual:

  SRMR                                           0.047

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  AS =~                                                                 
    AS1               1.000                               0.549    0.598
    AS2               1.138    0.137    8.326    0.000    0.625    0.618
    AS3               0.959    0.124    7.740    0.000    0.526    0.560
    AS4               0.902    0.121    7.475    0.000    0.495    0.535
    AS5               1.154    0.134    8.589    0.000    0.633    0.647
    AS6               0.674    0.094    7.142    0.000    0.370    0.505
    AS7               0.833    0.118    7.043    0.000    0.457    0.496
    AS8               0.908    0.120    7.539    0.000    0.498    0.541
    AS9               0.759    0.105    7.242    0.000    0.417    0.514
  AF =~                                                                 
    AF1               1.000                               0.506    0.564
    AF2               1.193    0.174    6.847    0.000    0.603    0.621
    AF3               0.740    0.137    5.402    0.000    0.374    0.423
    AF4               1.134    0.171    6.644    0.000    0.573    0.582
  MI =~                                                                 
    MI1               1.000                               0.476    0.570
    MI2               0.937    0.151    6.197    0.000    0.446    0.505
    MI3               1.201    0.182    6.590    0.000    0.571    0.558
    MI4               0.928    0.160    5.798    0.000    0.442    0.459
    MI5               0.698    0.140    4.992    0.000    0.332    0.377
  AUA =~                                                                
    AUA1              1.000                               0.550    0.541
    AUA2              0.988    0.141    6.988    0.000    0.544    0.609
    AUA3              0.788    0.123    6.427    0.000    0.434    0.528
    AUA4              1.091    0.153    7.112    0.000    0.600    0.631
  GRMS =~                                                               
    AS                1.000                               0.683    0.683
    AF                1.001    0.184    5.450    0.000    0.742    0.742
    MI                1.033    0.183    5.657    0.000    0.814    0.814
    AUA               1.244    0.222    5.594    0.000    0.847    0.847

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .AS1               0.540    0.050   10.844    0.000    0.540    0.642
   .AS2               0.630    0.059   10.679    0.000    0.630    0.618
   .AS3               0.606    0.055   11.116    0.000    0.606    0.687
   .AS4               0.611    0.054   11.264    0.000    0.611    0.714
   .AS5               0.558    0.054   10.413    0.000    0.558    0.582
   .AS6               0.400    0.035   11.421    0.000    0.400    0.745
   .AS7               0.640    0.056   11.463    0.000    0.640    0.754
   .AS8               0.600    0.053   11.230    0.000    0.600    0.708
   .AS9               0.484    0.043   11.377    0.000    0.484    0.736
   .AF1               0.548    0.055    9.902    0.000    0.548    0.682
   .AF2               0.579    0.064    9.040    0.000    0.579    0.614
   .AF3               0.641    0.057   11.212    0.000    0.641    0.821
   .AF4               0.640    0.066    9.652    0.000    0.640    0.661
   .MI1               0.471    0.048    9.902    0.000    0.471    0.676
   .MI2               0.579    0.055   10.611    0.000    0.579    0.744
   .MI3               0.722    0.072   10.051    0.000    0.722    0.689
   .MI4               0.731    0.066   11.000    0.000    0.731    0.789
   .MI5               0.665    0.058   11.510    0.000    0.665    0.858
   .AUA1              0.733    0.070   10.552    0.000    0.733    0.708
   .AUA2              0.500    0.051    9.760    0.000    0.500    0.629
   .AUA3              0.488    0.046   10.671    0.000    0.488    0.722
   .AUA4              0.544    0.058    9.439    0.000    0.544    0.601
   .AS                0.160    0.036    4.499    0.000    0.533    0.533
   .AF                0.115    0.034    3.342    0.001    0.449    0.449
   .MI                0.076    0.028    2.750    0.006    0.337    0.337
   .AUA               0.085    0.034    2.505    0.012    0.282    0.282
    GRMS              0.140    0.036    3.877    0.000    1.000    1.000

R-Square:
                   Estimate
    AS1               0.358
    AS2               0.382
    AS3               0.313
    AS4               0.286
    AS5               0.418
    AS6               0.255
    AS7               0.246
    AS8               0.292
    AS9               0.264
    AF1               0.318
    AF2               0.386
    AF3               0.179
    AF4               0.339
    MI1               0.324
    MI2               0.256
    MI3               0.311
    MI4               0.211
    MI5               0.142
    AUA1              0.292
    AUA2              0.371
    AUA3              0.278
    AUA4              0.399
    AS                0.467
    AF                0.551
    MI                0.663
    AUA               0.718
\end{verbatim}

As we plot this model we expect to see a ``second level'' factor predicting each of the ``first order'' factors. The indicator was set on GRM --\textgreater{} AS. Each of the four factors predicts only the items associated with their factor with one item for each factor (the first on the left) specified as the indicator variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(secondF, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{11-CFA_2ndOrder_files/figure-latex/unnamed-chunk-29-1.pdf} Again, among my first steps are also to write the code to export the results. The \emph{tidySEM} package has useful functions to export the fit statistics, parameter estimates, and correlations among the latent variables (i.e., factors).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{secondFFitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(secondF)}
\NormalTok{secondF\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(secondF, }\AttributeTok{digits=}\DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\CommentTok{\#because of the second order, there are no correlations among the LVs to request }
\CommentTok{\#to see each of the tables, remove the hashtab}
\CommentTok{\#secondFFitStats}
\CommentTok{\#secondF\_paramEsts}
\end{Highlighting}
\end{Shaded}

Next, I export them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(secondFFitStats, }\AttributeTok{file =} \StringTok{"secondFFitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(secondF\_paramEsts, }\AttributeTok{file =} \StringTok{"secondF\_paramEsts.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{interpreting-the-output-4}{%
\subsection{Interpreting the Output}\label{interpreting-the-output-4}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3780}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4512}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1707}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Our Results
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Criteria met?
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Factor loadings significant, strong, proper valence & AS: .50 to .65; AF: .42 to .62; MI: .38 to .57; AUA: .53 to .63; GRMS: .68 to .85 & Yes \\
Non-significant chi-square & \(\chi ^{2}(205) = 234.741, p = 0.076\) & Yes \\
\(CFI\geq .95\) & CFI = 0.975 & Yes \\
\(RMSEA\leq .05\) (but definitely \textless{} .10) & RMSEA = 0.022, 90\%CI(0.000, 0.034) & Yes \\
\(SRMR\leq .08\) (but definitely \textless{} .10) & SRMR = 0.047 & Yes \\
Combination rule: \(CFI \geq .95\) and \(SRMR \leq .08\) & CFI = 0.975 , SRS = 0.047 & Yes \\
\end{longtable}

\hypertarget{partial-write-up-2}{%
\subsection{Partial Write-up}\label{partial-write-up-2}}

\begin{quote}
\textbf{Second-order factor model}. Our next model represented a second order structure. Specifically, four first-order factors loaded onto a second factor model and demonstrated adequate fit to the data: \(\chi ^{2}(205) = 234.741, p = 0.076, CFI = 0.975, RMSEA = 0.022, 90%CI(0.000, 0.034), SRMR = .047
\). Factor loadings ranged from .50 to .65 for the AS scale, .42 to .62 for the AF scale, .38 to .57 for the MI scale, .53 to .63 for the AUA scale, and .68 to .85 for the GRMS total scale.
\end{quote}

Determining if models are nested vs.~hierarchically arranged can be confusing, especially when it comes to adding in second-order structures. That is, replacing the six correlations (in the correlated factors model) with the second-order factor (fixing the first of the first-order factors to 1.0, so adding only 3 paths to be estimated) is not a clear fixing or freeing of paths. We need to know if they are so that we know if it is appropriate to apply/interpret the \(\chi_{D}^{2}\) difference test.

Luckily, the Muthen's (creators of Mplus) have a \href{http://www.statmodel.com/discussion/messages/9/344.html?1518742498}{discussion} post devoted to this and it appears that our correlated factors model is the nesting model for the second-order structure. If there is a statistically significant difference in models, then the correlated factors model is superior.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{lavTestLRT}\NormalTok{(uncorrF, corrF, secondF)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Chi-Squared Difference Test

         Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff          Pr(>Chisq)
corrF   203 16662 16848 232.45                                               
secondF 205 16660 16839 234.74      2.289 0.02178       2              0.3185
uncorrF 209 16879 17042 461.10    226.361 0.42762       4 <0.0000000000000002
           
corrF      
secondF    
uncorrF ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Although there is not a statistically significant chi-square difference (\(\chi^{2}(2) = .37923, p = .827\) between the correlated factors and second order structure, the AIC and BIC favor (by a very small amount) the second-order model.

If our model fit was poor, we would want to inspect the modification indices and see if it would be justifiable to allow error covariances.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{modindices}\NormalTok{(secondF, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{minimum.value =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     lhs op  rhs    mi    epc sepc.lv sepc.all sepc.nox
303  AF1 ~~ AUA1 9.880 -0.131  -0.131   -0.207   -0.207
137 GRMS =~  MI5 9.635  1.388   0.520    0.591    0.591
76    AF =~  MI1 8.672  0.519   0.262    0.314    0.314
296  AF1 ~~  AF3 7.683  0.115   0.115    0.194    0.194
345  MI2 ~~  MI3 7.464  0.129   0.129    0.200    0.200
68    AF =~  AS2 7.396 -0.410  -0.207   -0.205   -0.205
119  AUA =~  MI5 6.946  0.525   0.289    0.328    0.328
129 GRMS =~  AF1 6.925 -0.953  -0.357   -0.399   -0.399
130 GRMS =~  AF2 6.303  1.048   0.393    0.405    0.405
111  AUA =~  AF1 6.125 -0.450  -0.248   -0.277   -0.277
309  AF2 ~~  MI1 5.928  0.088   0.088    0.169    0.169
262  AS7 ~~  MI4 5.764  0.101   0.101    0.148    0.148
214  AS4 ~~  MI4 5.745 -0.099  -0.099   -0.149   -0.149
121 GRMS =~  AS2 5.427 -0.614  -0.230   -0.228   -0.228
340  MI1 ~~  MI5 5.391 -0.089  -0.089   -0.158   -0.158
145  AS1 ~~  AS5 5.270  0.087   0.087    0.159    0.159
94    MI =~  AF1 5.171 -0.467  -0.222   -0.248   -0.248
189  AS3 ~~  AF1 5.066  0.084   0.084    0.145    0.145
184  AS3 ~~  AS5 4.864 -0.087  -0.087   -0.150   -0.150
310  AF2 ~~  MI2 4.801 -0.086  -0.086   -0.148   -0.148
77    AF =~  MI2 4.730 -0.400  -0.202   -0.230   -0.230
66    AS =~ AUA4 4.512  0.316   0.173    0.182    0.182
203  AS4 ~~  AS6 4.441 -0.066  -0.066   -0.133   -0.133
280  AS8 ~~ AUA3 4.375 -0.071  -0.071   -0.131   -0.131
95    MI =~  AF2 4.349  0.479   0.228    0.235    0.235
299  AF1 ~~  MI2 4.174 -0.075  -0.075   -0.134   -0.134
69    AF =~  AS3 4.137  0.294   0.148    0.158    0.158
80    AF =~  MI5 4.006  0.367   0.185    0.211    0.211
\end{verbatim}

The same AF1 \textasciitilde\textasciitilde{} AUA1 relationship is showing as the item that has a larger modification index than the others. As we saw earlier, freeing it to covary would improve the fit. However, our more parsimonious models (correlated factors, second-order) have excellent fit. Therefore, we will not respecify at this time.

\hypertarget{modeling-the-grmsaaw-as-a-bifactor-model}{%
\section{Modeling the GRMSAAW as a Bifactor Model}\label{modeling-the-grmsaaw-as-a-bifactor-model}}

\textbf{Bifactor models} are also known as \textbf{nested-factor} and \textbf{general-specific} models. Like the second-order model, they involve several specific, correlated constructs that make up a more general construct of interest. The big difference:

\begin{itemize}
\tightlist
\item
  \emph{g} in the bifactor model directly affects the indicators but is \emph{orthogonal}/unrelated to the specific factors
\item
  bifactor models where \emph{g} covaries with the specific factors may not be identified
\item
  bifactor models partition variance into three nonoverlapping sources:

  \begin{itemize}
  \tightlist
  \item
    specific factors
  \item
    the general factor (\emph{g})
  \item
    error
  \end{itemize}
\end{itemize}

Second-order and bifactor models make very different assumptions about whether \emph{g} is unrelated to the other factors (bifactor model) or covaries with/mediates those other factors (second-order model).

Take note that the base factor structure for the bifactor model is identical to the second-order structure. The difference is that the script fixes the relations between \emph{g} and each of the factors to 0.0; and the relations between each of the factors to each other as 0.0.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bifacM }\OtherTok{\textless{}{-}} \StringTok{" GRMS =\textasciitilde{} AS1 + AS2 + AS3 + AS4 + AS5 + AS6 + AS7 + AS8 + AS9 + AF1 + AF2 + AF3 + AF4 + MI1 + MI2 + MI3 + MI4 + MI5 + AUA1 + AUA2 + AUA3 + AUA4}

\StringTok{             AS =\textasciitilde{} AS1 + AS2 + AS3 + AS4 + AS5 + AS6 + AS7 + AS8 + AS9}
\StringTok{             AF =\textasciitilde{} AF1 + AF2 + AF3 + AF4 }
\StringTok{             MI =\textasciitilde{} MI1 + MI2 + MI3 + MI4 + MI5}
\StringTok{             AUA =\textasciitilde{} AUA1 + AUA2 + AUA3 + AUA4}
\StringTok{             }
\StringTok{            \#fixes the relations between g and each of the factors to 0.0 }
\StringTok{            GRMS \textasciitilde{}\textasciitilde{} 0*AS}
\StringTok{            GRMS \textasciitilde{}\textasciitilde{} 0*AF}
\StringTok{            GRMS \textasciitilde{}\textasciitilde{} 0*MI}
\StringTok{            GRMS \textasciitilde{}\textasciitilde{} 0*AUA}
\StringTok{            }
\StringTok{            \#fixes the relations (covariances) between each of the factors to 0.0}
\StringTok{            AS \textasciitilde{}\textasciitilde{} 0*AF}
\StringTok{            AS \textasciitilde{}\textasciitilde{} 0*MI}
\StringTok{            AS \textasciitilde{}\textasciitilde{} 0*AUA}
\StringTok{            AF \textasciitilde{}\textasciitilde{} 0*MI}
\StringTok{            AF \textasciitilde{}\textasciitilde{} 0*AUA}
\StringTok{            MI \textasciitilde{}\textasciitilde{} 0*AUA}
\StringTok{"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# On the first run I received a warning; it is not uncommon to add}
\CommentTok{\# the statement \textquotesingle{}check.gradient=FALSE\textquotesingle{} to force a solution. Then it}
\CommentTok{\# is important to closely inspect the results to see if things look}
\CommentTok{\# ok. If you get really stuck it is possible to change optimizers}
\CommentTok{\# through control statements}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{bifacF }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(bifacM, }\AttributeTok{data =}\NormalTok{ dfGRMSAAW, }\AttributeTok{check.gradient =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(bifacF, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rsquare =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 100 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        66

  Number of observations                           304

Model Test User Model:
                                                      
  Test statistic                               191.208
  Degrees of freedom                               187
  P-value (Chi-square)                           0.401

Model Test Baseline Model:

  Test statistic                              1439.317
  Degrees of freedom                               231
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.997
  Tucker-Lewis Index (TLI)                       0.996

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -8260.393
  Loglikelihood unrestricted model (H1)      -8164.789
                                                      
  Akaike (AIC)                               16652.786
  Bayesian (BIC)                             16898.110
  Sample-size adjusted Bayesian (SABIC)      16688.791

Root Mean Square Error of Approximation:

  RMSEA                                          0.009
  90 Percent confidence interval - lower         0.000
  90 Percent confidence interval - upper         0.027
  P-value H_0: RMSEA <= 0.050                    1.000
  P-value H_0: RMSEA >= 0.080                    0.000

Standardized Root Mean Square Residual:

  SRMR                                           0.039

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  GRMS =~                                                               
    AS1               1.000                               0.363    0.396
    AS2               0.907    0.192    4.719    0.000    0.329    0.326
    AS3               1.089    0.209    5.204    0.000    0.395    0.421
    AS4               0.917    0.191    4.789    0.000    0.333    0.359
    AS5               1.231    0.219    5.616    0.000    0.447    0.456
    AS6               0.779    0.160    4.880    0.000    0.283    0.386
    AS7               1.077    0.213    5.060    0.000    0.391    0.424
    AS8               0.936    0.193    4.847    0.000    0.340    0.369
    AS9               0.650    0.157    4.149    0.000    0.236    0.291
    AF1               0.877    0.212    4.136    0.000    0.318    0.355
    AF2               1.432    0.278    5.149    0.000    0.520    0.535
    AF3               0.765    0.200    3.829    0.000    0.278    0.314
    AF4               1.215    0.256    4.744    0.000    0.441    0.448
    MI1               1.154    0.231    5.001    0.000    0.419    0.501
    MI2               0.884    0.211    4.199    0.000    0.321    0.364
    MI3               1.109    0.253    4.380    0.000    0.402    0.393
    MI4               0.956    0.228    4.196    0.000    0.347    0.361
    MI5               0.992    0.220    4.504    0.000    0.360    0.409
    AUA1              1.211    0.264    4.580    0.000    0.439    0.432
    AUA2              1.235    0.250    4.947    0.000    0.448    0.502
    AUA3              0.979    0.213    4.596    0.000    0.355    0.432
    AUA4              1.509    0.288    5.240    0.000    0.547    0.576
  AS =~                                                                 
    AS1               1.000                               0.416    0.453
    AS2               1.398    0.252    5.548    0.000    0.581    0.575
    AS3               0.815    0.183    4.464    0.000    0.339    0.360
    AS4               0.899    0.191    4.718    0.000    0.374    0.404
    AS5               1.056    0.203    5.201    0.000    0.439    0.448
    AS6               0.571    0.140    4.080    0.000    0.238    0.324
    AS7               0.588    0.167    3.520    0.000    0.245    0.265
    AS8               0.861    0.187    4.617    0.000    0.358    0.389
    AS9               0.867    0.177    4.892    0.000    0.360    0.444
  AF =~                                                                 
    AF1               1.000                               0.620    0.691
    AF2               0.342    0.162    2.108    0.035    0.212    0.218
    AF3               0.446    0.196    2.270    0.023    0.276    0.312
    AF4               0.474    0.207    2.293    0.022    0.294    0.298
  MI =~                                                                 
    MI1               1.000                               0.190    0.227
    MI2               1.874    0.766    2.446    0.014    0.355    0.403
    MI3               2.959    1.340    2.208    0.027    0.561    0.548
    MI4               1.326    0.610    2.174    0.030    0.251    0.261
    MI5              -0.043    0.401   -0.107    0.915   -0.008   -0.009
  AUA =~                                                                
    AUA1              1.000                               0.419    0.412
    AUA2              0.794    0.370    2.147    0.032    0.333    0.373
    AUA3              0.590    0.275    2.145    0.032    0.247    0.301
    AUA4              0.444    0.245    1.813    0.070    0.186    0.196

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  GRMS ~~                                                               
    AS                0.000                               0.000    0.000
    AF                0.000                               0.000    0.000
    MI                0.000                               0.000    0.000
    AUA               0.000                               0.000    0.000
  AS ~~                                                                 
    AF                0.000                               0.000    0.000
    MI                0.000                               0.000    0.000
    AUA               0.000                               0.000    0.000
  AF ~~                                                                 
    MI                0.000                               0.000    0.000
    AUA               0.000                               0.000    0.000
  MI ~~                                                                 
    AUA               0.000                               0.000    0.000

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .AS1               0.536    0.050   10.644    0.000    0.536    0.638
   .AS2               0.574    0.064    8.986    0.000    0.574    0.563
   .AS3               0.612    0.054   11.277    0.000    0.612    0.693
   .AS4               0.606    0.054   11.129    0.000    0.606    0.708
   .AS5               0.566    0.054   10.518    0.000    0.566    0.591
   .AS6               0.401    0.035   11.531    0.000    0.401    0.746
   .AS7               0.637    0.055   11.605    0.000    0.637    0.750
   .AS8               0.605    0.054   11.220    0.000    0.605    0.713
   .AS9               0.472    0.044   10.829    0.000    0.472    0.718
   .AF1               0.318    0.156    2.041    0.041    0.318    0.396
   .AF2               0.628    0.059   10.673    0.000    0.628    0.666
   .AF3               0.628    0.060   10.552    0.000    0.628    0.804
   .AF4               0.688    0.067   10.333    0.000    0.688    0.710
   .MI1               0.486    0.045   10.741    0.000    0.486    0.697
   .MI2               0.548    0.067    8.199    0.000    0.548    0.705
   .MI3               0.572    0.132    4.333    0.000    0.572    0.546
   .MI4               0.742    0.067   11.024    0.000    0.742    0.802
   .MI5               0.646    0.057   11.354    0.000    0.646    0.833
   .AUA1              0.667    0.097    6.904    0.000    0.667    0.644
   .AUA2              0.484    0.063    7.679    0.000    0.484    0.608
   .AUA3              0.488    0.049    9.917    0.000    0.488    0.723
   .AUA4              0.570    0.055   10.420    0.000    0.570    0.630
    GRMS              0.132    0.043    3.053    0.002    1.000    1.000
    AS                0.173    0.051    3.399    0.001    1.000    1.000
    AF                0.384    0.165    2.327    0.020    1.000    1.000
    MI                0.036    0.026    1.373    0.170    1.000    1.000
    AUA               0.176    0.100    1.752    0.080    1.000    1.000

R-Square:
                   Estimate
    AS1               0.362
    AS2               0.437
    AS3               0.307
    AS4               0.292
    AS5               0.409
    AS6               0.254
    AS7               0.250
    AS8               0.287
    AS9               0.282
    AF1               0.604
    AF2               0.334
    AF3               0.196
    AF4               0.290
    MI1               0.303
    MI2               0.295
    MI3               0.454
    MI4               0.198
    MI5               0.167
    AUA1              0.356
    AUA2              0.392
    AUA3              0.277
    AUA4              0.370
\end{verbatim}

Providing a traditional diagram of the bifactor model requires some extra steps. The default from semPlot's \emph{semPaths()} function produces this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(bifacF, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{11-CFA_2ndOrder_files/figure-latex/unnamed-chunk-36-1.pdf} While it is an accurate depiction, I was seeking the traditional illustration. I found some instructive discussion on Sacha Epskamp's \emph{semPlot} \href{https://github.com/SachaEpskamp/semPlot/issues/4}{repo} on GitHub.

We can think of the variables in our model as numbered. The items take the first numbers, followed by \emph{g}, and then each of the factors. We need to represent them in a matrix of 0s and numbers. Let's start by mapping them out. The top row is the factors (4), the second row is items (22), the bottom row is g (1)

{[}1, {]} 0 0 0 24 0 0 0 0 25 0 0 0 0 0 26 0 0 0 0 0 27 0 0 {[}2, {]} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 {[}3, {]} 0 0 0 0 0 0 0 0 0 0 0 0 28 0 0 0 0 0 0 0 0 0 0

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\AttributeTok{nrow =} \DecValTok{3}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{22}\NormalTok{)}
\NormalTok{m[}\DecValTok{1}\NormalTok{, ] }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
    \DecValTok{0}\NormalTok{, }\DecValTok{27}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{m[}\DecValTok{2}\NormalTok{, ] }\OtherTok{=} \DecValTok{1}\SpecialCharTok{:}\DecValTok{22}
\NormalTok{m[}\DecValTok{3}\NormalTok{, ] }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
    \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{m}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]
[1,]    0    0    0    0   24    0    0    0    0     0    25     0     0     0
[2,]    1    2    3    4    5    6    7    8    9    10    11    12    13    14
[3,]    0    0    0    0    0    0    0    0    0     0     0    23     0     0
     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22]
[1,]     0    26     0     0     0     0    27     0
[2,]    15    16    17    18    19    20    21    22
[3,]     0     0     0     0     0     0     0     0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(bifacF, }\StringTok{"model"}\NormalTok{, }\StringTok{"std"}\NormalTok{, }\AttributeTok{layout =}\NormalTok{ m, }\AttributeTok{residuals =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{exoCov =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{11-CFA_2ndOrder_files/figure-latex/unnamed-chunk-38-1.pdf} Among my first steps are also to write the code to export the results. The \emph{tidySEM} package has useful functions to export the fit statistics, parameter estimates, and correlations among the latent variables (i.e., factors).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bifacFFitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(bifacF)}
\NormalTok{bifacF\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(bifacF, }\AttributeTok{digits=}\DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{bifacFCorrs }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_cors}\NormalTok{(bifacF, }\AttributeTok{digits=}\DecValTok{3}\NormalTok{)}
\CommentTok{\#to see each of the tables, remove the hashtab}
\CommentTok{\#bifacFFitStats}
\CommentTok{\#bifacF\_paramEsts}
\CommentTok{\#bifacFCorrs}
\end{Highlighting}
\end{Shaded}

Next, I export them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(bifacFFitStats, }\AttributeTok{file =} \StringTok{"bifacFFitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(bifacF\_paramEsts, }\AttributeTok{file =} \StringTok{"bifacF\_paramEsts.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(bifacFCorrs, }\AttributeTok{file =} \StringTok{"bifacFCorrs.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Troubleshooting tip}: If, while working with this function you get the error: \emph{``Error in file(file, ifelse(append,''a'', ``w'')) : cannot open the connection''}, it's because the .csv file that received your table is still open. R is just trying to write over it. A similar error happens when knitting.

\hypertarget{interpreting-the-output-5}{%
\subsection{Interpreting the Output}\label{interpreting-the-output-5}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3780}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4512}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1707}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Our Results
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Criteria met?
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Factor loadings significant, strong, proper valence & GRMS: .29 to .58; AS: .27 to .58; AF: .22 to .69; MI: -.01 to .55; AUA: .20 to .41 & \\
MI5 is near-zero and negative & & \\
Non-significant chi-square & \(\chi ^{2}(187) = 191.208, p = 0.401\) & Yes \\
\(CFI\geq .95\) & CFI = 0.997 & Yes \\
\(RMSEA\leq .05\) (but definitely \textless{} .10) & RMSEA = 0.009, 90\%CI(0.000, 0.027 & Yes \\
\(SRMR\leq .08\) (but definitely \textless{} .10) & SRMR = 0.039 & Yes \\
Combination rule: \(CFI \geq .95\) and \(SRMR \leq .08\) & CFI = 0.997, SRS = 0.039 & Yes \\
\end{longtable}

As promised, even in spite of the wiggly factor loadings, the model fit improves. This is another example of the nesting model generally having the best fit.

\hypertarget{partial-write-up-3}{%
\subsection{Partial Write-up}\label{partial-write-up-3}}

\begin{quote}
\textbf{Bifactor model}. The bifactor model regressed each item on its respective factor while simultaneously regressing each indicator onto an overall GRMS scale. This model had the best fit of those compared thus far: \(\chi ^{2}(187) = 164.080, p = .885\), CFI = 0.997 , RMSEA = 0.009, 90\%CI(0.000, 0.027), SRMR = 0.039. Factor loadings for the four factors ranged from .27 to .58 for the AS scale, .22 to .69 for the AF scale, -.01 to .55 for the MI scale, and .20 to .41 for the AUA scale. Factor loadings for the overall GRMSAAW (\emph{g}) ranged from .29 to .58.
\end{quote}

On the basis of this evaluation, we are finding all four models to be satisfactory (in terms of fit): the single-order uncorrelated factors (uncorrF), the single-order correlated factors model (corrF), the second order factor (secondF), and the bifactor model (bifacF). We can use \emph{lavaan's} \emph{lavTest()} function to compare them. No matter the order that we enter them, the function orders them according to their degrees of freedom.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{lavTestLRT}\NormalTok{(uncorrF, corrF, secondF, bifacF)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Chi-Squared Difference Test

         Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff            Pr(>Chisq)
bifacF  187 16653 16898 191.21                                                 
corrF   203 16662 16848 232.45     41.244 0.07204      16              0.000511
secondF 205 16660 16839 234.74      2.289 0.02178       2              0.318460
uncorrF 209 16879 17042 461.10    226.361 0.42762       4 < 0.00000000000000022
           
bifacF     
corrF   ***
secondF    
uncorrF ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We see that the bifacF and corrF models are statistically significantly different from each other. The AIC favors the bifacF; the BIC favors the corrF. We may be interested in knowing how it compares to the secondF model. The two models are statistically significantly different. The lower value of the AIC favors the bifactor model; the lower value of the BIC favors the second-order model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{lavTestLRT}\NormalTok{(secondF, bifacF)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Chi-Squared Difference Test

         Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(>Chisq)    
bifacF  187 16653 16898 191.21                                           
secondF 205 16660 16839 234.74     43.533 0.068309      18  0.0006725 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

In the article, Keum et al. \citeyearpar{keum_gendered_2018} reported the best fit for the bifactor model. They reported strong, significant, and properly valanced loadings for the \emph{g} factor as well as for each of the group factors. Our wiggly factor loadings on \emph{g} and the MI scale are likely an artifact of simulating the data from the EFA factor loadings.

\hypertarget{another-look-at-omega}{%
\section{Another Look at Omega}\label{another-look-at-omega}}

Now that we've had an introduction to CFA/SEM -- and the second-order and bifactor models in particular -- let's look again the \(\omega\) grouping of reliability estimates.

In prior lessons we used the \emph{psych} package's \emph{omegaSem()} function with raw data. The package estimated a family of model-based estimates that examine the correlations or covariances of the items and decomposed the test variance into that which is

\begin{itemize}
\tightlist
\item
  common to all items (\textbf{g}, a general factor),
\item
  specific to some items (\textbf{f}, orthogonal group factors), and
\item
  unique to each item (confounding \textbf{s} specific, and \textbf{e} error variance).
\end{itemize}

When using raw data or a correlation matrix as the object for the omega analysis, it is possible to specify the number of factors, but the procedure is \emph{exploratory} and there is no guarantee that the items will associate with the intended factor. When we are concerned with the omega reliability estimates for clearly specified factor structure we can feed our \emph{lavaan::cfa} models to \emph{semTools::reliability()} function and obtain the estimates.

\hypertarget{omega-h-for-bifactor-models}{%
\subsection{\texorpdfstring{Omega \emph{h} for Bifactor Models}{Omega h for Bifactor Models}}\label{omega-h-for-bifactor-models}}

In bifactor models the general factor captures the variance common across all items and the specific factors account for what is left over. Specific factors represent what is common across members of that factor, separate from what is claimed by \emph{g}.

In the context of a bifactor model, the reliability measure, \(\omega_{h}\), represents the proportion of total-score variance due to a single, general construct that influences all items, despite the multidimensional nature of the item set \citep{flora_supplemental_2020, flora_your_2020}.

Stated in terms of the GRMSAAW, \(\omega_{h}\) represents the extent to which the GRMSAAW total score provides a reliable measure of a construct represented by a general factor that influences all items in a multidimensional scale over and above the AS, AF, MI, and AUA subscales.

If we use the \emph{semTools::reliability()} function, we pass it the object we created from our \emph{lavaan::cfa()}. Flora's article and supplementary materials \citep{flora_supplemental_2020, flora_your_2020} provide an excellent description and review of how to specify and interpret \(\omega_{h}\) with \emph{semTools::reliability()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semTools}\SpecialCharTok{::}\FunctionTok{reliability}\NormalTok{(bifacF, }\AttributeTok{return.total =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            GRMS        AS        AF        MI       AUA     total
alpha  0.8485768 0.8015849 0.6327835 0.6130925 0.6665705 0.8485768
omega  0.8469630 0.6910784 0.4645562 0.3778777 0.3888254 0.8725861
omega2 0.7051557 0.4323840 0.2953065 0.2210012 0.2061386 0.8725861
omega3 0.7059886 0.4324453 0.2949845 0.2193985 0.2060547 0.8736167
avevar        NA        NA        NA        NA        NA 0.3285899
\end{verbatim}

In the case of the bifactor model, the estimates listed under the GRMS column pertain to the general GRMS factor. We typically focus on the omega2 and omega3 values as the indicators of \(\omega_{h}\). Flora \citeyearpar{flora_supplemental_2020} indicates that \emph{omega2} is calculated using the model-implied variance of the total score in its denominator and \emph{omega3} is calculated using the observed sample variance of \emph{X}. To the degree that these two values are different from each other, we may have concerns. In our data, omega2 = 0.705 and omega3 = 0.706. These values indicate the proportion of GRMS total-score variance that is due to a general factor, over-and-above, the influence of effects that are specific to the group factors (i.e., AS, AF, MI, AUA).

The omega2 and omega3 values in the AS through AUA columns are the \emph{omega-hierarchical-subscale}. These analyses indicate how well a given subscale reliably measures a narrower construct that is \emph{independent} from the broader higher-order construct that also influences the other subscales. Flora \citeyearpar{flora_your_2020} notates these as \(\omega_{h-ss}\) (omega-higherarchical-subscale). Specifically, \(\omega_{h-ss}\) represents the proportion of variance in a subscale that is due to the coresponding specific factor, over and above the influence of the general factor. Comparing the relative values to each other can provide some indication of the source of reliable variance. We see that the AS, AF, and AUA factors are considerably lower than the GRMS, however they are holding their own. Taken together, the omega values from our bifactor model suggest a strong general factor with subscales that are meaningful and useful. Our results paralleled the pattern reported in Keum et al. \citeyearpar{keum_gendered_2018}. In their follow-up investigation of construct validity, they used structural equation modeling with the \emph{bifactor} model to investigate the relationships between the GRMSAAW with relevant constructs.

In bifactor models, the multidimensionality of items (i.e., the existence of factors) is considered to be a ``nuisance'' \citep{flora_your_2020} for the measurement of a broad, general construct. This is different from hierarchical models such as the second-order factor structure. Since we can calculate \(\omega_{h}\) for it, let's look at it, next.

\hypertarget{omega_h-for-second-order-models}{%
\subsection{\texorpdfstring{\(\omega_{h}\) for Second Order Models}{\textbackslash omega\_\{h\} for Second Order Models}}\label{omega_h-for-second-order-models}}

In the second-order structure, the researcher hypothesizes that there is a broad, overarching construct indirectly influencing all items in a test through more conceptually narrow constructs that directly influence different groupings of items. This hypothesis implies that item-level data arise from a higher order model, in which the second-order factor, causes individual differences in the first-order factor, which directly influences the observed item responses.

In this case, \(\omega_{ho}\) (ho = higher order) \citep{flora_your_2020} represents the proportion of total-score variance that is due to the higher-order factor. As such, it represents the reliability of a total score for measuring a single construct that influences all items.

To use \emph{semTools} we switch functions to \emph{reliabilityL2}. The specification

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semTools}\SpecialCharTok{::}\FunctionTok{reliabilityL2}\NormalTok{(secondF, }\StringTok{"GRMS"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       omegaL1        omegaL2 partialOmegaL1 
     0.7038702      0.8547458      0.8420019 
\end{verbatim}

The estimate listed under \emph{omegaL1} (\(\omega_{ho}\)) represents the proportion of GRMSAAW total score variance due to the higher-order factor. The value of 0.704 is consistent with what we saw in the bifactor model.

We can apply the \emph{semTools::reliability()} function to the R object that holds the second-order results to obtain omega values for the subscales. Below the alpha coefficients, the omega values indicate how reliably each subscale measures its lower order factor. For example, 80\% of the total variance of a total score comprised of only the 9 items in the AS scale is explained by the AS lower order factor.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semTools}\SpecialCharTok{::}\FunctionTok{reliability}\NormalTok{(secondF)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              AS        AF        MI       AUA
alpha  0.8015849 0.6327835 0.6130925 0.6665705
omega  0.8045496 0.6370410 0.6184523 0.6666061
omega2 0.8045496 0.6370410 0.6184523 0.6666061
omega3 0.8052908 0.6352363 0.6193718 0.6637510
avevar 0.3194991 0.3111493 0.2500576 0.3361644
\end{verbatim}

Using the omega2 values, the subscale omegas range from .62 to .80. It is helpful to see how the omega values align with the alpha coefficients.

\hypertarget{partial-write-up-4}{%
\subsection{Partial Write-up}\label{partial-write-up-4}}

Given that we landed on the bifactor model as our final solution, here's how I might represent the omega results.

\begin{quote}
As estimates of model-based internal consistency associated with the bifactor model, we calculated omega hierarchical (\(\omega_{h}\)) and omega hierarchical subscale (\(\omega_{h-ss}\)). \(\omega_{h}\), represents the proportion of total-score variance due to a single, general construct that influences all items, despite the multidimensional nature of the item set \citep{flora_supplemental_2020, flora_your_2020}. Our \(\omega_{h}\) value of 0.71 indicates that 71\% of the variance of the GRMS total scores are attributable to individual differences on the general factor. \(\omega_{h-ss}\) for the subscales ranged from 0.21 to 0.43. Taken together, the omega values from our bifactor model suggest a strong general factor with subscales that are meaningful and useful.
\end{quote}

\hypertarget{preparing-an-overall-apa-style-results-section}{%
\section{Preparing an Overall APA Style Results Section}\label{preparing-an-overall-apa-style-results-section}}

\begin{quote}
\textbf{Model testing}. To evaluate the models we, we used confirmatory factor analysis (CFA) in the R package, \emph{lavaan} (v.0.6-17) with maximum likelihood estimation. Our sample size was 304. We selected fit criteria for their capacity to assess different aspects of the statistical analysis. As is common among SEM researchers, we reported the Chi-square goodness of fit (\(\chi^2\)). This evaluates the discrepancy between the unrestricted sample matrix and the restricted covariance matrix. Although the associated \(p\) value indicates adequate fit when the value is non-significant, it is widely recognized that large sample size can result in a statistically significant p value \citep{byrne_structural_2016}. The comparative fit index (CFI) is an incremental index, comparing the hypothesized model to the independent/baseline model. Adequate fit is determined when CFI values are at least .90 and perhaps higher than .95 \citep{kline_principles_2016}. The root mean square error of approximation (RMSEA) takes into account the error of approximation in the population and expresses it per degree of freedom. As such, the fit indicator considers the complexity of the model. Ideal values are equal to or less than .05, values less than .08 represent reasonable fit, and values between .08 and .10 represent mediocre fit. The standardized root mean residual (SRMR) is a standardized measure of the mean absolute covariance residual -- the overall difference between the observed and predicted correlations. Values greater than .10 may indicate poor fit and inspection of residuals is then advised. Because we were interested in comparing nested models we used the Chi-square difference test where a significant chi-square indicates statistically significant differences in models. Additionally, we used Akaike's Information Criterion (AIC) and the Bayesian Information Criterion (BIC) that take model complexity and sample size into consideration. Models with lower values on each are considered to be superior. Kline \citeyearpar{kline_principles_2016} advised researchers to be cautious when using these criteria as strict cut-offs. Elements such as sample size and model complexity should be considered when evaluating fit. Table 1 provides a side-by-side comparison of the resulting parameter estimates and fit statistics; Figures 1 and 2 provide a graphic representation of the models tested.
\end{quote}

\begin{quote}
To assess the factor structure of the GRMSAAW we examined five separate models: a unidimensional model, an uncorrelated factors model, a correlated factors model, a second-order model, and a bifactor models. Support for a unidimensional model would suggest that the model is best represented by a total scale score with no subfactors. Support for an uncorrelated factors model would suggest that the factors are largely independent. Support for a correlated factors model would suggest that the factors are related. Support for a second-order GRMS factor would suggest that the AS, AF, MI, and AUA subfactors represent facets of the higher order factor, GRMS. In the bifactor models, items for each scale are loaded onto both their respective subscale and the overall GRMS scale (\emph{g}). Support for this model would suggest that each subscale has both independent variance, and common variance that belongs to an underlying GRMS factor. When a bifactor model is the best representation of fit to the data, researchers can utilize bifactor indices to determine the proportion of variance accounted for by the subscales and the general factor, respectively.
\end{quote}

\begin{quote}
Our first model was unidimensional where each of the 24 items loaded onto a single factor representing overall, gendered racial microaggressions towards Asian American women. The Chi-square index was statistically significant (\(\chi ^{2}(209) = 444.451, p < .001\)) indicating likely misfit. The CFI value of .81 indicated poor fit. In contrast, the RMSEA = 0.061, 90\% CI(0.053, 0.069) and SRMR = 0.067 both fell within the ranges of acceptability. Our second model was an uncorrelated factors model. It demonstrated less than adequate fit to the data: \(\chi ^{2}(209) = 461.102, p < 0.001, CFI = 0.791, RMSEA = 0.063, 90%CI(0.055, 0.071), SRMR = 0.151
\). Our third model was a single-order, correlated factors where each of the 22 items loaded onto one of four factors and the factors were free to correlate. The Chi-square index was not statistically signficant (\(\chi ^{2}(203)=232.453, p = 0.076\)) indicating reasonable fit. The CFI value of 0.972 exceeded the recommendation of .95. The RMSEA = 0.022 (90\%CI{[}.000, 0.034{]}) was satisfactory. The SRMR value of 0.047 remained below the warning criteria of .10. Our fourth model represented a second order structure. Specifically, four first-order factors loaded onto a second factor model and demonstrated adequate fit to the data: \(\chi ^{2}(205) = 234.741, p = 0.076, CFI = 0.975, RMSEA = 0.022, 90%CI(0.000, 0.034), SRMR = .047
\). The fifth model, a bifactor model regressed each item on its respective factor while simultaneously regressing each indicator onto an overall GRMS scale. This model had the best fit of those compared thus far: \(\chi ^{2}(187) = 164.080, p = .885, CFI = 0.997 , RMSEA = 0.009, 90%CI(0.000, 0.027, SRMR = 0.039
\).
\end{quote}

\begin{quote}
As shown in our table of model comparisons, Chi-square difference tests between models showed statistically significant differences between the uncorrelated factors model and the correlated factors and second-order model, which do not differ from each other. Finally, there was a statistically significant difference between the second order factor model and the bifactor model (\(\chi ^{2}(18) = 43.533, p < .001\)). The CFI, RMSEA, SRMR, and AIC values favor the bifactor model; the BIC favored the second order model. Thus, all of the multidimensional models demonstrated adequate fit and are suitable for research and practice.
\end{quote}

\begin{quote}
As estimates of model-based internal consistency associated with the bifactor model, we calculated omega hierarchical (\(\omega_{h}\)) and omega hierarchical subscale (\(\omega_{h-ss}\)). \(\omega_{h}\), represents the proportion of total-score variance due to a single, general construct that influences all items, despite the multidimensional nature of the item set \citep{flora_supplemental_2020, flora_your_2020}. Our \(\omega_{h}\) value of 0.71 indicated that 71\% of the variance of the GRMS total scores are attributable to individual differences onthe general factor. \(\omega_{h-ss}\) for the subscales ranged from 0.21 to 0.43. Taken together, the omega values from our bifactor model suggest a strong general factor with subscales that are meaningful and useful.
\end{quote}

\hypertarget{a-conversation-with-dr.-keum}{%
\section{A Conversation with Dr.~Keum}\label{a-conversation-with-dr.-keum}}

Doctoral student Jadvir Gill (Industrial-Organizational Psychology) and I were able to interview the first author (Brian TaeHyuk Keum, PhD) about the article used for the research vignette \citep{keum_gendered_2018}. Here's a direct \href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=7dc9aaa5-2ca7-4398-b2aa-aef30126b169}{link} to that interview.

Among other things, we asked:

\begin{itemize}
\tightlist
\item
  What challenges did you encounter in the research process and recruiting participants?
\item
  Can you describe how the items (or subscales) captures intersectional microaggressions for the intended population?
\item
  How would you like to see the scale used in future science, practice, and advocacy?
\item
  How are you using this scale in your current and future research?
\end{itemize}

\hypertarget{practice-problems-9}{%
\section{Practice Problems}\label{practice-problems-9}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. In psychometrics, I strongly recommend that you have started with a dataset that has a minimum of three subscales and use it for all of the assignments in the OER. The suggestion for practice spans the \protect\hypertarget{CFA1st}{}{prior chapter} and this one. For this combination assignment, you should plan to:

\begin{itemize}
\tightlist
\item
  Prepare the data frame for CFA.
\item
  Specify and run unidimensional, single order (with correlated factors), second-order, and bifactor models.
\item
  Narrate the adequacy of fit with \(\chi ^{2}\), CFI, RMSEA, SRMR

  \begin{itemize}
  \tightlist
  \item
    Write a mini-results section for each
  \end{itemize}
\item
  Compare model fit with \(\chi ^{2}\Delta\), AIC, and BIC.
\item
  Calculate the appropriate omega reliability estimates for the \emph{g} and specific factors.
\item
  Write an APA style results sections with table(s) and figures.
\end{itemize}

\hypertarget{problem-1-play-around-with-this-simulation.-6}{%
\subsection{Problem \#1: Play around with this simulation.}\label{problem-1-play-around-with-this-simulation.-6}}

The least complex is to change the random seed in the research and rework the problem demonstrated in the lesson. The results \emph{should} map onto the ones obtained in the lecture.

\hypertarget{problem-2-use-simulated-data-from-other-lessons.-1}{%
\subsection{Problem \#2: Use simulated data from other lessons.}\label{problem-2-use-simulated-data-from-other-lessons.-1}}

The second option involves utilizing one of the simulated datasets available in this OER. The \protect\hyperlink{sims}{last lesson} in the OER contains three simulations that could be used for all of the statistics-based practice suggestions. Especially if you started with one of these examples in an earlier lesson, I highly recommend you continue with that.

Alternatively, Lewis and Neville's \citeyearpar{lewis_construction_2015} Gendered Racial Microaggressions Scale for Black Women was used in the lessons for exploratory factor analysis and Conover et al.'s \citeyearpar{conover_development_2017} Ableist Microaggressions Scale is used in the lesson on invariance testing. Both of these would be suitable for the CFA homework assignments.

\hypertarget{problem-3-try-something-entirely-new.-6}{%
\subsection{Problem \#3: Try something entirely new.}\label{problem-3-try-something-entirely-new.-6}}

As a third option, you are welcome to use data to which you have access and is suitable for CFA. This could include other simulated data, data found on an open access repository, data from the ReCentering Psych Stats survey described in the \protect\hyperlink{qualTRIX}{Qualtrics lesson}, or your own data (presuming you have permission to use it).

\hypertarget{grading-rubric-6}{%
\subsection{Grading Rubric}\label{grading-rubric-6}}

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5493}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1972}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Possible
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Earned
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1. Prepare data for CFA (items only df, reverse-scored) & 5 & \_\_\_\_\_ \\
2. Specify, run, and plot a unidimensional model & 5 & \_\_\_\_\_ \\
3. Narrate adequacy of fit with \(\chi ^{2}\), CFI, RMSEA, SRMR (write a mini-results section) & 5 & \_\_\_\_\_ \\
4. Specify, run, and plot a single-order model with correlated factors & 5 & \_\_\_\_\_ \\
5. Narrate adequacy of fit with \(\chi ^{2}\), CFI, RMSEA, SRMR (write a mini-results section) & 5 & \_\_\_\_\_ \\
6. Specify, run, and plot a second-order model & 5 & \_\_\_\_\_ \\
7. Narrate adequacy of fit with \(\chi ^{2}\), CFI, RMSEA, SRMR (write a mini-results section) & 5 & \_\_\_\_\_ \\
8. Specify, run, and plot a bifactor model & 5 & \_\_\_\_\_ \\
9. Narrate adequacy of fit with \(\chi ^{2}\), CFI, RMSEA, SRMR (write a mini-results section) & 5 & \_\_\_\_\_ \\
10. Compare model fit with \(\chi ^{2}\Delta\), AIC, BIC & 5 & \_\_\_\_\_ \\
11. Calculate omega hierarchical (\(\omega_{h}\)) and omega-hierarchical-subscales (\(\omega_{h-ss}\)) & 5 & \_\_\_\_\_ \\
12. APA style results with table(s) and figures & 5 & \_\_\_\_\_ \\
13. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 60 & \_\_\_\_\_ \\
\end{longtable}

\hypertarget{homeworked-example-6}{%
\section{Homeworked Example}\label{homeworked-example-6}}

\href{https://youtu.be/JEGhMqO4lWI}{Screencast Link}

For more information about the data used in this homeworked example, please refer to the description and codebook located at the end of the \href{https://lhbikos.github.io/ReCenterPsychStats/ReCintro.html\#introduction-to-the-data-set-used-for-homeworked-examples}{introduction} in first volume of ReCentering Psych Stats.

As a brief review, this data is part of an IRB-approved study, with consent to use in teaching demonstrations and to be made available to the general public via the open science framework. Hence, it is appropriate to use in this context. You will notice there are student- and teacher- IDs. These numbers are not actual student and teacher IDs, rather they were further re-identified so that they could not be connected to actual people.

Because this is an actual dataset, if you wish to work the problem along with me, you will need to download the \href{https://github.com/lhbikos/ReC_Psychometrics/blob/main/Worked_Examples/ReC.rds}{ReC.rds} data file from the Worked\_Examples folder in the ReC\_Psychometrics project on the GitHub.

The course evaluation items can be divided into three subscales:

\begin{itemize}
\tightlist
\item
  \textbf{Valued by the student} includes the items: ValObjectives, IncrUnderstanding, IncrInterest
\item
  \textbf{Traditional pedagogy} includes the items: ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation
\item
  \textbf{Socially responsive pedagogy} includes the items: InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration
\end{itemize}

In this Homeworked Example I will conduct all the analyses from the immediately prior (first order CFA models) and present CFA lessons. My hope is that the results will support my solution of three dimensions: valued-by-the-student, traditional pedagogy, socially responsive pedagogy. While the repetition of the prior lesson's homeworked example is somewhat redundant, I am hopeful that this code will provide a fairly complete set of code for someone who is analyzing their own data from the beginning.

\hypertarget{prepare-data-for-cfa-items-only-df-reverse-scored-1}{%
\subsection{Prepare data for CFA (items only df, reverse-scored)}\label{prepare-data-for-cfa-items-only-df-reverse-scored-1}}

We can upload the data from the .rds file. The file should be in the same folder as the .rmd file. I've named the df object that holds the data ``big.''

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{big }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"ReC.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For the demonstration of CFA models, I will create an items-only df.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{items }\OtherTok{\textless{}{-}}\NormalTok{ big }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(ValObjectives, IncrUnderstanding, IncrInterest, ClearResponsibilities,}
\NormalTok{        EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation,}
\NormalTok{        MultPerspectives, InclusvClassrm, DEIintegration, EquitableEval)}
\end{Highlighting}
\end{Shaded}

Let's quickly check the structure. The variables should be numeric or integer.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(items)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Classes 'data.table' and 'data.frame':  310 obs. of  12 variables:
 $ ValObjectives        : int  5 5 4 4 5 5 5 5 4 5 ...
 $ IncrUnderstanding    : int  2 3 4 3 4 4 5 2 4 5 ...
 $ IncrInterest         : int  5 3 4 2 4 3 5 3 2 5 ...
 $ ClearResponsibilities: int  5 5 4 4 5 4 5 4 4 5 ...
 $ EffectiveAnswers     : int  5 3 5 3 5 3 4 3 2 3 ...
 $ Feedback             : int  5 3 4 2 5 NA 5 4 4 5 ...
 $ ClearOrganization    : int  3 4 3 4 4 4 5 4 4 5 ...
 $ ClearPresentation    : int  4 4 4 2 5 3 4 4 4 5 ...
 $ MultPerspectives     : int  5 5 4 5 5 4 5 5 5 5 ...
 $ InclusvClassrm       : int  5 5 5 5 5 4 5 5 4 5 ...
 $ DEIintegration       : int  5 5 5 5 5 4 5 5 5 5 ...
 $ EquitableEval        : int  5 5 3 5 5 3 5 5 3 5 ...
 - attr(*, ".internal.selfref")=<externalptr> 
\end{verbatim}

\hypertarget{specify-and-run-a-unidimensional-model-1}{%
\subsection{Specify and run a unidimensional model}\label{specify-and-run-a-unidimensional-model-1}}

First we map the relations we want to analyze.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{uniD }\OtherTok{\textless{}{-}} \StringTok{"CourseEvals =\textasciitilde{} ValObjectives + IncrUnderstanding + IncrInterest + ClearResponsibilities + EffectiveAnswers + Feedback + ClearOrganization + ClearPresentation + MultPerspectives + InclusvClassrm + DEIintegration + EquitableEval"}
\end{Highlighting}
\end{Shaded}

We analyze the relations by naming that object in our \emph{lavaan} code.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{uniDfit }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(uniD, }\AttributeTok{data =}\NormalTok{ items)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(uniDfit, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rsquare =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 32 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        24

                                                  Used       Total
  Number of observations                           267         310

Model Test User Model:
                                                      
  Test statistic                               344.973
  Degrees of freedom                                54
  P-value (Chi-square)                           0.000

Model Test Baseline Model:

  Test statistic                              1940.157
  Degrees of freedom                                66
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.845
  Tucker-Lewis Index (TLI)                       0.810

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -3038.064
  Loglikelihood unrestricted model (H1)      -2865.578
                                                      
  Akaike (AIC)                                6124.129
  Bayesian (BIC)                              6210.223
  Sample-size adjusted Bayesian (SABIC)       6134.129

Root Mean Square Error of Approximation:

  RMSEA                                          0.142
  90 Percent confidence interval - lower         0.128
  90 Percent confidence interval - upper         0.157
  P-value H_0: RMSEA <= 0.050                    0.000
  P-value H_0: RMSEA >= 0.080                    1.000

Standardized Root Mean Square Residual:

  SRMR                                           0.074

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  CourseEvals =~                                                        
    ValObjectives     1.000                               0.309    0.515
    IncrUndrstndng    1.715    0.223    7.684    0.000    0.530    0.642
    IncrInterest      2.142    0.269    7.971    0.000    0.662    0.685
    ClearRspnsblts    2.065    0.239    8.652    0.000    0.638    0.808
    EffectivAnswrs    2.105    0.244    8.617    0.000    0.650    0.800
    Feedback          2.143    0.259    8.285    0.000    0.662    0.738
    ClearOrganiztn    2.678    0.314    8.516    0.000    0.828    0.780
    ClearPresenttn    2.521    0.285    8.832    0.000    0.779    0.846
    MultPerspectvs    2.067    0.246    8.392    0.000    0.639    0.757
    InclusvClassrm    1.246    0.170    7.324    0.000    0.385    0.592
    DEIintegration    1.015    0.174    5.820    0.000    0.314    0.424
    EquitableEval     1.435    0.179    8.027    0.000    0.443    0.694

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ValObjectives     0.265    0.024   11.254    0.000    0.265    0.735
   .IncrUndrstndng    0.401    0.037   10.970    0.000    0.401    0.588
   .IncrInterest      0.494    0.046   10.815    0.000    0.494    0.530
   .ClearRspnsblts    0.217    0.022    9.983    0.000    0.217    0.348
   .EffectivAnswrs    0.237    0.024   10.060    0.000    0.237    0.359
   .Feedback          0.367    0.035   10.555    0.000    0.367    0.455
   .ClearOrganiztn    0.439    0.043   10.250    0.000    0.439    0.391
   .ClearPresenttn    0.242    0.026    9.446    0.000    0.242    0.285
   .MultPerspectvs    0.304    0.029   10.431    0.000    0.304    0.427
   .InclusvClassrm    0.275    0.025   11.104    0.000    0.275    0.649
   .DEIintegration    0.449    0.040   11.372    0.000    0.449    0.820
   .EquitableEval     0.211    0.020   10.777    0.000    0.211    0.518
    CourseEvals       0.096    0.022    4.381    0.000    1.000    1.000

R-Square:
                   Estimate
    ValObjectives     0.265
    IncrUndrstndng    0.412
    IncrInterest      0.470
    ClearRspnsblts    0.652
    EffectivAnswrs    0.641
    Feedback          0.545
    ClearOrganiztn    0.609
    ClearPresenttn    0.715
    MultPerspectvs    0.573
    InclusvClassrm    0.351
    DEIintegration    0.180
    EquitableEval     0.482
\end{verbatim}

Let's plot the results to see if the figure resembles what we intended to specify.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(uniDfit, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{11-CFA_2ndOrder_files/figure-latex/unnamed-chunk-100-1.pdf}

\hypertarget{narrate-adequacy-of-fit-with-chi-2-cfi-rmsea-srmr-write-a-mini-results-section-1}{%
\subsection{\texorpdfstring{Narrate adequacy of fit with \(\chi ^{2}\), CFI, RMSEA, SRMR (write a mini-results section)}{Narrate adequacy of fit with \textbackslash chi \^{}\{2\}, CFI, RMSEA, SRMR (write a mini-results section)}}\label{narrate-adequacy-of-fit-with-chi-2-cfi-rmsea-srmr-write-a-mini-results-section-1}}

\begin{quote}
\textbf{Model testing}. To evaluate the models we, we used confirmatory factor analysis (CFA) in the R package, lavaan (v.0-6.17) with maximum likelihood estimation. Our sample size was 267 We selected fit criteria for their capacity to assess different aspects of the statistical analysis. As is common among SEM researchers, we reported the Chi-square goodness of fit (χ2). This evaluates the discrepancy between the unrestricted sample matrix and the restricted covariance matrix. Although the associated \emph{p}-value indicates adequate fit when the value is non-significant, it is widely recognized that a large sample size can result in a statistically significant \emph{p} value (Byrne, 2016b). The comparative fit index (CFI) is an incremental index, comparing the hypothesized model to the independent/baseline model. Adequate fit is determined when CFI values are at least .90 and perhaps higher than .95 \citep{kline_principles_2016}. The root mean square error of approximation (RMSEA) takes into account the error of approximation in the population and expresses it per degree of freedom. As such, the fit indicator considers the complexity of the model. Ideal values are equal to or less than .05, values less than .08 represent reasonable fit, and values between .08 and .10 represent mediocre fit. The standardized root mean residual (SRMR) is a standardized measure of the mean absolute covariance residual -- the overall difference between the observed and predicted correlations. Values greater than .10 may indicate poor fit and inspection of residuals is then advised. Kline (2016) advised researchers to be cautious when using these criteria as strict cut-offs. Elements such as sample size and model complexity should be considered when evaluating fit.
\end{quote}

\begin{quote}
Our first model was unidimensional where each of the 12 items loaded onto a single factor representing overall course evaluations. The Chi-square index was statistically significant \((\chi^2(54)=344.97, p<.001)\) indicating likely misfit. The CFI value of .85 indicated poor fit. The RMSEA = .14 (90\% CI {[}.13, .16{]}) suggested serious problems. The SRMR value of .07 was below the warning criteria of .10. The AIC and BIC values were 6124.13 and 6134.13, respectively, and will become useful in comparing subsequent models.
\end{quote}

\hypertarget{specify-and-run-a-single-order-model-with-correlated-factors-1}{%
\subsection{Specify and run a single-order model with correlated factors}\label{specify-and-run-a-single-order-model-with-correlated-factors-1}}

First we map the relations we want to analyze.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corrF }\OtherTok{\textless{}{-}} \StringTok{"TradPed =\textasciitilde{} ClearResponsibilities + EffectiveAnswers + Feedback + ClearOrganization + ClearPresentation  }
\StringTok{             Valued =\textasciitilde{} ValObjectives + IncrUnderstanding + IncrInterest }
\StringTok{             SCRPed =\textasciitilde{} MultPerspectives + InclusvClassrm + DEIintegration + EquitableEval}

\StringTok{  TradPed\textasciitilde{}\textasciitilde{}Valued}
\StringTok{  TradPed\textasciitilde{}\textasciitilde{}SCRPed}
\StringTok{  Valued\textasciitilde{}\textasciitilde{}SCRPed}
\StringTok{"}
\end{Highlighting}
\end{Shaded}

Next we run the analysis.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{corrF\_fit }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(corrF, }\AttributeTok{data =}\NormalTok{ items)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(corrF\_fit, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rsquare =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 42 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        27

                                                  Used       Total
  Number of observations                           267         310

Model Test User Model:
                                                      
  Test statistic                               224.795
  Degrees of freedom                                51
  P-value (Chi-square)                           0.000

Model Test Baseline Model:

  Test statistic                              1940.157
  Degrees of freedom                                66
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.907
  Tucker-Lewis Index (TLI)                       0.880

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -2977.975
  Loglikelihood unrestricted model (H1)      -2865.578
                                                      
  Akaike (AIC)                                6009.951
  Bayesian (BIC)                              6106.807
  Sample-size adjusted Bayesian (SABIC)       6021.201

Root Mean Square Error of Approximation:

  RMSEA                                          0.113
  90 Percent confidence interval - lower         0.098
  90 Percent confidence interval - upper         0.128
  P-value H_0: RMSEA <= 0.050                    0.000
  P-value H_0: RMSEA >= 0.080                    1.000

Standardized Root Mean Square Residual:

  SRMR                                           0.061

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed =~                                                            
    ClearRspnsblts    1.000                               0.652    0.826
    EffectivAnswrs    1.015    0.065   15.606    0.000    0.662    0.815
    Feedback          1.010    0.075   13.481    0.000    0.659    0.735
    ClearOrganiztn    1.295    0.086   15.106    0.000    0.845    0.797
    ClearPresenttn    1.204    0.072   16.680    0.000    0.785    0.853
  Valued =~                                                             
    ValObjectives     1.000                               0.334    0.557
    IncrUndrstndng    1.942    0.223    8.717    0.000    0.649    0.786
    IncrInterest      2.438    0.273    8.932    0.000    0.815    0.844
  SCRPed =~                                                             
    MultPerspectvs    1.000                               0.713    0.846
    InclusvClassrm    0.622    0.053   11.672    0.000    0.444    0.682
    DEIintegration    0.589    0.063    9.365    0.000    0.420    0.567
    EquitableEval     0.642    0.052   12.410    0.000    0.458    0.717

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed ~~                                                            
    Valued            0.171    0.026    6.640    0.000    0.785    0.785
    SCRPed            0.391    0.045    8.677    0.000    0.841    0.841
  Valued ~~                                                             
    SCRPed            0.164    0.026    6.254    0.000    0.688    0.688

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClearRspnsblts    0.199    0.021    9.456    0.000    0.199    0.319
   .EffectivAnswrs    0.222    0.023    9.618    0.000    0.222    0.336
   .Feedback          0.371    0.036   10.415    0.000    0.371    0.460
   .ClearOrganiztn    0.410    0.042    9.855    0.000    0.410    0.365
   .ClearPresenttn    0.232    0.026    8.939    0.000    0.232    0.273
   .ValObjectives     0.248    0.023   10.650    0.000    0.248    0.690
   .IncrUndrstndng    0.260    0.032    8.041    0.000    0.260    0.382
   .IncrInterest      0.268    0.043    6.308    0.000    0.268    0.288
   .MultPerspectvs    0.203    0.029    7.052    0.000    0.203    0.285
   .InclusvClassrm    0.226    0.023   10.028    0.000    0.226    0.534
   .DEIintegration    0.371    0.035   10.734    0.000    0.371    0.678
   .EquitableEval     0.198    0.020    9.685    0.000    0.198    0.486
    TradPed           0.426    0.053    8.085    0.000    1.000    1.000
    Valued            0.112    0.024    4.595    0.000    1.000    1.000
    SCRPed            0.509    0.063    8.039    0.000    1.000    1.000

R-Square:
                   Estimate
    ClearRspnsblts    0.681
    EffectivAnswrs    0.664
    Feedback          0.540
    ClearOrganiztn    0.635
    ClearPresenttn    0.727
    ValObjectives     0.310
    IncrUndrstndng    0.618
    IncrInterest      0.712
    MultPerspectvs    0.715
    InclusvClassrm    0.466
    DEIintegration    0.322
    EquitableEval     0.514
\end{verbatim}

Plotting the results. Does it look like what we intended to specify?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(corrF\_fit, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{11-CFA_2ndOrder_files/figure-latex/unnamed-chunk-103-1.pdf}

Code for saving the results as a .csv file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corrFitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(corrF\_fit)}
\NormalTok{corrF\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(corrF\_fit, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{corrFCorrs }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_cors}\NormalTok{(corrF\_fit, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\CommentTok{\# to see each of the tables, remove the hashtab corrFitStats}
\CommentTok{\# corrF\_paramEsts corrFCorrs}
\end{Highlighting}
\end{Shaded}

Next, I export them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(corrFitStats, }\AttributeTok{file =} \StringTok{"corrFitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(corrF\_paramEsts, }\AttributeTok{file =} \StringTok{"corrF\_paramEsts.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(corrFCorrs, }\AttributeTok{file =} \StringTok{"corrFCorrs.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{narrate-adequacy-of-fit-with-chi-2-cfi-rmsea-srmr-write-a-mini-results-section-2}{%
\subsection{\texorpdfstring{Narrate adequacy of fit with \(\chi ^{2}\), CFI, RMSEA, SRMR (write a mini-results section)}{Narrate adequacy of fit with \textbackslash chi \^{}\{2\}, CFI, RMSEA, SRMR (write a mini-results section)}}\label{narrate-adequacy-of-fit-with-chi-2-cfi-rmsea-srmr-write-a-mini-results-section-2}}

\begin{quote}
Our second model was a single-order, multidimensional model where each of the 12 items loaded onto one of four factors. Standardized pattern coefficients ranged between .74 and .85 on the TradPed factor, between .56 and .84 on the Valued factor, and between .57 and .85 on the SCRPed factor. The Chi-square index was statistically significant \((\chi^2(51) = 224.795, p < 0.001\) indicating some degree of misfit. The CFI value of .91 fell below the recommendation of .95. The RMSEA = .113 (90\% CI {[}.098, .128{]}) was higher than recommended. The SRMR value of .061 remained below the warning criteria of .10. The AIC and BIC values were 6009.95 and 6021.20, respectively.
\end{quote}

\hypertarget{specify-and-run-a-second-order-model}{%
\subsection{Specify and run a second-order model}\label{specify-and-run-a-second-order-model}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{secondM }\OtherTok{\textless{}{-}} \StringTok{"TradPed =\textasciitilde{} ClearResponsibilities + EffectiveAnswers + Feedback + ClearOrganization + ClearPresentation  }
\StringTok{             Valued =\textasciitilde{} ValObjectives + IncrUnderstanding + IncrInterest }
\StringTok{             SCRPed =\textasciitilde{} MultPerspectives + InclusvClassrm + DEIintegration + EquitableEval}
\StringTok{             Evals =\textasciitilde{} TradPed + Valued + SCRPed"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{secondF }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(secondM, }\AttributeTok{data =}\NormalTok{ items)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(secondF, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rsquare =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 37 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        27

                                                  Used       Total
  Number of observations                           267         310

Model Test User Model:
                                                      
  Test statistic                               224.795
  Degrees of freedom                                51
  P-value (Chi-square)                           0.000

Model Test Baseline Model:

  Test statistic                              1940.157
  Degrees of freedom                                66
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.907
  Tucker-Lewis Index (TLI)                       0.880

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -2977.975
  Loglikelihood unrestricted model (H1)      -2865.578
                                                      
  Akaike (AIC)                                6009.951
  Bayesian (BIC)                              6106.807
  Sample-size adjusted Bayesian (SABIC)       6021.201

Root Mean Square Error of Approximation:

  RMSEA                                          0.113
  90 Percent confidence interval - lower         0.098
  90 Percent confidence interval - upper         0.128
  P-value H_0: RMSEA <= 0.050                    0.000
  P-value H_0: RMSEA >= 0.080                    1.000

Standardized Root Mean Square Residual:

  SRMR                                           0.061

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed =~                                                            
    ClearRspnsblts    1.000                               0.652    0.826
    EffectivAnswrs    1.015    0.065   15.606    0.000    0.662    0.815
    Feedback          1.010    0.075   13.481    0.000    0.659    0.735
    ClearOrganiztn    1.295    0.086   15.106    0.000    0.845    0.797
    ClearPresenttn    1.204    0.072   16.680    0.000    0.785    0.853
  Valued =~                                                             
    ValObjectives     1.000                               0.334    0.557
    IncrUndrstndng    1.942    0.223    8.717    0.000    0.649    0.786
    IncrInterest      2.438    0.273    8.932    0.000    0.815    0.844
  SCRPed =~                                                             
    MultPerspectvs    1.000                               0.713    0.846
    InclusvClassrm    0.622    0.053   11.672    0.000    0.444    0.682
    DEIintegration    0.589    0.063    9.365    0.000    0.420    0.567
    EquitableEval     0.642    0.052   12.410    0.000    0.458    0.717
  Evals =~                                                              
    TradPed           1.000                               0.980    0.980
    Valued            0.419    0.055    7.630    0.000    0.801    0.801
    SCRPed            0.958    0.086   11.128    0.000    0.859    0.859

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClearRspnsblts    0.199    0.021    9.456    0.000    0.199    0.319
   .EffectivAnswrs    0.222    0.023    9.618    0.000    0.222    0.336
   .Feedback          0.371    0.036   10.415    0.000    0.371    0.460
   .ClearOrganiztn    0.410    0.042    9.855    0.000    0.410    0.365
   .ClearPresenttn    0.232    0.026    8.939    0.000    0.232    0.273
   .ValObjectives     0.248    0.023   10.650    0.000    0.248    0.690
   .IncrUndrstndng    0.260    0.032    8.041    0.000    0.260    0.382
   .IncrInterest      0.268    0.043    6.308    0.000    0.268    0.288
   .MultPerspectvs    0.203    0.029    7.052    0.000    0.203    0.285
   .InclusvClassrm    0.226    0.023   10.028    0.000    0.226    0.534
   .DEIintegration    0.371    0.035   10.734    0.000    0.371    0.678
   .EquitableEval     0.198    0.020    9.685    0.000    0.198    0.486
   .TradPed           0.017    0.023    0.728    0.467    0.039    0.039
   .Valued            0.040    0.010    3.895    0.000    0.358    0.358
   .SCRPed            0.134    0.032    4.219    0.000    0.263    0.263
    Evals             0.409    0.056    7.281    0.000    1.000    1.000

R-Square:
                   Estimate
    ClearRspnsblts    0.681
    EffectivAnswrs    0.664
    Feedback          0.540
    ClearOrganiztn    0.635
    ClearPresenttn    0.727
    ValObjectives     0.310
    IncrUndrstndng    0.618
    IncrInterest      0.712
    MultPerspectvs    0.715
    InclusvClassrm    0.466
    DEIintegration    0.322
    EquitableEval     0.514
    TradPed           0.961
    Valued            0.642
    SCRPed            0.737
\end{verbatim}

As we plot this model we expect to see a ``second level'' factor predicting each of the ``first order'' factors. The indicator was set on GRM --\textgreater{} AS. Each of the four factors predicts only the items associated with their factor with one item for each factor (the first on the left) specified as the indicator variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(secondF, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{11-CFA_2ndOrder_files/figure-latex/unnamed-chunk-108-1.pdf} Code for saving the results as a .csv file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{secondFFitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(secondF)}
\NormalTok{secondF\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(secondF, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\CommentTok{\# In a second order structure there are no correlations to request}
\CommentTok{\# secondFCorrs \textless{}{-} tidySEM::table\_cors(secondF, digits=3)}

\CommentTok{\# to see each of the tables, remove the hashtab secondFFitStats}
\CommentTok{\# secondF\_paramEsts}
\end{Highlighting}
\end{Shaded}

Next, I export them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(secondFFitStats, }\AttributeTok{file =} \StringTok{"secondFFitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(secondF\_paramEsts, }\AttributeTok{file =} \StringTok{"secondF\_paramEsts.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{narrate-adequacy-of-fit-with-chi-2-cfi-rmsea-srmr-write-a-mini-results-section-3}{%
\subsection{\texorpdfstring{Narrate adequacy of fit with \(\chi ^{2}\), CFI, RMSEA, SRMR (write a mini-results section)}{Narrate adequacy of fit with \textbackslash chi \^{}\{2\}, CFI, RMSEA, SRMR (write a mini-results section)}}\label{narrate-adequacy-of-fit-with-chi-2-cfi-rmsea-srmr-write-a-mini-results-section-3}}

\begin{quote}
Our next model represented a second order structure where three first-order factors loaded onto a second factor model. Across a variety of indices, model fit improved: \(\chi^{2}(51) = 224.80, p < .001, CFI = .907, RMSEA = .113, 90%CI(.098, .128), SRMR = .061
\). Factor loadings ranged from .75 to .85 for the TradPed scale, .56 to .84 for the Valued-by-Me scale, .57 to .85 for the SCRPed scale, and .80 to .98 for the total scale.
\end{quote}

\hypertarget{specify-and-run-a-bifactor-model}{%
\subsection{Specify and run a bifactor model}\label{specify-and-run-a-bifactor-model}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bifacM }\OtherTok{\textless{}{-}} \StringTok{"Evals =\textasciitilde{} ClearResponsibilities + EffectiveAnswers + Feedback + ClearOrganization + ClearPresentation + ValObjectives + IncrUnderstanding + IncrInterest + MultPerspectives + InclusvClassrm + DEIintegration + EquitableEval}
\StringTok{            TradPed =\textasciitilde{} ClearResponsibilities + EffectiveAnswers + Feedback + ClearOrganization + ClearPresentation  }
\StringTok{            Valued =\textasciitilde{} ValObjectives + IncrUnderstanding + IncrInterest }
\StringTok{            SCRPed =\textasciitilde{} MultPerspectives + InclusvClassrm + DEIintegration + EquitableEval}

\StringTok{          \#fixes the relations between g and each of the factors to 0.0 }
\StringTok{            Evals \textasciitilde{}\textasciitilde{} 0*TradPed}
\StringTok{            Evals \textasciitilde{}\textasciitilde{} 0*Valued}
\StringTok{            Evals \textasciitilde{}\textasciitilde{} 0*SCRPed}

\StringTok{          \#fixes the relations (covariances) between each of the factors to 0.0}
\StringTok{            TradPed \textasciitilde{}\textasciitilde{} 0*Valued}
\StringTok{            TradPed \textasciitilde{}\textasciitilde{} 0*SCRPed}
\StringTok{            Valued \textasciitilde{}\textasciitilde{} 0*SCRPed}
\StringTok{  }
\StringTok{"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{bifacF }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(bifacM, }\AttributeTok{data =}\NormalTok{ items)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in lavaan::lavaan(model = bifacM, data = items, int.ov.free = TRUE, : lavaan WARNING:
    the optimizer (NLMINB) claimed the model converged, but not all
    elements of the gradient are (near) zero; the optimizer may not
    have found a local solution use check.gradient = FALSE to skip
    this check.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(bifacF, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rsquare =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in lav_object_summary(object = object, header = header, fit.measures = fit.measures, : lavaan WARNING: fit measures not available if model did not converge
\end{verbatim}

\begin{verbatim}
lavaan 0.6.17 did NOT end normally after 1256 iterations
** WARNING ** Estimates below are most likely unreliable

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        36

                                                  Used       Total
  Number of observations                           267         310


Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Evals =~                                                              
    ClearRspnsblts    1.000                               0.665    0.842
    EffectivAnswrs    0.977       NA                      0.650    0.800
    Feedback          1.032       NA                      0.686    0.765
    ClearOrganiztn    1.251       NA                      0.832    0.785
    ClearPresenttn    1.227       NA                      0.816    0.886
    ValObjectives     0.436       NA                      0.290    0.484
    IncrUndrstndng    0.757       NA                      0.504    0.610
    IncrInterest      0.923       NA                      0.614    0.636
    MultPerspectvs    0.925       NA                      0.615    0.730
    InclusvClassrm    0.531       NA                      0.353    0.543
    DEIintegration    0.387       NA                      0.258    0.348
    EquitableEval     0.644       NA                      0.429    0.671
  TradPed =~                                                            
    ClearRspnsblts    1.000                               0.018    0.023
    EffectivAnswrs   -0.115       NA                     -0.002   -0.003
    Feedback          2.260       NA                      0.040    0.045
    ClearOrganiztn   -0.130       NA                     -0.002   -0.002
    ClearPresenttn -146.233       NA                     -2.614   -2.838
  Valued =~                                                             
    ValObjectives     1.000                               0.145    0.241
    IncrUndrstndng    2.548       NA                      0.368    0.446
    IncrInterest      4.310       NA                      0.623    0.645
  SCRPed =~                                                             
    MultPerspectvs    1.000                               0.267    0.316
    InclusvClassrm    1.302       NA                      0.347    0.534
    DEIintegration    2.188       NA                      0.584    0.788
    EquitableEval     0.415       NA                      0.111    0.173

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Evals ~~                                                              
    TradPed           0.000                               0.000    0.000
    Valued            0.000                               0.000    0.000
    SCRPed            0.000                               0.000    0.000
  TradPed ~~                                                            
    Valued            0.000                               0.000    0.000
    SCRPed            0.000                               0.000    0.000
  Valued ~~                                                             
    SCRPed            0.000                               0.000    0.000

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClearRspnsblts    0.182       NA                      0.182    0.291
   .EffectivAnswrs    0.238       NA                      0.238    0.361
   .Feedback          0.332       NA                      0.332    0.413
   .ClearOrganiztn    0.432       NA                      0.432    0.384
   .ClearPresenttn   -6.651       NA                     -6.651   -7.837
   .ValObjectives     0.255       NA                      0.255    0.708
   .IncrUndrstndng    0.292       NA                      0.292    0.429
   .IncrInterest      0.167       NA                      0.167    0.179
   .MultPerspectvs    0.262       NA                      0.262    0.368
   .InclusvClassrm    0.178       NA                      0.178    0.421
   .DEIintegration    0.141       NA                      0.141    0.257
   .EquitableEval     0.212       NA                      0.212    0.519
    Evals             0.443       NA                      1.000    1.000
    TradPed           0.000       NA                      1.000    1.000
    Valued            0.021       NA                      1.000    1.000
    SCRPed            0.071       NA                      1.000    1.000

R-Square:
                   Estimate
    ClearRspnsblts    0.709
    EffectivAnswrs    0.639
    Feedback          0.587
    ClearOrganiztn    0.616
    ClearPresenttn       NA
    ValObjectives     0.292
    IncrUndrstndng    0.571
    IncrInterest      0.821
    MultPerspectvs    0.632
    InclusvClassrm    0.579
    DEIintegration    0.743
    EquitableEval     0.481
\end{verbatim}

Unfortunately, it's all-too-common for complex models to fail to converge. When this happens it's a slow, tedious process to find a fix.

In the ReCentering Psych Stats example I was able to fix it by adding a \emph{check.gradient=FALSE} statement. So, let's try that

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{bifacF }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(bifacM, }\AttributeTok{data =}\NormalTok{ items, }\AttributeTok{check.gradient =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats, : lavaan WARNING:
    Could not compute standard errors! The information matrix could
    not be inverted. This may be a symptom that the model is not
    identified.
\end{verbatim}

\begin{verbatim}
Warning in lav_object_post_check(object): lavaan WARNING: some estimated ov
variances are negative
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(bifacF, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rsquare =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 1256 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        36

                                                  Used       Total
  Number of observations                           267         310

Model Test User Model:
                                                      
  Test statistic                               121.965
  Degrees of freedom                                42
  P-value (Chi-square)                           0.000

Model Test Baseline Model:

  Test statistic                              1940.157
  Degrees of freedom                                66
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.957
  Tucker-Lewis Index (TLI)                       0.933

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -2926.561
  Loglikelihood unrestricted model (H1)      -2865.578
                                                      
  Akaike (AIC)                                5925.121
  Bayesian (BIC)                              6054.262
  Sample-size adjusted Bayesian (SABIC)       5940.121

Root Mean Square Error of Approximation:

  RMSEA                                          0.084
  90 Percent confidence interval - lower         0.067
  90 Percent confidence interval - upper         0.102
  P-value H_0: RMSEA <= 0.050                    0.001
  P-value H_0: RMSEA >= 0.080                    0.679

Standardized Root Mean Square Residual:

  SRMR                                           0.041

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Evals =~                                                              
    ClearRspnsblts    1.000                               0.665    0.842
    EffectivAnswrs    0.977       NA                      0.650    0.800
    Feedback          1.032       NA                      0.686    0.765
    ClearOrganiztn    1.251       NA                      0.832    0.785
    ClearPresenttn    1.227       NA                      0.816    0.886
    ValObjectives     0.436       NA                      0.290    0.484
    IncrUndrstndng    0.757       NA                      0.504    0.610
    IncrInterest      0.923       NA                      0.614    0.636
    MultPerspectvs    0.925       NA                      0.615    0.730
    InclusvClassrm    0.531       NA                      0.353    0.543
    DEIintegration    0.387       NA                      0.258    0.348
    EquitableEval     0.644       NA                      0.429    0.671
  TradPed =~                                                            
    ClearRspnsblts    1.000                               0.018    0.023
    EffectivAnswrs   -0.115       NA                     -0.002   -0.003
    Feedback          2.260       NA                      0.040    0.045
    ClearOrganiztn   -0.130       NA                     -0.002   -0.002
    ClearPresenttn -146.233       NA                     -2.614   -2.838
  Valued =~                                                             
    ValObjectives     1.000                               0.145    0.241
    IncrUndrstndng    2.548       NA                      0.368    0.446
    IncrInterest      4.310       NA                      0.623    0.645
  SCRPed =~                                                             
    MultPerspectvs    1.000                               0.267    0.316
    InclusvClassrm    1.302       NA                      0.347    0.534
    DEIintegration    2.188       NA                      0.584    0.788
    EquitableEval     0.415       NA                      0.111    0.173

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Evals ~~                                                              
    TradPed           0.000                               0.000    0.000
    Valued            0.000                               0.000    0.000
    SCRPed            0.000                               0.000    0.000
  TradPed ~~                                                            
    Valued            0.000                               0.000    0.000
    SCRPed            0.000                               0.000    0.000
  Valued ~~                                                             
    SCRPed            0.000                               0.000    0.000

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClearRspnsblts    0.182       NA                      0.182    0.291
   .EffectivAnswrs    0.238       NA                      0.238    0.361
   .Feedback          0.332       NA                      0.332    0.413
   .ClearOrganiztn    0.432       NA                      0.432    0.384
   .ClearPresenttn   -6.651       NA                     -6.651   -7.837
   .ValObjectives     0.255       NA                      0.255    0.708
   .IncrUndrstndng    0.292       NA                      0.292    0.429
   .IncrInterest      0.167       NA                      0.167    0.179
   .MultPerspectvs    0.262       NA                      0.262    0.368
   .InclusvClassrm    0.178       NA                      0.178    0.421
   .DEIintegration    0.141       NA                      0.141    0.257
   .EquitableEval     0.212       NA                      0.212    0.519
    Evals             0.443       NA                      1.000    1.000
    TradPed           0.000       NA                      1.000    1.000
    Valued            0.021       NA                      1.000    1.000
    SCRPed            0.071       NA                      1.000    1.000

R-Square:
                   Estimate
    ClearRspnsblts    0.709
    EffectivAnswrs    0.639
    Feedback          0.587
    ClearOrganiztn    0.616
    ClearPresenttn       NA
    ValObjectives     0.292
    IncrUndrstndng    0.571
    IncrInterest      0.821
    MultPerspectvs    0.632
    InclusvClassrm    0.579
    DEIintegration    0.743
    EquitableEval     0.481
\end{verbatim}

That did not work.

I found an article on nonconvergence in bifactor models: \url{https://stackoverflow.com/questions/68837355/trouble-converging-bifactor-model-using-lavaan}

I took out the check.gradient command and swapped in the ``std.lv=TRUE'' command.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{bifacF }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(bifacM, }\AttributeTok{data =}\NormalTok{ items, }\AttributeTok{std.lv =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in lav_object_post_check(object): lavaan WARNING: some estimated ov
variances are negative
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(bifacF, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rsquare =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 521 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        36

                                                  Used       Total
  Number of observations                           267         310

Model Test User Model:
                                                      
  Test statistic                               121.965
  Degrees of freedom                                42
  P-value (Chi-square)                           0.000

Model Test Baseline Model:

  Test statistic                              1940.157
  Degrees of freedom                                66
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.957
  Tucker-Lewis Index (TLI)                       0.933

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -2926.561
  Loglikelihood unrestricted model (H1)      -2865.578
                                                      
  Akaike (AIC)                                5925.121
  Bayesian (BIC)                              6054.262
  Sample-size adjusted Bayesian (SABIC)       5940.121

Root Mean Square Error of Approximation:

  RMSEA                                          0.084
  90 Percent confidence interval - lower         0.067
  90 Percent confidence interval - upper         0.102
  P-value H_0: RMSEA <= 0.050                    0.001
  P-value H_0: RMSEA >= 0.080                    0.679

Standardized Root Mean Square Residual:

  SRMR                                           0.041

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Evals =~                                                              
    ClearRspnsblts    0.665    0.040   16.440    0.000    0.665    0.842
    EffectivAnswrs    0.650    0.043   15.284    0.000    0.650    0.800
    Feedback          0.686    0.048   14.204    0.000    0.686    0.765
    ClearOrganiztn    0.832    0.056   14.873    0.000    0.832    0.785
    ClearPresenttn    0.816    0.048   17.021    0.000    0.816    0.886
    ValObjectives     0.290    0.035    8.203    0.000    0.290    0.484
    IncrUndrstndng    0.504    0.047   10.784    0.000    0.504    0.610
    IncrInterest      0.614    0.054   11.355    0.000    0.614    0.636
    MultPerspectvs    0.615    0.045   13.572    0.000    0.615    0.730
    InclusvClassrm    0.353    0.038    9.350    0.000    0.353    0.543
    DEIintegration    0.258    0.045    5.718    0.000    0.258    0.348
    EquitableEval     0.429    0.035   12.164    0.000    0.429    0.671
  TradPed =~                                                            
    ClearRspnsblts   -0.018    0.234   -0.076    0.940   -0.018   -0.022
    EffectivAnswrs    0.002    0.028    0.073    0.942    0.002    0.002
    Feedback         -0.040    0.529   -0.076    0.940   -0.040   -0.045
    ClearOrganiztn    0.002    0.032    0.072    0.943    0.002    0.002
    ClearPresenttn    2.637   34.860    0.076    0.940    2.637    2.863
  Valued =~                                                             
    ValObjectives     0.145    0.042    3.472    0.001    0.145    0.241
    IncrUndrstndng    0.368    0.076    4.878    0.000    0.368    0.446
    IncrInterest      0.623    0.116    5.385    0.000    0.623    0.645
  SCRPed =~                                                             
    MultPerspectvs    0.267    0.042    6.327    0.000    0.267    0.316
    InclusvClassrm    0.347    0.042    8.251    0.000    0.347    0.534
    DEIintegration    0.584    0.059    9.967    0.000    0.584    0.788
    EquitableEval     0.111    0.034    3.224    0.001    0.111    0.173

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Evals ~~                                                              
    TradPed           0.000                               0.000    0.000
    Valued            0.000                               0.000    0.000
    SCRPed            0.000                               0.000    0.000
  TradPed ~~                                                            
    Valued            0.000                               0.000    0.000
    SCRPed            0.000                               0.000    0.000
  Valued ~~                                                             
    SCRPed            0.000                               0.000    0.000

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClearRspnsblts    0.182    0.021    8.669    0.000    0.182    0.291
   .EffectivAnswrs    0.238    0.025    9.441    0.000    0.238    0.361
   .Feedback          0.333    0.047    7.050    0.000    0.333    0.413
   .ClearOrganiztn    0.432    0.045    9.663    0.000    0.432    0.384
   .ClearPresenttn   -6.774  183.878   -0.037    0.971   -6.774   -7.982
   .ValObjectives     0.255    0.023   10.940    0.000    0.255    0.708
   .IncrUndrstndng    0.292    0.054    5.440    0.000    0.292    0.429
   .IncrInterest      0.167    0.137    1.221    0.222    0.167    0.179
   .MultPerspectvs    0.262    0.026    9.965    0.000    0.262    0.368
   .InclusvClassrm    0.178    0.024    7.298    0.000    0.178    0.421
   .DEIintegration    0.141    0.056    2.496    0.013    0.141    0.257
   .EquitableEval     0.212    0.019   10.967    0.000    0.212    0.519
    Evals             1.000                               1.000    1.000
    TradPed           1.000                               1.000    1.000
    Valued            1.000                               1.000    1.000
    SCRPed            1.000                               1.000    1.000

R-Square:
                   Estimate
    ClearRspnsblts    0.709
    EffectivAnswrs    0.639
    Feedback          0.587
    ClearOrganiztn    0.616
    ClearPresenttn       NA
    ValObjectives     0.292
    IncrUndrstndng    0.571
    IncrInterest      0.821
    MultPerspectvs    0.632
    InclusvClassrm    0.579
    DEIintegration    0.743
    EquitableEval     0.481
\end{verbatim}

Great! It ran, but now I have negative variances (i.e., a ``Heywood case'') so I need to fix that. In the ``variances'' you can see the negative values on: .ClearPresenttn

Here's an article about Heywood cases (i.e., where there are negative variances): \url{https://s3.amazonaws.com/assets.datacamp.com/production/course_6419/slides/chapter3.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{items }\OtherTok{\textless{}{-}} \FunctionTok{na.omit}\NormalTok{(items)}
\FunctionTok{var}\NormalTok{(items}\SpecialCharTok{$}\NormalTok{ClearPresentation)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.8518206
\end{verbatim}

I will add this statement:

ClearPresentation\textasciitilde\textasciitilde0.852*ClearPresentation

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bifacM }\OtherTok{\textless{}{-}} \StringTok{"Evals =\textasciitilde{} ClearResponsibilities + EffectiveAnswers + Feedback + ClearOrganization + ClearPresentation + ValObjectives + IncrUnderstanding + IncrInterest + MultPerspectives + InclusvClassrm + DEIintegration + EquitableEval}
\StringTok{            TradPed =\textasciitilde{} ClearResponsibilities + EffectiveAnswers + Feedback + ClearOrganization + ClearPresentation  }
\StringTok{            Valued =\textasciitilde{} ValObjectives + IncrUnderstanding + IncrInterest }
\StringTok{            SCRPed =\textasciitilde{} MultPerspectives + InclusvClassrm + DEIintegration + EquitableEval}
\StringTok{            ClearPresentation\textasciitilde{}\textasciitilde{}0.852*ClearPresentation}

\StringTok{          \#fixes the relations between g and each of the factors to 0.0 }
\StringTok{            Evals \textasciitilde{}\textasciitilde{} 0*TradPed}
\StringTok{            Evals \textasciitilde{}\textasciitilde{} 0*Valued}
\StringTok{            Evals \textasciitilde{}\textasciitilde{} 0*SCRPed}

\StringTok{          \#fixes the relations (covariances) between each of the factors to 0.0}
\StringTok{            TradPed \textasciitilde{}\textasciitilde{} 0*Valued}
\StringTok{            TradPed \textasciitilde{}\textasciitilde{} 0*SCRPed}
\StringTok{            Valued \textasciitilde{}\textasciitilde{} 0*SCRPed}
\StringTok{  }
\StringTok{"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240311}\NormalTok{)}
\NormalTok{bifacF }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(bifacM, }\AttributeTok{data =}\NormalTok{ items, }\AttributeTok{std.lv =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(bifacF, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rsquare =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 48 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        35

  Number of observations                           267

Model Test User Model:
                                                      
  Test statistic                               262.623
  Degrees of freedom                                43
  P-value (Chi-square)                           0.000

Model Test Baseline Model:

  Test statistic                              1940.157
  Degrees of freedom                                66
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.883
  Tucker-Lewis Index (TLI)                       0.820

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -2996.890
  Loglikelihood unrestricted model (H1)      -2865.578
                                                      
  Akaike (AIC)                                6063.779
  Bayesian (BIC)                              6189.333
  Sample-size adjusted Bayesian (SABIC)       6078.362

Root Mean Square Error of Approximation:

  RMSEA                                          0.138
  90 Percent confidence interval - lower         0.122
  90 Percent confidence interval - upper         0.155
  P-value H_0: RMSEA <= 0.050                    0.000
  P-value H_0: RMSEA >= 0.080                    1.000

Standardized Root Mean Square Residual:

  SRMR                                           0.084

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Evals =~                                                              
    ClearRspnsblts    0.621    0.045   13.943    0.000    0.621    0.786
    EffectivAnswrs    0.637    0.045   14.110    0.000    0.637    0.784
    Feedback          0.679    0.050   13.532    0.000    0.679    0.756
    ClearOrganiztn    0.798    0.061   13.135    0.000    0.798    0.753
    ClearPresenttn    0.721    0.070   10.301    0.000    0.721    0.613
    ValObjectives     0.293    0.037    7.996    0.000    0.293    0.488
    IncrUndrstndng    0.496    0.048   10.227    0.000    0.496    0.600
    IncrInterest      0.638    0.055   11.557    0.000    0.638    0.661
    MultPerspectvs    0.643    0.046   13.886    0.000    0.643    0.763
    InclusvClassrm    0.359    0.040    9.079    0.000    0.359    0.552
    DEIintegration    0.286    0.047    6.062    0.000    0.286    0.386
    EquitableEval     0.455    0.036   12.710    0.000    0.455    0.713
  TradPed =~                                                            
    ClearRspnsblts    0.394    0.170    2.313    0.021    0.394    0.498
    EffectivAnswrs    0.121    0.086    1.405    0.160    0.121    0.149
    Feedback          0.085    0.081    1.050    0.294    0.085    0.095
    ClearOrganiztn    0.215    0.136    1.582    0.114    0.215    0.203
    ClearPresenttn    0.118    0.112    1.052    0.293    0.118    0.100
  Valued =~                                                             
    ValObjectives     0.141    0.043    3.248    0.001    0.141    0.235
    IncrUndrstndng    0.385    0.081    4.752    0.000    0.385    0.466
    IncrInterest      0.578    0.111    5.188    0.000    0.578    0.598
  SCRPed =~                                                             
    MultPerspectvs    0.226    0.047    4.824    0.000    0.226    0.268
    InclusvClassrm    0.335    0.051    6.577    0.000    0.335    0.515
    DEIintegration    0.574    0.073    7.859    0.000    0.574    0.775
    EquitableEval     0.073    0.036    2.002    0.045    0.073    0.114

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Evals ~~                                                              
    TradPed           0.000                               0.000    0.000
    Valued            0.000                               0.000    0.000
    SCRPed            0.000                               0.000    0.000
  TradPed ~~                                                            
    Valued            0.000                               0.000    0.000
    SCRPed            0.000                               0.000    0.000
  Valued ~~                                                             
    SCRPed            0.000                               0.000    0.000

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClearPresenttn    0.852                               0.852    0.615
   .ClearRspnsblts    0.084    0.134    0.627    0.530    0.084    0.135
   .EffectivAnswrs    0.240    0.025    9.637    0.000    0.240    0.363
   .Feedback          0.337    0.034    9.812    0.000    0.337    0.419
   .ClearOrganiztn    0.440    0.050    8.847    0.000    0.440    0.392
   .ValObjectives     0.255    0.023   10.921    0.000    0.255    0.707
   .IncrUndrstndng    0.288    0.058    4.942    0.000    0.288    0.422
   .IncrInterest      0.191    0.120    1.598    0.110    0.191    0.205
   .MultPerspectvs    0.247    0.027    9.281    0.000    0.247    0.347
   .InclusvClassrm    0.182    0.028    6.557    0.000    0.182    0.430
   .DEIintegration    0.137    0.074    1.849    0.064    0.137    0.250
   .EquitableEval     0.195    0.019   10.102    0.000    0.195    0.479
    Evals             1.000                               1.000    1.000
    TradPed           1.000                               1.000    1.000
    Valued            1.000                               1.000    1.000
    SCRPed            1.000                               1.000    1.000

R-Square:
                   Estimate
    ClearPresenttn    0.385
    ClearRspnsblts    0.865
    EffectivAnswrs    0.637
    Feedback          0.581
    ClearOrganiztn    0.608
    ValObjectives     0.293
    IncrUndrstndng    0.578
    IncrInterest      0.795
    MultPerspectvs    0.653
    InclusvClassrm    0.570
    DEIintegration    0.750
    EquitableEval     0.521
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(bifacF, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{11-CFA_2ndOrder_files/figure-latex/unnamed-chunk-118-1.pdf} In making our map the first 12 values refer to the items, 13 refers to \emph{g}, and 14-16 point to the factors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\AttributeTok{nrow =} \DecValTok{3}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{12}\NormalTok{)}
\NormalTok{m[}\DecValTok{1}\NormalTok{, ] }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{14}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{16}\NormalTok{)}
\NormalTok{m[}\DecValTok{2}\NormalTok{, ] }\OtherTok{=} \DecValTok{1}\SpecialCharTok{:}\DecValTok{12}
\NormalTok{m[}\DecValTok{3}\NormalTok{, ] }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{m}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
[1,]   14    0    0    0    0   15    0    0    0     0     0    16
[2,]    1    2    3    4    5    6    7    8    9    10    11    12
[3,]    0    0    0    0    0   13    0    0    0     0     0     0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(bifacF, }\StringTok{"model"}\NormalTok{, }\StringTok{"std"}\NormalTok{, }\AttributeTok{layout =}\NormalTok{ m, }\AttributeTok{residuals =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{exoCov =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{11-CFA_2ndOrder_files/figure-latex/unnamed-chunk-120-1.pdf} Code for saving the results as a .csv file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bifacFFitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(bifacF)}
\NormalTok{bifacF\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(bifacF, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{bifacFCorrs }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_cors}\NormalTok{(bifacF, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\CommentTok{\# to see each of the tables, remove the hashtab corrFitStats}
\CommentTok{\# corrF\_paramEsts corrFCorrs}
\end{Highlighting}
\end{Shaded}

Next, I export them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(bifacFFitStats, }\AttributeTok{file =} \StringTok{"bifacFFitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(bifacF\_paramEsts, }\AttributeTok{file =} \StringTok{"bifacF\_paramEsts.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(bifacFCorrs, }\AttributeTok{file =} \StringTok{"bifacFCorrs.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{narrate-adequacy-of-fit-with-chi-2-cfi-rmsea-srmr-write-a-mini-results-section-4}{%
\subsection{\texorpdfstring{Narrate adequacy of fit with \(\chi ^{2}\), CFI, RMSEA, SRMR (write a mini-results section)}{Narrate adequacy of fit with \textbackslash chi \^{}\{2\}, CFI, RMSEA, SRMR (write a mini-results section)}}\label{narrate-adequacy-of-fit-with-chi-2-cfi-rmsea-srmr-write-a-mini-results-section-4}}

\begin{quote}
The bifactor model regressed each item on its respective factor while simultaneously regressing each indicator onto an overall evaluation scale. The initial specification failed to converge. Adding the ``std.lv = TRUE'' to the lavaan::cfa function resulted in convergence. It also identified a negative covariance (i.e., a ``Heywood'' case). We resolved this issue by respecifying the model with a multiplication of the variance of the item (ClearPresentation) onto itself. This model had the second best fit of those compared thus far: \(\chi^2(43)=262.62, p < .001, CFI = .883, RMSEA = .138, 90%CI [.122, .155], SRMR = .084
\) (the correlated factors and second order structures had the best fit). Factor loadings for the three factors ranged from .10 to .50 for the TradPed scale, .24 to .60 for the Valued-by-me scale, and .11 to .78 for the SCRPed scale. Factor loadings for the overall evaluation scale (g) ranged from .37 to .79.
\end{quote}

\hypertarget{compare-model-fit-with-chi-2delta-aic-bic-1}{%
\subsection{\texorpdfstring{Compare model fit with \(\chi ^{2}\Delta\), AIC, BIC}{Compare model fit with \textbackslash chi \^{}\{2\}\textbackslash Delta, AIC, BIC}}\label{compare-model-fit-with-chi-2delta-aic-bic-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{lavTestLRT}\NormalTok{(uniDfit, corrF\_fit, secondF, bifacF)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in lavaan::lavTestLRT(uniDfit, corrF_fit, secondF, bifacF): lavaan WARNING:
    Some restricted models fit better than less restricted models;
    either these models are not nested, or the less restricted model
    failed to reach a global optimum. Smallest difference =
    -37.8284756558401
\end{verbatim}

\begin{verbatim}
Warning in lavaan::lavTestLRT(uniDfit, corrF_fit, secondF, bifacF): lavaan
WARNING: some models have the same degrees of freedom
\end{verbatim}

\begin{verbatim}

Chi-Squared Difference Test

          Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff
bifacF    43 6063.8 6189.3 262.62                           
corrF_fit 51 6010.0 6106.8 224.79    -37.828 0.00000       8
secondF   51 6010.0 6106.8 224.79      0.000 0.00000       0
uniDfit   54 6124.1 6210.2 344.97    120.178 0.38248       3
                   Pr(>Chisq)    
bifacF                           
corrF_fit                   1    
secondF                          
uniDfit   <0.0000000000000002 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{lavTestLRT}\NormalTok{(secondF, bifacF)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in lavaan::lavTestLRT(secondF, bifacF): lavaan WARNING:
    Some restricted models fit better than less restricted models;
    either these models are not nested, or the less restricted model
    failed to reach a global optimum. Smallest difference =
    -37.8284756538201
\end{verbatim}

\begin{verbatim}

Chi-Squared Difference Test

        Df    AIC    BIC  Chisq Chisq diff RMSEA Df diff Pr(>Chisq)
bifacF  43 6063.8 6189.3 262.62                                    
secondF 51 6010.0 6106.8 224.79    -37.828     0       8          1
\end{verbatim}

\hypertarget{calculate-omega-hierarchical-omega_h-and-omega-hierarchical-subscales-omega_h-ss}{%
\subsection{\texorpdfstring{Calculate omega hierarchical (\(\omega_{h}\)) and omega-hierarchical-subscales (\(\omega_{h-ss}\))}{Calculate omega hierarchical (\textbackslash omega\_\{h\}) and omega-hierarchical-subscales (\textbackslash omega\_\{h-ss\})}}\label{calculate-omega-hierarchical-omega_h-and-omega-hierarchical-subscales-omega_h-ss}}

Given that our results favored the second-order structure, I will calculate model-based assessments of the omegas with that function (\emph{semTools::reliabilityL2}) and object (\emph{secondF}).

To use \emph{semTools} we switch functions to \emph{reliabilityL2}. The specification

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semTools}\SpecialCharTok{::}\FunctionTok{reliabilityL2}\NormalTok{(secondF, }\StringTok{"Evals"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       omegaL1        omegaL2 partialOmegaL1 
     0.8832659      0.9237686      0.9336447 
\end{verbatim}

The estimate listed under \emph{omegaL1} (\(\omega_{ho}\)) represents the proportion of the course evaluation total score variance due to the higher-order factor. The value is 0.88 and, being above .80, indicates strong reliability.

We can apply the \emph{semTools::reliability()} function to the second-order factor to obtain omega values for the subscales. Below the alpha coefficients, the omega values indicate how reliably each subscale measures its lower order factor. For example, 80\% of the total variance of a total score comprised of only the 9 items in the AS scale is explained by the AS lower order factor.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semTools}\SpecialCharTok{::}\FunctionTok{reliability}\NormalTok{(secondF)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         TradPed    Valued    SCRPed
alpha  0.8973027 0.7573447 0.8037983
omega  0.9005696 0.8062411 0.8057420
omega2 0.9005696 0.8062411 0.8057420
omega3 0.9019489 0.8109489 0.7868658
avevar 0.6470672 0.6063643 0.5223301
\end{verbatim}

Subscale omegas range from .81 to .90.

In this case, \(\omega_{ho}\) (ho = higher order) \citep{flora_your_2020} represents the proportion of total-score variance that is due to the higher-order factor. As such, it represents the reliability of a total score for measuring a single construct that influences all items.

\begin{quote}
We obtained estimates of model-based internal consistency associated with the second-order model. Specifically, we calculated \(\omega_{ho}\), which represents the proportion of GRMSAAW total score variance due to the higher-order factor. The value of 0.88 indicates strong reliability. Similarly, we obtained omega values for the subscales. These ranged were 0.90, 0.81, and 0.81 for Traditional Pedagogy, Valued-by-the-Student, and Socially Responsive Pedagogy, respectively. Taken together, the omega values from our second order model suggest suggest a strong general factor and strong subscales.
\end{quote}

\hypertarget{apa-style-results-with-tables-and-figure-1}{%
\subsection{APA style results with table(s) and figure}\label{apa-style-results-with-tables-and-figure-1}}

Because we have written mini-results throughout, we can assemble them into a full results section. Keep in mind that most CFA models will continue testing multidimensional models. Thus, the entire analysis continues in the next lesson and associated practice problem.

\begin{quote}
\textbf{Model testing}. To evaluate the models we, we used confirmatory factor analysis (CFA) in the R package, lavaan (v.0-6.17) with maximum likelihood estimation. Our sample size was 267 We selected fit criteria for their capacity to assess different aspects of the statistical analysis. As is common among SEM researchers, we reported the Chi-square goodness of fit (χ2). This evaluates the discrepancy between the unrestricted sample matrix and the restricted covariance matrix. Although the associated \emph{p}-value indicates adequate fit when the value is non-significant, it is widely recognized that a large sample size can result in a statistically significant \emph{p} value (Byrne, 2016b). The comparative fit index (CFI) is an incremental index, comparing the hypothesized model to the independent/baseline model. Adequate fit is determined when CFI values are at least .90 and perhaps higher than .95 \citep{kline_principles_2016}. The root mean square error of approximation (RMSEA) takes into account the error of approximation in the population and expresses it per degree of freedom. As such, the fit indicator considers the complexity of the model. Ideal values are equal to or less than .05, values less than .08 represent reasonable fit, and values between .08 and .10 represent mediocre fit. The standardized root mean residual is a standardized measure of the mean absolute covariance residual -- the overall difference between the observed and predicted correlations. Values greater than .10 may indicate poor fit and inspection of residuals is then advised. Kline (2016) advised researchers to be cautious when using these criteria as strict cut-offs. Elements such as sample size and model complexity should be considered when evaluating fit.
\end{quote}

\begin{quote}
Our first model was unidimensional where each of the 12 items loaded onto a single factor representing overall course evaluations. The Chi-square index was statistically significant \((\chi^2(54)=344.97, p<.001)\) indicating likely misfit. The CFI value of .85 indicated poor fit. The RMSEA = .14 (90\% CI {[}.13, .16{]}) suggested serious problems. The SRMR value of .07 was below the warning criteria of .10. The AIC and BIC values were 6124.13 and 6134.13, respectively, and will become useful in comparing subsequent models.
\end{quote}

\begin{quote}
Our second model was a single-order, multidimensional model where each of the 12 items loaded onto one of four factors. Standardized pattern coefficients ranged between .74 and .85 on the TradPed factor, between .56 and .84 on the Valued factor, and between .57 and .85 on the SCRPed factor. The Chi-square index was statistically significant \((\chi^2(51) = 224.795, p < 0.001\) indicating some degree of misfit. The CFI value of .907 met the recommendation of .90 but fell below the recommendation of .95. The RMSEA = .113 (90\% CI {[}.098, .128{]}) was higher than recommended. The SRMR value of .061 remained below the warning criteria of .10. The AIC and BIC values were 6009.95 and 6021.20, respectively.
\end{quote}

\begin{quote}
Our third model represented a second order structure where three first-order factors loaded onto a second factor model. Across a variety of indices, model fit improved: \(\chi^{2}(51) = 224.80, p < .001, CFI = .907, RMSEA = .113, 90%CI(.098, .128), SRMR = .061
\). Factor loadings ranged from .75 to .85 for the TradPed scale, .56 to .84 for the Valued-by-Me scale, .57 to .85 for the SCRPed scale, and .80 to .98 for the total scale.
\end{quote}

\begin{quote}
The bifactor model regressed each item on its respective factor while simultaneously regressing each indicator onto an overall evaluation scale. The initial specification failed to converge. Adding the ``std.lv = TRUE'' to the lavaan::cfa function resulted in convergence. It also identified a negative covariance (i.e., a ``Heywood'' case). We resolved this issue by respecifying the model with a multiplication of the variance of the item (ClearPresentation) onto itself. This model had the second best fit of those compared thus far: \(\chi^2(43)=262.62, p < .001, CFI = .883, RMSEA = .138, 90%CI [.122, .155], SRMR = .084
\) (the correlated factors and second order structures had the best fit). Factor loadings for the three factors ranged from .10 to .50 for the TradPed scale, .24 to .60 for the Valued-by-me scale, and .11 to .78 for the SCRPed scale. Factor loadings for the overall evaluation scale (g) ranged from .37 to .79.
\end{quote}

\begin{quote}
As shown in our table of model comparisons, the Chi-square difference tests between models showed statistically significant differences between the unidimensional and correlated factors model (\(\chi ^{2}(3) = 120.178, p < .001\)). The second-order model and bifactor model did not differ from each other (\(\chi ^{2}(8) = -37.828, p = 1.000\)). The CFI, RMSEA, SRMR, AIC, and BIC values all favored the second-order and correlated factors models (which did not differ from each other).
\end{quote}

\begin{quote}
We obtained estimates of model-based internal consistency associated with the second-order model. Specifically, we calculated \(\omega_{ho}\), which represents the proportion of GRMSAAW total score variance due to the higher-order factor. The value of 0.88 indicates strong reliability. Similarly, we obtained omega values for the subscales. These ranged were 0.90, 0.81, and 0.81 for Traditional Pedagogy, Valued-by-the-Student, and Socially Responsive Pedagogy, respectively. Taken together, the omega values from our second order model suggest a strong general factor and strong subscales.
\end{quote}

\hypertarget{explanation-to-grader-4}{%
\subsection{Explanation to grader}\label{explanation-to-grader-4}}

\hypertarget{Invariance}{%
\chapter{Invariance Testing}\label{Invariance}}

\href{https://youtube.com/playlist?list=PLtz5cFLQl4KPc98t5IRYFhuS7sULnaCK7\&si=Ed4oPAWJfQq0J5Lx}{Screencasted Lecture Link}

The focus of this lecture is invariance testing -- that is, evaluating if a scale operates equivalently across two samples.

\hypertarget{navigating-this-lesson-10}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-10}}

There is a little more than 1.5 hours of lecture. If you work through the materials with me it would be plan for an additional 1.5 hours.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}.

\hypertarget{learning-objectives-10}{%
\subsection{Learning Objectives}\label{learning-objectives-10}}

Focusing on this week's materials, make sure you can:

\begin{itemize}
\tightlist
\item
  Specify a series of models that will test for multigroup invariance.
\item
  Interpret model adequacy and fit.
\item
  Compare models on the basis of statistical criteria.
\item
  Recall which parameters (e.g., structures, loadings, intercepts, residuals) are constrained in configural, weak, strong, and strict models

  \begin{itemize}
  \tightlist
  \item
    and know, without evaluation, which of these models will (necessarily) have the best fit
  \end{itemize}
\item
  interpret \(\chi_{D}^{2}\) and \(\Delta CFI\) tests to determine if there are statisticaly significant differences in model fit.
\end{itemize}

\hypertarget{planning-for-practice-10}{%
\subsection{Planning for Practice}\label{planning-for-practice-10}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. The least complex is to change the random seed and rework the problem demonstrated in the lesson. The results \emph{should} map onto the ones obtained in the lecture.

The second option involves utilizing one of the simulated datasets available in this OER. The \protect\hyperlink{sims}{last lesson} in the OER contains three simulations that could be used for all of the statistics-based practice suggestions. Especially if you started with one of these examples in an earlier lesson, I highly recommend you continue with that.

As a third option, you are welcome to use data to which you have access and is suitable for CFA. This could include other simulated data, data found on an open access repository, data from the ReCentering Psych Stats survey described in the \protect\hyperlink{qualTRIX}{Qualtrics lesson}, or your own data (presuming you have permission to use it).

In any case, please plan to:

\begin{itemize}
\tightlist
\item
  Specify, interpret, and write up preliminary results for CFA models that examine

  \begin{itemize}
  \tightlist
  \item
    entire sample (making no distinction between groups)
  \item
    configural invariance
  \item
    weak invariance
  \item
    strong invariance
  \item
    strict invariance
  \end{itemize}
\item
  Create an APA style results section with appropriate table(s) and figure(s)
\item
  Talk about it with someone
\end{itemize}

\hypertarget{readings-resources-9}{%
\subsection{Readings \& Resources}\label{readings-resources-9}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

Byrne, B. M. (2016). Adaptation of assessment scales in cross-national research: Issues, guidelines, and caveats. \emph{International Perspectives in Psychology: Research, Practice, Consultation, 5}(1), 51--65. \url{https://doi.org/10.1037/ipp0000042}

Conover, K. J., Israel, T., \& Nylund-Gibson, K. (n.d.). Development and Validation of the Ableist Microaggressions Scale. The Counseling Psychologist, 30.

\begin{itemize}
\tightlist
\item
  Our research vignette for this lesson
\end{itemize}

Hirschfeld, G., \& von Brachel, R. (2014). Multiple-Group confirmatory factor analysis in R -- A tutorial in measurement invariance with continuous and ordinal indicators. 19(7), 12.

Kline, R. (2016). Principles and practice of structural equation modeling (Fourth ed., Methodology in the social sciences). New York: The Guilford Press.

\begin{itemize}
\tightlist
\item
  Chapter 16: Multiple-Samples Analysis and Measurement Invariance
\end{itemize}

Rosseel, Y. (2019). The \emph{lavaan} tutorial. Belgium: Department of Data Analysis, Ghent University. \url{http://lavaan.ugent.be/tutorial/tutorial.pdf}

\begin{itemize}
\tightlist
\item
  Section 8/Multiple groups
\end{itemize}

\hypertarget{packages-10}{%
\subsection{Packages}\label{packages-10}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# will install the package if not already installed}
\CommentTok{\# if(!require(MASS))\{install.packages(\textquotesingle{}MASS\textquotesingle{})\}}
\CommentTok{\# if(!require(sjstats))\{install.packages(\textquotesingle{}sjstats\textquotesingle{})\}}
\CommentTok{\# if(!require(psych))\{install.packages(\textquotesingle{}psych\textquotesingle{})\}}
\CommentTok{\# if(!require(tidyverse))\{install.packages(\textquotesingle{}tidyverse\textquotesingle{})\}}
\CommentTok{\# if(!require(lavaan))\{install.packages(\textquotesingle{}lavaan\textquotesingle{})\}}
\CommentTok{\# if(!require(semPlot))\{install.packages(\textquotesingle{}semPlot\textquotesingle{})\}}
\CommentTok{\# if(!require(semTable))\{install.packages(\textquotesingle{}semTable\textquotesingle{})\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{invariance-testing-aka-multiple-samples-sem-or-multiple-group-cfa-mg-cfa}{%
\section{Invariance Testing (aka Multiple-Samples SEM or Multiple-Group CFA {[}MG-CFA{]})}\label{invariance-testing-aka-multiple-samples-sem-or-multiple-group-cfa-mg-cfa}}

\hypertarget{introducing-the-topic-and-the-terminology}{%
\subsection{Introducing the Topic and the Terminology}\label{introducing-the-topic-and-the-terminology}}

As we enter this topic of \emph{invariance} it will be useful to take a few moments to look at the definitions. It is common for the use of the terms ``invariance'' and ``noninvariance'' to be confusing. As we look at each of these definitions I have often wondered why we don't start with ``variant'' or ``variation.''

\begin{itemize}
\item
  \textbf{Variance}: is not a term we are using today. However, recalling the notions of \emph{variable} (which contrasts with \emph{constant}), \emph{variation}, \emph{variability} may help you with \emph{invariance} (which means, it doesn't vary).
\item
  \textbf{Invariance} is synonymous with equivalence. That is, there are not statistically significant differences between the two versions/models being compared.
\item
  \textbf{Noninvariance} is synonymous with nonequivalence. That is, there are statistically significant differences between the two versions/models being compared
\item
  \textbf{Equality constraints} are imposed by the researcher when we specify (require) two or more parameters to be equal. The particular constraints could be placed on between factor loadings, covariances between factors, intercepts, error variances, error covariances, and so forth. Such constraints simplify the analysis because only one coefficient is needed rather than two. In a multiple-samples/groups, like invariance testing, a cross-group equality constraint forces the computer to derive equal estimates of the same parameter across all groups. This specification corresponds to the null hypothesis that the parameter is equal in all populations from which the samples are drawn. We then conduct formal difference tests to see if, in fact, the model fit is worse when the two groups are constrained to be equal on that parameter (or more likely, set of parameters).
\end{itemize}

\textbf{Measurement invariance} is a property when a set of indicators measures the same constructs with equal precision over different samples.

A scale is said to have measurement invariance (or, measurement equivalence) across groups if respondents with identical levels of the latent construct have the same expected raw scores on the measure \citep{hirschfeld_multiple-group_2014}.

We can think of this in several ways:

\begin{itemize}
\item
  whether values of model parameters of substantive interest vary in meaningful ways across different samples,
\item
  as an interaction -- whether sample membership \emph{moderates} the relations specified in a model; if there is evidence for a \emph{group x model} interaction, then the program must be allowed to derive separate estimates of some parameters in each sample in order for the model to have acceptable fit over all samples involved,
\item
  whether scores from the operationalization of a construct have the same meaning under differing conditions

  \begin{itemize}
  \tightlist
  \item
    these conditions could involve time of measurement, test administration methods, or populations (national samples, clinical/community samples, children/adults, and so forth)
  \item
    absence of invariance says that findings of differences between persons cannot be unambiguously isolated from differences owing to time, methods, group membership (thus, there would be no clear basis for drawing inferences from the scores)
  \end{itemize}
\end{itemize}

\textbf{Longitudinal measurement invariance} evaluates the stability in measurement parameters over time for the same population.

\textbf{Method invariance} is concerned with whether different methods of administration (online survey versus paper/pencil) are invariant.

My experience with invariance testing is the multiple language/cross-cultural/international context. Here we often ask, ``What makes a test culturally transferable?'' Byrne's \citeyearpar{byrne_adaptation_2016} article provides a current, excellent, thorough review. Highlights include:

\begin{itemize}
\tightlist
\item
  In the past we could claim that a test was culturally adaptive if it involved

  \begin{itemize}
  \tightlist
  \item
    Translation/backtranslation
  \item
    Replication of factor structure within the culture
  \item
    Replication of validity and reliability estimates
  \end{itemize}
\item
  Today, there is a movement toward testing adaptation

  \begin{itemize}
  \tightlist
  \item
    Including all the past steps, PLUS
  \item
    Invariance testing to explore the factor structure across cultures
  \item
    Investigation of item bias and construct relations
  \end{itemize}
\end{itemize}

In this lesson we will focus rather narrowly on Byrne's \citeyearpar{byrne_adaptation_2016} strategy for the statistical/psychometric evaluation of invariance. You might also be interested in Gerstein's, Systematic Test of Equivalence Procedure (STEP; \citeyearpar{gerstein_theory_2021}), which walks the researcher, item-by-item, through six step analysis of the cultural appropriateness of each item. The researcher is prompted to consider why items are and are not appropriate/relevant and how they might be modified.

\hypertarget{evaluation-strategies}{%
\subsection{Evaluation Strategies}\label{evaluation-strategies}}

There are two primary options for establishing multigroup invariance.

\hypertarget{free-baseline-approach}{%
\subsubsection{Free baseline approach}\label{free-baseline-approach}}

In the free baseline approach, testing for measurement invariance is a hierarchical, \emph{model trimming}, strategy. Specifically, the \emph{configural model} (the initial, unconstrained model) is gradually restricted by adding cross-group equality constraints in a sequence that corresponds to \emph{weak}, \emph{strong}, and \emph{strict} invariance. At the point that the invariance hypothesis cannot be retained, testing stops (i.e., more restricted models are not considered).

In the free baseline approach:

\begin{itemize}
\tightlist
\item
  Respecification moves from nesting to nested models.
\item
  Fit generally worsens in each subsequent model.
\item
  The goal is non-significant differences in fit with each additional set of cross-group equality constraints.
\end{itemize}

\hypertarget{constrained-baseline-approach}{%
\subsubsection{Constrained baseline approach}\label{constrained-baseline-approach}}

In the constrained baseline approach, testing for measurement invariance is a \emph{model building} approach where the most restricted model (\emph{strict}; with equal pattern coefficients, intercepts, and residuals) is the baseline. If necessary, these are sequentially released and compared backwards through the hierarchy (\emph{strong}, \emph{weak}, \emph{configural}) but some researchers will switch around the order in which constraints are released.

In the constrained baseline approach:

\begin{itemize}
\tightlist
\item
  Respecification moves from nested to nesting models.
\item
  Fit generally improves in each subsequent model
\item
  The goal is to have satisfactory fit in the most restricted model (but this often is not the case).
\end{itemize}

Ideally-and-theoretically, model trimming and building approaches will end up in the same place, but this is not guaranteed.

\hypertarget{invariance-testing-workflow}{%
\subsection{Invariance Testing Workflow}\label{invariance-testing-workflow}}

Today we will will use the \emph{free baseline} approach in testing the measurement equivalence of a scale across two groups.

\begin{figure}
\centering
\includegraphics{images/Invariance/Workflow_Invariance.png}
\caption{Image of a flowchart and decision-tree for multi-group invariance testing}
\end{figure}

Multigroup invariance testing involves:

\begin{itemize}
\tightlist
\item
  Structuring up the item-level dataset (i.e., reverse-coding any variables)
\item
  For the groups-of-interest, identifying a common \emph{baseline model} that meets acceptable standards for model fit. These standards include:

  \begin{itemize}
  \tightlist
  \item
    theoretical and statistical identification
  \item
    appropriate magnitude and direction of factor loadings.
  \end{itemize}
\item
  Specifying and comparing a series of increasingly restrictive models. These models include:

  \begin{itemize}
  \tightlist
  \item
    Configural invariance (the same CFA structure)
  \item
    Weak invariance (configural + pattern/factor loadings)
  \item
    Strong invariance (weak + item intercepts)
  \item
    Strict (strong + error variances and covariances)
  \end{itemize}
\item
  At the point (i.e., the model) that fit is unacceptable, consider investigating and reporting the source of partial measurement invariance.
\end{itemize}

\hypertarget{successive-gradations-of-measurement-invariance}{%
\subsection{Successive Gradations of Measurement Invariance}\label{successive-gradations-of-measurement-invariance}}

There are four levels of invariance. Invariance testing is a sequential and incremental process. Thus, each successive level of variance is prerequisite on meeting the criteria of the prior. Different authors use different names for these same notions.

\hypertarget{configural-invariance}{%
\subsubsection{Configural invariance}\label{configural-invariance}}

Configural invariance is the least restrictive level. It implies that the number of latent variables and the pattern of loadings on the latent variables on indicators are similar across the groups.

Configural invariance is tested by specifying the same CFA model in each group. Both the number of factors and the correspondence between factors and indicators are the same, but all parameters are freely estimated in each group.

\begin{itemize}
\item
  If this model is not consistent with the data, then measurement invariance does not hold at any subsequent level.
\item
  If the model is retained, it says that the same factors are manifested (in potentially different ways) in each group. Differences could include:

  \begin{itemize}
  \tightlist
  \item
    unequal pattern coefficients
  \item
    unequal intercepts
  \item
    unequal error variances
  \end{itemize}
\item
  If there is only configural variance, a different weighting scheme would be needed for each group.
\end{itemize}

\hypertarget{weak-invariance}{%
\subsubsection{Weak invariance}\label{weak-invariance}}

Weak invariance is sometimes termed \emph{pattern invariance} and \emph{metric invariance}. Weak invariance requires \emph{configural variance} plus equality of unstandardized pattern coefficients. That is, the magnitude of the loadings is similar across the groups.

The hypothesis of weak invariance is tested by:

\begin{itemize}
\tightlist
\item
  Imposing an equality constraint over groups on the unstandardized coefficient of each indicator. Then
\item
  Comparing with the chi-square difference (\(\chi_{D}^{2}\)) test the configural invariance model and the weak invariance model

  \begin{itemize}
  \tightlist
  \item
    In comparing models, if the fit of more restrictive invariance model tested \emph{is not appreciably worse} than the immediately prior restrictive model, then the more restrictive weak invariance hypothesis is retained.
  \item
    Thus, the weak invariance model would be compared to the configural invariance model. IF we use the \(\chi_{D}^{2}\) \textgreater{} .05 (and we'll learn later that there are better/more options), then we can claim weak invariance.
  \item
    Think back to what we learned about comparing nested/hierarchical models. As we continue through this invariance \emph{hierarchy} (configural, weak, strong, strict), each of the more restrictive measures will have worse fit (the prior, lesser restrictive model will be the nesting model with ``more sticks'' and fewer degrees of freedom). Therefore, we would really like there to be NO DIFFERENCE in model fit when we add between-group equality constraints.
  \end{itemize}
\item
  If weak invariance is supported, then we can claim that constructs are manifested the same way in each group. This means that:

  \begin{itemize}
  \tightlist
  \item
    slopes from regressing the indicators on their respective factors are equal across all groups, and
  \item
    factor scores can be calculated using the same weighting scheme in all groups tested
  \end{itemize}
\item
  If weak invariance is rejected, then\ldots{}

  \begin{itemize}
  \tightlist
  \item
    the factors (or at least a subset of items corresponding to those factors) have different meanings in different groups
  \item
    \emph{extreme response styles (ERS)} may affect response variability, for example, low ERS is the tendency to avoid endorsing the most extreme options (e.g., never, always); high ERS is the tendency to endorse the most extreme options
  \end{itemize}
\item
  If we can support weak invariance, we are justified in formally comparing estimated factor variances or covariances across different groups, but because indicators are affected by both factors and sources of unique (residual) variation, we need MORE in order to statistically compare observed variances or covariances over groups. This comes from the next level.
\end{itemize}

\hypertarget{strong-invariance}{%
\subsubsection{Strong invariance**}\label{strong-invariance}}

Strong invariance is also termed \emph{scalar invariance}; it is predicated on weak invariance. Strong invariance implies that the item loadings plus the item intercepts are similar across the groups. It also implies that there are no systematic response biases. It is required in order to meaningfully compare the means of latent variables across different groups.

\begin{itemize}
\tightlist
\item
  Item intercepts are considered to be the origin or starting value of the scale that your factor is based on. Thus, participants who have the same value on the latent construct should have equal values on the items on which the construct is based. These are related to the mean structure, hence you'll see some refer to this as means.
\item
  The intercept estimates the score on an indicator given a true score of zero on the corresponding factor
\item
  \emph{Equality of intercepts} says that different groups use the response scale of that indicator in the same way; that is, a person from one group and a person from a different group with the same level on the factor should obtain the same score on the indicator.
\end{itemize}

The hypothesis of strong invariance is tested by:

\begin{itemize}
\tightlist
\item
  Imposing equality constraints on unstandardized pattern coefficients and intercepts. Then,
\item
  Comparing this model with the model of the equality-constrained pattern coefficients (i.e., the weak invariance model) with the \(\chi_{D}^{2}\)

  \begin{itemize}
  \tightlist
  \item
    If the fit of the more restrictive invariance model tested \emph{is not appreciably worse} than the immediately prior restrictive model, then the more restrictive weak invariance hypothesis is retained.\\
  \item
    Thus, the strong invariance model would be compared to the weak invariance model. If \(\chi_{D}^{2}\) \textgreater{} .05, then we can claim strong invariance.
  \end{itemize}
\item
  If strong invariance is rejected, then we may be concerned about a \emph{differential additive (acquiescence) response style}: systematic influences unrelated to the factors that decrease or increase the overall level of responding on an indicator in a particular population

  \begin{itemize}
  \tightlist
  \item
    Example: if patients are weighed in street clothes in the clinic and in a gown at the hospital, an additive constant is added to true body weight, dependent upon where patients are tested; this contaminates the estimates of mean weight differences over the two clinics.\\
  \item
    If a response style affects all indicators, then invariance testing will not detect this pattern; instead the estimates of the construct will be influenced by response styles that are uniform over all indicators.
  \end{itemize}
\item
  \textbf{Differential item functioning} is the pattern that an indicator has appreciably unequal pattern coefficients or intercepts over groups; DIF violates measurement invariance.

  \begin{itemize}
  \tightlist
  \item
    A goal in multiple-samples CFA is to locate the indicator(s) responsible for rejecting the hypothesis of weak or strong invariance
  \item
    In test development, we flag items as candidates for revision or deletion
  \end{itemize}
\item
  If strong invariance is supported

  \begin{itemize}
  \tightlist
  \item
    group differences in estimated factor means will be unbiased\\
  \item
    group differences in indicators means or estimated factor scores will be directly related to the factor means and will not be distorted by differential additive response bias\\
  \item
    the factors have a common meaning over groups and any constant effects on the indicators are cancelled out when observed means are compared over groups\\
  \item
    strong invariance is the minimal level required for meaningful interpretation of group mean contrasts
  \end{itemize}
\end{itemize}

\hypertarget{strict-invariance}{%
\subsubsection{Strict invariance}\label{strict-invariance}}

Strict invariance requires strong invariance plus the equality in error variances and covariances across groups. This means that the indicators measure the same factors in each group with the same degree of precision. There are some rifts about what exactly constitutes strict invariance:

\begin{itemize}
\tightlist
\item
  residual invariance is required in order to claim that factors are measured identically across group (Deshon, 2004; Wu et al., 2007)\\
\item
  Because unique (residual) error variance reflects random measurement error and systematic variance, the sum of these two components must be equal across groups (Little, 2013). Kline \citeyearpar{kline_principles_2016} says that it may be too strict and somewhat unreasonable/unattainable. Little (2013) also cautioned against enforcing this requirement because if the sum of random and systematic parts of unique variance is not exactly equal, the amount of misfit due to equality-constrained residual variances must contaminate estimates elsewhere in the model.
\end{itemize}

\hypertarget{tests-for-model-comparison}{%
\subsection{Tests for Model Comparison}\label{tests-for-model-comparison}}

It is not sufficient to declare any level of invariance (i.e., constrained (configural, weak, string, or strong) to be adequate on the basis of the traditional evaluation of fit (i.e., strength and significance of factor loadings, fit indices). Rather the whole models must be compared through formal statistical comparison. There are several options and they all have caveats.

A non-significant chi-square difference test (\(\chi_{D}^{2} > .05\)) that compares less-and-more restrictive models indicates that the stricter invariance hypothesis should \emph{not} be rejected. That is, it supports invariance for the more restricted model.

\begin{itemize}
\tightlist
\item
  In large samples, this could be statistically significant, even though the absolute differences in parameter estimates are trivial.
\item
  Thus, the \(\chi_{D}^{2}\) could indicate lack of measurement invariance when the imposition of cross-group equality constraints makes relatively little difference in fit. Options for verifying:

  \begin{itemize}
  \tightlist
  \item
    compare unstandardized solutions across groups
  \item
    inspect changes in approximate fit indices
  \item
    BUT\ldots there are few guidelines for how to do this
  \end{itemize}
\end{itemize}

When the CFA change statistic is smaller than .01 (\(\Delta CFI < .01\)) there is evidence that the stricter invariance hypothesis should \emph{not} be rejected. That is, it supports invariance for the more restricted model.

\begin{itemize}
\tightlist
\item
  Simulation studies suggested that stability for different model characteristics such as number of indicators per factor. Here are some findings (guidelines?) for different testing scenarios:

  \begin{itemize}
  \tightlist
  \item
    In super large samples (\textasciitilde6,000) use \(\Delta CFI < .002\).
  \item
    When group sizes are small (\(n < 300\)) and unequal, use \(\Delta CFI < .005\) and \(\Delta RMSEA < .010\).
  \item
    When group sizes are larger (\(n > 300\)), equal, and the pattern of invariance was mixed (i.e., there are at least two invariant parameters, each of which is from a different category {[}pattern coefficient, intercept, residual variance{]}), use \(\Delta CFI < .010\) and \(\Delta RMSEA < .015\).
  \end{itemize}
\end{itemize}

\(\Delta NCI\) was also stable, but Kline \citeyearpar{kline_principles_2016} did not provide a threshold (and I don't see the NCI reported much in psychometrics papers.

The general practice seems to favor reporting both the \(\chi_{D}^{2}\) and \(\Delta CFI\). Even if \(\chi_{D}^{2} > .05\), a \(\Delta CFI < .01\) supports invariance between models.

\hypertarget{partial-measurement-invariance}{%
\subsection{Partial measurement invariance}\label{partial-measurement-invariance}}

The notion of partial measurement invariance was introduced by Byrne, Shavelson, and Muthen (1989) and is often used to describe an \emph{intermediate} state of invariance. For example, weak invariance assumes cross-group equality of each unstandardized pattern coefficient. If some, but not all pattern coefficients are invariant, then only \emph{partial weak invariance} can be claimed.

The researcher may investigate which pattern coefficients are noninvariant and relax (or free) them to differ across groups. Once freed, the research might choose to compare the models with the \(\chi_{D}^{2} > .05\), a \(\Delta CFI < .01\). Once enough pattern coefficients have been freed and the fit across models is equivalent, the researcher might continue the process of determining the degree of invariance in the more restricted evaluations (e.g., strong, strict).

Even if the researcher does not continue with testing for invariance in the increasingly restricting models, they have learned which pattern coefficients vary across groups.

\hypertarget{research-vignette-9}{%
\section{Research Vignette}\label{research-vignette-9}}

This lesson's research vignette emerges from Conover et al's Ableist Microaggressions Scale (AMS \citeyearpar{conover_development_2017}). The article reports on a series of three studies comprised the development, refinement, and psychometric evaluation of the AMS. I simulated data from the results of the exploratory factor analysis in the second study.

Conover et al. \citeyearpar{conover_development_2017} reported support for using a total scale score (22 items) or four, correlated, subscales. Below, I list the four subscales, their number of items, and a single example item. At the outset, let me provide a content warning. For those who hold this particular identity (or related identities) the content in the items may be upsetting. In other lessons, I often provide a variable name that gives an indication of the primary content of the item. In the case of the AMS, I will simply provide an abbreviation of the subscale name and its respective item number. This will allow us to easily inspect the alignment of the item with its intended factor, and hopefully minimize discomfort. If you are not a member of this particular identity, I encourage you to learn about these microaggressions by reading the article in its entirety. Please do not ask members of this group to explain why these microaggressions are harmful or ask if they have encountered them.

There are 20 items on the AMS scale. The frequency scaling ranged from 0(\emph{never}) to 5(\emph{very frequently}). Higher scores indicate higher frequency of microaggressions.

The four factors, number of items, and sample item are as follows:

\begin{itemize}
\tightlist
\item
  Helplessness (5 items)

  \begin{itemize}
  \tightlist
  \item
    ``People feel they need to do something to help me because I have a disability.''
  \item
    Abbreviated in the simulated data as ``Help\#''
  \end{itemize}
\item
  Minimization (3 items)

  \begin{itemize}
  \tightlist
  \item
    ``People minimize my disability or suggest that it could be worse.''
  \item
    Abbreviated in the simulated data as ``Min\#''
  \end{itemize}
\item
  Denial of Personhood (5 items)

  \begin{itemize}
  \tightlist
  \item
    ``People don't see me as a whole person because I have a disability.''
  \item
    Abbreviated in the simulated data as ``Pers\#''
  \end{itemize}
\item
  Otherization (7 items)

  \begin{itemize}
  \tightlist
  \item
    ``People indicate that they would not date a person with a disability.''
  \item
    Abbreviated in the simulated data as ``Oth\#''
  \end{itemize}
\end{itemize}

In the simulation below, I use the same factor loadings from the EFA and correlations between factors for both the mild and severe groupings. I use the reported means for the mild group, and arbitrarily make them higher for the severe group.

Below I walk through the data simulation. This is not an essential portion of the lesson, but I will lecture it in case you are interested. None of the items are negatively worded (relative to the other items), so there is no need to reverse-score any items.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulating the data for the respondents with mild disability}
\NormalTok{AMSmild\_generating\_model }\OtherTok{\textless{}{-}} \StringTok{"}
\StringTok{        \#measurement model}
\StringTok{        Help  =\textasciitilde{} .74*Help1 + .75*Help2 + .65*Help3 + .58*Help4 + .62*Help5}
\StringTok{        Minim =\textasciitilde{} .71*Min1 + .52*Min2 + .47*Min3}
\StringTok{        Person =\textasciitilde{} .71*Per1 + .84*Per2 + .74*Per3 + .56*Per4 + .42*Per5 }
\StringTok{        Other =\textasciitilde{} .89*Oth1 + .73*Oth2 + .70*Oth3 + .46*Oth4 + .41*Oth5 + .40*Oth6 + .32*Oth7}

\StringTok{        \#Means}
\StringTok{         Help \textasciitilde{} 1.96*1}
\StringTok{         Minim \textasciitilde{} 2.76*1}
\StringTok{         Person \textasciitilde{} 1.51*1}
\StringTok{         Other \textasciitilde{} 1.17*1}
\StringTok{        }
\StringTok{         \#Correlations}
\StringTok{         Help \textasciitilde{}\textasciitilde{} .27*Minim}
\StringTok{         Help \textasciitilde{}\textasciitilde{} .66*Person}
\StringTok{         Help \textasciitilde{}\textasciitilde{} .68* Other}
\StringTok{         Minim \textasciitilde{}\textasciitilde{} .36*Person}
\StringTok{         Minim \textasciitilde{}\textasciitilde{} .30*Other}
\StringTok{         Person \textasciitilde{}\textasciitilde{} .76*Other}
\StringTok{       }
\StringTok{         "}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240504}\NormalTok{)}
\NormalTok{AMSmild }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{simulateData}\NormalTok{(}\AttributeTok{model =}\NormalTok{ AMSmild\_generating\_model, }\AttributeTok{model.type =} \StringTok{"sem"}\NormalTok{,}
    \AttributeTok{meanstructure =}\NormalTok{ T, }\AttributeTok{sample.nobs =} \DecValTok{548}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# Adding a variable that denotes mild condition}

\NormalTok{AMSmild}\SpecialCharTok{$}\NormalTok{Group }\OtherTok{\textless{}{-}} \StringTok{"Mild"}


\NormalTok{AMSsev\_generating\_model }\OtherTok{\textless{}{-}} \StringTok{"}
\StringTok{        \#measurement model}
\StringTok{        Help  =\textasciitilde{} .68*Help1 + .76*Help2 + .57*Help3 + .62*Help4 + .72*Help5}
\StringTok{        Minim =\textasciitilde{} .75*Min1 + .59*Min2 + .51*Min3}
\StringTok{        Person =\textasciitilde{} .69*Per1 + .79*Per2 + .79*Per3 + .58*Per4 + .51*Per5 }
\StringTok{        Other =\textasciitilde{} .72*Oth1 + .71*Oth2 + .75*Oth3 + .51*Oth4 + .51*Oth5 + .36*Oth6 + .42*Oth7}

\StringTok{        \#means}
\StringTok{         Help \textasciitilde{} 3.5*1}
\StringTok{         Minim \textasciitilde{} 3.3*1}
\StringTok{         Person \textasciitilde{} 3.01*1}
\StringTok{         Other \textasciitilde{} 2.32*1}

\StringTok{        \#correlations}
\StringTok{         Help \textasciitilde{}\textasciitilde{} .31*Minim}
\StringTok{         Help \textasciitilde{}\textasciitilde{} .44*Person}
\StringTok{         Help \textasciitilde{}\textasciitilde{} .55* Other}
\StringTok{         Minim \textasciitilde{}\textasciitilde{} .40*Person}
\StringTok{         Minim \textasciitilde{}\textasciitilde{} .35*Other}
\StringTok{         Person \textasciitilde{}\textasciitilde{} .49*Other}
\StringTok{"}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240504}\NormalTok{)}
\NormalTok{AMSsev }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{simulateData}\NormalTok{(}\AttributeTok{model =}\NormalTok{ AMSsev\_generating\_model, }\AttributeTok{model.type =} \StringTok{"sem"}\NormalTok{,}
    \AttributeTok{meanstructure =}\NormalTok{ T, }\AttributeTok{sample.nobs =} \DecValTok{285}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\# used to retrieve column indices used in the rescaling script below}

\CommentTok{\# Adding a variable that denotes severe condition}
\NormalTok{AMSsev}\SpecialCharTok{$}\NormalTok{Group }\OtherTok{\textless{}{-}} \StringTok{"Severe"}

\CommentTok{\# Binding the separate groups together in a single file}

\NormalTok{dfAMSi }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{bind\_rows}\NormalTok{(AMSmild, AMSsev)}

\NormalTok{col\_index }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(dfAMSi))}

\CommentTok{\# The code below loops through each column of the dataframe and}
\CommentTok{\# assigns the scaling accordingly All rows are the iBel scales,}
\CommentTok{\# administrations A and B}


\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(dfAMSi)) \{}
    \ControlFlowTok{if}\NormalTok{ (i }\SpecialCharTok{\textgreater{}=} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{20}\NormalTok{) \{}
\NormalTok{        dfAMSi[, i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(dfAMSi[, i], }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{    \}}
\NormalTok{\}}

\CommentTok{\# Now round to zero}
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{dfAMSi }\OtherTok{\textless{}{-}}\NormalTok{ dfAMSi }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate\_if}\NormalTok{(is.numeric, round, }\AttributeTok{digits =} \DecValTok{0}\NormalTok{)}

\CommentTok{\# quick check of my work psych::describe(dfAMSi)}
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either a .csv file (think ``Excel lite'') or .rds object (preserves any formatting you might do). If you save the .csv file and bring it back in, you will lose any formatting (e.g., ordered factors will be interpreted as character variables).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(dfAMSi,}
\CommentTok{\# file=\textquotesingle{}dfAMSi.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE) bring}
\CommentTok{\# back the simulated dat from a .csv file dfAMSi \textless{}{-} read.csv}
\CommentTok{\# (\textquotesingle{}dfAMSi.csv\textquotesingle{}, header = TRUE) str(dfAMSi)}
\end{Highlighting}
\end{Shaded}

In this lesson I made the Severity variable a factor during the simulation. Importing the exported .csv file will lose that formating. Therefore, unless you need to use a .csv file outside of R, I recommend using the .rds file.

An .rds file preserves all formatting to variables prior to the export and re-import. If you already exported/imported the .csv file, you will need to re-run the simulation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(dfAMSi, \textquotesingle{}dfAMSi.rds\textquotesingle{}) bring back the simulated dat}
\CommentTok{\# from an .rds file}
\NormalTok{dfAMSi }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"dfAMSi.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's check the structure of the data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(dfAMSi)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   833 obs. of  21 variables:
 $ Help1: int  2 4 1 2 2 3 2 2 1 1 ...
 $ Help2: int  2 2 0 2 3 3 3 3 1 4 ...
 $ Help3: int  3 2 1 2 3 1 3 2 2 3 ...
 $ Help4: int  3 2 1 4 2 2 3 3 2 3 ...
 $ Help5: int  2 3 2 3 2 2 1 3 2 2 ...
 $ Min1 : int  2 4 1 2 3 3 3 3 3 3 ...
 $ Min2 : int  3 4 1 2 2 4 3 2 3 2 ...
 $ Min3 : int  3 3 3 1 2 3 2 3 4 3 ...
 $ Per1 : int  2 2 1 2 1 3 2 2 2 2 ...
 $ Per2 : int  3 3 1 2 2 3 3 3 1 2 ...
 $ Per3 : int  3 3 2 3 2 3 2 3 2 3 ...
 $ Per4 : int  3 4 1 2 3 2 3 2 3 2 ...
 $ Per5 : int  3 4 2 1 2 2 2 3 2 3 ...
 $ Oth1 : int  3 3 2 3 4 3 4 3 2 3 ...
 $ Oth2 : int  2 2 2 4 3 3 2 3 3 3 ...
 $ Oth3 : int  2 2 1 3 4 3 3 2 2 2 ...
 $ Oth4 : int  2 3 2 3 2 2 1 3 1 3 ...
 $ Oth5 : int  3 2 2 2 2 3 2 3 2 2 ...
 $ Oth6 : int  3 2 2 3 4 2 3 3 2 2 ...
 $ Oth7 : int  2 2 2 3 3 2 3 3 3 3 ...
 $ Group: chr  "Mild" "Mild" "Mild" "Mild" ...
\end{verbatim}

We need ``Group'' to be a factor.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dfAMSi[, }\StringTok{"Group"}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(dfAMSi[, }\StringTok{"Group"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\hypertarget{whole-group-and-baseline-analyses}{%
\section{Whole-Group and Baseline Analyses}\label{whole-group-and-baseline-analyses}}

Conover et al.\citeyearpar{conover_development_2017} conducted the invariance testing with the four-factor, correlated factors model. Let's start by simply by creating an overall measurement model from the dataset without regard to group membership.

\hypertarget{whole-group-cfa}{%
\subsection{Whole Group CFA}\label{whole-group-cfa}}

With the number of items per scale ranging from 3 to 7 on this multidimensional, first-order, factor structure we are sufficiently \emph{identified}. Remember, rule is at least 3 items/indicators per factor for unidimensional scales and 2 items/indicators per factor for a multidimensional scale.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240504}\NormalTok{)}
\NormalTok{AMS4CorrMod }\OtherTok{\textless{}{-}} \StringTok{"}
\StringTok{            Helplessness =\textasciitilde{} Help1 + Help2 + Help3 + Help4 + Help5}
\StringTok{            Minimization =\textasciitilde{} Min1 + Min2 + Min3}
\StringTok{            DenialPersonhood =\textasciitilde{} Per1 + Per2 + Per3 + Per4 + Per5 }
\StringTok{            Otherization =\textasciitilde{} Oth1 + Oth2 + Oth3 + Oth4 + Oth5 + Oth6 + Oth7}
\StringTok{"}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240504}\NormalTok{)}
\NormalTok{AMS4CorrFit }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(AMS4CorrMod, }\AttributeTok{data =}\NormalTok{ dfAMSi)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(AMS4CorrFit, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 48 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        46

  Number of observations                           833

Model Test User Model:
                                                      
  Test statistic                               155.238
  Degrees of freedom                               164
  P-value (Chi-square)                           0.676

Model Test Baseline Model:

  Test statistic                              3297.768
  Degrees of freedom                               190
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    1.000
  Tucker-Lewis Index (TLI)                       1.003

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)             -19314.357
  Loglikelihood unrestricted model (H1)     -19236.738
                                                      
  Akaike (AIC)                               38720.715
  Bayesian (BIC)                             38938.066
  Sample-size adjusted Bayesian (SABIC)      38791.986

Root Mean Square Error of Approximation:

  RMSEA                                          0.000
  90 Percent confidence interval - lower         0.000
  90 Percent confidence interval - upper         0.013
  P-value H_0: RMSEA <= 0.050                    1.000
  P-value H_0: RMSEA >= 0.080                    0.000

Standardized Root Mean Square Residual:

  SRMR                                           0.023

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness =~                                                          
    Help1                1.000                               0.433    0.549
    Help2                1.338    0.102   13.082    0.000    0.579    0.658
    Help3                1.186    0.098   12.049    0.000    0.514    0.574
    Help4                1.225    0.099   12.353    0.000    0.531    0.597
    Help5                1.401    0.107   13.081    0.000    0.607    0.657
  Minimization =~                                                          
    Min1                 1.000                               0.452    0.545
    Min2                 0.825    0.121    6.825    0.000    0.373    0.455
    Min3                 0.734    0.112    6.560    0.000    0.332    0.411
  DenialPersonhood =~                                                      
    Per1                 1.000                               0.532    0.590
    Per2                 1.043    0.074   14.073    0.000    0.555    0.675
    Per3                 1.052    0.075   14.012    0.000    0.559    0.670
    Per4                 0.810    0.069   11.774    0.000    0.431    0.521
    Per5                 0.783    0.072   10.817    0.000    0.416    0.467
  Otherization =~                                                          
    Oth1                 1.000                               0.553    0.629
    Oth2                 0.875    0.067   13.058    0.000    0.484    0.582
    Oth3                 0.946    0.070   13.527    0.000    0.523    0.611
    Oth4                 0.807    0.070   11.486    0.000    0.446    0.494
    Oth5                 0.622    0.059   10.581    0.000    0.344    0.448
    Oth6                 0.571    0.060    9.466    0.000    0.316    0.394
    Oth7                 0.621    0.063    9.897    0.000    0.344    0.414

Covariances:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness ~~                                                          
    Minimization         0.079    0.014    5.829    0.000    0.406    0.406
    DenialPersonhd       0.176    0.018    9.754    0.000    0.764    0.764
    Otherization         0.163    0.017    9.520    0.000    0.681    0.681
  Minimization ~~                                                          
    DenialPersonhd       0.116    0.018    6.590    0.000    0.481    0.481
    Otherization         0.126    0.018    6.838    0.000    0.503    0.503
  DenialPersonhood ~~                                                      
    Otherization         0.205    0.021    9.909    0.000    0.698    0.698

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .Help1             0.434    0.024   18.210    0.000    0.434    0.698
   .Help2             0.441    0.027   16.490    0.000    0.441    0.568
   .Help3             0.537    0.030   17.903    0.000    0.537    0.671
   .Help4             0.509    0.029   17.580    0.000    0.509    0.644
   .Help5             0.483    0.029   16.493    0.000    0.483    0.568
   .Min1              0.483    0.038   12.809    0.000    0.483    0.703
   .Min2              0.532    0.034   15.739    0.000    0.532    0.793
   .Min3              0.542    0.032   16.829    0.000    0.542    0.831
   .Per1              0.531    0.030   17.657    0.000    0.531    0.652
   .Per2              0.367    0.023   16.032    0.000    0.367    0.544
   .Per3              0.383    0.024   16.150    0.000    0.383    0.551
   .Per4              0.498    0.027   18.497    0.000    0.498    0.729
   .Per5              0.620    0.033   18.981    0.000    0.620    0.782
   .Oth1              0.468    0.028   16.664    0.000    0.468    0.605
   .Oth2              0.456    0.026   17.482    0.000    0.456    0.661
   .Oth3              0.459    0.027   16.998    0.000    0.459    0.626
   .Oth4              0.615    0.033   18.574    0.000    0.615    0.756
   .Oth5              0.471    0.025   18.987    0.000    0.471    0.799
   .Oth6              0.544    0.028   19.371    0.000    0.544    0.845
   .Oth7              0.570    0.030   19.236    0.000    0.570    0.828
    Helplessness      0.187    0.024    7.710    0.000    1.000    1.000
    Minimization      0.204    0.038    5.397    0.000    1.000    1.000
    DenialPersonhd    0.283    0.034    8.375    0.000    1.000    1.000
    Otherization      0.306    0.034    8.925    0.000    1.000    1.000
\end{verbatim}

Among my first steps are also to write the code to export the results. The \emph{tidySEM} package has useful functions to export the fit statistics, parameter estimates, and correlations among the latent variables (i.e., factors).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AllFitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(AMS4CorrFit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Registered S3 method overwritten by 'tidySEM':
  method          from  
  predict.MxModel OpenMx
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{All\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(AMS4CorrFit, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{AllCorrs }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_cors}\NormalTok{(AMS4CorrFit, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\CommentTok{\# to see each of the tables, remove the hashtab AllFitStats}
\CommentTok{\# All\_paramEsts AllCorrs}
\end{Highlighting}
\end{Shaded}

Next, I export them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(AllFitStats, }\AttributeTok{file =} \StringTok{"AllFitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(All\_paramEsts, }\AttributeTok{file =} \StringTok{"All\_paramEsts.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(AllCorrs, }\AttributeTok{file =} \StringTok{"AllCorrs.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{interpreting-the-output-6}{%
\subsection{Interpreting the Output}\label{interpreting-the-output-6}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3780}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4512}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1707}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Our Results
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Criteria met?
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Factor loadings significant, strong, proper valence & Help: 0.55 to 0.66; Min: 0.41 to 0.55; Pers: 0.47 to 0.68 Oth: 0.39 to 0.63 & Yes \\
Non-significant chi-square & \(\chi ^{2}(164) = 155.24, p = 0.676\) & Yes \\
\(CFI\geq .95\) & CFI = 1.000 & Yes \\
\(RMSEA\leq .05\) (but definitely \textless{} .10) & RMSEA = 0.000, 90\%CI(0.000, 0.013) & Yes \\
\(SRMR\leq .08\) (but definitely \textless{} .10) & SRMR = 0.023 & Yes \\
Combination rule: \(CFI \geq .95\) and \(SRMR \leq .08\) & CFI = 1.000, SRS = 0.023 & Yes \\
\end{longtable}

\hypertarget{partial-write-up-5}{%
\subsection{Partial Write-up}\label{partial-write-up-5}}

\begin{quote}
\textbf{Correlated factors model for all in sample}. The model where factors were free to covary demonstrated the following fit to the \url{data:$/chi} \^{}\{2\}(164) = 155.24, p = 0.676\$, CFI = 1.000, RMSEA = 0.000, 90\%CI(0.000, 0.013), SRMR = 0.023. Factor loadings ranged from 0.55 to 0.66 for the Helplessness scale, 0.41 to 0.55 for the Minimization scale, 0.47 to 0.68 for the Denial of Personhood scale, and 0.39 to 0.63 for the Otherization scale.
\end{quote}

Producing a figure can be useful to represent what we did to others as well as checking our own work. That is, ``Did we think we did what we intended?'' When the *what = ``col'', whatLabels = ``stand'') combination is shown, paths that are ``fixed'' are represented by dashed lines. Below, we expect to see each the four factors predicting only the items associated with their factor, one item for each factor (the first on the left) should be specified as the indicator variable (and represented with a dashed line), and the factors/latent variables should not be freed to covary (i.e., an uncorrelated traits or orthogonal model). Because they are ``fixed'' to be 0.00, they will be represented with dashed curves with double-headed arrows.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# displays standardized pattern coefficients}
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(AMS4CorrFit, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{12-Invariance_files/figure-latex/unnamed-chunk-11-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# displays estimates/unstandardized regression weights}
\CommentTok{\# semPlot::semPaths(AMS4CorrFit, layout = \textquotesingle{}tree\textquotesingle{}, style = \textquotesingle{}lisrel\textquotesingle{},}
\CommentTok{\# what = \textquotesingle{}col\textquotesingle{}, whatLabels = \textquotesingle{}est\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

Our fit is fairly similar to what Conover et al.~reported in their article. Specifically, their four-factor, correlated factors model, had a statistically significant chi-square. Regarding fit: CFI = .89, SRMR = .07, and RMSEA = .07 CI90\% (.06, .07). As researchers, they were satisfied with the result and they asked the question, ``Is measure invariant across disability severity.'' A first (but not complete) step is to evaluate the model, separately for the groups of interest. In their case it was mild (where they combined mild and moderate levels of severity) and severe (combining severe and very severe levels).

\hypertarget{baseline-model-when-severity-mild}{%
\subsection{Baseline Model when Severity = Mild}\label{baseline-model-when-severity-mild}}

Let's start by subsetting the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mild\_df }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(dfAMSi, Group }\SpecialCharTok{==} \StringTok{"Mild"}\NormalTok{)}
\NormalTok{severe\_df }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(dfAMSi, Group }\SpecialCharTok{==} \StringTok{"Severe"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's run the CFA model for those participants whose data were classified as ``mild.''

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240504}\NormalTok{)}
\NormalTok{MildFit }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(AMS4CorrMod, }\AttributeTok{data =}\NormalTok{ mild\_df)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(MildFit, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 46 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        46

  Number of observations                           548

Model Test User Model:
                                                      
  Test statistic                               165.227
  Degrees of freedom                               164
  P-value (Chi-square)                           0.458

Model Test Baseline Model:

  Test statistic                              1561.967
  Degrees of freedom                               190
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.999
  Tucker-Lewis Index (TLI)                       0.999

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)             -12518.845
  Loglikelihood unrestricted model (H1)     -12436.231
                                                      
  Akaike (AIC)                               25129.689
  Bayesian (BIC)                             25327.778
  Sample-size adjusted Bayesian (SABIC)      25181.755

Root Mean Square Error of Approximation:

  RMSEA                                          0.004
  90 Percent confidence interval - lower         0.000
  90 Percent confidence interval - upper         0.020
  P-value H_0: RMSEA <= 0.050                    1.000
  P-value H_0: RMSEA >= 0.080                    0.000

Standardized Root Mean Square Residual:

  SRMR                                           0.031

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness =~                                                          
    Help1                1.000                               0.435    0.574
    Help2                1.066    0.116    9.227    0.000    0.464    0.581
    Help3                0.966    0.115    8.382    0.000    0.420    0.499
    Help4                0.857    0.110    7.803    0.000    0.373    0.451
    Help5                0.959    0.113    8.473    0.000    0.417    0.507
  Minimization =~                                                          
    Min1                 1.000                               0.529    0.639
    Min2                 0.550    0.137    4.000    0.000    0.291    0.358
    Min3                 0.509    0.129    3.934    0.000    0.269    0.339
  DenialPersonhood =~                                                      
    Per1                 1.000                               0.422    0.501
    Per2                 1.254    0.132    9.487    0.000    0.530    0.691
    Per3                 1.082    0.121    8.912    0.000    0.457    0.597
    Per4                 0.736    0.105    7.007    0.000    0.311    0.405
    Per5                 0.736    0.110    6.717    0.000    0.311    0.382
  Otherization =~                                                          
    Oth1                 1.000                               0.554    0.645
    Oth2                 0.811    0.084    9.691    0.000    0.449    0.542
    Oth3                 0.819    0.083    9.892    0.000    0.454    0.558
    Oth4                 0.662    0.084    7.835    0.000    0.367    0.417
    Oth5                 0.511    0.073    7.040    0.000    0.283    0.369
    Oth6                 0.545    0.076    7.176    0.000    0.302    0.377
    Oth7                 0.448    0.073    6.104    0.000    0.248    0.315

Covariances:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness ~~                                                          
    Minimization         0.057    0.018    3.143    0.002    0.249    0.249
    DenialPersonhd       0.128    0.019    6.889    0.000    0.699    0.699
    Otherization         0.153    0.021    7.266    0.000    0.634    0.634
  Minimization ~~                                                          
    DenialPersonhd       0.088    0.019    4.612    0.000    0.396    0.396
    Otherization         0.098    0.023    4.201    0.000    0.335    0.335
  DenialPersonhood ~~                                                      
    Otherization         0.165    0.023    7.268    0.000    0.705    0.705

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .Help1             0.384    0.029   13.388    0.000    0.384    0.670
   .Help2             0.423    0.032   13.286    0.000    0.423    0.663
   .Help3             0.533    0.037   14.441    0.000    0.533    0.751
   .Help4             0.543    0.036   14.930    0.000    0.543    0.796
   .Help5             0.503    0.035   14.349    0.000    0.503    0.743
   .Min1              0.406    0.071    5.741    0.000    0.406    0.592
   .Min2              0.575    0.041   13.887    0.000    0.575    0.872
   .Min3              0.558    0.039   14.255    0.000    0.558    0.885
   .Per1              0.531    0.036   14.736    0.000    0.531    0.749
   .Per2              0.306    0.027   11.455    0.000    0.306    0.522
   .Per3              0.378    0.028   13.521    0.000    0.378    0.644
   .Per4              0.493    0.032   15.501    0.000    0.493    0.836
   .Per5              0.566    0.036   15.637    0.000    0.566    0.854
   .Oth1              0.431    0.035   12.460    0.000    0.431    0.584
   .Oth2              0.484    0.034   14.178    0.000    0.484    0.706
   .Oth3              0.456    0.033   13.976    0.000    0.456    0.689
   .Oth4              0.640    0.042   15.366    0.000    0.640    0.826
   .Oth5              0.509    0.033   15.667    0.000    0.509    0.864
   .Oth6              0.553    0.035   15.621    0.000    0.553    0.858
   .Oth7              0.561    0.035   15.935    0.000    0.561    0.901
    Helplessness      0.189    0.031    6.153    0.000    1.000    1.000
    Minimization      0.280    0.074    3.774    0.000    1.000    1.000
    DenialPersonhd    0.178    0.033    5.434    0.000    1.000    1.000
    Otherization      0.307    0.043    7.182    0.000    1.000    1.000
\end{verbatim}

Not surprisingly, our results are similar to the total group. I notice that the pattern coefficients wiggle around a little more (one as low as .13) but that the fit indices seem a little stronger.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4658}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3425}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1918}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Mild
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Severe
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Factor loadings: Help & 0.451 to 0.581 & \\
Factor loadings: Min & 0.339 to 0.639 & \\
Factor loadings: Pers & 0.382 to 0.691 & \\
Factor loadings: Oth & 0.315 to 0.645 & \\
Non-significant chi-square & \(p = 0.458\) & \\
\(CFI\geq .95\) & CFI = 0.999 & \\
\(SRMR\leq .08\) (but definitely \textless{} .10) & SRMR = 0.031 & \\
\(RMSEA\leq .05\) (but definitely \textless{} .10) & RMSEA = 0.004, 90\%CI(0.000, 0.020) & \\
\end{longtable}

\hypertarget{baseline-model-when-severity-severe}{%
\subsection{Baseline Model when Severity = Severe}\label{baseline-model-when-severity-severe}}

Let's run the CFA model again for those participants whose data were classified as ``severe.''

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240504}\NormalTok{)}
\NormalTok{SevereFit }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(AMS4CorrMod, }\AttributeTok{data =}\NormalTok{ severe\_df)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(SevereFit, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 57 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        46

  Number of observations                           285

Model Test User Model:
                                                      
  Test statistic                               162.345
  Degrees of freedom                               164
  P-value (Chi-square)                           0.522

Model Test Baseline Model:

  Test statistic                               892.916
  Degrees of freedom                               190
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    1.000
  Tucker-Lewis Index (TLI)                       1.003

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -6567.990
  Loglikelihood unrestricted model (H1)      -6486.817
                                                      
  Akaike (AIC)                               13227.980
  Bayesian (BIC)                             13395.994
  Sample-size adjusted Bayesian (SABIC)      13250.125

Root Mean Square Error of Approximation:

  RMSEA                                          0.000
  90 Percent confidence interval - lower         0.000
  90 Percent confidence interval - upper         0.026
  P-value H_0: RMSEA <= 0.050                    1.000
  P-value H_0: RMSEA >= 0.080                    0.000

Standardized Root Mean Square Residual:

  SRMR                                           0.044

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness =~                                                          
    Help1                1.000                               0.314    0.408
    Help2                1.454    0.285    5.096    0.000    0.457    0.553
    Help3                1.448    0.290    5.001    0.000    0.455    0.527
    Help4                1.313    0.262    5.007    0.000    0.412    0.529
    Help5                1.741    0.325    5.352    0.000    0.546    0.642
  Minimization =~                                                          
    Min1                 1.000                               0.322    0.407
    Min2                 1.151    0.317    3.633    0.000    0.371    0.472
    Min3                 1.331    0.362    3.676    0.000    0.429    0.527
  DenialPersonhood =~                                                      
    Per1                 1.000                               0.488    0.558
    Per2                 0.895    0.145    6.180    0.000    0.437    0.544
    Per3                 1.163    0.171    6.795    0.000    0.568    0.687
    Per4                 0.653    0.130    5.036    0.000    0.319    0.403
    Per5                 0.647    0.144    4.491    0.000    0.316    0.349
  Otherization =~                                                          
    Oth1                 1.000                               0.542    0.621
    Oth2                 0.823    0.114    7.202    0.000    0.446    0.578
    Oth3                 0.807    0.119    6.811    0.000    0.438    0.534
    Oth4                 0.838    0.127    6.619    0.000    0.454    0.514
    Oth5                 0.548    0.097    5.663    0.000    0.297    0.423
    Oth6                 0.566    0.106    5.325    0.000    0.307    0.393
    Oth7                 0.672    0.117    5.738    0.000    0.364    0.429

Covariances:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness ~~                                                          
    Minimization         0.026    0.012    2.100    0.036    0.259    0.259
    DenialPersonhd       0.083    0.021    3.972    0.000    0.543    0.543
    Otherization         0.091    0.022    4.123    0.000    0.535    0.535
  Minimization ~~                                                          
    DenialPersonhd       0.047    0.020    2.403    0.016    0.299    0.299
    Otherization         0.091    0.026    3.464    0.001    0.522    0.522
  DenialPersonhood ~~                                                      
    Otherization         0.126    0.029    4.372    0.000    0.476    0.476

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .Help1             0.494    0.045   10.995    0.000    0.494    0.834
   .Help2             0.474    0.048    9.831    0.000    0.474    0.694
   .Help3             0.536    0.053   10.097    0.000    0.536    0.722
   .Help4             0.437    0.043   10.083    0.000    0.437    0.720
   .Help5             0.426    0.050    8.568    0.000    0.426    0.588
   .Min1              0.522    0.054    9.754    0.000    0.522    0.834
   .Min2              0.481    0.055    8.742    0.000    0.481    0.778
   .Min3              0.477    0.063    7.638    0.000    0.477    0.722
   .Per1              0.527    0.055    9.660    0.000    0.527    0.688
   .Per2              0.455    0.046    9.829    0.000    0.455    0.704
   .Per3              0.361    0.048    7.469    0.000    0.361    0.528
   .Per4              0.523    0.048   10.983    0.000    0.523    0.837
   .Per5              0.722    0.064   11.260    0.000    0.722    0.878
   .Oth1              0.467    0.050    9.390    0.000    0.467    0.614
   .Oth2              0.397    0.040    9.910    0.000    0.397    0.666
   .Oth3              0.480    0.046   10.329    0.000    0.480    0.715
   .Oth4              0.575    0.055   10.493    0.000    0.575    0.736
   .Oth5              0.406    0.037   11.066    0.000    0.406    0.821
   .Oth6              0.516    0.046   11.207    0.000    0.516    0.846
   .Oth7              0.587    0.053   11.031    0.000    0.587    0.816
    Helplessness      0.099    0.033    3.029    0.002    1.000    1.000
    Minimization      0.104    0.042    2.455    0.014    1.000    1.000
    DenialPersonhd    0.238    0.056    4.223    0.000    1.000    1.000
    Otherization      0.294    0.059    4.985    0.000    1.000    1.000
\end{verbatim}

Our visual inspection of the similarity of psychometric characteristics suggests that the measure is functioning similarly across the two levels of severity.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4928}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3043}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2029}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Mild
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Severe
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Factor loadings: Help & 0.451 to 0.581 & 0.408 to 0.642 \\
Factor loadings: Min & 0.339 to 0.639 & 0.407 to 0.527 \\
Factor loadings: Pers & 0.382 to 0.691 & 0.349 to 0.687 \\
Factor loadings: Oth & 0.315 to 0.645 & 0.393 to 0.621 \\
Non-significant chi-square & \(p = 0.458\) & \(p = 0.522\) \\
\(CFI\geq .95\) & CFI = 0.999 & CFI = 1.000 \\
\(SRMR\leq .08\) (but definitely \textless{} .10) & SRMR = 0.044 & \\
\(RMSEA\leq .05\) (but definitely \textless{} .10) & RMSEA = 0.004, 90\%CI(0.000, 0.026) & \\
\end{longtable}

This, though, does not constitute a formal evaluation. Thus, we continue with testing for multigroup invariance.

\hypertarget{configural-invariance-1}{%
\section{Configural Invariance}\label{configural-invariance-1}}

Configural invariance is our least restrictive level. We are essentially specifying ONE STRUCTURE -- four correlated factors, each with 3 to 7 items/indicators. Each model is allowed to have its own loadings, error variances, and so forth. It's only the structure (the \emph{configuration}) that is consistent.

The same model we had before works. We create the configural model simply by specifying \emph{group = ``Severity''} in the \emph{cfa()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240504}\NormalTok{)}
\NormalTok{configural }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(AMS4CorrMod, }\AttributeTok{data =}\NormalTok{ dfAMSi, }\AttributeTok{group =} \StringTok{"Group"}\NormalTok{)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(configural, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 87 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                       132

  Number of observations per group:                   
    Mild                                           548
    Severe                                         285

Model Test User Model:
                                                      
  Test statistic                               327.572
  Degrees of freedom                               328
  P-value (Chi-square)                           0.496
  Test statistic for each group:
    Mild                                       165.227
    Severe                                     162.345

Model Test Baseline Model:

  Test statistic                              2454.883
  Degrees of freedom                               380
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    1.000
  Tucker-Lewis Index (TLI)                       1.000

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)             -19086.834
  Loglikelihood unrestricted model (H1)     -18923.048
                                                      
  Akaike (AIC)                               38437.669
  Bayesian (BIC)                             39061.373
  Sample-size adjusted Bayesian (SABIC)      38642.187

Root Mean Square Error of Approximation:

  RMSEA                                          0.000
  90 Percent confidence interval - lower         0.000
  90 Percent confidence interval - upper         0.018
  P-value H_0: RMSEA <= 0.050                    1.000
  P-value H_0: RMSEA >= 0.080                    0.000

Standardized Root Mean Square Residual:

  SRMR                                           0.034

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured


Group 1 [Mild]:

Latent Variables:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness =~                                                          
    Help1                1.000                               0.435    0.574
    Help2                1.066    0.116    9.227    0.000    0.464    0.581
    Help3                0.966    0.115    8.382    0.000    0.420    0.499
    Help4                0.857    0.110    7.803    0.000    0.373    0.451
    Help5                0.959    0.113    8.473    0.000    0.417    0.507
  Minimization =~                                                          
    Min1                 1.000                               0.529    0.639
    Min2                 0.550    0.137    4.000    0.000    0.291    0.358
    Min3                 0.509    0.129    3.934    0.000    0.269    0.339
  DenialPersonhood =~                                                      
    Per1                 1.000                               0.422    0.501
    Per2                 1.254    0.132    9.487    0.000    0.530    0.691
    Per3                 1.082    0.121    8.913    0.000    0.457    0.597
    Per4                 0.736    0.105    7.007    0.000    0.311    0.405
    Per5                 0.736    0.110    6.717    0.000    0.311    0.382
  Otherization =~                                                          
    Oth1                 1.000                               0.554    0.645
    Oth2                 0.811    0.084    9.691    0.000    0.449    0.542
    Oth3                 0.819    0.083    9.892    0.000    0.454    0.558
    Oth4                 0.662    0.084    7.835    0.000    0.367    0.417
    Oth5                 0.511    0.073    7.040    0.000    0.283    0.369
    Oth6                 0.545    0.076    7.176    0.000    0.302    0.377
    Oth7                 0.448    0.073    6.104    0.000    0.248    0.315

Covariances:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness ~~                                                          
    Minimization         0.057    0.018    3.143    0.002    0.249    0.249
    DenialPersonhd       0.128    0.019    6.889    0.000    0.699    0.699
    Otherization         0.153    0.021    7.266    0.000    0.634    0.634
  Minimization ~~                                                          
    DenialPersonhd       0.088    0.019    4.612    0.000    0.396    0.396
    Otherization         0.098    0.023    4.201    0.000    0.335    0.335
  DenialPersonhood ~~                                                      
    Otherization         0.165    0.023    7.269    0.000    0.705    0.705

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .Help1             1.978    0.032   61.199    0.000    1.978    2.614
   .Help2             2.400    0.034   70.342    0.000    2.400    3.005
   .Help3             2.137    0.036   59.392    0.000    2.137    2.537
   .Help4             2.226    0.035   63.090    0.000    2.226    2.695
   .Help5             2.020    0.035   57.490    0.000    2.020    2.456
   .Min1              2.401    0.035   67.896    0.000    2.401    2.900
   .Min2              2.343    0.035   67.532    0.000    2.343    2.885
   .Min3              2.631    0.034   77.574    0.000    2.631    3.314
   .Per1              2.086    0.036   57.955    0.000    2.086    2.476
   .Per2              2.109    0.033   64.478    0.000    2.109    2.754
   .Per3              2.553    0.033   78.028    0.000    2.553    3.333
   .Per4              2.381    0.033   72.580    0.000    2.381    3.100
   .Per5              2.148    0.035   61.774    0.000    2.148    2.639
   .Oth1              2.673    0.037   72.839    0.000    2.673    3.112
   .Oth2              2.628    0.035   74.257    0.000    2.628    3.172
   .Oth3              2.117    0.035   60.924    0.000    2.117    2.603
   .Oth4              2.407    0.038   64.037    0.000    2.407    2.736
   .Oth5              2.489    0.033   75.903    0.000    2.489    3.242
   .Oth6              2.620    0.034   76.425    0.000    2.620    3.265
   .Oth7              2.104    0.034   62.432    0.000    2.104    2.667

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .Help1             0.384    0.029   13.388    0.000    0.384    0.670
   .Help2             0.423    0.032   13.286    0.000    0.423    0.663
   .Help3             0.533    0.037   14.441    0.000    0.533    0.751
   .Help4             0.543    0.036   14.930    0.000    0.543    0.796
   .Help5             0.503    0.035   14.349    0.000    0.503    0.743
   .Min1              0.406    0.071    5.741    0.000    0.406    0.592
   .Min2              0.575    0.041   13.887    0.000    0.575    0.872
   .Min3              0.558    0.039   14.255    0.000    0.558    0.885
   .Per1              0.531    0.036   14.736    0.000    0.531    0.749
   .Per2              0.306    0.027   11.455    0.000    0.306    0.522
   .Per3              0.378    0.028   13.521    0.000    0.378    0.644
   .Per4              0.493    0.032   15.501    0.000    0.493    0.836
   .Per5              0.566    0.036   15.637    0.000    0.566    0.854
   .Oth1              0.431    0.035   12.460    0.000    0.431    0.584
   .Oth2              0.484    0.034   14.178    0.000    0.484    0.706
   .Oth3              0.456    0.033   13.976    0.000    0.456    0.689
   .Oth4              0.640    0.042   15.366    0.000    0.640    0.826
   .Oth5              0.509    0.033   15.667    0.000    0.509    0.864
   .Oth6              0.553    0.035   15.621    0.000    0.553    0.858
   .Oth7              0.561    0.035   15.935    0.000    0.561    0.901
    Helplessness      0.189    0.031    6.153    0.000    1.000    1.000
    Minimization      0.280    0.074    3.774    0.000    1.000    1.000
    DenialPersonhd    0.178    0.033    5.434    0.000    1.000    1.000
    Otherization      0.307    0.043    7.182    0.000    1.000    1.000


Group 2 [Severe]:

Latent Variables:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness =~                                                          
    Help1                1.000                               0.314    0.408
    Help2                1.454    0.285    5.096    0.000    0.457    0.553
    Help3                1.448    0.290    5.001    0.000    0.455    0.527
    Help4                1.313    0.262    5.007    0.000    0.412    0.529
    Help5                1.741    0.325    5.352    0.000    0.546    0.642
  Minimization =~                                                          
    Min1                 1.000                               0.322    0.407
    Min2                 1.151    0.317    3.633    0.000    0.371    0.472
    Min3                 1.331    0.362    3.676    0.000    0.429    0.527
  DenialPersonhood =~                                                      
    Per1                 1.000                               0.488    0.558
    Per2                 0.895    0.145    6.180    0.000    0.437    0.544
    Per3                 1.163    0.171    6.796    0.000    0.568    0.687
    Per4                 0.653    0.130    5.036    0.000    0.319    0.403
    Per5                 0.647    0.144    4.491    0.000    0.316    0.349
  Otherization =~                                                          
    Oth1                 1.000                               0.542    0.621
    Oth2                 0.823    0.114    7.202    0.000    0.446    0.578
    Oth3                 0.807    0.119    6.811    0.000    0.438    0.534
    Oth4                 0.838    0.127    6.619    0.000    0.454    0.514
    Oth5                 0.548    0.097    5.663    0.000    0.297    0.423
    Oth6                 0.566    0.106    5.325    0.000    0.307    0.393
    Oth7                 0.672    0.117    5.738    0.000    0.364    0.429

Covariances:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness ~~                                                          
    Minimization         0.026    0.012    2.100    0.036    0.259    0.259
    DenialPersonhd       0.083    0.021    3.972    0.000    0.543    0.543
    Otherization         0.091    0.022    4.123    0.000    0.535    0.535
  Minimization ~~                                                          
    DenialPersonhd       0.047    0.020    2.403    0.016    0.299    0.299
    Otherization         0.091    0.026    3.464    0.001    0.522    0.522
  DenialPersonhood ~~                                                      
    Otherization         0.126    0.029    4.372    0.000    0.476    0.476

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .Help1             2.411    0.046   52.851    0.000    2.411    3.131
   .Help2             3.140    0.049   64.193    0.000    3.140    3.802
   .Help3             2.733    0.051   53.536    0.000    2.733    3.171
   .Help4             2.996    0.046   64.929    0.000    2.996    3.846
   .Help5             2.860    0.050   56.731    0.000    2.860    3.360
   .Min1              2.712    0.047   57.873    0.000    2.712    3.428
   .Min2              2.677    0.047   57.464    0.000    2.677    3.404
   .Min3              2.849    0.048   59.141    0.000    2.849    3.503
   .Per1              2.698    0.052   52.077    0.000    2.698    3.085
   .Per2              2.660    0.048   55.883    0.000    2.660    3.310
   .Per3              3.133    0.049   63.960    0.000    3.133    3.789
   .Per4              2.986    0.047   63.795    0.000    2.986    3.779
   .Per5              2.730    0.054   50.838    0.000    2.730    3.011
   .Oth1              3.025    0.052   58.540    0.000    3.025    3.468
   .Oth2              3.021    0.046   66.060    0.000    3.021    3.913
   .Oth3              2.667    0.049   54.944    0.000    2.667    3.255
   .Oth4              2.818    0.052   53.832    0.000    2.818    3.189
   .Oth5              2.867    0.042   68.820    0.000    2.867    4.077
   .Oth6              2.842    0.046   61.425    0.000    2.842    3.638
   .Oth7              2.481    0.050   49.362    0.000    2.481    2.924

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .Help1             0.494    0.045   10.995    0.000    0.494    0.834
   .Help2             0.474    0.048    9.831    0.000    0.474    0.694
   .Help3             0.536    0.053   10.098    0.000    0.536    0.722
   .Help4             0.437    0.043   10.083    0.000    0.437    0.720
   .Help5             0.426    0.050    8.568    0.000    0.426    0.588
   .Min1              0.522    0.054    9.754    0.000    0.522    0.834
   .Min2              0.481    0.055    8.742    0.000    0.481    0.778
   .Min3              0.477    0.063    7.638    0.000    0.477    0.722
   .Per1              0.527    0.055    9.660    0.000    0.527    0.688
   .Per2              0.455    0.046    9.829    0.000    0.455    0.704
   .Per3              0.361    0.048    7.469    0.000    0.361    0.528
   .Per4              0.523    0.048   10.983    0.000    0.523    0.837
   .Per5              0.722    0.064   11.260    0.000    0.722    0.878
   .Oth1              0.467    0.050    9.390    0.000    0.467    0.614
   .Oth2              0.397    0.040    9.910    0.000    0.397    0.666
   .Oth3              0.480    0.046   10.329    0.000    0.480    0.715
   .Oth4              0.575    0.055   10.493    0.000    0.575    0.736
   .Oth5              0.406    0.037   11.066    0.000    0.406    0.821
   .Oth6              0.516    0.046   11.207    0.000    0.516    0.846
   .Oth7              0.587    0.053   11.031    0.000    0.587    0.816
    Helplessness      0.099    0.033    3.029    0.002    1.000    1.000
    Minimization      0.104    0.042    2.455    0.014    1.000    1.000
    DenialPersonhd    0.238    0.056    4.223    0.000    1.000    1.000
    Otherization      0.294    0.059    4.985    0.000    1.000    1.000
\end{verbatim}

Let's format these results into tables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ConfigFitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(configural)}
\NormalTok{Config\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(configural, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{ConfigCorrs }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_cors}\NormalTok{(configural, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\CommentTok{\# to see each of the tables, remove the hashtab ConfigFitStats}
\CommentTok{\# Config\_paramEsts ConfigCorrs}
\end{Highlighting}
\end{Shaded}

Then, export them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(ConfigFitStats, }\AttributeTok{file =} \StringTok{"ConfigFitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(Config\_paramEsts, }\AttributeTok{file =} \StringTok{"Config\_paramEsts.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(ConfigCorrs, }\AttributeTok{file =} \StringTok{"ConfigCorrs.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Examining the plots can help us understand what we've just done. This will result in two tables, one for each of the models. Recall, we are requiring the structure to be the same, but allowing the values to vary.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# semPlot::semPaths(configural, layout = \textquotesingle{}tree\textquotesingle{}, style = \textquotesingle{}lisrel\textquotesingle{},}
\CommentTok{\# what = \textquotesingle{}col\textquotesingle{}, whatLabels = \textquotesingle{}stand\textquotesingle{})}
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(configural, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"est"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{12-Invariance_files/figure-latex/unnamed-chunk-18-1.pdf} \includegraphics{12-Invariance_files/figure-latex/unnamed-chunk-18-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# If R stalls, open the console. I received the instruction, \textquotesingle{}Hit}
\CommentTok{\# \textless{}Return\textgreater{} to see next plot:\textquotesingle{} Then it ran!}
\end{Highlighting}
\end{Shaded}

\emph{semPath()} automatically produced TWO figures. Toggling between them, we see the configuration is the same, but some of the values change on the paths. In the next models we'll tighten those down.

\hypertarget{interpreting-the-output-7}{%
\subsection{Interpreting the Output}\label{interpreting-the-output-7}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3780}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4512}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1707}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Our Results
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Criteria met?
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Mild: factor loadings significant, strong, proper valence & Help: 0.45 to 0.58; Min: 0.34 to 0.64; Pers: 0.38 to 0.69; Oth: 0.32 to 0.65 & \\
Yes & & \\
Severe: factor loadings significant, strong, proper valence & Help: 0.41 to 0.64; Min: 0.41 to 0.47; Pers: 0.35 to 0.56; Oth: 0.39 to 0.62 & \\
Yes & & \\
Non-significant chi-square & \(\chi ^{2}(328) = 327.57, p = 0.496\) & Yes \\
\(CFI\geq .95\) or \(CFI\geq .90\) & CFI = 1.000 & Yes \\
\(RMSEA\leq .05\) (but definitely \textless{} .10) & RMSEA = 0.000, 90\%CI(0.000, 0.018) & Yes \\
\(SRMR\leq .08\) (but definitely \textless{} .10) & SRMR = 0.032 & Yes \\
Combination rule: \(CFI \geq .95\) and \(SRMR \leq .08\) & CFI = 1.000 SRMR = 0.034 & Yes \\
\end{longtable}

\hypertarget{partial-write-up-6}{%
\subsection{Partial Write-up}\label{partial-write-up-6}}

\begin{quote}
\textbf{Configural Model}. The configural model, which constrained only the relative configuration of variables in the model to be the same in both groups had adequate fit to the data: \(\chi ^{2}(328) = 327.57, p = 0.496\), CFI = 1.000, SRMR = 0.034, RMSEA = 0.000, 90\%CI(0.000, 0.018).
\end{quote}

\hypertarget{weak-invariance-1}{%
\section{Weak Invariance}\label{weak-invariance-1}}

Weak invariance is predicated on configural invariance and it adds cross-group equality constraints on the pattern (factor) loadings.

A priori, we know this will not (can not) be better than configural invariance. We are simply hoping that it is the same or not statistically, significantly different.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240504}\NormalTok{)}
\NormalTok{weak }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(AMS4CorrMod, }\AttributeTok{data =}\NormalTok{ dfAMSi, }\AttributeTok{group =} \StringTok{"Group"}\NormalTok{, }\AttributeTok{group.equal =} \StringTok{"loadings"}\NormalTok{)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(weak, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 65 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                       132
  Number of equality constraints                    16

  Number of observations per group:                   
    Mild                                           548
    Severe                                         285

Model Test User Model:
                                                      
  Test statistic                               352.993
  Degrees of freedom                               344
  P-value (Chi-square)                           0.357
  Test statistic for each group:
    Mild                                       173.702
    Severe                                     179.291

Model Test Baseline Model:

  Test statistic                              2454.883
  Degrees of freedom                               380
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.996
  Tucker-Lewis Index (TLI)                       0.995

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)             -19099.545
  Loglikelihood unrestricted model (H1)     -18923.048
                                                      
  Akaike (AIC)                               38431.089
  Bayesian (BIC)                             38979.193
  Sample-size adjusted Bayesian (SABIC)      38610.817

Root Mean Square Error of Approximation:

  RMSEA                                          0.008
  90 Percent confidence interval - lower         0.000
  90 Percent confidence interval - upper         0.020
  P-value H_0: RMSEA <= 0.050                    1.000
  P-value H_0: RMSEA >= 0.080                    0.000

Standardized Root Mean Square Residual:

  SRMR                                           0.039

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured


Group 1 [Mild]:

Latent Variables:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness =~                                                          
    Help1                1.000                               0.393    0.528
    Help2   (.p2.)       1.161    0.110   10.543    0.000    0.456    0.572
    Help3   (.p3.)       1.084    0.110    9.866    0.000    0.426    0.504
    Help4   (.p4.)       0.982    0.103    9.554    0.000    0.385    0.465
    Help5   (.p5.)       1.150    0.111   10.317    0.000    0.452    0.541
  Minimization =~                                                          
    Min1                 1.000                               0.434    0.530
    Min2    (.p7.)       0.750    0.133    5.625    0.000    0.326    0.399
    Min3    (.p8.)       0.751    0.134    5.622    0.000    0.326    0.407
  DenialPersonhood =~                                                      
    Per1                 1.000                               0.439    0.518
    Per2    (.10.)       1.141    0.100   11.455    0.000    0.501    0.664
    Per3    (.11.)       1.097    0.098   11.195    0.000    0.481    0.620
    Per4    (.12.)       0.720    0.083    8.730    0.000    0.316    0.411
    Per5    (.13.)       0.701    0.087    8.047    0.000    0.307    0.378
  Otherization =~                                                          
    Oth1                 1.000                               0.542    0.635
    Oth2    (.15.)       0.819    0.068   12.115    0.000    0.444    0.537
    Oth3    (.16.)       0.816    0.068   11.999    0.000    0.443    0.547
    Oth4    (.17.)       0.720    0.070   10.253    0.000    0.391    0.440
    Oth5    (.18.)       0.520    0.058    8.993    0.000    0.282    0.368
    Oth6    (.19.)       0.556    0.062    8.977    0.000    0.301    0.375
    Oth7    (.20.)       0.518    0.062    8.301    0.000    0.281    0.352

Covariances:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness ~~                                                          
    Minimization         0.045    0.014    3.104    0.002    0.262    0.262
    DenialPersonhd       0.120    0.016    7.415    0.000    0.699    0.699
    Otherization         0.135    0.018    7.530    0.000    0.636    0.636
  Minimization ~~                                                          
    DenialPersonhd       0.077    0.017    4.517    0.000    0.405    0.405
    Otherization         0.084    0.020    4.179    0.000    0.356    0.356
  DenialPersonhood ~~                                                      
    Otherization         0.167    0.021    7.987    0.000    0.703    0.703

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .Help1             1.978    0.032   62.311    0.000    1.978    2.662
   .Help2             2.400    0.034   70.483    0.000    2.400    3.011
   .Help3             2.137    0.036   59.244    0.000    2.137    2.531
   .Help4             2.226    0.035   62.823    0.000    2.226    2.684
   .Help5             2.020    0.036   56.683    0.000    2.020    2.421
   .Min1              2.401    0.035   68.644    0.000    2.401    2.932
   .Min2              2.343    0.035   67.268    0.000    2.343    2.874
   .Min3              2.631    0.034   76.874    0.000    2.631    3.284
   .Per1              2.086    0.036   57.629    0.000    2.086    2.462
   .Per2              2.109    0.032   65.455    0.000    2.109    2.796
   .Per3              2.553    0.033   77.033    0.000    2.553    3.291
   .Per4              2.381    0.033   72.525    0.000    2.381    3.098
   .Per5              2.148    0.035   61.873    0.000    2.148    2.643
   .Oth1              2.673    0.036   73.298    0.000    2.673    3.131
   .Oth2              2.628    0.035   74.445    0.000    2.628    3.180
   .Oth3              2.117    0.035   61.266    0.000    2.117    2.617
   .Oth4              2.407    0.038   63.475    0.000    2.407    2.712
   .Oth5              2.489    0.033   75.949    0.000    2.489    3.244
   .Oth6              2.620    0.034   76.426    0.000    2.620    3.265
   .Oth7              2.104    0.034   61.765    0.000    2.104    2.638

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .Help1             0.398    0.028   14.249    0.000    0.398    0.721
   .Help2             0.427    0.031   13.649    0.000    0.427    0.673
   .Help3             0.532    0.037   14.545    0.000    0.532    0.746
   .Help4             0.540    0.036   14.958    0.000    0.540    0.784
   .Help5             0.492    0.035   14.104    0.000    0.492    0.707
   .Min1              0.482    0.047   10.160    0.000    0.482    0.719
   .Min2              0.559    0.041   13.582    0.000    0.559    0.841
   .Min3              0.536    0.040   13.416    0.000    0.536    0.834
   .Per1              0.525    0.036   14.683    0.000    0.525    0.732
   .Per2              0.319    0.026   12.373    0.000    0.319    0.560
   .Per3              0.370    0.028   13.281    0.000    0.370    0.616
   .Per4              0.491    0.032   15.530    0.000    0.491    0.831
   .Per5              0.566    0.036   15.707    0.000    0.566    0.857
   .Oth1              0.435    0.034   12.947    0.000    0.435    0.597
   .Oth2              0.486    0.034   14.444    0.000    0.486    0.711
   .Oth3              0.458    0.032   14.305    0.000    0.458    0.700
   .Oth4              0.635    0.041   15.312    0.000    0.635    0.806
   .Oth5              0.509    0.032   15.757    0.000    0.509    0.865
   .Oth6              0.553    0.035   15.710    0.000    0.553    0.859
   .Oth7              0.557    0.035   15.820    0.000    0.557    0.876
    Helplessness      0.154    0.024    6.399    0.000    1.000    1.000
    Minimization      0.189    0.044    4.307    0.000    1.000    1.000
    DenialPersonhd    0.192    0.030    6.427    0.000    1.000    1.000
    Otherization      0.294    0.037    7.962    0.000    1.000    1.000


Group 2 [Severe]:

Latent Variables:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness =~                                                          
    Help1                1.000                               0.407    0.510
    Help2   (.p2.)       1.161    0.110   10.543    0.000    0.473    0.570
    Help3   (.p3.)       1.084    0.110    9.866    0.000    0.442    0.515
    Help4   (.p4.)       0.982    0.103    9.554    0.000    0.400    0.517
    Help5   (.p5.)       1.150    0.111   10.317    0.000    0.468    0.565
  Minimization =~                                                          
    Min1                 1.000                               0.432    0.534
    Min2    (.p7.)       0.750    0.133    5.625    0.000    0.324    0.415
    Min3    (.p8.)       0.751    0.134    5.622    0.000    0.324    0.405
  DenialPersonhood =~                                                      
    Per1                 1.000                               0.455    0.525
    Per2    (.10.)       1.141    0.100   11.455    0.000    0.519    0.625
    Per3    (.11.)       1.097    0.098   11.195    0.000    0.499    0.618
    Per4    (.12.)       0.720    0.083    8.730    0.000    0.328    0.415
    Per5    (.13.)       0.701    0.087    8.047    0.000    0.319    0.351
  Otherization =~                                                          
    Oth1                 1.000                               0.565    0.640
    Oth2    (.15.)       0.819    0.068   12.115    0.000    0.462    0.596
    Oth3    (.16.)       0.816    0.068   11.999    0.000    0.461    0.557
    Oth4    (.17.)       0.720    0.070   10.253    0.000    0.407    0.468
    Oth5    (.18.)       0.520    0.058    8.993    0.000    0.294    0.417
    Oth6    (.19.)       0.556    0.062    8.977    0.000    0.314    0.402
    Oth7    (.20.)       0.518    0.062    8.301    0.000    0.292    0.352

Covariances:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness ~~                                                          
    Minimization         0.042    0.020    2.142    0.032    0.240    0.240
    DenialPersonhd       0.105    0.020    5.251    0.000    0.565    0.565
    Otherization         0.119    0.023    5.175    0.000    0.516    0.516
  Minimization ~~                                                          
    DenialPersonhd       0.060    0.022    2.669    0.008    0.305    0.305
    Otherization         0.130    0.029    4.450    0.000    0.531    0.531
  DenialPersonhood ~~                                                      
    Otherization         0.120    0.025    4.801    0.000    0.466    0.466

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .Help1             2.411    0.047   50.902    0.000    2.411    3.015
   .Help2             3.140    0.049   63.944    0.000    3.140    3.788
   .Help3             2.733    0.051   53.787    0.000    2.733    3.186
   .Help4             2.996    0.046   65.419    0.000    2.996    3.875
   .Help5             2.860    0.049   58.177    0.000    2.860    3.446
   .Min1              2.712    0.048   56.634    0.000    2.712    3.355
   .Min2              2.677    0.046   57.887    0.000    2.677    3.429
   .Min3              2.849    0.047   60.155    0.000    2.849    3.563
   .Per1              2.698    0.051   52.630    0.000    2.698    3.118
   .Per2              2.660    0.049   54.068    0.000    2.660    3.203
   .Per3              3.133    0.048   65.507    0.000    3.133    3.880
   .Per4              2.986    0.047   63.887    0.000    2.986    3.784
   .Per5              2.730    0.054   50.677    0.000    2.730    3.002
   .Oth1              3.025    0.052   57.831    0.000    3.025    3.426
   .Oth2              3.021    0.046   65.767    0.000    3.021    3.896
   .Oth3              2.667    0.049   54.354    0.000    2.667    3.220
   .Oth4              2.818    0.052   54.700    0.000    2.818    3.240
   .Oth5              2.867    0.042   68.742    0.000    2.867    4.072
   .Oth6              2.842    0.046   61.423    0.000    2.842    3.638
   .Oth7              2.481    0.049   50.360    0.000    2.481    2.983

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .Help1             0.473    0.045   10.530    0.000    0.473    0.740
   .Help2             0.464    0.047    9.972    0.000    0.464    0.675
   .Help3             0.541    0.052   10.477    0.000    0.541    0.735
   .Help4             0.438    0.042   10.440    0.000    0.438    0.733
   .Help5             0.469    0.047   10.023    0.000    0.469    0.681
   .Min1              0.467    0.058    8.110    0.000    0.467    0.715
   .Min2              0.505    0.050   10.114    0.000    0.505    0.828
   .Min3              0.534    0.052   10.226    0.000    0.534    0.836
   .Per1              0.542    0.052   10.354    0.000    0.542    0.724
   .Per2              0.420    0.045    9.244    0.000    0.420    0.609
   .Per3              0.403    0.043    9.337    0.000    0.403    0.618
   .Per4              0.515    0.046   11.086    0.000    0.515    0.828
   .Per5              0.725    0.064   11.375    0.000    0.725    0.877
   .Oth1              0.460    0.048    9.515    0.000    0.460    0.591
   .Oth2              0.388    0.039   10.000    0.000    0.388    0.644
   .Oth3              0.473    0.046   10.386    0.000    0.473    0.690
   .Oth4              0.591    0.054   10.973    0.000    0.591    0.781
   .Oth5              0.409    0.037   11.208    0.000    0.409    0.826
   .Oth6              0.512    0.045   11.278    0.000    0.512    0.839
   .Oth7              0.606    0.053   11.460    0.000    0.606    0.876
    Helplessness      0.166    0.030    5.514    0.000    1.000    1.000
    Minimization      0.186    0.050    3.705    0.000    1.000    1.000
    DenialPersonhd    0.207    0.037    5.613    0.000    1.000    1.000
    Otherization      0.319    0.048    6.674    0.000    1.000    1.000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{anova}\NormalTok{(configural, weak)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Chi-Squared Difference Test

            Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(>Chisq)  
configural 328 38438 39061 327.57                                         
weak       344 38431 38979 352.99      25.42 0.037598      16    0.06275 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Let's format these results into tables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{WeakFitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(weak)}
\NormalTok{Weak\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(weak, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{WeakCorrs }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_cors}\NormalTok{(weak, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\CommentTok{\# to see each of the tables, remove the hashtag WeakFitStats}
\CommentTok{\# Weak\_paramEsts WeakCorrs}
\end{Highlighting}
\end{Shaded}

Then, export them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(WeakFitStats, }\AttributeTok{file =} \StringTok{"WeakFitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(Weak\_paramEsts, }\AttributeTok{file =} \StringTok{"Weak\_paramEsts.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(WeakCorrs, }\AttributeTok{file =} \StringTok{"WeakCorrs.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{interpreting-the-output-8}{%
\subsection{Interpreting the Output}\label{interpreting-the-output-8}}

Note that although the ``Std.all'' values differ from each other, the ``Estimates'' (factor loadings) are identical across Mild and Severe groups. Each also has a ``label'' (e.g., .p2., .p3.) which indicates that they have been constrained to be equal. The ``Std.all'' differ between degree of disability severity due to the difference in standard deviations of the indicators.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3780}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4512}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1707}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Our Results
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Criteria met?
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Mild: factor loadings significant, strong, proper valence & Help: 0.47 to 0.57; Min: 0.40 to 0.53; Pers: 0.38 to 0.66; Oth: 0.35 to 0.64 & \\
Yes & & \\
Severe: factor loadings significant, strong, proper valence & Help: 0.41 to 0.57; Min: 0.41 to 0.53; Pers: 0.35 to 0.63; Oth: 0.35 to 0.64 & \\
Yes & & \\
Non-significant chi-square & \(\chi ^{2}(344) = 353.00, p = 0.357\) & Yes \\
\(CFI\geq .95\) or \(CFI\geq .90\) & CFI = 0.996 & Yes \\
\(RMSEA\leq .05\) (but definitely \textless{} .10) & RMSEA = 0.008, 90\%CI(0.000, 0.020) & Yes \\
\(SRMR\leq .08\) (but definitely \textless{} .10) & SRMR = 0.035 & Yes \\
Combination rule: \(CFI \geq .95\) and \(SRMR \leq .08\) & CFI = 1.000, SRMR = 0.035 & Yes \\
\end{longtable}

\hypertarget{partial-write-up-7}{%
\subsection{Partial Write-up}\label{partial-write-up-7}}

\begin{quote}
\textbf{Weak invariance model}. The weak invariance model constrained the configuration of variables and all factor loadings to be constant across groups. Fit indices were comparable to the configural model: \(\chi ^{2}(344) = 353.00, p = 0.357\), CFI = 0.996, SRMR = 0.039, RMSEA = 0.008, 90\%CI(0.000, 0.020. Invariance of the factor loadings was supported by the non-significant difference tests that assessed model similarity: \(\chi_{D}^{2}(16) = 25.42, p = 0.063\); \(\Delta CFI = 0.004\)
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The CFI difference test is calculated by simple subtraction}
\DecValTok{1} \SpecialCharTok{{-}} \FloatTok{0.996}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.004
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# semPlot::semPaths(weak, layout = \textquotesingle{}tree\textquotesingle{}, style = \textquotesingle{}lisrel\textquotesingle{}, what =}
\CommentTok{\# \textquotesingle{}col\textquotesingle{}, whatLabels = \textquotesingle{}stand\textquotesingle{})}
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(weak, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"est"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{12-Invariance_files/figure-latex/unnamed-chunk-23-1.pdf} \includegraphics{12-Invariance_files/figure-latex/unnamed-chunk-23-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# If R stalls, open the console. I received the instruction, \textquotesingle{}Hit}
\CommentTok{\# \textless{}Return\textgreater{} to see next plot:\textquotesingle{} Then it ran!}
\end{Highlighting}
\end{Shaded}

\hypertarget{strong-invariance-1}{%
\section{Strong Invariance}\label{strong-invariance-1}}

Strong invariance is predicated on configural and weak invariance, but also constrains the indicator means/intercepts.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240504}\NormalTok{)}
\NormalTok{strong }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(AMS4CorrMod, }\AttributeTok{data =}\NormalTok{ dfAMSi, }\AttributeTok{group =} \StringTok{"Group"}\NormalTok{, }\AttributeTok{group.equal =} \FunctionTok{c}\NormalTok{(}\StringTok{"loadings"}\NormalTok{,}
    \StringTok{"intercepts"}\NormalTok{))}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(strong, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 87 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                       136
  Number of equality constraints                    36

  Number of observations per group:                   
    Mild                                           548
    Severe                                         285

Model Test User Model:
                                                      
  Test statistic                               412.746
  Degrees of freedom                               360
  P-value (Chi-square)                           0.029
  Test statistic for each group:
    Mild                                       210.341
    Severe                                     202.405

Model Test Baseline Model:

  Test statistic                              2454.883
  Degrees of freedom                               380
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.975
  Tucker-Lewis Index (TLI)                       0.973

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)             -19129.421
  Loglikelihood unrestricted model (H1)     -18923.048
                                                      
  Akaike (AIC)                               38458.843
  Bayesian (BIC)                             38931.346
  Sample-size adjusted Bayesian (SABIC)      38613.781

Root Mean Square Error of Approximation:

  RMSEA                                          0.019
  90 Percent confidence interval - lower         0.007
  90 Percent confidence interval - upper         0.027
  P-value H_0: RMSEA <= 0.050                    1.000
  P-value H_0: RMSEA >= 0.080                    0.000

Standardized Root Mean Square Residual:

  SRMR                                           0.045

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured


Group 1 [Mild]:

Latent Variables:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness =~                                                          
    Help1                1.000                               0.329    0.452
    Help2   (.p2.)       1.368    0.106   12.897    0.000    0.451    0.566
    Help3   (.p3.)       1.198    0.101   11.831    0.000    0.395    0.473
    Help4   (.p4.)       1.291    0.103   12.520    0.000    0.425    0.504
    Help5   (.p5.)       1.464    0.112   13.092    0.000    0.482    0.571
  Minimization =~                                                          
    Min1                 1.000                               0.416    0.509
    Min2    (.p7.)       0.851    0.120    7.087    0.000    0.354    0.432
    Min3    (.p8.)       0.748    0.111    6.728    0.000    0.311    0.389
  DenialPersonhood =~                                                      
    Per1                 1.000                               0.444    0.524
    Per2    (.10.)       1.044    0.074   14.168    0.000    0.464    0.625
    Per3    (.11.)       1.036    0.074   14.036    0.000    0.460    0.600
    Per4    (.12.)       0.817    0.068   11.957    0.000    0.363    0.463
    Per5    (.13.)       0.786    0.072   10.914    0.000    0.349    0.423
  Otherization =~                                                          
    Oth1                 1.000                               0.496    0.591
    Oth2    (.15.)       0.873    0.066   13.176    0.000    0.433    0.526
    Oth3    (.16.)       0.951    0.070   13.628    0.000    0.472    0.576
    Oth4    (.17.)       0.803    0.070   11.516    0.000    0.398    0.447
    Oth5    (.18.)       0.620    0.058   10.693    0.000    0.308    0.397
    Oth6    (.19.)       0.571    0.060    9.517    0.000    0.283    0.355
    Oth7    (.20.)       0.616    0.063    9.838    0.000    0.305    0.379

Covariances:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness ~~                                                          
    Minimization         0.036    0.012    3.106    0.002    0.263    0.263
    DenialPersonhd       0.103    0.013    7.913    0.000    0.706    0.706
    Otherization         0.103    0.014    7.588    0.000    0.632    0.632
  Minimization ~~                                                          
    DenialPersonhd       0.075    0.016    4.612    0.000    0.404    0.404
    Otherization         0.074    0.018    4.232    0.000    0.360    0.360
  DenialPersonhood ~~                                                      
    Otherization         0.156    0.018    8.416    0.000    0.706    0.706

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .Help1   (.51.)    1.944    0.029   66.425    0.000    1.944    2.667
   .Help2   (.52.)    2.400    0.033   73.804    0.000    2.400    3.013
   .Help3   (.53.)    2.119    0.034   63.226    0.000    2.119    2.537
   .Help4   (.54.)    2.254    0.034   66.598    0.000    2.254    2.673
   .Help5   (.55.)    2.037    0.034   59.104    0.000    2.037    2.410
   .Min1    (.56.)    2.394    0.033   71.944    0.000    2.394    2.930
   .Min2    (.57.)    2.362    0.033   72.669    0.000    2.362    2.882
   .Min3    (.58.)    2.621    0.031   83.690    0.000    2.621    3.279
   .Per1    (.59.)    2.089    0.034   61.496    0.000    2.089    2.462
   .Per2    (.60.)    2.087    0.030   68.542    0.000    2.087    2.812
   .Per3    (.61.)    2.538    0.031   81.214    0.000    2.538    3.307
   .Per4    (.62.)    2.418    0.031   77.819    0.000    2.418    3.081
   .Per5    (.63.)    2.179    0.033   66.537    0.000    2.179    2.638
   .Oth1    (.64.)    2.630    0.034   78.432    0.000    2.630    3.135
   .Oth2    (.65.)    2.616    0.032   81.837    0.000    2.616    3.182
   .Oth3    (.66.)    2.146    0.033   65.814    0.000    2.146    2.618
   .Oth4    (.67.)    2.414    0.034   70.725    0.000    2.414    2.711
   .Oth5    (.68.)    2.519    0.029   86.645    0.000    2.519    3.252
   .Oth6    (.69.)    2.601    0.030   86.656    0.000    2.601    3.258
   .Oth7    (.70.)    2.129    0.031   69.263    0.000    2.129    2.646

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .Help1             0.423    0.028   15.147    0.000    0.423    0.796
   .Help2             0.431    0.031   13.925    0.000    0.431    0.680
   .Help3             0.542    0.036   14.982    0.000    0.542    0.777
   .Help4             0.530    0.036   14.696    0.000    0.530    0.746
   .Help5             0.482    0.035   13.866    0.000    0.482    0.674
   .Min1              0.495    0.044   11.338    0.000    0.495    0.741
   .Min2              0.546    0.041   13.253    0.000    0.546    0.813
   .Min3              0.542    0.039   14.020    0.000    0.542    0.848
   .Per1              0.522    0.035   14.706    0.000    0.522    0.726
   .Per2              0.335    0.025   13.319    0.000    0.335    0.609
   .Per3              0.377    0.027   13.755    0.000    0.377    0.640
   .Per4              0.484    0.032   15.237    0.000    0.484    0.786
   .Per5              0.560    0.036   15.494    0.000    0.560    0.821
   .Oth1              0.457    0.033   13.768    0.000    0.457    0.650
   .Oth2              0.489    0.033   14.609    0.000    0.489    0.723
   .Oth3              0.449    0.032   13.994    0.000    0.449    0.669
   .Oth4              0.634    0.041   15.289    0.000    0.634    0.800
   .Oth5              0.505    0.032   15.617    0.000    0.505    0.842
   .Oth6              0.557    0.035   15.831    0.000    0.557    0.874
   .Oth7              0.554    0.035   15.701    0.000    0.554    0.856
    Helplessness      0.109    0.016    6.725    0.000    1.000    1.000
    Minimization      0.173    0.037    4.734    0.000    1.000    1.000
    DenialPersonhd    0.197    0.026    7.454    0.000    1.000    1.000
    Otherization      0.246    0.031    7.963    0.000    1.000    1.000


Group 2 [Severe]:

Latent Variables:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness =~                                                          
    Help1                1.000                               0.345    0.442
    Help2   (.p2.)       1.368    0.106   12.897    0.000    0.473    0.569
    Help3   (.p3.)       1.198    0.101   11.831    0.000    0.414    0.487
    Help4   (.p4.)       1.291    0.103   12.520    0.000    0.446    0.564
    Help5   (.p5.)       1.464    0.112   13.092    0.000    0.506    0.602
  Minimization =~                                                          
    Min1                 1.000                               0.417    0.517
    Min2    (.p7.)       0.851    0.120    7.087    0.000    0.355    0.451
    Min3    (.p8.)       0.748    0.111    6.728    0.000    0.312    0.391
  DenialPersonhood =~                                                      
    Per1                 1.000                               0.461    0.531
    Per2    (.10.)       1.044    0.074   14.168    0.000    0.481    0.588
    Per3    (.11.)       1.036    0.074   14.036    0.000    0.477    0.595
    Per4    (.12.)       0.817    0.068   11.957    0.000    0.376    0.465
    Per5    (.13.)       0.786    0.072   10.914    0.000    0.362    0.392
  Otherization =~                                                          
    Oth1                 1.000                               0.517    0.594
    Oth2    (.15.)       0.873    0.066   13.176    0.000    0.451    0.584
    Oth3    (.16.)       0.951    0.070   13.628    0.000    0.492    0.584
    Oth4    (.17.)       0.803    0.070   11.516    0.000    0.415    0.477
    Oth5    (.18.)       0.620    0.058   10.693    0.000    0.321    0.449
    Oth6    (.19.)       0.571    0.060    9.517    0.000    0.295    0.379
    Oth7    (.20.)       0.616    0.063    9.838    0.000    0.318    0.380

Covariances:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness ~~                                                          
    Minimization         0.034    0.016    2.153    0.031    0.239    0.239
    DenialPersonhd       0.089    0.017    5.379    0.000    0.560    0.560
    Otherization         0.094    0.018    5.303    0.000    0.525    0.525
  Minimization ~~                                                          
    DenialPersonhd       0.060    0.022    2.737    0.006    0.312    0.312
    Otherization         0.115    0.025    4.506    0.000    0.532    0.532
  DenialPersonhood ~~                                                      
    Otherization         0.113    0.023    4.926    0.000    0.474    0.474

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .Help1   (.51.)    1.944    0.029   66.425    0.000    1.944    2.486
   .Help2   (.52.)    2.400    0.033   73.804    0.000    2.400    2.890
   .Help3   (.53.)    2.119    0.034   63.226    0.000    2.119    2.492
   .Help4   (.54.)    2.254    0.034   66.598    0.000    2.254    2.850
   .Help5   (.55.)    2.037    0.034   59.104    0.000    2.037    2.425
   .Min1    (.56.)    2.394    0.033   71.944    0.000    2.394    2.970
   .Min2    (.57.)    2.362    0.033   72.669    0.000    2.362    3.004
   .Min3    (.58.)    2.621    0.031   83.690    0.000    2.621    3.285
   .Per1    (.59.)    2.089    0.034   61.496    0.000    2.089    2.407
   .Per2    (.60.)    2.087    0.030   68.542    0.000    2.087    2.553
   .Per3    (.61.)    2.538    0.031   81.214    0.000    2.538    3.168
   .Per4    (.62.)    2.418    0.031   77.819    0.000    2.418    2.991
   .Per5    (.63.)    2.179    0.033   66.537    0.000    2.179    2.358
   .Oth1    (.64.)    2.630    0.034   78.432    0.000    2.630    3.022
   .Oth2    (.65.)    2.616    0.032   81.837    0.000    2.616    3.386
   .Oth3    (.66.)    2.146    0.033   65.814    0.000    2.146    2.546
   .Oth4    (.67.)    2.414    0.034   70.725    0.000    2.414    2.773
   .Oth5    (.68.)    2.519    0.029   86.645    0.000    2.519    3.529
   .Oth6    (.69.)    2.601    0.030   86.656    0.000    2.601    3.343
   .Oth7    (.70.)    2.129    0.031   69.263    0.000    2.129    2.540
    Hlplssn           0.541    0.044   12.314    0.000    1.567    1.567
    Minmztn           0.332    0.051    6.518    0.000    0.797    0.797
    DnlPrsn           0.604    0.050   12.008    0.000    1.311    1.311
    Othrztn           0.485    0.049    9.804    0.000    0.938    0.938

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .Help1             0.492    0.045   11.034    0.000    0.492    0.805
   .Help2             0.466    0.046   10.115    0.000    0.466    0.676
   .Help3             0.552    0.051   10.771    0.000    0.552    0.763
   .Help4             0.427    0.042   10.155    0.000    0.427    0.682
   .Help5             0.450    0.046    9.753    0.000    0.450    0.637
   .Min1              0.476    0.055    8.720    0.000    0.476    0.733
   .Min2              0.493    0.050    9.798    0.000    0.493    0.797
   .Min3              0.539    0.051   10.493    0.000    0.539    0.847
   .Per1              0.541    0.052   10.349    0.000    0.541    0.718
   .Per2              0.437    0.045    9.782    0.000    0.437    0.654
   .Per3              0.414    0.043    9.688    0.000    0.414    0.645
   .Per4              0.512    0.047   10.829    0.000    0.512    0.784
   .Per5              0.722    0.064   11.220    0.000    0.722    0.846
   .Oth1              0.490    0.049   10.072    0.000    0.490    0.647
   .Oth2              0.393    0.039   10.155    0.000    0.393    0.659
   .Oth3              0.468    0.046   10.173    0.000    0.468    0.659
   .Oth4              0.585    0.054   10.938    0.000    0.585    0.772
   .Oth5              0.407    0.037   11.076    0.000    0.407    0.798
   .Oth6              0.518    0.046   11.371    0.000    0.518    0.856
   .Oth7              0.601    0.053   11.374    0.000    0.601    0.856
    Helplessness      0.119    0.021    5.803    0.000    1.000    1.000
    Minimization      0.174    0.044    3.983    0.000    1.000    1.000
    DenialPersonhd    0.212    0.034    6.202    0.000    1.000    1.000
    Otherization      0.267    0.040    6.679    0.000    1.000    1.000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{anova}\NormalTok{(configural, weak, strong)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Chi-Squared Difference Test

            Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff   Pr(>Chisq)    
configural 328 38438 39061 327.57                                             
weak       344 38431 38979 352.99     25.420 0.037598      16      0.06275 .  
strong     360 38459 38931 412.75     59.754 0.081029      16 0.0000005758 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Let's format these results into tables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{strongFitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(strong)}
\NormalTok{strong\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(strong, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{strongCorrs }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_cors}\NormalTok{(strong, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\CommentTok{\# to see each of the tables, remove the hashtag strongFitStats}
\CommentTok{\# strong\_paramEsts strongCorrs}
\end{Highlighting}
\end{Shaded}

Then, export them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(strongFitStats, }\AttributeTok{file =} \StringTok{"strongFitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(strong\_paramEsts, }\AttributeTok{file =} \StringTok{"strong\_paramEsts.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(strongCorrs, }\AttributeTok{file =} \StringTok{"strongCorrs.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# semPlot::semPaths(strong, layout = \textquotesingle{}tree\textquotesingle{}, style = \textquotesingle{}lisrel\textquotesingle{}, what =}
\CommentTok{\# \textquotesingle{}col\textquotesingle{}, whatLabels = \textquotesingle{}stand\textquotesingle{})}
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(strong, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"est"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{12-Invariance_files/figure-latex/unnamed-chunk-27-1.pdf} \includegraphics{12-Invariance_files/figure-latex/unnamed-chunk-27-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# If R stalls, open the console. I received the instruction, \textquotesingle{}Hit}
\CommentTok{\# \textless{}Return\textgreater{} to see next plot:\textquotesingle{} Then it ran!}
\end{Highlighting}
\end{Shaded}

\hypertarget{interpreting-the-output-9}{%
\subsection{Interpreting the Output}\label{interpreting-the-output-9}}

Note that although the ``Std.all'' values differ from each other, the ``Estimates'' (factor loadings) are identical across Mild and Severe groups. Each also has a ``label'' (e.g., .p2., .p3.) which indicates that they have been constrained to be equal. The ``Std.all'' differ between degree of disability severity due to the difference in standard deviations of the indicators.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3780}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4512}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1707}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Our Results
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Criteria met?
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Mild: factor loadings significant, strong, proper valence & Help: 0.45 to 0.57; Min: 0.39 to 0.51; Pers: 0.42 to 0.63; Oth: 0.36 to 0.59 & \\
Yes & & \\
Severe: factor loadings significant, strong, proper valence & Help: 0.44 to 0.60; Min: 0.39 to 0.52; Pers: 0.39 to 0.60; Oth: 0.38 to 0.59 & \\
Yes & & \\
Non-significant chi-square & \(\chi ^{2}(360) = 412.75 p = 0.029\) & No \\
\(CFI\geq .95\) or \(CFI\geq .90\) & CFI = 0.975 & Yes \\
\(RMSEA\leq .05\) (but definitely \textless{} .10) & RMSEA = 0.019, CI90\%(0.007 to 0.027) & Yes \\
\(SRMR\leq .08\) (but definitely \textless{} .10) & SRMR = 0.045 & Yes \\
Combination rule: \(CFI \geq .95\) and \(SRMR \leq .08\) & CFI = .805, SRMR = 0.036 & Yes \\
\end{longtable}

\hypertarget{partial-write-up-8}{%
\subsection{Partial Write-up}\label{partial-write-up-8}}

\begin{quote}
\textbf{Strong invariance model}. In the strong invariance model, configuration, factor loadings, and indicator means/intercepts were constrained to be the same for each group. Fit indices were less than ideal: \(\chi ^{2}(360) = 412.75 p = 0.029\), CFI = 0.975, SRMR = 0.045, RMSEA =0.019, CI90\%(0.007 to 0.027). The difference tests that evaluated model similarity suggested there was factorial noninvariance: (\(\chi_{D}^{2}(16) = 59.754, p = 0.01\); \(\Delta CFI = 0.021\). Given that the \(\chi_{D}^{2}\) test is statistically significant and the \(\Delta CFI > 0.01\) we cannot claim strong invariance and we therefore do not test stricter models.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
 \FloatTok{0.996} \SpecialCharTok{{-}} \FloatTok{0.975}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.021
\end{verbatim}

\textbf{Should I be worried if measurement invariance stops here?}\\
Byrne \citeyearpar{byrne_structural_2016} wrote, ``Historically, the Joreskog tradition of invariance testing held that the equality of error variances and their covariances should also be tested. However, it is now widely accepted that to do so represents an overly restrictive test of the data'' (p.~230).

Further, in an awesome article examining the factorial invariance of the Calling \& Vocation Questionnaire \citep{autin_career_2017} in a binational sample, strict invariance (the next level of restraint) was not even mentioned. Further, after strong invariance was not achieved the authors wrote, ``Therefore, poor fit in only this model does not necessarily indicate the factor structure operates differently for different groups'' (p.~695).

SO\ldots as a researcher, I would be happy if I had configural (just the shape) and weak (factor loadings) invariance.

Plus..a little later in the lecture we head into \emph{partial measurement invariance.}

If we fail at this stage (or at any earlier stage), we would normally not continue. Because this lesson is for training, we will continue onto the last model.

\hypertarget{strict-invariance-1}{%
\section{Strict Invariance}\label{strict-invariance-1}}

Strict invariance is predicated on configural, weak, and strong invariance. To that, it adds cross-group equality constraints on the residuals.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240504}\NormalTok{)}
\NormalTok{strict }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(AMS4CorrMod, }\AttributeTok{data =}\NormalTok{ dfAMSi, }\AttributeTok{group =} \StringTok{"Group"}\NormalTok{, }\AttributeTok{group.equal =} \FunctionTok{c}\NormalTok{(}\StringTok{"loadings"}\NormalTok{,}
    \StringTok{"intercepts"}\NormalTok{, }\StringTok{"residuals"}\NormalTok{))}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(strict, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{, )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 81 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                       136
  Number of equality constraints                    56

  Number of observations per group:                   
    Mild                                           548
    Severe                                         285

Model Test User Model:
                                                      
  Test statistic                               439.692
  Degrees of freedom                               380
  P-value (Chi-square)                           0.019
  Test statistic for each group:
    Mild                                       221.270
    Severe                                     218.422

Model Test Baseline Model:

  Test statistic                              2454.883
  Degrees of freedom                               380
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.971
  Tucker-Lewis Index (TLI)                       0.971

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)             -19142.894
  Loglikelihood unrestricted model (H1)     -18923.048
                                                      
  Akaike (AIC)                               38445.789
  Bayesian (BIC)                             38823.792
  Sample-size adjusted Bayesian (SABIC)      38569.739

Root Mean Square Error of Approximation:

  RMSEA                                          0.019
  90 Percent confidence interval - lower         0.009
  90 Percent confidence interval - upper         0.027
  P-value H_0: RMSEA <= 0.050                    1.000
  P-value H_0: RMSEA >= 0.080                    0.000

Standardized Root Mean Square Residual:

  SRMR                                           0.046

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured


Group 1 [Mild]:

Latent Variables:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness =~                                                          
    Help1                1.000                               0.326    0.438
    Help2   (.p2.)       1.383    0.107   12.902    0.000    0.450    0.560
    Help3   (.p3.)       1.212    0.102   11.826    0.000    0.395    0.472
    Help4   (.p4.)       1.299    0.105   12.389    0.000    0.423    0.515
    Help5   (.p5.)       1.477    0.113   13.041    0.000    0.481    0.574
  Minimization =~                                                          
    Min1                 1.000                               0.424    0.520
    Min2    (.p7.)       0.841    0.119    7.044    0.000    0.357    0.440
    Min3    (.p8.)       0.738    0.110    6.711    0.000    0.313    0.391
  DenialPersonhood =~                                                      
    Per1                 1.000                               0.439    0.518
    Per2    (.10.)       1.030    0.072   14.230    0.000    0.453    0.596
    Per3    (.11.)       1.033    0.073   14.122    0.000    0.454    0.588
    Per4    (.12.)       0.819    0.068   12.052    0.000    0.360    0.456
    Per5    (.13.)       0.790    0.071   11.061    0.000    0.347    0.405
  Otherization =~                                                          
    Oth1                 1.000                               0.497    0.587
    Oth2    (.15.)       0.876    0.067   13.107    0.000    0.435    0.542
    Oth3    (.16.)       0.953    0.070   13.637    0.000    0.474    0.574
    Oth4    (.17.)       0.801    0.070   11.449    0.000    0.398    0.452
    Oth5    (.18.)       0.620    0.059   10.577    0.000    0.308    0.409
    Oth6    (.19.)       0.573    0.060    9.500    0.000    0.284    0.360
    Oth7    (.20.)       0.619    0.063    9.887    0.000    0.308    0.377

Covariances:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness ~~                                                          
    Minimization         0.036    0.012    3.096    0.002    0.259    0.259
    DenialPersonhd       0.102    0.013    7.884    0.000    0.712    0.712
    Otherization         0.102    0.013    7.560    0.000    0.631    0.631
  Minimization ~~                                                          
    DenialPersonhd       0.075    0.016    4.614    0.000    0.405    0.405
    Otherization         0.075    0.018    4.238    0.000    0.355    0.355
  DenialPersonhood ~~                                                      
    Otherization         0.155    0.018    8.404    0.000    0.711    0.711

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .Help1   (.51.)    1.943    0.030   65.457    0.000    1.943    2.612
   .Help2   (.52.)    2.400    0.033   73.272    0.000    2.400    2.986
   .Help3   (.53.)    2.119    0.034   63.095    0.000    2.119    2.531
   .Help4   (.54.)    2.252    0.033   67.814    0.000    2.252    2.741
   .Help5   (.55.)    2.037    0.034   59.474    0.000    2.037    2.430
   .Min1    (.56.)    2.394    0.033   71.865    0.000    2.394    2.934
   .Min2    (.57.)    2.361    0.032   73.225    0.000    2.361    2.916
   .Min3    (.58.)    2.622    0.031   83.617    0.000    2.622    3.277
   .Per1    (.59.)    2.088    0.034   61.558    0.000    2.088    2.460
   .Per2    (.60.)    2.084    0.031   67.533    0.000    2.084    2.744
   .Per3    (.61.)    2.537    0.031   80.958    0.000    2.537    3.284
   .Per4    (.62.)    2.419    0.031   77.688    0.000    2.419    3.067
   .Per5    (.63.)    2.183    0.034   65.090    0.000    2.183    2.546
   .Oth1    (.64.)    2.628    0.034   77.964    0.000    2.628    3.108
   .Oth2    (.65.)    2.617    0.032   82.717    0.000    2.617    3.258
   .Oth3    (.66.)    2.147    0.033   65.562    0.000    2.147    2.604
   .Oth4    (.67.)    2.415    0.034   71.063    0.000    2.415    2.740
   .Oth5    (.68.)    2.516    0.029   87.481    0.000    2.516    3.342
   .Oth6    (.69.)    2.601    0.030   86.991    0.000    2.601    3.292
   .Oth7    (.70.)    2.130    0.031   68.797    0.000    2.130    2.612

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .Help1   (.21.)    0.447    0.024   18.643    0.000    0.447    0.808
   .Help2   (.22.)    0.443    0.026   16.948    0.000    0.443    0.686
   .Help3   (.23.)    0.545    0.030   18.272    0.000    0.545    0.778
   .Help4   (.24.)    0.496    0.028   17.699    0.000    0.496    0.735
   .Help5   (.25.)    0.471    0.028   16.684    0.000    0.471    0.670
   .Min1    (.26.)    0.486    0.037   13.237    0.000    0.486    0.730
   .Min2    (.27.)    0.529    0.033   15.800    0.000    0.529    0.806
   .Min3    (.28.)    0.542    0.032   16.988    0.000    0.542    0.847
   .Per1    (.29.)    0.528    0.030   17.766    0.000    0.528    0.732
   .Per2    (.30.)    0.372    0.023   16.379    0.000    0.372    0.645
   .Per3    (.31.)    0.391    0.024   16.557    0.000    0.391    0.655
   .Per4    (.32.)    0.493    0.027   18.526    0.000    0.493    0.792
   .Per5    (.33.)    0.615    0.032   19.010    0.000    0.615    0.836
   .Oth1    (.34.)    0.468    0.028   16.733    0.000    0.468    0.655
   .Oth2    (.35.)    0.456    0.026   17.522    0.000    0.456    0.706
   .Oth3    (.36.)    0.455    0.027   16.979    0.000    0.455    0.670
   .Oth4    (.37.)    0.618    0.033   18.638    0.000    0.618    0.796
   .Oth5    (.38.)    0.472    0.025   19.023    0.000    0.472    0.833
   .Oth6    (.39.)    0.544    0.028   19.385    0.000    0.544    0.870
   .Oth7    (.40.)    0.570    0.030   19.266    0.000    0.570    0.858
    Hlplssn           0.106    0.016    6.684    0.000    1.000    1.000
    Minmztn           0.180    0.037    4.881    0.000    1.000    1.000
    DnlPrsn           0.193    0.026    7.442    0.000    1.000    1.000
    Othrztn           0.247    0.031    7.955    0.000    1.000    1.000


Group 2 [Severe]:

Latent Variables:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness =~                                                          
    Help1                1.000                               0.342    0.455
    Help2   (.p2.)       1.383    0.107   12.902    0.000    0.473    0.579
    Help3   (.p3.)       1.212    0.102   11.826    0.000    0.414    0.489
    Help4   (.p4.)       1.299    0.105   12.389    0.000    0.444    0.533
    Help5   (.p5.)       1.477    0.113   13.041    0.000    0.505    0.593
  Minimization =~                                                          
    Min1                 1.000                               0.411    0.508
    Min2    (.p7.)       0.841    0.119    7.044    0.000    0.345    0.429
    Min3    (.p8.)       0.738    0.110    6.711    0.000    0.303    0.381
  DenialPersonhood =~                                                      
    Per1                 1.000                               0.471    0.544
    Per2    (.10.)       1.030    0.072   14.230    0.000    0.485    0.623
    Per3    (.11.)       1.033    0.073   14.122    0.000    0.487    0.614
    Per4    (.12.)       0.819    0.068   12.052    0.000    0.386    0.481
    Per5    (.13.)       0.790    0.071   11.061    0.000    0.372    0.429
  Otherization =~                                                          
    Oth1                 1.000                               0.514    0.600
    Oth2    (.15.)       0.876    0.067   13.107    0.000    0.450    0.555
    Oth3    (.16.)       0.953    0.070   13.637    0.000    0.490    0.587
    Oth4    (.17.)       0.801    0.070   11.449    0.000    0.412    0.464
    Oth5    (.18.)       0.620    0.059   10.577    0.000    0.319    0.421
    Oth6    (.19.)       0.573    0.060    9.500    0.000    0.294    0.371
    Oth7    (.20.)       0.619    0.063    9.887    0.000    0.318    0.388

Covariances:
                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Helplessness ~~                                                          
    Minimization         0.036    0.016    2.234    0.026    0.255    0.255
    DenialPersonhd       0.090    0.017    5.439    0.000    0.558    0.558
    Otherization         0.093    0.018    5.289    0.000    0.527    0.527
  Minimization ~~                                                          
    DenialPersonhd       0.060    0.022    2.704    0.007    0.308    0.308
    Otherization         0.115    0.026    4.494    0.000    0.543    0.543
  DenialPersonhood ~~                                                      
    Otherization         0.113    0.023    4.914    0.000    0.466    0.466

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .Help1   (.51.)    1.943    0.030   65.457    0.000    1.943    2.587
   .Help2   (.52.)    2.400    0.033   73.272    0.000    2.400    2.940
   .Help3   (.53.)    2.119    0.034   63.095    0.000    2.119    2.503
   .Help4   (.54.)    2.252    0.033   67.814    0.000    2.252    2.705
   .Help5   (.55.)    2.037    0.034   59.474    0.000    2.037    2.390
   .Min1    (.56.)    2.394    0.033   71.865    0.000    2.394    2.959
   .Min2    (.57.)    2.361    0.032   73.225    0.000    2.361    2.934
   .Min3    (.58.)    2.622    0.031   83.617    0.000    2.622    3.292
   .Per1    (.59.)    2.088    0.034   61.558    0.000    2.088    2.412
   .Per2    (.60.)    2.084    0.031   67.533    0.000    2.084    2.674
   .Per3    (.61.)    2.537    0.031   80.958    0.000    2.537    3.202
   .Per4    (.62.)    2.419    0.031   77.688    0.000    2.419    3.020
   .Per5    (.63.)    2.183    0.034   65.090    0.000    2.183    2.515
   .Oth1    (.64.)    2.628    0.034   77.964    0.000    2.628    3.071
   .Oth2    (.65.)    2.617    0.032   82.717    0.000    2.617    3.225
   .Oth3    (.66.)    2.147    0.033   65.562    0.000    2.147    2.575
   .Oth4    (.67.)    2.415    0.034   71.063    0.000    2.415    2.721
   .Oth5    (.68.)    2.516    0.029   87.481    0.000    2.516    3.323
   .Oth6    (.69.)    2.601    0.030   86.991    0.000    2.601    3.277
   .Oth7    (.70.)    2.130    0.031   68.797    0.000    2.130    2.599
    Hlplssn           0.536    0.043   12.323    0.000    1.567    1.567
    Minmztn           0.333    0.051    6.525    0.000    0.812    0.812
    DnlPrsn           0.606    0.050   12.024    0.000    1.286    1.286
    Othrztn           0.484    0.049    9.810    0.000    0.942    0.942

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .Help1   (.21.)    0.447    0.024   18.643    0.000    0.447    0.793
   .Help2   (.22.)    0.443    0.026   16.948    0.000    0.443    0.665
   .Help3   (.23.)    0.545    0.030   18.272    0.000    0.545    0.761
   .Help4   (.24.)    0.496    0.028   17.699    0.000    0.496    0.716
   .Help5   (.25.)    0.471    0.028   16.684    0.000    0.471    0.649
   .Min1    (.26.)    0.486    0.037   13.237    0.000    0.486    0.742
   .Min2    (.27.)    0.529    0.033   15.800    0.000    0.529    0.816
   .Min3    (.28.)    0.542    0.032   16.988    0.000    0.542    0.855
   .Per1    (.29.)    0.528    0.030   17.766    0.000    0.528    0.704
   .Per2    (.30.)    0.372    0.023   16.379    0.000    0.372    0.612
   .Per3    (.31.)    0.391    0.024   16.557    0.000    0.391    0.623
   .Per4    (.32.)    0.493    0.027   18.526    0.000    0.493    0.768
   .Per5    (.33.)    0.615    0.032   19.010    0.000    0.615    0.816
   .Oth1    (.34.)    0.468    0.028   16.733    0.000    0.468    0.639
   .Oth2    (.35.)    0.456    0.026   17.522    0.000    0.456    0.692
   .Oth3    (.36.)    0.455    0.027   16.979    0.000    0.455    0.655
   .Oth4    (.37.)    0.618    0.033   18.638    0.000    0.618    0.785
   .Oth5    (.38.)    0.472    0.025   19.023    0.000    0.472    0.823
   .Oth6    (.39.)    0.544    0.028   19.385    0.000    0.544    0.863
   .Oth7    (.40.)    0.570    0.030   19.266    0.000    0.570    0.849
    Hlplssn           0.117    0.020    5.838    0.000    1.000    1.000
    Minmztn           0.169    0.042    4.001    0.000    1.000    1.000
    DnlPrsn           0.222    0.035    6.398    0.000    1.000    1.000
    Othrztn           0.264    0.039    6.697    0.000    1.000    1.000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{anova}\NormalTok{(configural, weak, strong, strict)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Chi-Squared Difference Test

            Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff   Pr(>Chisq)    
configural 328 38438 39061 327.57                                             
weak       344 38431 38979 352.99     25.420 0.037598      16      0.06275 .  
strong     360 38459 38931 412.75     59.754 0.081029      16 0.0000005758 ***
strict     380 38446 38824 439.69     26.946 0.028876      20      0.13679    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Let's format these results into tables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{strictFitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(strict)}
\NormalTok{strict\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(strict, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{strictCorrs }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_cors}\NormalTok{(strict, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\CommentTok{\# to see each of the tables, remove the hashtag strictFitStats}
\CommentTok{\# strict\_paramEsts strictCorrs}
\end{Highlighting}
\end{Shaded}

Then, export them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(strictFitStats, }\AttributeTok{file =} \StringTok{"strictFitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(strict\_paramEsts, }\AttributeTok{file =} \StringTok{"strict\_paramEsts.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(strictCorrs, }\AttributeTok{file =} \StringTok{"strictCorrs.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# semPlot::semPaths(strict, layout = \textquotesingle{}tree\textquotesingle{}, style = \textquotesingle{}lisrel\textquotesingle{}, what =}
\CommentTok{\# \textquotesingle{}col\textquotesingle{}, whatLabels = \textquotesingle{}stand\textquotesingle{})}
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(strict, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"est"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{12-Invariance_files/figure-latex/unnamed-chunk-32-1.pdf} \includegraphics{12-Invariance_files/figure-latex/unnamed-chunk-32-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# If R stalls, open the console. I received the instruction, \textquotesingle{}Hit}
\CommentTok{\# \textless{}Return\textgreater{} to see next plot:\textquotesingle{} Then it ran!}
\end{Highlighting}
\end{Shaded}

\hypertarget{interpreting-the-output-10}{%
\subsection{Interpreting the Output}\label{interpreting-the-output-10}}

Note that although the ``Std.all'' values differ from each other, the ``Estimates'' (factor loadings) are identical across Mild and Severe groups. Each also has a ``label'' (e.g., .p2., .p3.) which indicates that they have been constrained to be equal. The ``Std.all'' differ between degree of disability severity due to the difference in standard deviations of the indicators.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3780}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4512}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1707}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Our Results
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Criteria met?
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Mild: factor loadings significant, strong, proper valence & Help: .44 to .57; Min: .44 to .52; Pers: .41 to .60; Oth: .36 to .58 & \\
Severe: factor loadings significant, strong, proper valence & Help: .46 to .59; Min: .38 to .51 Pers: .43 to .62; Oth: .37 to .60 & \\
Yes & & \\
Non-significant chi-square & \(\chi ^{2}(380) = 439.69 p < 0.019\) & No \\
\(CFI\geq .95\) or \(CFI\geq .90\) & CFI = 0.971 & No \\
\(RMSEA\leq .05\) (but definitely \textless{} .10) & RMSEA = 0.019, CI90\%(0.009 to 0.027) & Yes(ish) \\
\(SRMR\leq .08\) (but definitely \textless{} .10) & SRMR = 0.046 & Yes(ish) \\
Combination rule: \(CFI \geq .95\) and \(SRMR \leq .08\) & CFI = 0.971, SRMR = 0.046 & No \\
\end{longtable}

\hypertarget{partial-write-up-9}{%
\subsection{Partial Write-up}\label{partial-write-up-9}}

\begin{quote}
\textbf{Strict invariance model}. In the strict invariance model, configuration, factor loadings, indicator means/intercepts, and residuals were constrained to be the same for each group. Fit indices were less than ideal: \(\chi ^{2}(380) = 439.69 p < 0.019\) , CFI = 0.971, SRMR = 0.046, RMSEA = 0.019, (90\%CI = 0.009 to 0.027). Although the non-significant chi-square difference test and the change CFI test indicate that the strong and strict invariance models are not statistically significant from each other (\(\chi_{D}^{2}(20) = 26.946, p = 0.137\); \(\Delta CFI = 0.004\)) our earlier data indicated that we cannot claim invariance beyond the weak model.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# CFI difference test, calculated by hand}
\FloatTok{0.975} \SpecialCharTok{{-}} \FloatTok{0.971}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.004
\end{verbatim}

\hypertarget{partial-measurement-invariance-1}{%
\section{Partial Measurement Invariance}\label{partial-measurement-invariance-1}}

\emph{Partial measurement invariance} can be seen as an intermediate state of invariance and whatever stage (beyond configural; weak, strong, strict) that the criteria for invariance is not met. For example:

\begin{itemize}
\tightlist
\item
  if the model failed at weak invariance, tests of partial measurement invariance could determine which factor loadings are (and are not) invariant across groups;
\item
  if the model failed at strong invariance, tests of partial measurement invariance could determine which intercepts/means are (and are not) invariant across groups;
\item
  (this one is aspirational) if the model failed at strict invariance, tests of partial measurement invariance could determine which residuals are (and are not) invariant across groups.
\end{itemize}

Using some of the investigative tools in \emph{lavaan} and the associated packages, researchers can identify which elements are noninvariant. They can free the constraints until the fit statistics are acceptable and the the Chi-square difference and \(\Delta CFI\) tests are no longer significant.

Conover et al. \citeyearpar{conover_development_2017} reported that the AMS was invariant at configural and weak invariance (i.e., constraining factor loadings to be equal). At the level of strong invariance (i.e., adding constraints to the intercepts), results, the majority of fit indices remained acceptable (CFI = .90, RMSEA = .06, SRMR = .08). However the chi-square difference and CFI change tests were statistically significant: \(\chi_{D}^{2}(20) = 78.83, p < .01\); \(\Delta CFI = -.010\). The Conover et al.~article did not report further investigation regarding partial measurement invariance.

\hypertarget{apa-style-write-up-of-the-results}{%
\section{APA Style Write-up of the Results}\label{apa-style-write-up-of-the-results}}

As in the Conover et al. \citeyearpar{conover_development_2017} article, the write-up of invariance testing would likely be part of a multi-stage evaluation. Therefore, this section would be preceded by a variety of steps in a psychometric evaluation. Here is an example of how I might write this up.

\hypertarget{measurement-invariance-across-disability-severity}{%
\subsection{Measurement Invariance Across Disability Severity}\label{measurement-invariance-across-disability-severity}}

\begin{quote}
To test if the factor structures of the AMS were stable across disability severity, we used measurement invariance analyses. First, we constructed CFA models with the \emph{lavaan} (v. 0.6-17) package in R and created two groups representing mild and severe. Within \emph{lavaan} we successivly constrained parameters representing the configural, weak (loadings), strong (intercepts), and strict (residuals) structures \citep{hirschfeld_multiple-group_2014, kline_principles_2016}. A poor fit in any of these models suggests that the aspect being constrained does not operate consistently for the different groups. The degree of invariance was determined jointly when \(\chi_{D}^2, p > .05\); and a \(\Delta CFI < .01\).
\end{quote}

\begin{quote}
The configural model constrains only the relative configuration of variables in the model to be the same in both groups. In other words, no factor loadings or indicator means are constrained to be the same across groups, but the organization of indicators is the same for both groups. Next, in weak factorial invariance, the configuration of variables and all factor loadings are constrained to be the same for each group. Poor fit here suggests that the factor loadings vary in size between the two groups. In strong factorial invariance, both the configuration, factor loadings, and the indicator means are constrained to be the same for each group. A reduction in fit here, but not in the previous steps, suggests that indicators have different means in both groups, which might be expected when comparing two groups of people, such as in a t-test. Therefore, poor fit in only this model does not necessarily indicate the factor structure operates differently for different groups. Finally, strict factorial invariance requires strong invariance and equality in error variances and covariances across groups. This means that the indicators measure the same factors in each group with the same degree of precision.
\end{quote}

\begin{quote}
We selected fit criteria for their capacity to assess different aspects of the statistical analysis. As is common among SEM researchers, we reported the chi-square goodness of fit (\(\chi^2\)). This evaluates the discrepancy between the unrestricted sample matrix and the restricted covariance matrix. Although the associated \(p\) value indicates adequate fit when the value is non-significant, it is widely recognized that large sample size can result in a statistically significant \(p\) value \citep{byrne_structural_2016-1}. The comparative fit index (CFI) is an incremental index, comparing the hypothesized model with the baseline model, and should be at least .90 and perhaps higher than .95 \citep{hu_cutoff_1999}. The root mean square error of approximation (RMSEA) takes into account the error of approximation in the population and expresses it per degree of freedom. As such, the fit indicator considers the complexity of the model. Ideal values are equal to or less than .05, values less than .08 represent reasonable fit, and values between .08 and .10 represent mediocre fit. The standardized root mean residual (SRMR) is a standardized measure of the mean absolute covariance residual -- the overall difference between the observed and predicted correlations. Values greater than .10 may indicate poor fit and inspection of residuals is then advised. Researchers have advised caution when using these criteria as strict cutoffs, and other factors such as sample size and model complexity should be considered \citep{kline_principles_2016}.
\end{quote}

\begin{quote}
AMS items each were loaded on their respective correlated factors. The configural model, which constrained only the relative configuration of variables in the model to be the same in both groups had adequate fit to the data: \(\chi ^{2}(328) = 327.57, p = 0.496\), CFI = 1.000, SRMR = 0.034, RMSEA = 0.000, 90\%CI(0.000, 0.018). The weak invariance model constrained the configuration of variables and all factor loadings to be constant across groups. Fit indices were comparable to the configural model: \(\chi ^{2}(344) = 353.00, p = 0.357\), CFI = 0.996, SRMR = 0.039, RMSEA = 0.008, 90\%CI(0.000, 0.020. Invariance of the factor loadings was supported by the non-significant difference tests that assessed model similarity: \(\chi_{D}^{2}(16) = 25.42, p = 0.063\); \(\Delta CFI = 0.004\). In the strong invariance model, configuration, factor loadings, and indicator means/intercepts were constrained to be the same for each group. Fit indices were less than ideal: \(\chi ^{2}(360) = 412.75 p = 0.029\), CFI = 0.975, SRMR = 0.045 , RMSEA =0.019, CI90\%(0.007 to 0.027). The difference tests that evaluated model similarity suggested there was factorial noninvariance: (\(\chi_{D}^{2}(16) = 59.754, p = 0.01\); \(\Delta CFI = 0.021\). Given that the \(\chi_{D}^{2}\) test is statistically significant and the \(\Delta CFI 0.01\) we cannot claim strong invariance and we therefore do not test stricter models. Because we found noninvariance at the strong level, we did not attempt to model strict invariance.
\end{quote}

\begin{quote}
Overall, this analysis suggests that the factor structure of the AMS was stable for mild/moderate and severe/very severe levels of disability. Figure 1 provides an illustration of the factor structure. Tables 1 and 2 provide fit indices for each of the factor structures and a summary of the measurement invariance tests.
\end{quote}

\hypertarget{practice-problems-10}{%
\section{Practice Problems}\label{practice-problems-10}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty The least complex is to change the random seed and rework the problem demonstrated in the lesson. The results \emph{should} map onto the ones obtained in the lecture.

The second option would be to adapt one of the codes in the \protect\hyperlink{sims}{simluations chapter} to create two groups for which invariance testing would be appropriate for that measure.

As a third option, you are welcome to use data to which you have access and is suitable for invariance testing. In either case, you will be expected to:

\begin{itemize}
\tightlist
\item
  Specify, interpret, and write up preliminary results for CFA models that examine

  \begin{itemize}
  \tightlist
  \item
    entire sample (making no distinction between groups)
  \item
    configural invariance
  \item
    weak invariance
  \item
    strong invariance
  \item
    strict invariance
  \end{itemize}
\item
  Create an APA style results section with appropriate table(s) and figure(s)
\item
  Talk about it with someone
\end{itemize}

\hypertarget{problem-1-play-around-with-this-simulation.-7}{%
\subsection{Problem \#1: Play around with this simulation.}\label{problem-1-play-around-with-this-simulation.-7}}

Copy the script for the simulation and then change the number in ``set.seed(211023)'' from 211023 to something else. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go.

\hypertarget{problem-2-adapt-one-of-the-simulated-data-sets.}{%
\subsection{Problem \#2: Adapt one of the simulated data sets.}\label{problem-2-adapt-one-of-the-simulated-data-sets.}}

The \protect\hyperlink{sims}{Simulations} includes simulated data from many of the research vignettes used in this volume. Using guidance provided in this lesson, adapt one of those simulations to include at least two groups for which invariance testing would be appropriate.

\hypertarget{problem-3-try-something-entirely-new.-7}{%
\subsection{Problem \#3: Try something entirely new.}\label{problem-3-try-something-entirely-new.-7}}

Using data for which you have permission and access (e.g., IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository) complete the multi-group invariance testing process.

\hypertarget{grading-rubric-7}{%
\subsection{Grading Rubric}\label{grading-rubric-7}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5493}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1972}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Possible
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Earned
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1. Check and, if needed, format data & 5 & \_\_\_\_\_ \\
2. Specify, evaluate, and interpret the CFA for the entire sample (making no distinction between groups). Write up the preliminary results. & 5 & \_\_\_\_\_ \\
3. Specify, evaluate, and interpret the CFA for configural invariance. Write up the preliminary results. & 5 & \_\_\_\_\_ \\
4. Specify, evaluate, and interpret the CFA for weak invariance. Conduct the analysis to compare fit the weak and configural models. Write up the preliminary results. & 5 & \_\_\_\_\_ \\
5. Specify, evaluate, and interpret the CFA for strong invariance. Conduct the analysis to compare fit the strong and weak models. Write up the preliminary results. & 5 & \_\_\_\_\_ \\
6. Specify, evaluate, and interpret the CFA for strict invariance. Conduct the analysis to compare fit the strict and strong models. Write up the preliminary results. & 5 & \_\_\_\_\_ \\
7. Create an APA style results section. Do not report any invariance tests past the one that failed. Include a table(s) and figure(s) & 10 & \_\_\_\_\_ \\
8. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 45 & \_\_\_\_\_ \\
\end{longtable}

\hypertarget{homeworked-example-7}{%
\section{Homeworked Example}\label{homeworked-example-7}}

\href{https://youtu.be/DK0-gWSa7MI}{Screencast Link}

For more information about the data used in this homeworked example, please refer to the description and codebook located at the end of the \href{https://lhbikos.github.io/ReCenterPsychStats/ReCintro.html\#introduction-to-the-data-set-used-for-homeworked-examples}{introduction} in first volume of ReCentering Psych Stats.

As a brief review, this data is part of an IRB-approved study, with consent to use in teaching demonstrations and to be made available to the general public via the open science framework. Hence, it is appropriate to use in this context. You will notice there are student- and teacher- IDs. These numbers are not actual student and teacher IDs, rather they were further re-identified so that they could not be connected to actual people.

Because this is an actual dataset, if you wish to work the problem along with me, you will need to download the \href{https://github.com/lhbikos/ReC_Psychometrics/blob/main/Worked_Examples/ReC.rds}{ReC.rds} data file from the Worked\_Examples folder in the ReC\_Psychometrics project on the GitHub.

The course evaluation items can be divided into three subscales:

\begin{itemize}
\tightlist
\item
  \textbf{Valued by the student} includes the items: ValObjectives, IncrUnderstanding, IncrInterest
\item
  \textbf{Traditional pedagogy} includes the items: ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation
\item
  \textbf{Socially responsive pedagogy} includes the items: InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration
\end{itemize}

In this Homeworked Example I will assess for invariance of the a first-level, correlated factors model for the course evaluation data across students in the Clinical Psychology (CPY) and Industrial-Organizational Psychology (ORG) doctoral programs, asking ``Is the structure of the course evaluation item data invariant across CPY and IOP?''

\hypertarget{check-and-if-needed-format-data-3}{%
\subsection*{Check and, if needed, format data}\label{check-and-if-needed-format-data-3}}


\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{big }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"ReC.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The invariance testing will be easiest if I trim the dataset to the items and the grouping variable (i.e., Dept).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{items }\OtherTok{\textless{}{-}}\NormalTok{ big }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Dept, ValObjectives, IncrUnderstanding, IncrInterest,}
\NormalTok{        ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization,}
\NormalTok{        ClearPresentation, MultPerspectives, InclusvClassrm, DEIintegration,}
\NormalTok{        EquitableEval)}
\end{Highlighting}
\end{Shaded}

Let's check the structure of the data. The items should be numerical or integer; the Dept variable must be a factor.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(items)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Classes 'data.table' and 'data.frame':  310 obs. of  13 variables:
 $ Dept                 : chr  "CPY" "CPY" "CPY" "CPY" ...
 $ ValObjectives        : int  5 5 4 4 5 5 5 5 4 5 ...
 $ IncrUnderstanding    : int  2 3 4 3 4 4 5 2 4 5 ...
 $ IncrInterest         : int  5 3 4 2 4 3 5 3 2 5 ...
 $ ClearResponsibilities: int  5 5 4 4 5 4 5 4 4 5 ...
 $ EffectiveAnswers     : int  5 3 5 3 5 3 4 3 2 3 ...
 $ Feedback             : int  5 3 4 2 5 NA 5 4 4 5 ...
 $ ClearOrganization    : int  3 4 3 4 4 4 5 4 4 5 ...
 $ ClearPresentation    : int  4 4 4 2 5 3 4 4 4 5 ...
 $ MultPerspectives     : int  5 5 4 5 5 4 5 5 5 5 ...
 $ InclusvClassrm       : int  5 5 5 5 5 4 5 5 4 5 ...
 $ DEIintegration       : int  5 5 5 5 5 4 5 5 5 5 ...
 $ EquitableEval        : int  5 5 3 5 5 3 5 5 3 5 ...
 - attr(*, ".internal.selfref")=<externalptr> 
\end{verbatim}

Let's change the Dept variable to be a factor.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{items}\SpecialCharTok{$}\NormalTok{Dept }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(items}\SpecialCharTok{$}\NormalTok{Dept)}
\end{Highlighting}
\end{Shaded}

\hypertarget{specify-evaluate-and-interpret-the-cfa-for-the-entire-sample-making-no-distinction-between-groups.-write-up-the-preliminary-results.}{%
\subsection*{Specify, evaluate, and interpret the CFA for the entire sample (making no distinction between groups). Write up the preliminary results.}\label{specify-evaluate-and-interpret-the-cfa-for-the-entire-sample-making-no-distinction-between-groups.-write-up-the-preliminary-results.}}


\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corrF }\OtherTok{\textless{}{-}} \StringTok{"TradPed =\textasciitilde{} ClearResponsibilities + EffectiveAnswers + Feedback + ClearOrganization + ClearPresentation  }
\StringTok{             Valued =\textasciitilde{} ValObjectives + IncrUnderstanding + IncrInterest }
\StringTok{             SCRPed =\textasciitilde{} MultPerspectives + InclusvClassrm + DEIintegration + EquitableEval}

\StringTok{  TradPed\textasciitilde{}\textasciitilde{}Valued}
\StringTok{  TradPed\textasciitilde{}\textasciitilde{}SCRPed}
\StringTok{  Valued\textasciitilde{}\textasciitilde{}SCRPed}
\StringTok{"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240505}\NormalTok{)}
\NormalTok{corrF\_fit }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(corrF, }\AttributeTok{data =}\NormalTok{ items)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(corrF\_fit, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rsquare =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 42 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        27

                                                  Used       Total
  Number of observations                           267         310

Model Test User Model:
                                                      
  Test statistic                               224.795
  Degrees of freedom                                51
  P-value (Chi-square)                           0.000

Model Test Baseline Model:

  Test statistic                              1940.157
  Degrees of freedom                                66
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.907
  Tucker-Lewis Index (TLI)                       0.880

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -2977.975
  Loglikelihood unrestricted model (H1)      -2865.578
                                                      
  Akaike (AIC)                                6009.951
  Bayesian (BIC)                              6106.807
  Sample-size adjusted Bayesian (SABIC)       6021.201

Root Mean Square Error of Approximation:

  RMSEA                                          0.113
  90 Percent confidence interval - lower         0.098
  90 Percent confidence interval - upper         0.128
  P-value H_0: RMSEA <= 0.050                    0.000
  P-value H_0: RMSEA >= 0.080                    1.000

Standardized Root Mean Square Residual:

  SRMR                                           0.061

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed =~                                                            
    ClearRspnsblts    1.000                               0.652    0.826
    EffectivAnswrs    1.015    0.065   15.606    0.000    0.662    0.815
    Feedback          1.010    0.075   13.481    0.000    0.659    0.735
    ClearOrganiztn    1.295    0.086   15.106    0.000    0.845    0.797
    ClearPresenttn    1.204    0.072   16.680    0.000    0.785    0.853
  Valued =~                                                             
    ValObjectives     1.000                               0.334    0.557
    IncrUndrstndng    1.942    0.223    8.717    0.000    0.649    0.786
    IncrInterest      2.438    0.273    8.932    0.000    0.815    0.844
  SCRPed =~                                                             
    MultPerspectvs    1.000                               0.713    0.846
    InclusvClassrm    0.622    0.053   11.672    0.000    0.444    0.682
    DEIintegration    0.589    0.063    9.365    0.000    0.420    0.567
    EquitableEval     0.642    0.052   12.410    0.000    0.458    0.717

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed ~~                                                            
    Valued            0.171    0.026    6.640    0.000    0.785    0.785
    SCRPed            0.391    0.045    8.677    0.000    0.841    0.841
  Valued ~~                                                             
    SCRPed            0.164    0.026    6.254    0.000    0.688    0.688

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClearRspnsblts    0.199    0.021    9.456    0.000    0.199    0.319
   .EffectivAnswrs    0.222    0.023    9.618    0.000    0.222    0.336
   .Feedback          0.371    0.036   10.415    0.000    0.371    0.460
   .ClearOrganiztn    0.410    0.042    9.855    0.000    0.410    0.365
   .ClearPresenttn    0.232    0.026    8.939    0.000    0.232    0.273
   .ValObjectives     0.248    0.023   10.650    0.000    0.248    0.690
   .IncrUndrstndng    0.260    0.032    8.041    0.000    0.260    0.382
   .IncrInterest      0.268    0.043    6.308    0.000    0.268    0.288
   .MultPerspectvs    0.203    0.029    7.052    0.000    0.203    0.285
   .InclusvClassrm    0.226    0.023   10.028    0.000    0.226    0.534
   .DEIintegration    0.371    0.035   10.734    0.000    0.371    0.678
   .EquitableEval     0.198    0.020    9.685    0.000    0.198    0.486
    TradPed           0.426    0.053    8.085    0.000    1.000    1.000
    Valued            0.112    0.024    4.595    0.000    1.000    1.000
    SCRPed            0.509    0.063    8.039    0.000    1.000    1.000

R-Square:
                   Estimate
    ClearRspnsblts    0.681
    EffectivAnswrs    0.664
    Feedback          0.540
    ClearOrganiztn    0.635
    ClearPresenttn    0.727
    ValObjectives     0.310
    IncrUndrstndng    0.618
    IncrInterest      0.712
    MultPerspectvs    0.715
    InclusvClassrm    0.466
    DEIintegration    0.322
    EquitableEval     0.514
\end{verbatim}

Among my first steps are also to write the code to export the results. The \emph{tidySEM} package has useful functions to export the fit statistics, parameter estimates, and correlations among the latent variables (i.e., factors).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corrF\_FitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(corrF\_fit)}
\NormalTok{corrF\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(corrF\_fit, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{corrF\_Corrs }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_cors}\NormalTok{(corrF\_fit, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\CommentTok{\# to see each of the tables, remove the hashtag corrF\_FitStats}
\CommentTok{\# bifacF\_paramEsts bifacFCorrs}
\end{Highlighting}
\end{Shaded}

Next, I export them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(corrF\_FitStats, }\AttributeTok{file =} \StringTok{"corrF\_FitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(corrF\_paramEsts, }\AttributeTok{file =} \StringTok{"corrF\_paramEsts.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(corrF\_Corrs, }\AttributeTok{file =} \StringTok{"corrF\_Corrs"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Troubleshooting tip}: If, while working with this function you get the error: \emph{``Error in file(file, ifelse(append,''a'', ``w'')) : cannot open the connection''}, it's because the .csv file that received your table is still open. R is just trying to write over it. A similar error happens when knitting.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(corrF\_fit, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{12-Invariance_files/figure-latex/unnamed-chunk-78-1.pdf}

\begin{quote}
We evaluated a single-order, multidimensional model where each of the 12 items loaded onto one of three correlated factors. Standardized pattern coefficients ranged between 0.74 and 0.85 on the TradPed factor, between 0.56 and 0.84 on the Valued factor, and between 0.57 and 0.85 on the SCRPed factor. The Chi-square index was statistically significant \((\chi^2(51) = 224.795, p < 0.001\) indicating some degree of misfit. The CFI value of 0.91 exceeded the bare minimum recommendation of 0.90. The RMSEA = 0.113 (90\% CI {[}0.098, 0.128{]}) was higher than recommended value of 0.10. The SRMR value of 0.061 remained below the warning criteria of 0.10. The AIC and BIC values were 6009.95 and 6106.81, respectively.
\end{quote}

At the outset, let me acknowledge that starting the invariance testing with fit indices that are on the margins of acceptability suggests that we may not get very far.

\hypertarget{specify-evaluate-and-interpret-the-cfa-for-configural-invariance.-write-up-the-preliminary-results.}{%
\subsection*{Specify, evaluate, and interpret the CFA for configural invariance. Write up the preliminary results.}\label{specify-evaluate-and-interpret-the-cfa-for-configural-invariance.-write-up-the-preliminary-results.}}


The only addition to the prior code is to add the grouping variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{configural }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(corrF, }\AttributeTok{data =}\NormalTok{ items, }\AttributeTok{group =} \StringTok{"Dept"}\NormalTok{)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(configural, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 71 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        78

  Number of observations per group:               Used       Total
    CPY                                            202         232
    ORG                                             65          78

Model Test User Model:
                                                      
  Test statistic                               339.428
  Degrees of freedom                               102
  P-value (Chi-square)                           0.000
  Test statistic for each group:
    CPY                                        202.821
    ORG                                        136.608

Model Test Baseline Model:

  Test statistic                              2156.987
  Degrees of freedom                               132
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.883
  Tucker-Lewis Index (TLI)                       0.848

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -2897.967
  Loglikelihood unrestricted model (H1)      -2728.252
                                                      
  Akaike (AIC)                                5951.933
  Bayesian (BIC)                              6231.739
  Sample-size adjusted Bayesian (SABIC)       5984.433

Root Mean Square Error of Approximation:

  RMSEA                                          0.132
  90 Percent confidence interval - lower         0.117
  90 Percent confidence interval - upper         0.148
  P-value H_0: RMSEA <= 0.050                    0.000
  P-value H_0: RMSEA >= 0.080                    1.000

Standardized Root Mean Square Residual:

  SRMR                                           0.073

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured


Group 1 [CPY]:

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed =~                                                            
    ClearRspnsblts    1.000                               0.580    0.764
    EffectivAnswrs    1.017    0.087   11.659    0.000    0.590    0.782
    Feedback          1.048    0.098   10.661    0.000    0.608    0.724
    ClearOrganiztn    1.393    0.121   11.540    0.000    0.808    0.775
    ClearPresenttn    1.380    0.105   13.134    0.000    0.801    0.865
  Valued =~                                                             
    ValObjectives     1.000                               0.360    0.603
    IncrUndrstndng    1.960    0.226    8.668    0.000    0.705    0.820
    IncrInterest      2.251    0.259    8.688    0.000    0.810    0.823
  SCRPed =~                                                             
    MultPerspectvs    1.000                               0.681    0.875
    InclusvClassrm    0.830    0.061   13.701    0.000    0.566    0.812
    DEIintegration    0.736    0.072   10.187    0.000    0.501    0.656
    EquitableEval     0.590    0.056   10.501    0.000    0.402    0.671

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed ~~                                                            
    Valued            0.185    0.030    6.227    0.000    0.885    0.885
    SCRPed            0.329    0.045    7.373    0.000    0.833    0.833
  Valued ~~                                                             
    SCRPed            0.160    0.028    5.663    0.000    0.653    0.653

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClearRspnsblts    4.510    0.053   84.410    0.000    4.510    5.939
   .EffectivAnswrs    4.416    0.053   83.154    0.000    4.416    5.851
   .Feedback          4.327    0.059   73.232    0.000    4.327    5.153
   .ClearOrganiztn    4.109    0.073   56.009    0.000    4.109    3.941
   .ClearPresenttn    4.287    0.065   65.775    0.000    4.287    4.628
   .ValObjectives     4.554    0.042  108.498    0.000    4.554    7.634
   .IncrUndrstndng    4.287    0.060   70.863    0.000    4.287    4.986
   .IncrInterest      3.941    0.069   56.961    0.000    3.941    4.008
   .MultPerspectvs    4.490    0.055   81.955    0.000    4.490    5.766
   .InclusvClassrm    4.649    0.049   94.832    0.000    4.649    6.672
   .DEIintegration    4.550    0.054   84.599    0.000    4.550    5.952
   .EquitableEval     4.604    0.042  109.361    0.000    4.604    7.695

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClearRspnsblts    0.240    0.027    8.916    0.000    0.240    0.416
   .EffectivAnswrs    0.221    0.025    8.775    0.000    0.221    0.388
   .Feedback          0.335    0.037    9.163    0.000    0.335    0.475
   .ClearOrganiztn    0.434    0.049    8.831    0.000    0.434    0.399
   .ClearPresenttn    0.217    0.028    7.607    0.000    0.217    0.252
   .ValObjectives     0.227    0.025    9.223    0.000    0.227    0.636
   .IncrUndrstndng    0.242    0.035    6.920    0.000    0.242    0.328
   .IncrInterest      0.311    0.046    6.832    0.000    0.311    0.322
   .MultPerspectvs    0.143    0.024    5.963    0.000    0.143    0.235
   .InclusvClassrm    0.166    0.022    7.588    0.000    0.166    0.341
   .DEIintegration    0.333    0.036    9.125    0.000    0.333    0.570
   .EquitableEval     0.197    0.022    9.045    0.000    0.197    0.550
    TradPed           0.337    0.054    6.279    0.000    1.000    1.000
    Valued            0.129    0.029    4.462    0.000    1.000    1.000
    SCRPed            0.464    0.062    7.515    0.000    1.000    1.000


Group 2 [ORG]:

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed =~                                                            
    ClearRspnsblts    1.000                               0.810    0.924
    EffectivAnswrs    1.101    0.086   12.866    0.000    0.892    0.920
    Feedback          0.875    0.129    6.808    0.000    0.709    0.682
    ClearOrganiztn    1.096    0.118    9.325    0.000    0.888    0.810
    ClearPresenttn    1.015    0.081   12.502    0.000    0.822    0.911
  Valued =~                                                             
    ValObjectives     1.000                               0.224    0.366
    IncrUndrstndng    2.193    0.824    2.661    0.008    0.490    0.693
    IncrInterest      3.450    1.262    2.735    0.006    0.771    0.865
  SCRPed =~                                                             
    MultPerspectvs    1.000                               0.901    0.895
    InclusvClassrm    0.146    0.068    2.155    0.031    0.131    0.278
    DEIintegration    0.242    0.093    2.596    0.009    0.218    0.331
    EquitableEval     0.569    0.095    6.002    0.000    0.513    0.687

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed ~~                                                            
    Valued            0.106    0.047    2.263    0.024    0.586    0.586
    SCRPed            0.577    0.128    4.497    0.000    0.791    0.791
  Valued ~~                                                             
    SCRPed            0.161    0.067    2.415    0.016    0.798    0.798

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClearRspnsblts    4.431    0.109   40.755    0.000    4.431    5.055
   .EffectivAnswrs    4.369    0.120   36.321    0.000    4.369    4.505
   .Feedback          4.108    0.129   31.857    0.000    4.108    3.951
   .ClearOrganiztn    3.892    0.136   28.602    0.000    3.892    3.548
   .ClearPresenttn    4.215    0.112   37.642    0.000    4.215    4.669
   .ValObjectives     4.523    0.076   59.745    0.000    4.523    7.410
   .IncrUndrstndng    4.338    0.088   49.425    0.000    4.338    6.130
   .IncrInterest      4.138    0.111   37.392    0.000    4.138    4.638
   .MultPerspectvs    4.308    0.125   34.506    0.000    4.308    4.280
   .InclusvClassrm    4.738    0.059   80.735    0.000    4.738   10.014
   .DEIintegration    4.523    0.082   55.348    0.000    4.523    6.865
   .EquitableEval     4.523    0.093   48.854    0.000    4.523    6.060

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClearRspnsblts    0.112    0.028    3.960    0.000    0.112    0.146
   .EffectivAnswrs    0.145    0.036    4.070    0.000    0.145    0.154
   .Feedback          0.578    0.106    5.453    0.000    0.578    0.535
   .ClearOrganiztn    0.415    0.080    5.153    0.000    0.415    0.345
   .ClearPresenttn    0.139    0.033    4.260    0.000    0.139    0.170
   .ValObjectives     0.323    0.058    5.527    0.000    0.323    0.866
   .IncrUndrstndng    0.260    0.058    4.492    0.000    0.260    0.520
   .IncrInterest      0.201    0.092    2.182    0.029    0.201    0.252
   .MultPerspectvs    0.202    0.088    2.294    0.022    0.202    0.199
   .InclusvClassrm    0.207    0.037    5.640    0.000    0.207    0.923
   .DEIintegration    0.386    0.069    5.611    0.000    0.386    0.890
   .EquitableEval     0.294    0.059    5.009    0.000    0.294    0.528
    TradPed           0.656    0.135    4.867    0.000    1.000    1.000
    Valued            0.050    0.036    1.400    0.161    1.000    1.000
    SCRPed            0.811    0.192    4.229    0.000    1.000    1.000
\end{verbatim}

Among my first steps are also to write the code to export the results. The \emph{tidySEM} package has useful functions to export the fit statistics, parameter estimates, and correlations among the latent variables (i.e., factors).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{configural\_FitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(configural)}
\NormalTok{configural\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(configural, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{configural\_Corrs }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_cors}\NormalTok{(configural, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\CommentTok{\# to see each of the tables, remove the hashtag configural\_FitStats}
\CommentTok{\# configural\_paramEsts configural\_Corrs}
\end{Highlighting}
\end{Shaded}

Next, I export them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(configural\_FitStats, }\AttributeTok{file =} \StringTok{"configural\_FitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(configural\_paramEsts, }\AttributeTok{file =} \StringTok{"configural\_paramEsts.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(configural\_Corrs, }\AttributeTok{file =} \StringTok{"configural\_Corrs.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can also plot our work

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(configural, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"est"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{12-Invariance_files/figure-latex/unnamed-chunk-82-1.pdf} \includegraphics{12-Invariance_files/figure-latex/unnamed-chunk-82-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# If the procedure stalls open the Consult and follow the}
\CommentTok{\# instructions to Hit Return to see the plot}
\end{Highlighting}
\end{Shaded}

\begin{quote}
\textbf{Configural Model.} The configural model, which constrained only the relative configuration of variables in the model to be the same in both groups had less than adequate fit to the data: \(\chi^2 (102)=339.43, p < 0.001, CFI = 0.883, SRMR = 0.073, RMSEA = 0.132, 90%CI(0.117, 0.148)
\). For CPY, standardized pattern coefficients ranged between 0.72 and 0.87 on the TradPed factor, between 0.60 and 0.82 on the Valued factor, and between 0.66 and 0.88 on the SCRPed factor. For ORG, standardized pattern coefficients ranged between 0.58 and 0.92 for the TradPed factor, between 0.37 and 0.87 on the Valued factor, and between 0.28 and 0.90 on the SCRPed factor.
\end{quote}

At this point we notice that the factor loadings for CPY are consistently above 0.60. In contrast, some factor loadings dip as low as 0.28. Excepting the SRMR (which is below the 0.08) the remaining fit indices would suggest that we cannot claim configural invariance between these two groups. Because this is a demonstration, I will continue with all of the steps.

\hypertarget{specify-evaluate-and-interpret-the-cfa-for-the-entire-sample-making-no-distinction-between-groups.-write-up-the-preliminary-results.---specify-evaluate-and-interpret-the-cfa-for-weak-invariance.-conduct-the-analysis-to-compare-fit-the-weak-and-configural-models.-write-up-the-preliminary-results.}{%
\subsection*{Specify, evaluate, and interpret the CFA for the entire sample (making no distinction between groups). Write up the preliminary results. \{-\} Specify, evaluate, and interpret the CFA for weak invariance. Conduct the analysis to compare fit the weak and configural models. Write up the preliminary results.}\label{specify-evaluate-and-interpret-the-cfa-for-the-entire-sample-making-no-distinction-between-groups.-write-up-the-preliminary-results.---specify-evaluate-and-interpret-the-cfa-for-weak-invariance.-conduct-the-analysis-to-compare-fit-the-weak-and-configural-models.-write-up-the-preliminary-results.}}


To the code we wrote for testing configural invariance, we add the \emph{group.equal = ``loadings} command.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{weak }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(corrF, }\AttributeTok{data =}\NormalTok{ items, }\AttributeTok{group =} \StringTok{"Dept"}\NormalTok{, }\AttributeTok{group.equal =} \StringTok{"loadings"}\NormalTok{)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(weak, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 55 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        78
  Number of equality constraints                     9

  Number of observations per group:               Used       Total
    CPY                                            202         232
    ORG                                             65          78

Model Test User Model:
                                                      
  Test statistic                               400.290
  Degrees of freedom                               111
  P-value (Chi-square)                           0.000
  Test statistic for each group:
    CPY                                        213.288
    ORG                                        187.001

Model Test Baseline Model:

  Test statistic                              2156.987
  Degrees of freedom                               132
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.857
  Tucker-Lewis Index (TLI)                       0.830

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -2928.397
  Loglikelihood unrestricted model (H1)      -2728.252
                                                      
  Akaike (AIC)                                5994.795
  Bayesian (BIC)                              6242.315
  Sample-size adjusted Bayesian (SABIC)       6023.544

Root Mean Square Error of Approximation:

  RMSEA                                          0.140
  90 Percent confidence interval - lower         0.125
  90 Percent confidence interval - upper         0.155
  P-value H_0: RMSEA <= 0.050                    0.000
  P-value H_0: RMSEA >= 0.080                    1.000

Standardized Root Mean Square Residual:

  SRMR                                           0.101

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured


Group 1 [CPY]:

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed =~                                                            
    ClrRspn           1.000                               0.619    0.788
    EffctvA (.p2.)    1.041    0.062   16.874    0.000    0.644    0.811
    Feedbck (.p3.)    0.977    0.074   13.277    0.000    0.605    0.726
    ClrOrgn (.p4.)    1.260    0.083   15.218    0.000    0.780    0.762
    ClrPrsn (.p5.)    1.190    0.066   17.996    0.000    0.737    0.834
  Valued =~                                                             
    VlObjct           1.000                               0.355    0.598
    IncrUnd (.p7.)    1.952    0.214    9.121    0.000    0.693    0.812
    IncrInt (.p8.)    2.330    0.253    9.226    0.000    0.827    0.834
  SCRPed =~                                                             
    MltPrsp           1.000                               0.696    0.881
    InclsvC (.10.)    0.743    0.055   13.417    0.000    0.517    0.773
    DEIntgr (.11.)    0.693    0.066   10.435    0.000    0.483    0.638
    EqtblEv (.12.)    0.626    0.053   11.750    0.000    0.436    0.707

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed ~~                                                            
    Valued            0.192    0.029    6.542    0.000    0.875    0.875
    SCRPed            0.362    0.046    7.837    0.000    0.840    0.840
  Valued ~~                                                             
    SCRPed            0.163    0.028    5.783    0.000    0.658    0.658

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClearRspnsblts    4.510    0.055   81.633    0.000    4.510    5.744
   .EffectivAnswrs    4.416    0.056   78.997    0.000    4.416    5.558
   .Feedback          4.327    0.059   73.822    0.000    4.327    5.194
   .ClearOrganiztn    4.109    0.072   57.069    0.000    4.109    4.015
   .ClearPresenttn    4.287    0.062   69.002    0.000    4.287    4.855
   .ValObjectives     4.554    0.042  108.931    0.000    4.554    7.664
   .IncrUndrstndng    4.287    0.060   71.430    0.000    4.287    5.026
   .IncrInterest      3.941    0.070   56.440    0.000    3.941    3.971
   .MultPerspectvs    4.490    0.056   80.795    0.000    4.490    5.685
   .InclusvClassrm    4.649    0.047   98.789    0.000    4.649    6.951
   .DEIintegration    4.550    0.053   85.411    0.000    4.550    6.009
   .EquitableEval     4.604    0.043  106.079    0.000    4.604    7.464

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClearRspnsblts    0.233    0.027    8.729    0.000    0.233    0.379
   .EffectivAnswrs    0.216    0.025    8.490    0.000    0.216    0.342
   .Feedback          0.328    0.036    9.146    0.000    0.328    0.473
   .ClearOrganiztn    0.439    0.049    8.932    0.000    0.439    0.419
   .ClearPresenttn    0.237    0.029    8.188    0.000    0.237    0.304
   .ValObjectives     0.227    0.025    9.243    0.000    0.227    0.643
   .IncrUndrstndng    0.247    0.035    7.081    0.000    0.247    0.340
   .IncrInterest      0.300    0.046    6.546    0.000    0.300    0.305
   .MultPerspectvs    0.139    0.024    5.692    0.000    0.139    0.223
   .InclusvClassrm    0.180    0.022    8.183    0.000    0.180    0.402
   .DEIintegration    0.340    0.037    9.218    0.000    0.340    0.594
   .EquitableEval     0.190    0.022    8.816    0.000    0.190    0.500
    TradPed           0.383    0.052    7.385    0.000    1.000    1.000
    Valued            0.126    0.027    4.619    0.000    1.000    1.000
    SCRPed            0.485    0.063    7.656    0.000    1.000    1.000


Group 2 [ORG]:

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed =~                                                            
    ClrRspn           1.000                               0.760    0.907
    EffctvA (.p2.)    1.041    0.062   16.874    0.000    0.792    0.889
    Feedbck (.p3.)    0.977    0.074   13.277    0.000    0.743    0.695
    ClrOrgn (.p4.)    1.260    0.083   15.218    0.000    0.958    0.835
    ClrPrsn (.p5.)    1.190    0.066   17.996    0.000    0.905    0.926
  Valued =~                                                             
    VlObjct           1.000                               0.285    0.460
    IncrUnd (.p7.)    1.952    0.214    9.121    0.000    0.556    0.761
    IncrInt (.p8.)    2.330    0.253    9.226    0.000    0.664    0.772
  SCRPed =~                                                             
    MltPrsp           1.000                               0.534    0.594
    InclsvC (.10.)    0.743    0.055   13.417    0.000    0.397    0.669
    DEIntgr (.11.)    0.693    0.066   10.435    0.000    0.370    0.542
    EqtblEv (.12.)    0.626    0.053   11.750    0.000    0.335    0.506

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed ~~                                                            
    Valued            0.126    0.038    3.361    0.001    0.583    0.583
    SCRPed            0.272    0.073    3.722    0.000    0.669    0.669
  Valued ~~                                                             
    SCRPed            0.113    0.032    3.518    0.000    0.740    0.740

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClearRspnsblts    4.431    0.104   42.624    0.000    4.431    5.287
   .EffectivAnswrs    4.369    0.110   39.553    0.000    4.369    4.906
   .Feedback          4.108    0.133   30.969    0.000    4.108    3.841
   .ClearOrganiztn    3.892    0.142   27.348    0.000    3.892    3.392
   .ClearPresenttn    4.215    0.121   34.783    0.000    4.215    4.314
   .ValObjectives     4.523    0.077   58.826    0.000    4.523    7.296
   .IncrUndrstndng    4.338    0.091   47.876    0.000    4.338    5.938
   .IncrInterest      4.138    0.107   38.771    0.000    4.138    4.809
   .MultPerspectvs    4.308    0.112   38.629    0.000    4.308    4.791
   .InclusvClassrm    4.738    0.074   64.420    0.000    4.738    7.990
   .DEIintegration    4.523    0.085   53.338    0.000    4.523    6.616
   .EquitableEval     4.523    0.082   55.126    0.000    4.523    6.837

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClearRspnsblts    0.124    0.029    4.221    0.000    0.124    0.177
   .EffectivAnswrs    0.166    0.037    4.519    0.000    0.166    0.210
   .Feedback          0.591    0.109    5.419    0.000    0.591    0.517
   .ClearOrganiztn    0.399    0.080    4.990    0.000    0.399    0.303
   .ClearPresenttn    0.136    0.036    3.796    0.000    0.136    0.142
   .ValObjectives     0.303    0.056    5.377    0.000    0.303    0.789
   .IncrUndrstndng    0.224    0.058    3.901    0.000    0.224    0.420
   .IncrInterest      0.300    0.079    3.776    0.000    0.300    0.405
   .MultPerspectvs    0.523    0.106    4.920    0.000    0.523    0.647
   .InclusvClassrm    0.194    0.043    4.480    0.000    0.194    0.552
   .DEIintegration    0.330    0.065    5.109    0.000    0.330    0.706
   .EquitableEval     0.326    0.062    5.221    0.000    0.326    0.744
    TradPed           0.578    0.115    5.041    0.000    1.000    1.000
    Valued            0.081    0.024    3.328    0.001    1.000    1.000
    SCRPed            0.286    0.080    3.590    0.000    1.000    1.000
\end{verbatim}

Let's write and export the data into .csv files that we can view through Excel.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{weak\_FitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(weak)}
\NormalTok{weak\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(weak, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{weak\_Corrs }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_cors}\NormalTok{(weak, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\CommentTok{\# to see each of the tables, remove the hashtag weak\_FitStats}
\CommentTok{\# weak\_paramEsts weak\_Corrs}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(weak\_FitStats, }\AttributeTok{file =} \StringTok{"weak\_FitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(weak\_paramEsts, }\AttributeTok{file =} \StringTok{"weak\_paramEsts.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(weak\_Corrs, }\AttributeTok{file =} \StringTok{"weak\_Corrs.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

with the \emph{lavaan::anova} function we can formally compare the configural and weak tests.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{anova}\NormalTok{(configural, weak)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Chi-Squared Difference Test

            Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff      Pr(>Chisq)
configural 102 5951.9 6231.7 339.43                                           
weak       111 5994.8 6242.3 400.29     60.862 0.20776       9 0.0000000009145
              
configural    
weak       ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The chi-square difference tests indicates that they are statistically significantly different from each other: \(\chi_{D}^2(9)=60.86,p < .001\).

Using the CFI values from the configural and weak models we hand-calculate (see below) the change CFI statistic: \(\Delta CFI = .026\). These statisticially significant differences suggest that the weak invariance model is worse.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#CFI diff}
\NormalTok{.}\DecValTok{883} \SpecialCharTok{{-}}\NormalTok{ .}\DecValTok{857} 
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.026
\end{verbatim}

\begin{quote}
\textbf{Weak invariance model.} The weak invariance model constrained the configuration of variables and all factor loadings to be constant across groups. Fit suggested poor fit: \(\chi^2(111)=400.29, p < 0.001, CFI = 0.857, SRMR = 0.101, RMSEA = 0.140 (90%CI = 0.125, 0.156)
\). Noninvariance of the factor loadings was supported by the significant difference tests that assessed model similarity: \(\chi_{D}^2(9)=60.86,p < .001; \Delta CFI = .026\). For CPY, standardized pattern coefficients ranged between 0.73 and 0.83 on the TradPed factor, between 0.60 and 0.83 on the Valued factor, and between 0.64 and 0.88 on the SCRPed factor. For ORG, standardized pattern coefficients ranged between 0.70 and 0.93 for the TradPed factor, between 0.46 and 0.77 on the Valued factor, and between 0.54 and 0.67 on the SCRPed factor.
\end{quote}

As noted before, we could not claim configural invariance, so we must conclude that this model is also noninvariant.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(weak, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"est"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{12-Invariance_files/figure-latex/unnamed-chunk-88-1.pdf} \includegraphics{12-Invariance_files/figure-latex/unnamed-chunk-88-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Remember to open the consult and hit ENTER twice to view the plots}
\end{Highlighting}
\end{Shaded}

\hypertarget{specify-evaluate-and-interpret-the-cfa-for-strong-invariance.-conduct-the-analysis-to-compare-fit-the-strong-and-weak-models.-write-up-the-preliminary-results.}{%
\subsection*{Specify, evaluate, and interpret the CFA for strong invariance. Conduct the analysis to compare fit the strong and weak models. Write up the preliminary results.}\label{specify-evaluate-and-interpret-the-cfa-for-strong-invariance.-conduct-the-analysis-to-compare-fit-the-strong-and-weak-models.-write-up-the-preliminary-results.}}


To the weak invariance code we add ``intercepts'' to a concatonated list in the \emph{group.equal} command.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{strong }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(corrF, }\AttributeTok{data =}\NormalTok{ items, }\AttributeTok{group =} \StringTok{"Dept"}\NormalTok{, }\AttributeTok{group.equal =} \FunctionTok{c}\NormalTok{(}\StringTok{"loadings"}\NormalTok{,}
    \StringTok{"intercepts"}\NormalTok{))}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(strong, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 107 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        81
  Number of equality constraints                    21

  Number of observations per group:               Used       Total
    CPY                                            202         232
    ORG                                             65          78

Model Test User Model:
                                                      
  Test statistic                               413.226
  Degrees of freedom                               120
  P-value (Chi-square)                           0.000
  Test statistic for each group:
    CPY                                        215.909
    ORG                                        197.317

Model Test Baseline Model:

  Test statistic                              2156.987
  Degrees of freedom                               132
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.855
  Tucker-Lewis Index (TLI)                       0.841

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -2934.866
  Loglikelihood unrestricted model (H1)      -2728.252
                                                      
  Akaike (AIC)                                5989.731
  Bayesian (BIC)                              6204.966
  Sample-size adjusted Bayesian (SABIC)       6014.731

Root Mean Square Error of Approximation:

  RMSEA                                          0.135
  90 Percent confidence interval - lower         0.121
  90 Percent confidence interval - upper         0.150
  P-value H_0: RMSEA <= 0.050                    0.000
  P-value H_0: RMSEA >= 0.080                    1.000

Standardized Root Mean Square Residual:

  SRMR                                           0.103

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured


Group 1 [CPY]:

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed =~                                                            
    ClrRspn           1.000                               0.619    0.788
    EffctvA (.p2.)    1.039    0.062   16.868    0.000    0.643    0.810
    Feedbck (.p3.)    0.982    0.074   13.309    0.000    0.607    0.728
    ClrOrgn (.p4.)    1.265    0.083   15.243    0.000    0.783    0.763
    ClrPrsn (.p5.)    1.188    0.066   18.011    0.000    0.735    0.833
  Valued =~                                                             
    VlObjct           1.000                               0.353    0.595
    IncrUnd (.p7.)    1.960    0.216    9.074    0.000    0.692    0.812
    IncrInt (.p8.)    2.351    0.256    9.185    0.000    0.830    0.835
  SCRPed =~                                                             
    MltPrsp           1.000                               0.697    0.882
    InclsvC (.10.)    0.740    0.055   13.350    0.000    0.515    0.771
    DEIntgr (.11.)    0.690    0.066   10.401    0.000    0.481    0.636
    EqtblEv (.12.)    0.626    0.053   11.757    0.000    0.436    0.707

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed ~~                                                            
    Valued            0.191    0.029    6.525    0.000    0.875    0.875
    SCRPed            0.363    0.046    7.840    0.000    0.841    0.841
  Valued ~~                                                             
    SCRPed            0.162    0.028    5.773    0.000    0.658    0.658

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClrRspn (.31.)    4.513    0.052   86.495    0.000    4.513    5.750
   .EffctvA (.32.)    4.429    0.054   82.644    0.000    4.429    5.584
   .Feedbck (.33.)    4.307    0.057   75.837    0.000    4.307    5.159
   .ClrOrgn (.34.)    4.082    0.069   59.369    0.000    4.082    3.980
   .ClrPrsn (.35.)    4.299    0.060   71.957    0.000    4.299    4.872
   .VlObjct (.36.)    4.539    0.039  115.407    0.000    4.539    7.650
   .IncrUnd (.37.)    4.276    0.058   73.163    0.000    4.276    5.017
   .IncrInt (.38.)    3.961    0.069   57.724    0.000    3.961    3.983
   .MltPrsp (.39.)    4.479    0.055   81.030    0.000    4.479    5.666
   .InclsvC (.40.)    4.675    0.046  102.360    0.000    4.675    6.991
   .DEIntgr (.41.)    4.550    0.050   91.154    0.000    4.550    6.016
   .EqtblEv (.42.)    4.595    0.042  109.486    0.000    4.595    7.449

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClearRspnsblts    0.233    0.027    8.728    0.000    0.233    0.379
   .EffectivAnswrs    0.216    0.025    8.498    0.000    0.216    0.344
   .Feedback          0.328    0.036    9.137    0.000    0.328    0.471
   .ClearOrganiztn    0.439    0.049    8.924    0.000    0.439    0.418
   .ClearPresenttn    0.238    0.029    8.199    0.000    0.238    0.306
   .ValObjectives     0.227    0.025    9.254    0.000    0.227    0.646
   .IncrUndrstndng    0.248    0.035    7.095    0.000    0.248    0.341
   .IncrInterest      0.300    0.046    6.515    0.000    0.300    0.304
   .MultPerspectvs    0.139    0.025    5.678    0.000    0.139    0.223
   .InclusvClassrm    0.181    0.022    8.205    0.000    0.181    0.406
   .DEIintegration    0.341    0.037    9.224    0.000    0.341    0.596
   .EquitableEval     0.190    0.022    8.811    0.000    0.190    0.500
    TradPed           0.383    0.052    7.388    0.000    1.000    1.000
    Valued            0.125    0.027    4.595    0.000    1.000    1.000
    SCRPed            0.486    0.063    7.655    0.000    1.000    1.000


Group 2 [ORG]:

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed =~                                                            
    ClrRspn           1.000                               0.761    0.908
    EffctvA (.p2.)    1.039    0.062   16.868    0.000    0.790    0.887
    Feedbck (.p3.)    0.982    0.074   13.309    0.000    0.747    0.693
    ClrOrgn (.p4.)    1.265    0.083   15.243    0.000    0.962    0.834
    ClrPrsn (.p5.)    1.188    0.066   18.011    0.000    0.904    0.926
  Valued =~                                                             
    VlObjct           1.000                               0.283    0.454
    IncrUnd (.p7.)    1.960    0.216    9.074    0.000    0.554    0.758
    IncrInt (.p8.)    2.351    0.256    9.185    0.000    0.664    0.769
  SCRPed =~                                                             
    MltPrsp           1.000                               0.537    0.592
    InclsvC (.10.)    0.740    0.055   13.350    0.000    0.397    0.656
    DEIntgr (.11.)    0.690    0.066   10.401    0.000    0.370    0.540
    EqtblEv (.12.)    0.626    0.053   11.757    0.000    0.336    0.506

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed ~~                                                            
    Valued            0.126    0.037    3.357    0.001    0.584    0.584
    SCRPed            0.277    0.074    3.745    0.000    0.678    0.678
  Valued ~~                                                             
    SCRPed            0.114    0.032    3.530    0.000    0.750    0.750

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClrRspn (.31.)    4.513    0.052   86.495    0.000    4.513    5.386
   .EffctvA (.32.)    4.429    0.054   82.644    0.000    4.429    4.976
   .Feedbck (.33.)    4.307    0.057   75.837    0.000    4.307    3.995
   .ClrOrgn (.34.)    4.082    0.069   59.369    0.000    4.082    3.537
   .ClrPrsn (.35.)    4.299    0.060   71.957    0.000    4.299    4.404
   .VlObjct (.36.)    4.539    0.039  115.407    0.000    4.539    7.289
   .IncrUnd (.37.)    4.276    0.058   73.163    0.000    4.276    5.855
   .IncrInt (.38.)    3.961    0.069   57.724    0.000    3.961    4.583
   .MltPrsp (.39.)    4.479    0.055   81.030    0.000    4.479    4.941
   .InclsvC (.40.)    4.675    0.046  102.360    0.000    4.675    7.728
   .DEIntgr (.41.)    4.550    0.050   91.154    0.000    4.550    6.629
   .EqtblEv (.42.)    4.595    0.042  109.486    0.000    4.595    6.919
    TradPed          -0.088    0.107   -0.820    0.412   -0.116   -0.116
    Valued            0.049    0.049    0.991    0.322    0.172    0.172
    SCRPed           -0.040    0.097   -0.416    0.678   -0.075   -0.075

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClearRspnsblts    0.124    0.029    4.209    0.000    0.124    0.176
   .EffectivAnswrs    0.168    0.037    4.530    0.000    0.168    0.212
   .Feedback          0.605    0.112    5.421    0.000    0.605    0.520
   .ClearOrganiztn    0.406    0.081    4.994    0.000    0.406    0.305
   .ClearPresenttn    0.136    0.036    3.795    0.000    0.136    0.143
   .ValObjectives     0.308    0.057    5.385    0.000    0.308    0.794
   .IncrUndrstndng    0.227    0.058    3.917    0.000    0.227    0.425
   .IncrInterest      0.306    0.081    3.791    0.000    0.306    0.409
   .MultPerspectvs    0.534    0.108    4.924    0.000    0.534    0.649
   .InclusvClassrm    0.208    0.046    4.558    0.000    0.208    0.569
   .DEIintegration    0.334    0.065    5.113    0.000    0.334    0.709
   .EquitableEval     0.328    0.063    5.219    0.000    0.328    0.744
    TradPed           0.578    0.115    5.042    0.000    1.000    1.000
    Valued            0.080    0.024    3.308    0.001    1.000    1.000
    SCRPed            0.288    0.081    3.558    0.000    1.000    1.000
\end{verbatim}

Let's export the data into .csv files that we can manipulate outside of the R environment.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{strong\_FitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(strong)}
\NormalTok{strong\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(strong, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{strong\_Corrs }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_cors}\NormalTok{(strong, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\CommentTok{\# to see each of the tables, remove the hashtag strong\_FitStats}
\CommentTok{\# strong\_paramEsts strong\_Corrs}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(strong\_FitStats, }\AttributeTok{file =} \StringTok{"strong\_FitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(strong\_paramEsts, }\AttributeTok{file =} \StringTok{"strong\_paramEsts.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(strong\_Corrs, }\AttributeTok{file =} \StringTok{"strong\_Corrs.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

with the \emph{lavaan::anova} function we can formally compare the configural, weak, and strong tests.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{anova}\NormalTok{(configural, weak, strong)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Chi-Squared Difference Test

            Df    AIC    BIC  Chisq Chisq diff    RMSEA Df diff      Pr(>Chisq)
configural 102 5951.9 6231.7 339.43                                            
weak       111 5994.8 6242.3 400.29     60.862 0.207759       9 0.0000000009145
strong     120 5989.7 6205.0 413.23     12.936 0.057238       9          0.1655
              
configural    
weak       ***
strong        
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Within this overall picture of poorly fitting models, the two are not statistically significantly different from each other: \(\chi_{D}^2(9) = 12.936,p = 0.166\).

Below we can calculate the change CFI test from the CFI values from the weak and strong tests: \(\Delta CFI=.002\)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# CFI change test}
\FloatTok{0.857} \SpecialCharTok{{-}} \FloatTok{0.855}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.002
\end{verbatim}

Don't be fooled by this test that falls within the \(\Delta CFI < 0.01\) criteria. It doesn't help us that our ``poor fit'' doesn't differ significantly across models.

\begin{quote}
\textbf{Strong invariance model.} In the strong invariance model, configuration, factor loadings, and indicator means/intercepts were constrained to be the same for each group. Fit indices were less than ideal: \(\chi^2(120) = 413.226, p < 0.001, CFI = 0.855, SRMR = 0.103, RMSEA = 0.135 (90%CI = 0.121, 0.150)
\). The difference tests that evaluated model similarity between the weak and strong constraints were not statistically significant: \(\chi_{D}^2(9) = 12.936,p = 0.166; \Delta CFI=.002\).
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(strong, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"est"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{12-Invariance_files/figure-latex/unnamed-chunk-94-1.pdf} \includegraphics{12-Invariance_files/figure-latex/unnamed-chunk-94-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Depress ENTER a couple of times if R appears to freeze this will}
\CommentTok{\# display both plots}
\end{Highlighting}
\end{Shaded}

\hypertarget{specify-evaluate-and-interpret-the-cfa-for-strict-invariance.-conduct-the-analysis-to-compare-fit-the-strict-and-strong-models.-write-up-the-preliminary-results.}{%
\subsection*{Specify, evaluate, and interpret the CFA for strict invariance. Conduct the analysis to compare fit the strict and strong models. Write up the preliminary results.}\label{specify-evaluate-and-interpret-the-cfa-for-strict-invariance.-conduct-the-analysis-to-compare-fit-the-strict-and-strong-models.-write-up-the-preliminary-results.}}


To the weak invariance code we add ``residuals'' to a concatonated list in the \emph{group.equal} command.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{strict }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(corrF, }\AttributeTok{data =}\NormalTok{ items, }\AttributeTok{group =} \StringTok{"Dept"}\NormalTok{, }\AttributeTok{group.equal =} \FunctionTok{c}\NormalTok{(}\StringTok{"loadings"}\NormalTok{,}
    \StringTok{"intercepts"}\NormalTok{, }\StringTok{"residuals"}\NormalTok{))}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(strict, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{, )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 82 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        81
  Number of equality constraints                    33

  Number of observations per group:               Used       Total
    CPY                                            202         232
    ORG                                             65          78

Model Test User Model:
                                                      
  Test statistic                               459.401
  Degrees of freedom                               132
  P-value (Chi-square)                           0.000
  Test statistic for each group:
    CPY                                        240.235
    ORG                                        219.166

Model Test Baseline Model:

  Test statistic                              2156.987
  Degrees of freedom                               132
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.838
  Tucker-Lewis Index (TLI)                       0.838

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -2957.953
  Loglikelihood unrestricted model (H1)      -2728.252
                                                      
  Akaike (AIC)                                6011.906
  Bayesian (BIC)                              6184.094
  Sample-size adjusted Bayesian (SABIC)       6031.906

Root Mean Square Error of Approximation:

  RMSEA                                          0.136
  90 Percent confidence interval - lower         0.123
  90 Percent confidence interval - upper         0.150
  P-value H_0: RMSEA <= 0.050                    0.000
  P-value H_0: RMSEA >= 0.080                    1.000

Standardized Root Mean Square Residual:

  SRMR                                           0.116

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured


Group 1 [CPY]:

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed =~                                                            
    ClrRspn           1.000                               0.610    0.800
    EffctvA (.p2.)    1.040    0.066   15.724    0.000    0.634    0.810
    Feedbck (.p3.)    1.010    0.077   13.173    0.000    0.616    0.706
    ClrOrgn (.p4.)    1.300    0.088   14.795    0.000    0.792    0.773
    ClrPrsn (.p5.)    1.231    0.073   16.748    0.000    0.750    0.848
  Valued =~                                                             
    VlObjct           1.000                               0.347    0.571
    IncrUnd (.p7.)    2.004    0.226    8.856    0.000    0.696    0.819
    IncrInt (.p8.)    2.390    0.268    8.926    0.000    0.830    0.834
  SCRPed =~                                                             
    MltPrsp           1.000                               0.707    0.843
    InclsvC (.10.)    0.622    0.053   11.698    0.000    0.440    0.678
    DEIntgr (.11.)    0.591    0.063    9.426    0.000    0.418    0.566
    EqtblEv (.12.)    0.644    0.052   12.496    0.000    0.455    0.716

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed ~~                                                            
    Valued            0.185    0.029    6.388    0.000    0.873    0.873
    SCRPed            0.374    0.048    7.805    0.000    0.869    0.869
  Valued ~~                                                             
    SCRPed            0.166    0.029    5.688    0.000    0.675    0.675

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClrRspn (.31.)    4.514    0.052   87.278    0.000    4.514    5.924
   .EffctvA (.32.)    4.429    0.053   83.153    0.000    4.429    5.656
   .Feedbck (.33.)    4.297    0.058   74.126    0.000    4.297    4.927
   .ClrOrgn (.34.)    4.087    0.069   59.115    0.000    4.087    3.988
   .ClrPrsn (.35.)    4.298    0.061   70.788    0.000    4.298    4.860
   .VlObjct (.36.)    4.535    0.040  114.691    0.000    4.535    7.453
   .IncrUnd (.37.)    4.277    0.058   73.215    0.000    4.277    5.032
   .IncrInt (.38.)    3.961    0.069   57.702    0.000    3.961    3.981
   .MltPrsp (.39.)    4.470    0.058   77.085    0.000    4.470    5.329
   .InclsvC (.40.)    4.685    0.043  108.763    0.000    4.685    7.229
   .DEIntgr (.41.)    4.557    0.048   95.088    0.000    4.557    6.173
   .EqtblEv (.42.)    4.600    0.043  107.915    0.000    4.600    7.233

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClrRspn (.16.)    0.209    0.021    9.784    0.000    0.209    0.360
   .EffctvA (.17.)    0.211    0.022    9.657    0.000    0.211    0.344
   .Feedbck (.18.)    0.381    0.036   10.573    0.000    0.381    0.501
   .ClrOrgn (.19.)    0.422    0.042   10.079    0.000    0.422    0.402
   .ClrPrsn (.20.)    0.219    0.024    8.978    0.000    0.219    0.280
   .VlObjct (.21.)    0.250    0.023   10.729    0.000    0.250    0.674
   .IncrUnd (.22.)    0.238    0.030    7.866    0.000    0.238    0.329
   .IncrInt (.23.)    0.301    0.041    7.406    0.000    0.301    0.304
   .MltPrsp (.24.)    0.204    0.028    7.179    0.000    0.204    0.290
   .InclsvC (.25.)    0.227    0.023   10.077    0.000    0.227    0.540
   .DEIntgr (.26.)    0.370    0.034   10.748    0.000    0.370    0.679
   .EqtblEv (.27.)    0.197    0.020    9.721    0.000    0.197    0.488
    TradPed           0.372    0.051    7.259    0.000    1.000    1.000
    Valued            0.121    0.027    4.447    0.000    1.000    1.000
    SCRPed            0.499    0.069    7.249    0.000    1.000    1.000


Group 2 [ORG]:

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed =~                                                            
    ClrRspn           1.000                               0.738    0.850
    EffctvA (.p2.)    1.040    0.066   15.724    0.000    0.767    0.858
    Feedbck (.p3.)    1.010    0.077   13.173    0.000    0.745    0.770
    ClrOrgn (.p4.)    1.300    0.088   14.795    0.000    0.959    0.828
    ClrPrsn (.p5.)    1.231    0.073   16.748    0.000    0.908    0.889
  Valued =~                                                             
    VlObjct           1.000                               0.278    0.487
    IncrUnd (.p7.)    2.004    0.226    8.856    0.000    0.558    0.753
    IncrInt (.p8.)    2.390    0.268    8.926    0.000    0.665    0.772
  SCRPed =~                                                             
    MltPrsp           1.000                               0.724    0.849
    InclsvC (.10.)    0.622    0.053   11.698    0.000    0.451    0.687
    DEIntgr (.11.)    0.591    0.063    9.426    0.000    0.428    0.576
    EqtblEv (.12.)    0.644    0.052   12.496    0.000    0.466    0.724

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  TradPed ~~                                                            
    Valued            0.127    0.037    3.450    0.001    0.616    0.616
    SCRPed            0.414    0.094    4.425    0.000    0.775    0.775
  Valued ~~                                                             
    SCRPed            0.153    0.040    3.843    0.000    0.760    0.760

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClrRspn (.31.)    4.514    0.052   87.278    0.000    4.514    5.200
   .EffctvA (.32.)    4.429    0.053   83.153    0.000    4.429    4.952
   .Feedbck (.33.)    4.297    0.058   74.126    0.000    4.297    4.439
   .ClrOrgn (.34.)    4.087    0.069   59.115    0.000    4.087    3.527
   .ClrPrsn (.35.)    4.298    0.061   70.788    0.000    4.298    4.207
   .VlObjct (.36.)    4.535    0.040  114.691    0.000    4.535    7.931
   .IncrUnd (.37.)    4.277    0.058   73.215    0.000    4.277    5.773
   .IncrInt (.38.)    3.961    0.069   57.702    0.000    3.961    4.595
   .MltPrsp (.39.)    4.470    0.058   77.085    0.000    4.470    5.236
   .InclsvC (.40.)    4.685    0.043  108.763    0.000    4.685    7.146
   .DEIntgr (.41.)    4.557    0.048   95.088    0.000    4.557    6.124
   .EqtblEv (.42.)    4.600    0.043  107.915    0.000    4.600    7.141
    TradPed          -0.096    0.105   -0.912    0.362   -0.130   -0.130
    Valued            0.047    0.048    0.978    0.328    0.169    0.169
    SCRPed           -0.099    0.113   -0.878    0.380   -0.136   -0.136

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .ClrRspn (.16.)    0.209    0.021    9.784    0.000    0.209    0.277
   .EffctvA (.17.)    0.211    0.022    9.657    0.000    0.211    0.264
   .Feedbck (.18.)    0.381    0.036   10.573    0.000    0.381    0.407
   .ClrOrgn (.19.)    0.422    0.042   10.079    0.000    0.422    0.315
   .ClrPrsn (.20.)    0.219    0.024    8.978    0.000    0.219    0.210
   .VlObjct (.21.)    0.250    0.023   10.729    0.000    0.250    0.763
   .IncrUnd (.22.)    0.238    0.030    7.866    0.000    0.238    0.433
   .IncrInt (.23.)    0.301    0.041    7.406    0.000    0.301    0.405
   .MltPrsp (.24.)    0.204    0.028    7.179    0.000    0.204    0.280
   .InclsvC (.25.)    0.227    0.023   10.077    0.000    0.227    0.528
   .DEIntgr (.26.)    0.370    0.034   10.748    0.000    0.370    0.669
   .EqtblEv (.27.)    0.197    0.020    9.721    0.000    0.197    0.475
    TradPed           0.544    0.112    4.855    0.000    1.000    1.000
    Valued            0.077    0.023    3.333    0.001    1.000    1.000
    SCRPed            0.525    0.116    4.533    0.000    1.000    1.000
\end{verbatim}

Let's export the data into .csv files that we can manipulate outside of the R environment.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{strict\_FitStats }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(strict)}
\NormalTok{strict\_paramEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(strict, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{strict\_Corrs }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_cors}\NormalTok{(strict, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\CommentTok{\# to see each of the tables, remove the hashtag strict\_FitStats}
\CommentTok{\# strict\_paramEsts strict\_Corrs}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(strict\_FitStats, }\AttributeTok{file =} \StringTok{"strict\_FitStats.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(strict\_paramEsts, }\AttributeTok{file =} \StringTok{"strict\_paramEsts.csv"}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(strict\_Corrs, }\AttributeTok{file =} \StringTok{"strict\_Corrs.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

with the \emph{lavaan::anova} function we can formally compare the configural, weak, strong, and strict tests.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{anova}\NormalTok{(configural, weak, strong, strict)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Chi-Squared Difference Test

            Df    AIC    BIC  Chisq Chisq diff    RMSEA Df diff      Pr(>Chisq)
configural 102 5951.9 6231.7 339.43                                            
weak       111 5994.8 6242.3 400.29     60.862 0.207759       9 0.0000000009145
strong     120 5989.7 6205.0 413.23     12.936 0.057238       9          0.1655
strict     132 6011.9 6184.1 459.40     46.175 0.146057      12 0.0000064729187
              
configural    
weak       ***
strong        
strict     ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The Chi-square difference test indicates fatalistically significant differences between the strict and strong models: \(\chi_{D}^2(12) = 46.175, p < 0.001\).

Below we hand calculate the change CFI test. This difference between the CFI tests from the strong and strict models is 0.017 and slightly exceeds the \(\Delta CFI < 0.01\). criteria.

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.855} \SpecialCharTok{{-}} \FloatTok{0.838}  \CommentTok{\#change CFI test}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.017
\end{verbatim}

\begin{quote}
\textbf{Strict invariance model.} In the strict invariance model, configuration, factor loadings, indicator means/intercepts, and residuals were constrained to be the same for each group. Fit indices were less than ideal: \(\chi^2(132) = 459.401, p < .001, CFI = 0.838, SRMS = 0.116, RMSEA = 0.1360(90%CI = 0.123, 0.150)
\). Factorial noninvariance was already suggested in the restriction from weak to strong, this continues to be true: \(\chi_{D}^2(12) = 46.175, p < 0.001; \Delta CFI = 0.017\). For CPY, standardized pattern coefficients ranged between 0.71 and 0.85 on the TradPed factor, between 0.57 and 0.83 on the Valued factor, and between 0.68 and 0.84 on the SCRPed factor. For ORG, standardized pattern coefficients ranged between 0.77 and 0.89 for the TradPed factor, between 0.49 and 0.77 on the Valued factor, and between 0.58 and 0.85 on the SCRPed factor.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(strict, }\AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{,}
    \AttributeTok{whatLabels =} \StringTok{"est"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{12-Invariance_files/figure-latex/unnamed-chunk-100-1.pdf} \includegraphics{12-Invariance_files/figure-latex/unnamed-chunk-100-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# If R appears to stall depress the ENTER key twice to see both plots}
\end{Highlighting}
\end{Shaded}

\hypertarget{create-an-apa-style-results-section.-do-not-report-any-invariance-tests-past-the-one-that-failed.-include-a-tables-and-figures.}{%
\subsection*{Create an APA style results section. Do not report any invariance tests past the one that failed. Include a table(s) and figure(s).}\label{create-an-apa-style-results-section.-do-not-report-any-invariance-tests-past-the-one-that-failed.-include-a-tables-and-figures.}}


\begin{quote}
To test if the factor structures of the course evaluations were stable across department (CPY and ORG), we used measurement invariance analyses. First, we constructed CFA models with the lavaan (v. 0.6-17) package in R and created two groups representing CPY and ORG. Within lavaan we successivly constrained parameters representing the configural, weak (loadings), strong (intercepts), and strict (residuals) structures (Hirschfeld \& von Brachel, 2014; Kline, 2016). A poor fit in any of these models suggests that the aspect being constrained does not operate consistently for the different groups. The degree of invariance was determined jointly when \(\chi^2 p > .05\) and a \(\Delta CFI < .01\).
\end{quote}

\begin{quote}
The configural model constrains only the relative configuration of variables in the model to be the same in both groups. In other words, no factor loadings or indicator means are constrained to be the same across groups, but the organization of indicators is the same for both groups. Next, in weak factorial invariance, the configuration of variables and all factor loadings are constrained to be the same for each group. Poor fit here suggests that the factor loadings vary in size between the two groups. In strong factorial invariance, both the configuration, factor loadings, and the indicator means are constrained to be the same for each group. A reduction in fit here, but not in the previous steps, suggests that indicators have different means in both groups, which might be expected when comparing two groups of people, such as in a t-test. Therefore, poor fit in only this model does not necessarily indicate the factor structure operates differently for different groups. Finally, strict factorial invariance requires strong invariance and equality in error variances and covariances across groups. This means that the indicators measure the same factors in each group with the same degree of precision.
\end{quote}

\begin{quote}
We selected fit criteria for their capacity to assess different aspects of the statistical analysis. As is common among SEM researchers, we reported the chi-square goodness of fit (\(\chi^2\)). This evaluates the discrepancy between the unrestricted sample matrix and the restricted covariance matrix. Although the associated p value indicates adequate fit when the value is non-significant, it is widely recognized that large sample size can result in a statistically significant \emph{p} value (Byrne, 2016). The comparative fit index (CFI) is an incremental index, comparing the hypothesized model with the baseline model, and should be at least .90 and perhaps higher than .95 (Hu \& Bentler, 1999). The root mean square error of approximation (RMSEA) takes into account the error of approximation in the population and expresses it per degree of freedom. As such, the fit indicator considers the complexity of the model. Ideal values are equal to or less than .05, values less than .08 represent reasonable fit, and values between .08 and .10 represent mediocre fit. The standardized root mean residual (SRMR) is a standardized measure of the mean absolute covariance residual -- the overall difference between the observed and predicted correlations. Values greater than .10 may indicate poor fit and inspection of residuals is then advised. Researchers have advised caution when using these criteria as strict cutoffs, and other factors such as sample size and model complexity should be considered (Kline, 2016).
\end{quote}

\begin{quote}
Course evaluation items each were loaded on their respective correlated factors. The configural model, which constrained only the relative configuration of variables in the model to be the same in both groups had less than adequate fit to the data: \(\chi^2 (102)=339.43, p < 0.001, CFI = 0.883, SRMR = 0.073, RMSEA = 0.132, 90%CI(0.117, 0.148)
\). Even though some of the fit criteria were less than adequate, we cautiously proceeded to the next stage of invariance testing. The weak invariance model constrained the configuration of variables and all factor loadings to be constant across groups. Unfortunately, all indicators suggested poor fit: \(\chi^2(111)=400.29, p < 0.001, CFI = 0.857, SRMR = 0.101, RMSEA = 0.140 (90%CI = 0.125, 0.156)
\). Further, noninvariance of the factor loadings was supported by the significant difference tests that assessed model similarity: \(\chi_{D}^2(9)=60.86,p < .001; \Delta CFI = .026\). Because we found noninvariance at the weak level, we did not attempt to model strong nor strict invariance.
\end{quote}

\hypertarget{hybrid}{%
\chapter{Hybrid Models}\label{hybrid}}

\href{https://www.youtube.com/playlist?list=PLtz5cFLQl4KPukMYJhxX3ZJ7meMNQ6Dku}{Screencasted Lecture Link}

Psychometrics courses usually focus on evaluating the psychometric properties of an instrument and outlining the steps/procedures in instrument development. I believe it is also important to understand how to incorporate those psychometrically credible measures in research designs and particularly, SEM. Thus, the purpose of this lecture is to walk through a real dataset from missing data analysis, to analyzing and managing missing data, to assessing the distributional characteristics of the data, to creating a measurement model, and finally recrafting it as a structural mode.

Our goal is:

\begin{itemize}
\tightlist
\item
  Starting with a raw dataset:

  \begin{itemize}
  \tightlist
  \item
    examine missing patterns mechanism and managing missing data,
  \item
    evaluate the distributional characteristics of the data, and
  \item
    conducting preliminary analyses.
  \end{itemize}
\item
  Specify our measurement model (all the measures, allowing them all to correlate).\\
\item
  Briefly examine \emph{identification} in the measurement and structural portions of the model.\\
\item
  Specify and evaluate our a priorily defined structural model.
\item
  Write it up!
\end{itemize}

At the outset, please know that this lesson (a) skips a few steps and (b) introduces (entirely too quickly) some steps that will be new to those who are not exposed to structural equation modeling (SEM). While somewhat regrettable, this is intentional. In the program where I have taught, we teach psychometrics before the multivariate class and this is the last lesson in psychometrics. One intent is to provide an advanced cognitive organizer for what is yet to come in the \href{https://lhbikos.github.io/ReC_MultivModel/SEM.html}{SEM lessons} in multivariate modeling. In-so-doing, I'm hoping to show how the entire psychometric process is critical to our final, tested, models.

\hypertarget{navigating-this-lesson-11}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-11}}

There is about 1 hour and 45 minutes of lecture. If you work through the materials with me it would be plan for an additional two hours.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}.

\hypertarget{learning-objectives-11}{%
\subsection{Learning Objectives}\label{learning-objectives-11}}

Focusing on this week's materials, make sure you can:

\begin{itemize}
\tightlist
\item
  Identify steps in preparing data for structural equation modeling.
\item
  Differentiate a measurement model from a structural model and know which one \emph{will} have better fit.
\item
  List the general steps in evaluating a hybrid model.
\item
  Specify and interpret the results of a measurement model.
\item
  Specify and interpret the results of a structural model.
\end{itemize}

\hypertarget{planning-for-practice-11}{%
\subsection{Planning for Practice}\label{planning-for-practice-11}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty The least complex is to import the latest \href{https://spupsych.az1.qualtrics.com/jfe/form/SV_b2cClqAlLGQ6nLU}{Rate-a-Recent-Course: A ReCentering Psych Stats Exercise}data from Qualtrics and rework the problem as written in the lesson. For an increased challenge, swap out one or more variables/scales. And for a maximal challenge, try something entirely new with data (similated or real) that you have permission to use.

Regardless of your choic(es) please work through the following:

\begin{itemize}
\tightlist
\item
  Structure up your dataframe.
\item
  Analyze and manage missing data.
\item
  Evaluate the assumptions for multivariate analysis.
\item
  Conduct appropriate preliminary analyses.
\item
  Specify and evaluate a measurement model.
\item
  Prepare an APA style results section with table(s) and figure(s).
\item
  Explain it to somebody.
\end{itemize}

\hypertarget{readings-resources-10}{%
\subsection{Readings \& Resources}\label{readings-resources-10}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

Kline, R. (2016). Principles and practice of structural equation modeling (Fourth ed., Methodology in the social sciences). New York: The Guilford Press.

\begin{itemize}
\tightlist
\item
  Chapter 4, Data Preparation and Psychometrics Review\\
\item
  Chapter 10, Specification and Identification of Structural Regression Models
\item
  Chapter 11, Estimation and Local Fit Testing
\item
  Chapter 13, Analysis of CFA Models
\end{itemize}

Little, T. D., Cunningham, W. A., Shahar, G., \& Widaman, K. F. (2002). To parcel or not to parcel: Exploring the question, weighing the merits. Structural Equation Modeling, 9(2), 151--173. \url{https://doi.org/10.1207/S15328007SEM0902_1}

Little, T. D., Rhemtulla, M., Gibson, K., \& Schoemann, A. M. (2013). Why the items versus parcels controversy needn't be one. Psychological Methods, 18(3), 285--300. \url{https://doi.org/10.1037/a0033266}

Rosseel, Y. (2019). The \emph{lavaan} tutorial. Belgium: Department of Data Analysis, Ghent University. \url{http://lavaan.ugent.be/tutorial/tutorial.pdf}

\hypertarget{packages-11}{%
\subsection{Packages}\label{packages-11}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# will install the package if not already installed}
\CommentTok{\# if(!require(psych))\{install.packages(\textquotesingle{}psych\textquotesingle{})\}}
\CommentTok{\# if(!require(lavaan))\{install.packages(\textquotesingle{}lavaan\textquotesingle{})\}}
\CommentTok{\# if(!require(semPlot))\{install.packages(\textquotesingle{}semPlot\textquotesingle{})\}}
\CommentTok{\# if(!require(psych))\{install.packages(\textquotesingle{}psych\textquotesingle{})\}}
\CommentTok{\# if(!require(semTable))\{install.packages(\textquotesingle{}semTable\textquotesingle{})\}}
\CommentTok{\# if(!require(semTools))\{install.packages(\textquotesingle{}semTools\textquotesingle{})\}}
\CommentTok{\# if(!require(semptools))\{install.packages(\textquotesingle{}semptools\textquotesingle{})\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{introducing-the-statistic}{%
\section{Introducing the Statistic}\label{introducing-the-statistic}}

The model we are testing in this lesson is \emph{hybrid} that is, it contains both CFA and the structural paths. Although there are several detour along the way, the analytic approach has two large stages:

\begin{itemize}
\tightlist
\item
  Testing the \emph{measurement model} which includes of each of the factors and its indicators with covariances between each of the latent variables.

  \begin{itemize}
  \tightlist
  \item
    The measurement model will have the best fit because all of the structural paths are saturated (i.e., there is a covariance between them).
  \item
    If the fit of the measurement model is below the threshold, a technique like parceling could be helpful, but we will not get there today. More information on that is found in \href{https://lhbikos.github.io/ReC_MultivModel/MeasMod.html\#parceling}{ReCentering Psych Stats: Multivariate Modeling}.
  \end{itemize}
\item
  Test the \emph{structural model.} This means we delete the covariancs and respecify the model to include the directional paths and covariances we hypothesized.
\end{itemize}

\begin{figure}
\centering
\includegraphics{images/Hybrid/WrkFlw_Hybrid.png}
\caption{Image of a flowchart and decision-tree for evaluating hybrid SEM models}
\end{figure}

The steps in working the STATISTIC generally include,

\begin{itemize}
\tightlist
\item
  Structuring -up your dataframe(s)

  \begin{itemize}
  \tightlist
  \item
    Reverse-score any items are negatively worded (i.e., the item is scaled opposite the other items)
  \item
    Ensure proper formatting of variables (e.g., numerical, factor)
  \item
    Conduct a missing data analysis and manage missing data.
  \end{itemize}
\item
  Preliminary analyses

  \begin{itemize}
  \tightlist
  \item
    Evaluate assumptions for multivariate analyses.
  \item
    Calculate internal consistency coefficients for any measures that are ``scales''
  \item
    Create a correlation table with means and standard deviations
  \end{itemize}
\item
  Specify and evaluate a measurement model. In this just-identified (saturated) model, all latent variables are specified as covarying

  \begin{itemize}
  \tightlist
  \item
    In the event of poor fit, respecify LVs with multiple indicators with \href{https://lhbikos.github.io/ReC_MultivModel/MeasMod.html\#parceling}{parcels}.
  \end{itemize}
\item
  Specify and evaluate a structural model by replacing the covariances with paths that represent the a priori hypotheses.

  \begin{itemize}
  \tightlist
  \item
    These models could take a variety of forms.
  \item
    It is possible to respecify models through trimming or building approaches.
  \item
    Nested models can be compared with \(\chi_{D}^{2}\) and \(\Delta{CFI}\) tests.
  \end{itemize}
\end{itemize}

\hypertarget{research-vignette-10}{%
\section{Research Vignette}\label{research-vignette-10}}

The research vignette comes from the open survey titled, \href{https://spupsych.az1.qualtrics.com/jfe/form/SV_b2cClqAlLGQ6nLU}{Rate-a-Recent-Course: A ReCentering Psych Stats Exercise}. A series of lessons devoted to preparing data for analysis provide information about the specific variables and link to the codebook. They are available in the \href{https://lhbikos.github.io/ReC_MultivariateModeling/}{Multivariate Modeling}volume.

If you are 18 years or older and have recently taken any type of course (e.g., college, graduate, continuing education), please consider taking the survey. Each time someone responds to the survey, it will allow users to follow along with a slightly different datasea.

The Rate-a-Recent-Course has a number of scales and variables. We will use four variables/scales to specify a parallel mediation predicting the perceptions of campus climate for students who are Black from the percent of classmates who are Black, the proportion of instructional staff who are BIPOC (Black, Indigenous, and Persons of Color), and course evaluation ratings that assess the degree to which the pedagogy is socially responsive.

\includegraphics{images/Hybrid/parallel_model.png} Variables in the model:

\begin{itemize}
\tightlist
\item
  Perceived Campus Climate for Black Students includes 6 items, one of which was reverse scored. This scale was adapted from Szymanski et al.'s \citeyearpar{szymanski_perceptions_2020} Campus Climate for LGBTQ students. It has not been evaluated for use with other groups. The Szymanski et al.~analysis suggested that it could be used as a total scale score, or divided into three items each that assess the college's response and experienced stigma. Items were assessed on a 7-point scale ranging from \emph{strongly disagree} to \emph{strongly agree} with higher scores indicating a more hostile campus climate. Example items from our revised scale include:

  \begin{itemize}
  \tightlist
  \item
    College response: ``My college is unresponsive to the needs of Black students.''
  \item
    Stigma: ``Anti-Black racism is visible in my college.''
  \end{itemize}
\item
  Course evaluation items assessed the the degree to which the pedagogy of course course reviewed by the respondent was socially and culturally responsive. In developing this survey, we chose items after reviewing evaluation items from several institutions of higher education and reviewing evaluative tools for open education resources. Eleven items were assessed on a 5-point scale ranging from \emph{strongly disagree} to \emph{strongly agree} with higher scores indicating a more positive evaluation of the course. Example items include:

  \begin{itemize}
  \tightlist
  \item
    ``Course content included materials authored by members of communities that are often marginalized (e.g., BIPOC, LGBTQ+, emerging economies).''
  \item
    ``A land acknowledgement was made (i.e., formal statement naming the indigenous people who originally inhabited the land).''
  \item
    ``Course materials (e.g., textbooks, articles, videos/podcasts) were free/no-cost to the students.''
  \end{itemize}
\item
  Percent of Black classmates was a single item that asked respondents to estimate the proportion of students in various racial categories.
\item
  Percent of BIPOC instructional staff, similarly, asked respondents to identify the racial category of each member of their instructional staff.
\end{itemize}

Our design has notable limitations. Briefly, (a) owing to the open source aspect of the data we do not ask about the demographic characteristics of the respondent; (b) the items that ask respondents to \emph{guess} the identities of the instructional staff and to place them in broad categories, (c) we do not provide options for ``write-in'' responses. We made these decisions after extensive conversation with stakeholders. The primary reason for these decisions was to prevent potential harm (a) to respondents who could be identified if/when the revealed private information in this open-source survey, and (b) trolls who would write inappropriate or harmful comments.

\emph{I would like to assess the model by having the instructional staff variable to be the \%Black instructional staff. At the time that this lecture is being prepared, there is not sufficient Black representation in this variable to model this.}

\hypertarget{importing-and-preparing-the-data}{%
\section{Importing and Preparing the Data}\label{importing-and-preparing-the-data}}

\textbf{This section of the lesson describes how to import data directly from Qualtrics and do some formatting. Because the survey remains open, if you import the data from Qualtrics you will get different results than are in the lesson. If you want to get the same results as me, \href{https://github.com/lhbikos/ReC_Psychometrics/blob/main/Model_df211010.rds}{download the dataset from the GitHub}, save it in the same place as your working .rmd file, and skip to ``START HERE''. For practice, you might consider downloading the data directly from Qualtrics for updated results.}

Three chapters in the \href{https://lhbikos.github.io/ReC_MultivariateModeling/scrub.html}{Multivariate Modeling} volume of ReCentering Psych Stats provide greater detail about the process of importing and preparing data from the \href{https://spupsych.az1.qualtrics.com/jfe/form/SV_b2cClqAlLGQ6nLU}{Rate-a-Recent-Course: A ReCentering Psych Stats Exercise} survey.

We start with an intRavenous import directly from Qualtrics. This is a two-step process.

\begin{itemize}
\tightlist
\item
  Establishing a connection to the Qualtrics account by supplying the base URL and API credentials.

  \begin{itemize}
  \tightlist
  \item
    Be very careful with these, they provide access to everything in your Qualtrics account. This Qualtrics account has only this survey and nothing else.
  \item
    If the API token becomes operational, please let me know. Qualtrics security may have a protocol to replace/disable them.
  \end{itemize}
\item
  Naming the survey (via its identification number) and importing the results.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The hashtagged line of code makes the connection to the}
\CommentTok{\# institution\textquotesingle{}s Qualtrics account and the individual Qualtrics}
\CommentTok{\# account within that institutional brand. Once that connection is}
\CommentTok{\# made, hashtag it out to avoid glitches. If you are changing from}
\CommentTok{\# one account to another you will likely need to restart R.}
\CommentTok{\# qualtRics::qualtrics\_api\_credentials(api\_key =}
\CommentTok{\# \textquotesingle{}mUgPMySYkiWpMFkwHale1QE5HNmh5LRUaA8d9PDg\textquotesingle{}, base\_url =}
\CommentTok{\# \textquotesingle{}spupsych.az1.qualtrics.com\textquotesingle{}, overwrite = TRUE, install = TRUE)}
\CommentTok{\# surveys \textless{}{-} qualtRics::all\_surveys() QTRX\_df}
\CommentTok{\# \textless{}{-}qualtRics::fetch\_survey(surveyID = \textquotesingle{}SV\_b2cClqAlLGQ6nLU\textquotesingle{},}
\CommentTok{\# time\_zone = NULL, verbose = FALSE, label=TRUE, force\_request =}
\CommentTok{\# TRUE, import\_id = FALSE) convert=FALSE,}
\end{Highlighting}
\end{Shaded}

In the next set of code, I quickly prepare the data that I will use for the hybrid SEM. In the next set of script we:

\begin{itemize}
\tightlist
\item
  Delete ``previews'' (those ``tester'' surveys taken prior to the official launch).
\item
  Rename a few variables to make them easier to manipulate.

  \begin{itemize}
  \tightlist
  \item
    Most variable naming was completed inside the Qualtrics survey, prior to importing, but some variables were impossible to rename and we did not anticipate all of our needs.
  \end{itemize}
\item
  Create an ID number for each case and moving it to the front of the dataframe.
\item
  Create a df that includes only the variables needed to specify the hybrid model.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# eliminating previews QTRX\_df \textless{}{-} dplyr::filter (QTRX\_df,}
\CommentTok{\# DistributionChannel != \textquotesingle{}preview\textquotesingle{})}

\CommentTok{\# renaming variables that start with numbers QTRX\_df \textless{}{-}}
\CommentTok{\# dplyr::rename(QTRX\_df, iRace1 = \textquotesingle{}1\_iRace\textquotesingle{}, iRace2 = \textquotesingle{}2\_iRace\textquotesingle{},}
\CommentTok{\# iRace3 = \textquotesingle{}3\_iRace\textquotesingle{}, iRace4 = \textquotesingle{}4\_iRace\textquotesingle{}, iRace5 = \textquotesingle{}5\_iRace\textquotesingle{}, iRace6}
\CommentTok{\# = \textquotesingle{}6\_iRace\textquotesingle{}, iRace7 = \textquotesingle{}7\_iRace\textquotesingle{}, iRace8 = \textquotesingle{}8\_iRace\textquotesingle{}, iRace9 =}
\CommentTok{\# \textquotesingle{}9\_iRace\textquotesingle{}, iRace10 = \textquotesingle{}10\_iRace\textquotesingle{})}

\CommentTok{\# renaming variables about classmates race/ethnicity QTRX\_df \textless{}{-}}
\CommentTok{\# dplyr::rename(QTRX\_df, cmBiMulti = Race\_10, cmBlack = Race\_1,}
\CommentTok{\# cmNBPoC = Race\_7, cmWhite = Race\_8, cmUnsure = Race\_2)}

\FunctionTok{library}\NormalTok{(tidyverse)  }\CommentTok{\#opening this package to be able to use pipes}
\CommentTok{\# creating ID variable and moving it to the front QTRX\_df \textless{}{-} QTRX\_df}
\CommentTok{\# \%\textgreater{}\% dplyr::mutate(ID = row\_number()) QTRX\_df \textless{}{-}}
\CommentTok{\# QTRX\_df\%\textgreater{}\%dplyr::select(ID, everything())}

\CommentTok{\# downsizing df to have just variables of interest Model\_df \textless{}{-}(select}
\CommentTok{\# (QTRX\_df, ID, iRace1, iRace2, iRace3, iRace4, iRace5, iRace6,}
\CommentTok{\# iRace7, iRace8, iRace9, iRace10, cmBiMulti, cmBlack, cmNBPoC,}
\CommentTok{\# cmWhite, cmUnsure, Blst\_1:Blst\_6, cEval\_8, cEval\_9, cEval\_10,}
\CommentTok{\# cEval\_11, cEval\_12, cEval\_13, cEval\_14, cEval\_15, cEval\_20,}
\CommentTok{\# cEval\_16,cEval\_17))}
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the imported data to your computing environment as either a .csv file (think ``Excel lite'') or .rds object (preserves any formatting you might do). If you save the .csv file and bring it back in, you will lose any formatting (e.g., ordered factors will be interpreted as character variables).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(Model\_df,}
\CommentTok{\# file=\textquotesingle{}Model\_df.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE)}
\CommentTok{\# bring back the simulated dat from a .csv file Model\_df \textless{}{-} read.csv}
\CommentTok{\# (\textquotesingle{}Model\_df.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

An .rds file preserves all formatting to variables prior to the export and re-import. For the purpose of this chapter, you don't need to do either. That is, you can re-simulate the data each time you work the problem.

\textbf{START HERE} to upload the {[}data you downloaded from the GitHub{]}(\url{https://github.com/lhbikos/ReC_Psychometrics/blob/main/Model_df211010.rds}. This will produce the same results in this lesson

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(Model\_df, \textquotesingle{}Model\_df.rds\textquotesingle{}) bring back the simulated dat}
\CommentTok{\# from an .rds file}
\NormalTok{Model\_df }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"Model\_df211010.rds"}\NormalTok{)  }\CommentTok{\#For this lesson, I saved and imported this set of data; use it if you want the same results as are in the lesson and screencasted lecture}
\CommentTok{\# Model\_df \textless{}{-} readRDS(\textquotesingle{}Model\_df.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

As a multicategorical variable, race/ethnicity frequently takes some thought and manipulation. I would have liked to have evaluated instructor race as the proportion of the instructional staff who is Black. At this time, there is so little variability in the instructional staff variable that we are using proportion of instructional staff who is BIPOC.

Given that classes may be teamtaught (and/or include teaching assistants) in the survey, respondents indicated how many instructional staff taught their class. For each, the respondent indicated the race/ethnicity of the instructor. It was possible to list up to 10 instructors per class. We need to get these 10 responses summarized as one variable representing the proportion of instructional faculty (per respondent/class) who were BIPOC. The code below:

\begin{itemize}
\tightlist
\item
  Transforms each race identification variable into a factor.
\item
  Calculates the proportion of BIPOC instructional faculty for each respondent's class.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# str(Model\_df$iRace1)}

\NormalTok{Model\_df}\SpecialCharTok{$}\NormalTok{tRace1 }\OtherTok{=} \FunctionTok{factor}\NormalTok{(Model\_df}\SpecialCharTok{$}\NormalTok{iRace1, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{), }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Black"}\NormalTok{,}
    \StringTok{"nBpoc"}\NormalTok{, }\StringTok{"BiMulti"}\NormalTok{, }\StringTok{"White"}\NormalTok{, }\StringTok{"NotNotice"}\NormalTok{))}
\NormalTok{Model\_df}\SpecialCharTok{$}\NormalTok{tRace2 }\OtherTok{=} \FunctionTok{factor}\NormalTok{(Model\_df}\SpecialCharTok{$}\NormalTok{iRace2, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{), }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Black"}\NormalTok{,}
    \StringTok{"nBpoc"}\NormalTok{, }\StringTok{"BiMulti"}\NormalTok{, }\StringTok{"White"}\NormalTok{, }\StringTok{"NotNotice"}\NormalTok{))}
\NormalTok{Model\_df}\SpecialCharTok{$}\NormalTok{tRace3 }\OtherTok{=} \FunctionTok{factor}\NormalTok{(Model\_df}\SpecialCharTok{$}\NormalTok{iRace3, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{), }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Black"}\NormalTok{,}
    \StringTok{"nBpoc"}\NormalTok{, }\StringTok{"BiMulti"}\NormalTok{, }\StringTok{"White"}\NormalTok{, }\StringTok{"NotNotice"}\NormalTok{))}
\NormalTok{Model\_df}\SpecialCharTok{$}\NormalTok{tRace4 }\OtherTok{=} \FunctionTok{factor}\NormalTok{(Model\_df}\SpecialCharTok{$}\NormalTok{iRace4, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{), }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Black"}\NormalTok{,}
    \StringTok{"nBpoc"}\NormalTok{, }\StringTok{"BiMulti"}\NormalTok{, }\StringTok{"White"}\NormalTok{, }\StringTok{"NotNotice"}\NormalTok{))}
\NormalTok{Model\_df}\SpecialCharTok{$}\NormalTok{tRace5 }\OtherTok{=} \FunctionTok{factor}\NormalTok{(Model\_df}\SpecialCharTok{$}\NormalTok{iRace5, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{), }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Black"}\NormalTok{,}
    \StringTok{"nBpoc"}\NormalTok{, }\StringTok{"BiMulti"}\NormalTok{, }\StringTok{"White"}\NormalTok{, }\StringTok{"NotNotice"}\NormalTok{))}
\NormalTok{Model\_df}\SpecialCharTok{$}\NormalTok{tRace6 }\OtherTok{=} \FunctionTok{factor}\NormalTok{(Model\_df}\SpecialCharTok{$}\NormalTok{iRace6, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{), }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Black"}\NormalTok{,}
    \StringTok{"nBpoc"}\NormalTok{, }\StringTok{"BiMulti"}\NormalTok{, }\StringTok{"White"}\NormalTok{, }\StringTok{"NotNotice"}\NormalTok{))}
\NormalTok{Model\_df}\SpecialCharTok{$}\NormalTok{tRace7 }\OtherTok{=} \FunctionTok{factor}\NormalTok{(Model\_df}\SpecialCharTok{$}\NormalTok{iRace7, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{), }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Black"}\NormalTok{,}
    \StringTok{"nBpoc"}\NormalTok{, }\StringTok{"BiMulti"}\NormalTok{, }\StringTok{"White"}\NormalTok{, }\StringTok{"NotNotice"}\NormalTok{))}
\NormalTok{Model\_df}\SpecialCharTok{$}\NormalTok{tRace8 }\OtherTok{=} \FunctionTok{factor}\NormalTok{(Model\_df}\SpecialCharTok{$}\NormalTok{iRace8, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{), }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Black"}\NormalTok{,}
    \StringTok{"nBpoc"}\NormalTok{, }\StringTok{"BiMulti"}\NormalTok{, }\StringTok{"White"}\NormalTok{, }\StringTok{"NotNotice"}\NormalTok{))}
\NormalTok{Model\_df}\SpecialCharTok{$}\NormalTok{tRace9 }\OtherTok{=} \FunctionTok{factor}\NormalTok{(Model\_df}\SpecialCharTok{$}\NormalTok{iRace9, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{), }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Black"}\NormalTok{,}
    \StringTok{"nBpoc"}\NormalTok{, }\StringTok{"BiMulti"}\NormalTok{, }\StringTok{"White"}\NormalTok{, }\StringTok{"NotNotice"}\NormalTok{))}
\NormalTok{Model\_df}\SpecialCharTok{$}\NormalTok{tRace10 }\OtherTok{=} \FunctionTok{factor}\NormalTok{(Model\_df}\SpecialCharTok{$}\NormalTok{iRace10, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{),}
    \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Black"}\NormalTok{, }\StringTok{"nBpoc"}\NormalTok{, }\StringTok{"BiMulti"}\NormalTok{, }\StringTok{"White"}\NormalTok{, }\StringTok{"NotNotice"}\NormalTok{))}
\CommentTok{\# checking to see that they are factors glimpse(Model\_df)}

\CommentTok{\# counting non{-}White instructional staff by creating the variable}
\CommentTok{\# \textquotesingle{}count.BIPOC\textquotesingle{} by summing across tRace1 thru tRace10 and assigning a}
\CommentTok{\# count of \textquotesingle{}1\textquotesingle{} each time the factor value was Black, nBpoc, or}
\CommentTok{\# BiMulti}
\NormalTok{Model\_df}\SpecialCharTok{$}\NormalTok{count.BIPOC }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(Model\_df[}\FunctionTok{c}\NormalTok{(}\StringTok{"tRace1"}\NormalTok{, }\StringTok{"tRace2"}\NormalTok{, }\StringTok{"tRace3"}\NormalTok{,}
    \StringTok{"tRace4"}\NormalTok{, }\StringTok{"tRace5"}\NormalTok{, }\StringTok{"tRace6"}\NormalTok{, }\StringTok{"tRace7"}\NormalTok{, }\StringTok{"tRace8"}\NormalTok{, }\StringTok{"tRace9"}\NormalTok{, }\StringTok{"tRace10"}\NormalTok{)],}
    \DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{sum}\NormalTok{(x }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"Black"}\NormalTok{, }\StringTok{"nBpoc"}\NormalTok{, }\StringTok{"BiMulti"}\NormalTok{)))}

\CommentTok{\# created a variable that counted the number of non{-}missing values}
\CommentTok{\# across the tRace1 thru tRace10 vars}
\NormalTok{Model\_df}\SpecialCharTok{$}\NormalTok{count.nMiss }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(Model\_df[}\FunctionTok{c}\NormalTok{(}\StringTok{"tRace1"}\NormalTok{, }\StringTok{"tRace2"}\NormalTok{, }\StringTok{"tRace3"}\NormalTok{,}
    \StringTok{"tRace4"}\NormalTok{, }\StringTok{"tRace5"}\NormalTok{, }\StringTok{"tRace6"}\NormalTok{, }\StringTok{"tRace7"}\NormalTok{, }\StringTok{"tRace8"}\NormalTok{, }\StringTok{"tRace9"}\NormalTok{, }\StringTok{"tRace10"}\NormalTok{)],}
    \DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{sum}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(x)))}

\CommentTok{\# calculate proportion of BIPOC instructional faculty for each case}
\NormalTok{Model\_df}\SpecialCharTok{$}\NormalTok{iBIPOC\_pr }\OtherTok{=}\NormalTok{ Model\_df}\SpecialCharTok{$}\NormalTok{count.BIPOC}\SpecialCharTok{/}\NormalTok{Model\_df}\SpecialCharTok{$}\NormalTok{count.nMiss}
\end{Highlighting}
\end{Shaded}

The scale assessing perceptions of campus climate for Black students had six items. One was worded in the opposite direction of the rest, therefore we must reverse-score it. Following the reverse-coding, I once again trimmed the dataframe so that it includes only the variables we need for the next step.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{Model\_df }\OtherTok{\textless{}{-}}\NormalTok{ Model\_df }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{rBlst\_1 =} \DecValTok{8} \SpecialCharTok{{-}}\NormalTok{ Blst\_1)  }\CommentTok{\#if you had multiple items, you could add a pipe (\%\textgreater{}\%) at the end of the line and add more until the last one}

\CommentTok{\# selecting the variables we want}
\NormalTok{Model\_df }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Model\_df, ID, iBIPOC\_pr, cmBlack, rBlst\_1, Blst\_2}\SpecialCharTok{:}\NormalTok{Blst\_6,}
\NormalTok{    cEval\_8}\SpecialCharTok{:}\NormalTok{cEval\_17)}
\end{Highlighting}
\end{Shaded}

\hypertarget{analyzing-and-managing-missingness}{%
\subsection{Analyzing and Managing Missingness}\label{analyzing-and-managing-missingness}}

The series of lessons on data preparation in the \href{https://lhbikos.github.io/ReC_MultivModel/}{Multivariate Modeling} volume provide a more detailed review of analyzing and managing missing data. Much of the script below is copied from those lessons and my review and explanation in this lesson is significantly shorter.

Structural equation models lend themselves to managing missing data with Parent's \citeyearpar{parent_handling_2013} \emph{available information analysis} (AIA) approach. My approach is to:

\begin{itemize}
\tightlist
\item
  Create a dataframe that includes only the variables that will be used in the analysis.
\item
  Delete all cases with greater than 20\% missingness.
\item
  If scale scores (or parcels) are used, calculate them if \textasciitilde80\% of the data for the calculation is present.
\item
  Use the \emph{full information maximum likelihood} (FIML) estimation procedure in \emph{lavaan}; this allows item-level missingness.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cases1 }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(Model\_df)  }\CommentTok{\#I produced this object for the sole purpose of feeding the number of cases into the inline text, below}
\NormalTok{cases1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 70
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\CommentTok{\# Create a variable (n\_miss) that counts the number missing}
\NormalTok{Model\_df}\SpecialCharTok{$}\NormalTok{n\_miss }\OtherTok{\textless{}{-}}\NormalTok{ Model\_df }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(iBIPOC\_pr}\SpecialCharTok{:}\NormalTok{cEval\_17) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    is.na }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rowSums}

\CommentTok{\# Create a proportion missing by dividing n\_miss by the total number}
\CommentTok{\# of variables (21) Sort in order of descending frequency to get a}
\CommentTok{\# sense of the missingness}
\NormalTok{Model\_df }\OtherTok{\textless{}{-}}\NormalTok{ Model\_df }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop\_miss =}\NormalTok{ (n\_miss}\SpecialCharTok{/}\DecValTok{21}\NormalTok{) }\SpecialCharTok{*} \DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n\_miss))}

\NormalTok{PrMiss1 }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(Model\_df}\SpecialCharTok{$}\NormalTok{prop\_miss)}
\NormalTok{PrMiss1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   vars  n  mean    sd median trimmed mad min   max range skew kurtosis   se
X1    1 70 27.48 40.41      0   23.04   0   0 90.48 90.48 0.87     -1.2 4.83
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MissMin1 }\OtherTok{\textless{}{-}}\NormalTok{ formattable}\SpecialCharTok{::}\FunctionTok{digits}\NormalTok{(PrMiss1}\SpecialCharTok{$}\NormalTok{min, }\DecValTok{0}\NormalTok{)  }\CommentTok{\#this object is displayed below and I use input from  it for the inline text used in the write{-}up}
\NormalTok{MissMax1 }\OtherTok{\textless{}{-}}\NormalTok{ formattable}\SpecialCharTok{::}\FunctionTok{digits}\NormalTok{(PrMiss1}\SpecialCharTok{$}\NormalTok{max, }\DecValTok{0}\NormalTok{)}
\NormalTok{MissMin1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MissMax1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 90
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CellsMissing1 }\OtherTok{\textless{}{-}}\NormalTok{ formattable}\SpecialCharTok{::}\FunctionTok{percent}\NormalTok{(}\FunctionTok{mean}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(Model\_df)))  }\CommentTok{\#percent missing across df}
\NormalTok{RowsMissing1 }\OtherTok{\textless{}{-}}\NormalTok{ formattable}\SpecialCharTok{::}\FunctionTok{percent}\NormalTok{(}\FunctionTok{mean}\NormalTok{(}\FunctionTok{complete.cases}\NormalTok{(Model\_df)))  }\CommentTok{\#percent of rows with nonmissing data}
\NormalTok{CellsMissing1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 26.23%
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RowsMissing1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 60.00%
\end{verbatim}

Our initial inspection of the data indicated that 70 attempted the survey. The proportion of missingness in the responses ranged from 0 to 90. Across the dataframe there was 26\% of missingness across the cells. Approximately 60\% of the cases had nonmissing data.

Let's conduct an analysis of missingness with the \emph{mice::md.pattern()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{missingness }\OtherTok{\textless{}{-}}\NormalTok{ mice}\SpecialCharTok{::}\FunctionTok{md.pattern}\NormalTok{(Model\_df, }\AttributeTok{plot =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rotate.names =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{13_HybridModels_files/figure-latex/unnamed-chunk-11-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{missingness}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   ID n_miss prop_miss cmBlack cEval_8 cEval_12 cEval_13 cEval_14 cEval_16
42  1      1         1       1       1        1        1        1        1
1   1      1         1       1       1        1        1        1        1
1   1      1         1       1       1        1        1        1        1
1   1      1         1       1       1        1        1        1        1
1   1      1         1       1       1        1        1        1        1
1   1      1         1       1       1        1        1        1        1
1   1      1         1       1       1        1        1        1        1
2   1      1         1       1       1        1        1        1        1
1   1      1         1       0       0        0        0        0        0
19  1      1         1       0       0        0        0        0        0
    0      0         0      20      20       20       20       20       20
   iBIPOC_pr cEval_9 cEval_10 cEval_15 cEval_20 cEval_17 Blst_2 Blst_3 Blst_5
42         1       1        1        1        1        1      1      1      1
1          1       1        1        1        1        1      1      1      1
1          1       1        1        1        1        1      0      1      1
1          1       1        1        1        1        1      0      0      0
1          1       1        1        1        1        0      1      1      1
1          1       1        1        0        0        1      1      1      0
1          1       0        0        1        1        1      1      0      1
2          0       1        1        1        1        1      1      1      1
1          1       0        0        0        0        0      0      0      0
19         0       0        0        0        0        0      0      0      0
          21      21       21       21       21       21     22     22     22
   Blst_6 cEval_11 Blst_4 rBlst_1    
42      1        1      1       1   0
1       1        1      1       0   1
1       0        1      0       0   4
1       0        1      0       0   6
1       1        1      1       1   1
1       1        0      1       0   5
1       1        0      0       0   6
2       1        1      1       1   1
1       0        0      0       0  18
19      0        0      0       0  19
       22       22     23      25 404
\end{verbatim}

We need to decide what is our retention threshold. Twenty percent seems to be a general rule of thumb. Let's delete all cases with missingness at 20\% or greater.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Model\_df }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(Model\_df, prop\_miss }\SpecialCharTok{\textless{}=} \DecValTok{20}\NormalTok{)  }\CommentTok{\#update df to have only those with at least 20\% of complete data (this is an arbitrary decision)}

\NormalTok{Model\_df }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Model\_df, iBIPOC\_pr}\SpecialCharTok{:}\NormalTok{cEval\_17)  }\CommentTok{\#the variable selection just lops off the proportion missing}

\NormalTok{CasesIncluded }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(Model\_df)}
\NormalTok{CasesIncluded  }\CommentTok{\#this object is displayed below and I use input from  it for the inline text used in the write{-}up}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 47
\end{verbatim}

We should check the missingness characteristics again.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CellsMissing2 }\OtherTok{\textless{}{-}}\NormalTok{ formattable}\SpecialCharTok{::}\FunctionTok{percent}\NormalTok{(}\FunctionTok{mean}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(Model\_df)))  }\CommentTok{\#percent missing across df}
\NormalTok{RowsMissing2 }\OtherTok{\textless{}{-}}\NormalTok{ formattable}\SpecialCharTok{::}\FunctionTok{percent}\NormalTok{(}\FunctionTok{mean}\NormalTok{(}\FunctionTok{complete.cases}\NormalTok{(Model\_df)))  }\CommentTok{\#percent of rows with nonmissing data}
\NormalTok{CellsMissing2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.90%
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RowsMissing2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 89.36%
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{missingness2 }\OtherTok{\textless{}{-}}\NormalTok{ mice}\SpecialCharTok{::}\FunctionTok{md.pattern}\NormalTok{(Model\_df, }\AttributeTok{plot =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rotate.names =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{13_HybridModels_files/figure-latex/unnamed-chunk-14-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{missingness2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   cmBlack Blst_3 Blst_5 cEval_8 cEval_9 cEval_10 cEval_11 cEval_12 cEval_13
42       1      1      1       1       1        1        1        1        1
1        1      1      1       1       1        1        1        1        1
2        1      1      1       1       1        1        1        1        1
1        1      1      1       1       1        1        1        1        1
1        1      1      1       1       1        1        1        1        1
         0      0      0       0       0        0        0        0        0
   cEval_14 cEval_15 cEval_20 cEval_16 Blst_2 Blst_4 Blst_6 cEval_17 iBIPOC_pr
42        1        1        1        1      1      1      1        1         1
1         1        1        1        1      1      1      1        1         1
2         1        1        1        1      1      1      1        1         0
1         1        1        1        1      1      1      1        0         1
1         1        1        1        1      0      0      0        1         1
          0        0        0        0      1      1      1        1         2
   rBlst_1  
42       1 0
1        0 1
2        1 1
1        1 1
1        0 4
         2 8
\end{verbatim}

Write up of results so far:

\begin{quote}
Our initial inspection of the data indicated that 70 attempted the survey. The proportion of missingness in the responses ranged from 0 to 90. Across the dataframe there was 26\% of missingness across the cells. Approximately 60\% of the cases had nonmissing data. The predominant pattern of missingness included individuals opening the survey without completing any of the items. Beyond that, our inspection of a missingness map indicated a haphazard pattern of missingness \citep{enders_applied_2010}.
\end{quote}

\begin{quote}
We decided to delete all cases with greater than 20\% missinness. We reinspected the missingness of the dataset with 47 cases. Across the dataframe there was less than 1\% of missingness across the cells. Approximately 89\% of the cases had nonmissing data.
\end{quote}

\hypertarget{assessing-the-distributional-characteristics-of-the-data}{%
\subsection{Assessing the Distributional Characteristics of the Data}\label{assessing-the-distributional-characteristics-of-the-data}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(Model\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
          vars  n mean   sd median trimmed  mad min max range  skew kurtosis
iBIPOC_pr    1 45 0.33 0.40   0.25    0.30 0.37   0   1     1  0.76    -1.06
cmBlack      2 47 6.66 7.64   5.00    5.46 7.41   0  29    29  1.29     0.85
rBlst_1      3 45 3.42 1.60   3.00    3.35 1.48   1   7     6  0.48    -0.60
Blst_2       4 46 2.67 1.59   2.00    2.58 1.48   1   6     5  0.30    -1.42
Blst_3       5 47 2.11 1.29   2.00    1.95 1.48   1   6     5  1.06     0.25
Blst_4       6 46 2.35 1.25   2.00    2.24 1.48   1   5     4  0.61    -0.80
Blst_5       7 47 2.00 1.20   2.00    1.87 1.48   1   5     4  0.89    -0.58
Blst_6       8 46 2.26 1.34   2.00    2.08 1.48   1   6     5  0.99     0.04
cEval_8      9 47 4.30 0.83   4.00    4.44 1.48   1   5     4 -1.69     3.98
cEval_9     10 47 3.81 1.06   4.00    3.87 1.48   2   5     3 -0.49    -1.00
cEval_10    11 47 4.00 0.98   4.00    4.13 1.48   1   5     4 -0.96     0.53
cEval_11    12 47 3.40 1.15   3.00    3.41 1.48   1   5     4  0.02    -1.21
cEval_12    13 47 3.85 1.27   4.00    3.97 1.48   1   5     4 -0.73    -0.83
cEval_13    14 47 3.83 1.05   4.00    3.97 0.00   1   5     4 -1.21     1.18
cEval_14    15 47 3.72 1.36   4.00    3.87 1.48   1   5     4 -0.97    -0.30
cEval_15    16 47 3.26 1.19   3.00    3.31 1.48   1   5     4 -0.41    -0.78
cEval_20    17 47 3.55 1.14   4.00    3.67 1.48   1   5     4 -0.73    -0.10
cEval_16    18 47 3.13 1.30   3.00    3.15 1.48   1   5     4 -0.23    -1.11
cEval_17    19 46 2.67 1.43   2.00    2.61 1.48   1   5     4  0.35    -1.31
            se
iBIPOC_pr 0.06
cmBlack   1.11
rBlst_1   0.24
Blst_2    0.23
Blst_3    0.19
Blst_4    0.18
Blst_5    0.17
Blst_6    0.20
cEval_8   0.12
cEval_9   0.15
cEval_10  0.14
cEval_11  0.17
cEval_12  0.18
cEval_13  0.15
cEval_14  0.20
cEval_15  0.17
cEval_20  0.17
cEval_16  0.19
cEval_17  0.21
\end{verbatim}

Regarding the distributional characteristics of the data, skew and kurtosis values of the variables fell below the values of 3 (skew) and 8 to 20 (kurtosis) that Kline suggests are concerning \citeyearpar{kline_principles_2016}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Model\_df}\SpecialCharTok{$}\NormalTok{Mahal }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{outlier}\NormalTok{(Model\_df)}
\end{Highlighting}
\end{Shaded}

\includegraphics{13_HybridModels_files/figure-latex/unnamed-chunk-16-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\CommentTok{\# str(item\_scores\_df$Mahal)}
\NormalTok{Model\_df}\SpecialCharTok{$}\NormalTok{MOutlier }\OtherTok{\textless{}{-}} \FunctionTok{if\_else}\NormalTok{(Model\_df}\SpecialCharTok{$}\NormalTok{Mahal }\SpecialCharTok{\textgreater{}}\NormalTok{ (}\FunctionTok{median}\NormalTok{(Model\_df}\SpecialCharTok{$}\NormalTok{Mahal) }\SpecialCharTok{+}
\NormalTok{    (}\DecValTok{3} \SpecialCharTok{*} \FunctionTok{sd}\NormalTok{(Model\_df}\SpecialCharTok{$}\NormalTok{Mahal))), }\ConstantTok{TRUE}\NormalTok{, }\ConstantTok{FALSE}\NormalTok{)}

\NormalTok{OutlierCount }\OtherTok{\textless{}{-}}\NormalTok{ Model\_df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{count}\NormalTok{(MOutlier)}
\NormalTok{OutlierCount}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
  MOutlier     n
  <lgl>    <int>
1 FALSE       47
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NumOutliers }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(Model\_df) }\SpecialCharTok{{-}}\NormalTok{ OutlierCount  }\CommentTok{\#calculating how many outliers}
\NormalTok{NumOutliers  }\CommentTok{\#this object is used for the inline text for the reesults}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  MOutlier n
1       47 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NumOutliers}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  MOutlier n
1       47 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(Model\_df)  }\CommentTok{\#shows us the first 6 rows of the data so we can see the new variables (Mahal, MOutlier)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 21
  iBIPOC_pr cmBlack rBlst_1 Blst_2 Blst_3 Blst_4 Blst_5 Blst_6 cEval_8 cEval_9
      <dbl>   <dbl>   <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl>   <dbl>
1     1           0      NA     NA      2     NA      2     NA       5       4
2     0           5       2      6      2      2      4      1       5       5
3   NaN           5       4      5      4      5      3      4       4       4
4   NaN          14       3      4      1      3      1      2       4       3
5     0.25        0      NA      1      1      1      1      1       4       5
6     0.333       0       3      3      5      2      2      2       5       5
# i 11 more variables: cEval_10 <dbl>, cEval_11 <dbl>, cEval_12 <dbl>,
#   cEval_13 <dbl>, cEval_14 <dbl>, cEval_15 <dbl>, cEval_20 <dbl>,
#   cEval_16 <dbl>, cEval_17 <dbl>, Mahal <dbl>, MOutlier <lgl>
\end{verbatim}

We evaluated multivariate normality with the Mahalanobis distance test. Specifically, we used the \emph{outlier()} function in the \emph{psych} package and included all continuous variables in the calculation. Our visual inspection of the Q-Q plot suggested that the plotted line strayed from the straight line as the quantiles increased. Additionally, we appended the Mahalanobis distance scores as a variable to the data. Analyzing this variable, we found that no exceed three standard deviations beyond the median. \emph{Thus, with no outliers, we assumed multivariate normality and proceeded with the data analysis}

\hypertarget{preliminary-analyses}{%
\subsection{Preliminary Analyses}\label{preliminary-analyses}}

\hypertarget{internal-consistency-coefficients}{%
\subsubsection{Internal Consistency Coefficients}\label{internal-consistency-coefficients}}

Most research projects start with some preliminary statistics. Even though we will be using item-level data in our hybrid model, for any instruments that are scales, we typically compute internal consistency coefficients and include these values in the last sentence of in the description of the respective measure. In this example we used two scales: Perceptions of the Campus Climate for Black Students and the Course Evaluation items that evaluated the degree to which the pedagogy was socially and culturally responsive. A more thorough description of internal consistency coefficients are found in the \href{rxx}{reliability} chapter of this volume.

My process for calculating alpha coefficients is to first create a subset of item-level data that is consistently scaled in the same direction. That is, reverse score any items before creating the subset of data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClimateItems }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Model\_df, rBlst\_1, Blst\_2, Blst\_3, Blst\_4,}
\NormalTok{    Blst\_5, Blst\_6)}
\NormalTok{CEvalItems }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Model\_df, cEval\_8, cEval\_9, cEval\_10, cEval\_11,}
\NormalTok{    cEval\_12, cEval\_13, cEval\_14, cEval\_15, cEval\_16, cEval\_17, cEval\_20)}
\end{Highlighting}
\end{Shaded}

Next, in separate analyses, we apply the \emph{psych::alpha()} function to the scale items.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClimateAlpha }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(ClimateItems)}
\NormalTok{ClimateAlpha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = ClimateItems)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r
      0.87      0.88    0.88      0.55 7.4 0.029  2.5 1.1     0.53

    95% confidence boundaries 
         lower alpha upper
Feldt     0.81  0.87  0.92
Duhachek  0.82  0.87  0.93

 Reliability if an item is dropped:
        raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r
rBlst_1      0.88      0.89    0.88      0.62 8.0    0.027 0.016  0.60
Blst_2       0.86      0.87    0.86      0.56 6.5    0.034 0.026  0.53
Blst_3       0.87      0.88    0.87      0.59 7.3    0.031 0.020  0.57
Blst_4       0.83      0.84    0.83      0.52 5.4    0.039 0.019  0.50
Blst_5       0.83      0.83    0.82      0.50 5.1    0.040 0.018  0.47
Blst_6       0.83      0.84    0.83      0.52 5.3    0.039 0.019  0.48

 Item statistics 
         n raw.r std.r r.cor r.drop mean  sd
rBlst_1 45  0.68  0.65  0.53   0.51  3.4 1.6
Blst_2  46  0.78  0.76  0.70   0.65  2.7 1.6
Blst_3  47  0.69  0.70  0.61   0.57  2.1 1.3
Blst_4  46  0.86  0.86  0.85   0.79  2.3 1.3
Blst_5  47  0.88  0.89  0.89   0.84  2.0 1.2
Blst_6  46  0.86  0.87  0.86   0.79  2.3 1.3

Non missing response frequency for each item
           1    2    3    4    5    6    7 miss
rBlst_1 0.09 0.24 0.22 0.22 0.09 0.09 0.04 0.04
Blst_2  0.37 0.15 0.09 0.24 0.13 0.02 0.00 0.02
Blst_3  0.43 0.30 0.09 0.15 0.02 0.02 0.00 0.00
Blst_4  0.30 0.33 0.15 0.15 0.07 0.00 0.00 0.02
Blst_5  0.47 0.28 0.06 0.17 0.02 0.00 0.00 0.00
Blst_6  0.35 0.35 0.11 0.11 0.07 0.02 0.00 0.02
\end{verbatim}

We learn that the Cronbach's alpha coefficient for the scale assessing perceptions of campus climate for students who are Black is 0.881. This exceeds the recommended thresshold of .80. I would simply add a sentence similar to the following to the end of my description of the scale in the Method/Measures section: In our study the estimated internal consistency reliability of the total scale score assessing campus climate was 0.881.

Let's repeat the process for the items assessing the degree to which the pedagogy was socially and culturally responsive.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CEvalAlpha }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(CEvalItems)}
\NormalTok{CEvalAlpha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = CEvalItems)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r
      0.88      0.89    0.92      0.42 7.8 0.025  3.6 0.8     0.41

    95% confidence boundaries 
         lower alpha upper
Feldt     0.83  0.88  0.93
Duhachek  0.83  0.88  0.93

 Reliability if an item is dropped:
         raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r
cEval_8       0.87      0.88    0.90      0.42 7.2    0.027 0.017  0.42
cEval_9       0.88      0.88    0.91      0.43 7.6    0.026 0.018  0.43
cEval_10      0.87      0.87    0.90      0.41 6.8    0.028 0.019  0.40
cEval_11      0.87      0.87    0.91      0.41 6.9    0.028 0.022  0.39
cEval_12      0.88      0.89    0.92      0.44 7.8    0.025 0.018  0.43
cEval_13      0.87      0.87    0.90      0.41 6.8    0.028 0.019  0.40
cEval_14      0.87      0.87    0.90      0.41 6.9    0.029 0.019  0.41
cEval_15      0.86      0.87    0.90      0.40 6.6    0.030 0.017  0.39
cEval_16      0.87      0.88    0.91      0.42 7.2    0.028 0.021  0.40
cEval_17      0.87      0.88    0.91      0.42 7.2    0.027 0.017  0.41
cEval_20      0.87      0.88    0.90      0.42 7.2    0.028 0.017  0.41

 Item statistics 
          n raw.r std.r r.cor r.drop mean   sd
cEval_8  47  0.63  0.67  0.65   0.57  4.3 0.83
cEval_9  47  0.57  0.60  0.56   0.48  3.8 1.06
cEval_10 47  0.72  0.75  0.73   0.66  4.0 0.98
cEval_11 47  0.72  0.72  0.68   0.64  3.4 1.15
cEval_12 47  0.55  0.56  0.49   0.44  3.9 1.27
cEval_13 47  0.72  0.74  0.73   0.66  3.8 1.05
cEval_14 47  0.74  0.72  0.70   0.66  3.7 1.36
cEval_15 47  0.80  0.79  0.79   0.74  3.3 1.19
cEval_16 47  0.68  0.66  0.61   0.59  3.1 1.30
cEval_17 46  0.70  0.66  0.63   0.59  2.7 1.43
cEval_20 47  0.68  0.66  0.64   0.60  3.6 1.14

Non missing response frequency for each item
            1    2    3    4    5 miss
cEval_8  0.02 0.02 0.04 0.47 0.45 0.00
cEval_9  0.00 0.17 0.15 0.38 0.30 0.00
cEval_10 0.02 0.06 0.15 0.43 0.34 0.00
cEval_11 0.02 0.23 0.30 0.21 0.23 0.00
cEval_12 0.04 0.17 0.11 0.26 0.43 0.00
cEval_13 0.06 0.04 0.13 0.53 0.23 0.00
cEval_14 0.15 0.02 0.13 0.36 0.34 0.00
cEval_15 0.11 0.15 0.26 0.36 0.13 0.00
cEval_16 0.15 0.17 0.23 0.30 0.15 0.00
cEval_17 0.26 0.28 0.13 0.17 0.15 0.02
cEval_20 0.09 0.06 0.26 0.40 0.19 0.00
\end{verbatim}

The alpha coefficient for the course evaluation items assessing a socially and culturally responsive pedagogy was 0.887. I would add this sentence to the description of this measure.

\hypertarget{means-sds-r-matrix}{%
\subsubsection{Means, SDs, r-matrix}\label{means-sds-r-matrix}}

Means, standard deviations, and a correlation matrix are also commonly reported. Because two of our constructs are scales, we will need to calculate their means for cases that have met the minimum thresshold for nonmissingness.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create lists of the items}
\NormalTok{ClimateVars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"rBlst\_1"}\NormalTok{, }\StringTok{"Blst\_2"}\NormalTok{, }\StringTok{"Blst\_3"}\NormalTok{, }\StringTok{"Blst\_4"}\NormalTok{, }\StringTok{"Blst\_5"}\NormalTok{, }\StringTok{"Blst\_6"}\NormalTok{)}
\NormalTok{CEvalVars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"cEval\_8"}\NormalTok{, }\StringTok{"cEval\_9"}\NormalTok{, }\StringTok{"cEval\_10"}\NormalTok{, }\StringTok{"cEval\_11"}\NormalTok{, }\StringTok{"cEval\_12"}\NormalTok{,}
    \StringTok{"cEval\_13"}\NormalTok{, }\StringTok{"cEval\_14"}\NormalTok{, }\StringTok{"cEval\_15"}\NormalTok{, }\StringTok{"cEval\_16"}\NormalTok{, }\StringTok{"cEval\_17"}\NormalTok{, }\StringTok{"cEval\_20"}\NormalTok{)}

\CommentTok{\# calculate means for when a specified proportion of items are}
\CommentTok{\# non{-}missing}
\NormalTok{Model\_df}\SpecialCharTok{$}\NormalTok{ClimateM }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(Model\_df[, ClimateVars], }\FloatTok{0.8}\NormalTok{)  }\CommentTok{\#will create the mean for each individual if 80\% of variables are present (this means there must be at least 5 of 6)}
\NormalTok{Model\_df}\SpecialCharTok{$}\NormalTok{CEvalM }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(Model\_df[, CEvalVars], }\FloatTok{0.8}\NormalTok{)  }\CommentTok{\#will create the mean for each individual if 80\% of variables are present (this means there must be at least 9 of 11)}
\end{Highlighting}
\end{Shaded}

The \emph{apaTables::cor.table} function creates the standard table that will include the means, standard deviations, and correlation matrix.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(Model\_df[}\FunctionTok{c}\NormalTok{(}\StringTok{"ClimateM"}\NormalTok{, }\StringTok{"CEvalM"}\NormalTok{, }\StringTok{"iBIPOC\_pr"}\NormalTok{,}
    \StringTok{"cmBlack"}\NormalTok{)], }\AttributeTok{landscape =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{table.number =} \DecValTok{1}\NormalTok{, }\AttributeTok{filename =} \StringTok{"Table1\_Prelim.doc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Table 1 

Means, standard deviations, and correlations with confidence intervals
 

  Variable     M    SD   1            2           3          
  1. ClimateM  2.46 1.09                                     
                                                             
  2. CEvalM    3.60 0.80 -.30*                               
                         [-.55, -.02]                        
                                                             
  3. iBIPOC_pr 0.33 0.40 -.26         .24                    
                         [-.52, .04]  [-.06, .50]            
                                                             
  4. cmBlack   6.66 7.64 -.19         .08         -.07       
                         [-.45, .11]  [-.21, .36] [-.36, .23]
                                                             

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

\hypertarget{summary-of-data-preparation}{%
\subsection{Summary of Data Preparation}\label{summary-of-data-preparation}}

\begin{quote}
We began by creating a dataset that included only the variables of interest. Our initial inspection of the data indicated that 70 attempted the survey. The proportion of missingness in the responses ranged from 0 to 90. Across the dataframe there was 26\% of missingness across the cells. Approximately 60\% of the cases had nonmissing data. The predominant pattern of missingness included individuals opening the survey without completing any of the items. Beyond that, our inspection of a missingness map indicated a haphazard pattern of missingness \citep{enders_applied_2010}.
\end{quote}

\begin{quote}
Using Parent's \emph{available item analysis} {[}AIA; -\citet{parent_handling_2013}{]} as a guide, we deleted all cases where there was greater than 20\% of data missing. We reinspected the missingness of the dataset with 47 cases. Across the dataframe there was less than 1\% of missingness across the cells. Approximately 89\% of the cases had nonmissing data.
\end{quote}

\begin{quote}
Regarding the distributional characteristics of the data, skew and kurtosis values of the variables fell below the values of 3 (skew) and 8 to 20 (kurtosis) that Kline suggests are concerning \citeyearpar{kline_principles_2016}. We evaluated multivariate normality with the Mahalanobis distance test. Specifically, we used the \emph{outlier()} function in the \emph{psych} package and included all continuous variables in the calculation. Our visual inspection of the Q-Q plot suggested that the plotted line strayed from the straight line as the quantiles increased. Additionally, we appended the Mahalanobis distance scores as a variable to the data. Analyzing this variable, we found that no exceed three standard deviations beyond the median. Thus, with no outliers, we assumed multivariate normality and proceeded with the data analysis. Means, standard deviations, and a correlation matrix are found in Table 1.
\end{quote}

\hypertarget{the-measurement-model-specification-and-evaluation}{%
\section{The Measurement Model: Specification and Evaluation}\label{the-measurement-model-specification-and-evaluation}}

Structural regression (e.g., structural equation, hybrid) models include both \emph{measurement} and \emph{structural} portions. The \textbf{measurement model} examines the relationship between latent variables and their measures.

\begin{itemize}
\tightlist
\item
  Testing the measurement model means \emph{saturating} it, such that \(df = 0\) and it is \emph{just-identified}.\\
\item
  Essentially, the measurement model is a correlated factors model. However, rather than having subscales of a larger scale, these are all the LVs involved in your model.\\
\item
  Testing the measurement model points out any misfit in the measurement model (that you need to fix). Heywood cases(usually a negative error variance, which is an impossible solution) are an example of a problem that would need to be addressed before fixing.
\end{itemize}

The \textbf{structural model} evaluates the hypothesized relations between the latent variables.

\begin{itemize}
\tightlist
\item
  The structural model is typically more parsimonious (i.e., not saturated) than the measurement model and is characterized by directional paths (not covariances) between some (not all) of the variables.
\end{itemize}

The specification of our measurement model resembles the first-order, correlated traits specifications in prior lessons. What differs is that we include all latent variables and their specifications. Below, there are no surprises about the Climate and CourseEval latent variables, because these are traditional scales and they have at least three items/indicators. In contrast, latent variables with one and two indicators requires special treatment.

For two-indicator latent variables, Little et al. \citeyearpar{little_statistical_2002} recommended placing an equality constraint on the two loadings associated with the construct because this would locate the construct at the true intersection of the two selected indicators. Procedurally this is fairly straightforward. If we wanted to create a latent variable from the proportions of (a) instructional staff and (b) classmates who are Black we would simply assign labels to the two indicators:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#PrBlack =\textasciitilde{} v1*iBIPOC\_pr + v1*cmBlack}
\end{Highlighting}
\end{Shaded}

For single indicator latent variables, Little et al. \citeyearpar{little_statistical_2002} wrote, ``a single-indicator latent variable is essentially equivalent to a manifest variable. In this case, the error of measurement is either fixed at zero or fixed at a non-zero estimate of unreliability; additionally a second corresponding parameter would also need to be fixed because of issue of identification.''

Our proportion of instructional staff who are BIPOC and estimated proportion of classmates who are Black were estimated with one item each. In order to include single items as latent variables, we set the observed variable to be 0.00. In essence, this says that the latent variable will account for all of the variance in the observed variable. Note that for each of the single-item variables, there are two lines of code. The first, defines the LV from the item; the second specifies the error variance of the single observed variable to be 0.00.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{msmt }\OtherTok{\textless{}{-}} \StringTok{"  }
\StringTok{\#latent variable definitions for the factors with 3 or more indicators}
\StringTok{   Climate =\textasciitilde{} rBlst\_1 + Blst\_4 + Blst\_6 + Blst\_2 + Blst\_3 + Blst\_5}
\StringTok{   CourseEval =\textasciitilde{} cEval\_8 + cEval\_9 + cEval\_10 + cEval\_11 + cEval\_12 + cEval\_13 + cEval\_14 + cEval\_15 + cEval\_20 + cEval\_16 + cEval\_17}
\StringTok{   }
\StringTok{  \#latent variable definitions for the factors with 1 indicator; we set variance of the observed variable to be 0.00; this says that the LV will account for all of the variance in the observed variable}
\StringTok{   tBIPOC =\textasciitilde{} iBIPOC\_pr \#for the factor }\SpecialCharTok{\textbackslash{}"}\StringTok{t}\SpecialCharTok{\textbackslash{}"}\StringTok{ is teacher; for variable }\SpecialCharTok{\textbackslash{}"}\StringTok{i}\SpecialCharTok{\textbackslash{}"}\StringTok{ is instructor}
\StringTok{   sBlack =\textasciitilde{} cmBlack \#for factor }\SpecialCharTok{\textbackslash{}"}\StringTok{s}\SpecialCharTok{\textbackslash{}"}\StringTok{ is student; for variable }\SpecialCharTok{\textbackslash{}"}\StringTok{cm}\SpecialCharTok{\textbackslash{}"}\StringTok{ is classmates}
\StringTok{  }
\StringTok{   iBIPOC\_pr \textasciitilde{}\textasciitilde{} 0*iBIPOC\_pr \#this specifies the error variance of the single observed variable to be 0.00}
\StringTok{   cmBlack \textasciitilde{}\textasciitilde{} 0*cmBlack}
\StringTok{   "}
\end{Highlighting}
\end{Shaded}

\hypertarget{managing-missing-data-with-fiml}{%
\subsubsection{Managing missing data with FIML}\label{managing-missing-data-with-fiml}}

If the data contain missing values, the default behavior in \emph{lavaan} is listwise deletion. If we can presume that the missing mechanism is MCAR or MAR (e.g., there is no systematic missingness), we can specify a \emph{full information maximum likelihood} (FIML) estimation procedure with the argument \emph{missing = ``ml''} (or its alias \emph{missing = ``fiml''}). Recall that we retained cases if they had 20\% or less missing. Usin the ``fiml'' option is part of the AIA approach \citep{parent_handling_2013}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{msmt\_fit }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(msmt, }\AttributeTok{data =}\NormalTok{ Model\_df, }\AttributeTok{missing =} \StringTok{"fiml"}\NormalTok{, }\AttributeTok{check.gradient =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\# msmt\_fit \textless{}{-} lavaan::cfa(msmt, data = Model\_df, missing = \textquotesingle{}fiml\textquotesingle{},}
\CommentTok{\# estimator = \textquotesingle{}ML\textquotesingle{}, bounds = \textquotesingle{}wide\textquotesingle{})}
\NormalTok{m1fitsum }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(msmt\_fit, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{rsquare =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\# missing = \textquotesingle{}fiml\textquotesingle{},}
\NormalTok{m1fitsum}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 80 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        61

  Number of observations                            47
  Number of missing patterns                         5

Model Test User Model:
                                                      
  Test statistic                               239.689
  Degrees of freedom                               148
  P-value (Chi-square)                           0.000

Model Test Baseline Model:

  Test statistic                               590.701
  Degrees of freedom                               171
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.782
  Tucker-Lewis Index (TLI)                       0.748
                                                      
  Robust Comparative Fit Index (CFI)             0.774
  Robust Tucker-Lewis Index (TLI)                0.739

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -1282.408
  Loglikelihood unrestricted model (H1)      -1162.563
                                                      
  Akaike (AIC)                                2686.815
  Bayesian (BIC)                              2799.674
  Sample-size adjusted Bayesian (SABIC)       2608.355

Root Mean Square Error of Approximation:

  RMSEA                                          0.115
  90 Percent confidence interval - lower         0.087
  90 Percent confidence interval - upper         0.141
  P-value H_0: RMSEA <= 0.050                    0.000
  P-value H_0: RMSEA >= 0.080                    0.980
                                                      
  Robust RMSEA                                   0.118
  90 Percent confidence interval - lower         0.090
  90 Percent confidence interval - upper         0.144
  P-value H_0: Robust RMSEA <= 0.050             0.000
  P-value H_0: Robust RMSEA >= 0.080             0.985

Standardized Root Mean Square Residual:

  SRMR                                           0.100

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Observed
  Observed information based on                Hessian

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Climate =~                                                            
    rBlst_1           1.000                               0.851    0.538
    Blst_4            1.242    0.322    3.856    0.000    1.057    0.860
    Blst_6            1.343    0.347    3.866    0.000    1.142    0.867
    Blst_2            1.267    0.368    3.441    0.001    1.078    0.687
    Blst_3            0.935    0.289    3.232    0.001    0.796    0.624
    Blst_5            1.247    0.319    3.909    0.000    1.061    0.895
  CourseEval =~                                                         
    cEval_8           1.000                               0.508    0.617
    cEval_9           1.067    0.338    3.153    0.002    0.542    0.519
    cEval_10          1.352    0.331    4.087    0.000    0.687    0.710
    cEval_11          1.503    0.404    3.719    0.000    0.763    0.668
    cEval_12          1.184    0.409    2.895    0.004    0.601    0.479
    cEval_13          1.475    0.361    4.089    0.000    0.749    0.722
    cEval_14          1.870    0.505    3.701    0.000    0.949    0.704
    cEval_15          1.824    0.460    3.961    0.000    0.926    0.788
    cEval_20          1.413    0.428    3.301    0.001    0.717    0.637
    cEval_16          1.549    0.457    3.391    0.001    0.786    0.613
    cEval_17          1.835    0.539    3.403    0.001    0.932    0.654
  tBIPOC =~                                                             
    iBIPOC_pr         1.000                               0.396    1.000
  sBlack =~                                                             
    cmBlack           1.000                               7.560    1.000

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Climate ~~                                                            
    CourseEval       -0.138    0.086   -1.604    0.109   -0.319   -0.319
    tBIPOC           -0.075    0.057   -1.320    0.187   -0.224   -0.224
    sBlack           -1.519    1.067   -1.423    0.155   -0.236   -0.236
  CourseEval ~~                                                         
    tBIPOC            0.054    0.034    1.588    0.112    0.267    0.267
    sBlack            0.382    0.605    0.631    0.528    0.099    0.099
  tBIPOC ~~                                                             
    sBlack           -0.201    0.442   -0.455    0.649   -0.067   -0.067

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .rBlst_1           3.402    0.234   14.507    0.000    3.402    2.151
   .Blst_4            2.346    0.180   13.030    0.000    2.346    1.909
   .Blst_6            2.259    0.193   11.712    0.000    2.259    1.715
   .Blst_2            2.672    0.230   11.605    0.000    2.672    1.704
   .Blst_3            2.106    0.186   11.321    0.000    2.106    1.651
   .Blst_5            2.000    0.173   11.571    0.000    2.000    1.688
   .cEval_8           4.298    0.120   35.804    0.000    4.298    5.223
   .cEval_9           3.809    0.152   24.997    0.000    3.809    3.646
   .cEval_10          4.000    0.141   28.342    0.000    4.000    4.134
   .cEval_11          3.404    0.167   20.433    0.000    3.404    2.980
   .cEval_12          3.851    0.183   21.047    0.000    3.851    3.070
   .cEval_13          3.830    0.151   25.295    0.000    3.830    3.690
   .cEval_14          3.723    0.197   18.936    0.000    3.723    2.762
   .cEval_15          3.255    0.171   18.987    0.000    3.255    2.769
   .cEval_20          3.553    0.164   21.629    0.000    3.553    3.155
   .cEval_16          3.128    0.187   16.727    0.000    3.128    2.440
   .cEval_17          2.704    0.209   12.923    0.000    2.704    1.898
   .iBIPOC_pr         0.330    0.059    5.585    0.000    0.330    0.831
   .cmBlack           6.660    1.103    6.039    0.000    6.660    0.881

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .iBIPOC_pr         0.000                               0.000    0.000
   .cmBlack           0.000                               0.000    0.000
   .rBlst_1           1.776    0.388    4.579    0.000    1.776    0.710
   .Blst_4            0.394    0.111    3.544    0.000    0.394    0.261
   .Blst_6            0.430    0.126    3.425    0.001    0.430    0.248
   .Blst_2            1.298    0.295    4.393    0.000    1.298    0.528
   .Blst_3            0.994    0.217    4.577    0.000    0.994    0.611
   .Blst_5            0.279    0.094    2.986    0.003    0.279    0.199
   .cEval_8           0.419    0.098    4.295    0.000    0.419    0.619
   .cEval_9           0.797    0.176    4.542    0.000    0.797    0.731
   .cEval_10          0.465    0.119    3.920    0.000    0.465    0.496
   .cEval_11          0.723    0.165    4.374    0.000    0.723    0.554
   .cEval_12          1.212    0.260    4.666    0.000    1.212    0.770
   .cEval_13          0.516    0.127    4.062    0.000    0.516    0.479
   .cEval_14          0.916    0.221    4.139    0.000    0.916    0.504
   .cEval_15          0.524    0.145    3.608    0.000    0.524    0.379
   .cEval_20          0.754    0.178    4.229    0.000    0.754    0.594
   .cEval_16          1.025    0.229    4.483    0.000    1.025    0.624
   .cEval_17          1.161    0.275    4.230    0.000    1.161    0.572
    Climate           0.724    0.381    1.901    0.057    1.000    1.000
    CourseEval        0.258    0.119    2.172    0.030    1.000    1.000
    tBIPOC            0.157    0.033    4.757    0.000    1.000    1.000
    sBlack           57.161   11.791    4.848    0.000    1.000    1.000

R-Square:
                   Estimate
    iBIPOC_pr         1.000
    cmBlack           1.000
    rBlst_1           0.290
    Blst_4            0.739
    Blst_6            0.752
    Blst_2            0.472
    Blst_3            0.389
    Blst_5            0.801
    cEval_8           0.381
    cEval_9           0.269
    cEval_10          0.504
    cEval_11          0.446
    cEval_12          0.230
    cEval_13          0.521
    cEval_14          0.496
    cEval_15          0.621
    cEval_20          0.406
    cEval_16          0.376
    cEval_17          0.428
\end{verbatim}

\hypertarget{interpreting-the-output-11}{%
\subsubsection{Interpreting the Output}\label{interpreting-the-output-11}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3780}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4512}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1707}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Our Results
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Criteria met?
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Factor loadings significant, strong, proper valence & & Yes \\
Non-significant chi-square & \(\chi ^{2}(148) = 239.689, *p* < 0.001\) & No \\
\(CFI\geq .95\) & CFI = 0.782 & No \\
\(RMSEA\leq .05\) (but definitely \textless{} .10) & RMSEA = 0.115, 90\%CI(0.087, 0.141) & No \\
\(SRMR\leq .08\) (but definitely \textless{} .10) & SRMR = 0.100 & No \\
Combination rule: \(CFI \geq .95\) and \(SRMR \leq .08\) & CFI = `0.782 , SRMR = 0.100 & No \\
\end{longtable}

\begin{quote}
\textbf{Measurement model}. A model that allowed the latent variables (representing the measurement models of all the latent variables) to correlate had clearly unacceptable fit to the data: \(\chi ^{2}(148) = 239.689, *p* < 0.001, CFI = 0.782, RMSEA = 0.115, 90%CI(0.087, 0.141)
\).
\end{quote}

Before discussing our options, let's look at what we have just specified and evaluated.

The following code can be used to write a table to a .csv file for use in creating tables for APA style results.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vbls }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{rBlst\_1 =} \StringTok{"My university provides a supportive environment for Black students"}\NormalTok{,}
    \AttributeTok{Blst\_4 =} \StringTok{"My university is unresponsive to the needs of Black students"}\NormalTok{,}
    \AttributeTok{Blst\_6 =} \StringTok{"My university is cold and uncaring toward Black students and race{-}related issues"}\NormalTok{,}
    \AttributeTok{Blst\_2 =} \StringTok{"Anti{-}Black racism is visible in my campus"}\NormalTok{, }\AttributeTok{Blst\_3 =} \StringTok{"Negative attitudes toward persons who are Black are openly expressed in my university"}\NormalTok{,}
    \AttributeTok{Blst\_5 =} \StringTok{"Students who are Black are harassed in my university"}\NormalTok{, }\AttributeTok{cEval\_8 =} \StringTok{"Students felt respected"}\NormalTok{,}
    \AttributeTok{cEval\_9 =} \StringTok{"A sense of community developed among the course participants"}\NormalTok{,}
    \AttributeTok{cEval\_10 =} \StringTok{"The learning environment was inclusive for students with diverse backgrounds and abilities"}\NormalTok{,}
    \AttributeTok{cEval\_11 =} \StringTok{"Elements of universal design were used to increase accessibility"}\NormalTok{,}
    \AttributeTok{cEval\_l2 =} \StringTok{"Course materials were free or no cost to students"}\NormalTok{, }\AttributeTok{cEval\_13 =} \StringTok{"Where applicable, issues were considered from multiple perspectives"}\NormalTok{,}
    \AttributeTok{cEval\_14 =} \StringTok{"There was a discussion about race ethnicity culture and course content"}\NormalTok{,}
    \AttributeTok{cEval\_15 =} \StringTok{"Course content included topics related to social justice"}\NormalTok{,}
    \AttributeTok{cEval\_16 =} \StringTok{"Students and instructors shared personal pronouns"}\NormalTok{, }\AttributeTok{cEval\_17 =} \StringTok{"A land acknowledgement was made"}\NormalTok{,}
    \AttributeTok{cEval\_20 =} \StringTok{"Course content included topics related to social justice"}\NormalTok{,}
    \AttributeTok{iBIPOC\_pr =} \StringTok{"Proportion of Instructors who are BIPOC"}\NormalTok{, }\AttributeTok{cmBlack =} \StringTok{"Proportion of Classmates who are Black"}\NormalTok{)}

\NormalTok{Table }\OtherTok{\textless{}{-}}\NormalTok{ semTable}\SpecialCharTok{::}\FunctionTok{semTable}\NormalTok{(msmt\_fit, }\AttributeTok{columnLabels =} \FunctionTok{c}\NormalTok{(}\AttributeTok{eststars =} \StringTok{"Estimate"}\NormalTok{,}
    \AttributeTok{se =} \StringTok{"SE"}\NormalTok{, }\AttributeTok{p =} \StringTok{"p{-}value"}\NormalTok{), }\AttributeTok{fits =} \FunctionTok{c}\NormalTok{(}\StringTok{"chisq"}\NormalTok{, }\StringTok{"df"}\NormalTok{, }\StringTok{"pvalue"}\NormalTok{, }\StringTok{"cfi"}\NormalTok{,}
    \StringTok{"rmsea"}\NormalTok{, }\StringTok{"rmsea.ci.lower"}\NormalTok{, }\StringTok{"rmsea.ci.upper"}\NormalTok{, }\StringTok{"srmr"}\NormalTok{, }\StringTok{"aic"}\NormalTok{, }\StringTok{"bic"}\NormalTok{),}
    \AttributeTok{varLabels =}\NormalTok{ vbls, }\AttributeTok{file =} \StringTok{"msmt\_fit"}\NormalTok{, }\AttributeTok{type =} \StringTok{"csv"}\NormalTok{, }\AttributeTok{print.results =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_m1 }\OtherTok{\textless{}{-}}\NormalTok{ semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(msmt\_fit, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{, }\AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{,}
    \AttributeTok{sizeMan =} \DecValTok{5}\NormalTok{, }\AttributeTok{node.width =} \DecValTok{1}\NormalTok{, }\AttributeTok{edge.label.cex =} \FloatTok{0.75}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{,}
    \AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{13_HybridModels_files/figure-latex/unnamed-chunk-26-1.pdf} We can further edit the \emph{semPlot::semPath} object to illustrate how all the latent variables are free to covary.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#IMPORTANT:  Must use the node names (take directly from the SemPlot) assigned by SemPlot}
\CommentTok{\#You can change them as the last thing}
\NormalTok{m1\_msmt }\OtherTok{\textless{}{-}}\NormalTok{ semptools}\SpecialCharTok{::}\FunctionTok{layout\_matrix}\NormalTok{(}\AttributeTok{sBl =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{tBI =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{CrE =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),}
                                  \AttributeTok{Clm =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\CommentTok{\#m\_msmt \#can check to see if it is what you thought you did}

\CommentTok{\#tell where you want the indicators to face}
\NormalTok{m1\_point\_to }\OtherTok{\textless{}{-}}\NormalTok{ semptools}\SpecialCharTok{::}\FunctionTok{layout\_matrix}\NormalTok{ (}\AttributeTok{left =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                      \AttributeTok{left =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                      \AttributeTok{up =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),}
                                      \AttributeTok{down =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\CommentTok{\#the next two codes {-}{-} indicator\_order and indicator\_factor are paired together, they specify the order of observed variables for each factor}
\NormalTok{m1\_indicator\_order }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"cmB"}\NormalTok{,}
                     \StringTok{"iBI"}\NormalTok{,}
                     \StringTok{"cE\_8"}\NormalTok{,}\StringTok{"cE\_9"}\NormalTok{,}\StringTok{"cE\_10"}\NormalTok{,}\StringTok{"cE\_11"}\NormalTok{,}\StringTok{"cE\_12"}\NormalTok{,}\StringTok{"cE\_13"}\NormalTok{,}\StringTok{"cE\_14"}\NormalTok{,}\StringTok{"cE\_15"}\NormalTok{,}\StringTok{"cE\_2"}\NormalTok{,}\StringTok{"cE\_16"}\NormalTok{,}\StringTok{"cE\_17"}\NormalTok{,}
                    \StringTok{"rB\_"}\NormalTok{, }\StringTok{"B\_4"}\NormalTok{, }\StringTok{"B\_6"}\NormalTok{, }\StringTok{"B\_2"}\NormalTok{, }\StringTok{"B\_3"}\NormalTok{, }\StringTok{"B\_5"}\NormalTok{)}
\NormalTok{m1\_indicator\_factor }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"sBl"}\NormalTok{,}
                      \StringTok{"tBI"}\NormalTok{,}
                      \StringTok{"CrE"}\NormalTok{,}\StringTok{"CrE"}\NormalTok{,}\StringTok{"CrE"}\NormalTok{,}\StringTok{"CrE"}\NormalTok{,}\StringTok{"CrE"}\NormalTok{,}\StringTok{"CrE"}\NormalTok{,}\StringTok{"CrE"}\NormalTok{,}\StringTok{"CrE"}\NormalTok{,}\StringTok{"CrE"}\NormalTok{,}\StringTok{"CrE"}\NormalTok{,}\StringTok{"CrE"}\NormalTok{,}
                      \StringTok{"Clm"}\NormalTok{, }\StringTok{"Clm"}\NormalTok{, }\StringTok{"Clm"}\NormalTok{, }\StringTok{"Clm"}\NormalTok{, }\StringTok{"Clm"}\NormalTok{, }\StringTok{"Clm"}\NormalTok{)}
\CommentTok{\#next set of code pushes the indicator variables away from the factor}
\NormalTok{m1\_indicator\_push }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{sBl =} \FloatTok{2.5}\NormalTok{, }\CommentTok{\#pushing the 1{-}item indicators only a little way away}
                    \AttributeTok{tBI =} \FloatTok{2.5}\NormalTok{,}
                    \AttributeTok{CrE =} \DecValTok{2}\NormalTok{,}\DecValTok{5}\NormalTok{, }\CommentTok{\#pushing the multi{-}item indicators further away)}
                    \AttributeTok{Clm =} \FloatTok{2.5}\NormalTok{)}
\NormalTok{m1\_indicator\_spread }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{CrE =} \DecValTok{2}\NormalTok{, }\CommentTok{\#spreading the boxes away from each other}
                    \AttributeTok{Clm =} \DecValTok{2}\NormalTok{)}

\NormalTok{msmtplot1 }\OtherTok{\textless{}{-}}\NormalTok{ semptools}\SpecialCharTok{::}\FunctionTok{set\_sem\_layout}\NormalTok{(plot\_m1,}
                                \AttributeTok{indicator\_order =}\NormalTok{ m1\_indicator\_order,}
                                \AttributeTok{indicator\_factor =}\NormalTok{ m1\_indicator\_factor,}
                                \AttributeTok{factor\_layout =}\NormalTok{ m1\_msmt,}
                                \AttributeTok{factor\_point\_to =}\NormalTok{ m1\_point\_to,}
                                \AttributeTok{indicator\_push =}\NormalTok{ m1\_indicator\_push,}
                                \AttributeTok{indicator\_spread =}\NormalTok{ m1\_indicator\_spread)}
\FunctionTok{plot}\NormalTok{(msmtplot1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{13_HybridModels_files/figure-latex/unnamed-chunk-27-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#changing node labels}
\NormalTok{msmtplot1b }\OtherTok{\textless{}{-}}\NormalTok{ semptools}\SpecialCharTok{::}\FunctionTok{change\_node\_label}\NormalTok{(msmtplot1,}
                                   \FunctionTok{c}\NormalTok{(}\AttributeTok{sBl =} \StringTok{"stntBlack"}\NormalTok{,}
                                     \AttributeTok{tBI =} \StringTok{"tchBIPOC"}\NormalTok{,}
                                     \AttributeTok{CrE =} \StringTok{"Evals"}\NormalTok{,}
                                     \AttributeTok{Clm =} \StringTok{"Climate"}\NormalTok{),}
                                   \AttributeTok{label.cex =} \FloatTok{1.1}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(msmtplot1b)}
\end{Highlighting}
\end{Shaded}

\includegraphics{13_HybridModels_files/figure-latex/unnamed-chunk-27-2.pdf}

As we can see in the figure, our measurement model has allowed all the latent variables to correlate. Unfortunately, the fit is extremely sub par. In other words: this fit stinks. For this lesson, I will move onto testing a structural model. However, there is hope. Researchers might consider parceling. This is explained more fully in \href{https://lhbikos.github.io/ReC_MultivModel/MeasMod.html\#parceling}{ReCentering Psych Stats: Multivariate Modeling}.

\hypertarget{the-structural-model-specification-and-evaluation}{%
\section{The Structural Model: Specification and Evaluation}\label{the-structural-model-specification-and-evaluation}}

Here's a quick reminder of the hypothesized model. The model is \emph{hybrid} because it include measurement models (the CFAs for the two Course Evaluation and Perceptions of Campus Climate for Black Students scales), plus the hypothesized paths.

\begin{figure}
\centering
\includegraphics{images/Hybrid/parallel_model.png}
\caption{Image of the proposed statistical model}
\end{figure}

Having just confirmed that our measurement model is adequate, we now replace the covariances between latent variables with the paths (directional) and covariances we hypothesize. These paths and covariances are \emph{soft} hypotheses. That is, we are ``freeing'' them to relate. The \emph{hard} hypotheses are where no path/covariance exists and the relationship between these variables is ``fixed'' to zero. This is directly related to degrees of freedom and the identification status (just-identified, over-identified, underidentified) of the model.

\hypertarget{model-identification}{%
\subsection{Model Identification}\label{model-identification}}

There are two necessary elements for identifying any type of SEM \citep{kline_principles_2016}, these include

\begin{itemize}
\tightlist
\item
  having degrees of freedom greater-than-or-equal to zero (\(df_{M}\geq 0\)), and
\item
  assigning a scale to every latent variable (including disturances or error terms).

  \begin{itemize}
  \tightlist
  \item
    We covered this criterion in the lessons on CFA.
  \end{itemize}
\end{itemize}

In the case of the specification of standard CFA models (i.e., the models we use in the psychometric evaluation of measures and surveys), the extent of our ``your model must be identified'' conversation stopped at:

\begin{itemize}
\tightlist
\item
  unidimensional models need to have a minimum of 3 items/indicators (manifest variables) per factor/scale (latent variable)\\
\item
  multidimensional models need to have a minimum of 2 items/indicators (manifest variables) per factor/scale (latent variable)\\
\item
  second order factors need three first-order factors in order to be identified\\
\item
  nonstandard models include error variances that are free to correlate -- they need closer scrutiny with regard to identification status
\end{itemize}

Model identification, though, is more complicated than that. Let's take a closer look at model identification in hybrid models as it relates to the \(df_{M}\geq 0\) criteria.

\textbf{Underidentified or undetermined} models have fewer observations (knowns) than free model parameters (unknowns). This results in negative degrees of freedom (\(df_{M}\leq 0\)). This means that it is impossible to find a unique set of estimates. The classic example for this is: \(a + b = 6\) where there are an infinite number of solutions.

\textbf{Just-identified or just-determined} models have an equal number of observations (knowns) as free parameters (unknowns). This results in zero degrees of freedom (\(df_{M}= 0\)). Just-identified scenarios will result in a unique solution. The classic example for this is

\[a + b = 6\] \[2a + b = 10\] The unique solution is \emph{a} = 4, \emph{b} = 2.

\textbf{Over-identified or overdetermined} models have more observations (knowns) than free parameters (unknowns). This results in positive degrees of freedom (\(df_{M}> 0\)). In this circumstance, there is no single solution, but one can be calculated when a statistical criterion is applied. For exampe, there is no single solution that satisfies all three of these formulas:

\[a + b = 6\] \[2a + b = 10\] \[3a + b = 12\]

When we add this instruction ``Find value of \emph{a} and \emph{b} that yield total scores such that the sum of squared differences between the observations (6, 10, 12) and these total scores is as small as possible.'' Curious about the answer? An excellent description is found in Kline \citeyearpar{kline_principles_2016}.

Model identification is an incredibly complex topic. It is possible to have theoretically identified models and yet they are statistically unidentified and then the researcher must hunt for the source of the problem. For this lesson, I will simply walk through the steps that are commonly used in determining the identification status of a structural model.

\hypertarget{model-identification-for-the-overall-sem}{%
\subsubsection{Model identification for the overall SEM}\label{model-identification-for-the-overall-sem}}

In order to be evaluated, structural models need to be \emph{just identifed} (\(df_M = 0\)) or \emph{overidentified} (\(df_M > 0\)). Computer programs are not (yet) good at estimating identification status because it is based on symbolism and not numbers. Therefore, we researchers must do the mental math to ensure that our \emph{knowns} (measured/observed variables) are equal (just-identified) or greater than (overidentified) our \emph{unknowns} (parameters that will be estimated).

We calculate the \emph{knowns} by identifying the number of measured variables (\emph{n}) and popping that number into this equation: \(\frac{n(n+1)}{2}\). \emph{Unknowns} are counted and include: measurement regression paths, structural regression paths, error covariances, residual error variances, and covariances.

Lets calculate this for our model.

\begin{itemize}
\tightlist
\item
  \textbf{Knowns}: There are 19 observed variables, so we have 190(19(19+1)/2) pieces of information from which to drive the parameters of the model.
\item
  \textbf{Unknowns}: We must estimate the following parameters

  \begin{itemize}
  \tightlist
  \item
    17 measurement regression paths (we don't count the marker variables or the single-indicator items)
  \item
    5 structural regression paths
  \item
    17 error covariances (1 for each indicator variable)
  \item
    2 residual error variances (any endogenous latent variable has one of these)
  \item
    0 covariances
  \item
    We have a total of: 41 unknowns
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{19}\SpecialCharTok{*}\NormalTok{(}\DecValTok{19}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{/}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 190
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{17} \SpecialCharTok{+} \DecValTok{5} \SpecialCharTok{+} \DecValTok{17} \SpecialCharTok{+} \DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 41
\end{verbatim}

Our overall model is overidentified with \(df_M = 41\). We know this because subtracted the unknowns (41) from the knowns (190). If we calculated this correctly, 41 will be the degrees of freedom associated with the chi-square test.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{190{-}41}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 149
\end{verbatim}

\hypertarget{model-identification-for-the-structural-portion-of-the-model}{%
\subsubsection{Model identification for the structural portion of the model}\label{model-identification-for-the-structural-portion-of-the-model}}

It is possible to have an overidentified model but still be underidentified in the structural portion. In order to be evaluated, structural models need to be \emph{just identifed} (\(df_M = 0\)) or \emph{overidentified} (\(df_M > 0\)). Before continuing, it is essential to understand that the structural part is (generally) the relations between the latent variables (although in some models there could be observed variables). In our case, our structural model consists only of four latent variables.

\begin{figure}
\centering
\includegraphics{images/Hybrid/structural_model.png}
\caption{A red circle identifies the structural portion of our hybrid model}
\end{figure}

Especially for the structural portion of the model, statistical packages are not (yet) good at estimating identification status because it is based on symbolism and not numbers. Therefore, we researchers must make the calculations to ensure that our \emph{knowns} are equal (just-identified) or greater than (overidentified) our \emph{unknowns}.

\begin{itemize}
\item
  \textbf{Knowns}: \(\frac{k(k+1)}{2}\) where \emph{k} is the number of \emph{constructs} (humoR: \emph{k}onstructs?)in the model. In our case, we have four constructs: 4(4+1)/2 = 10
\item
  \textbf{Unknowns}: are calculated with the following

  \begin{itemize}
  \tightlist
  \item
    Exogenous (predictor) variables (1 variance estimated for each): we have 2 (stntBlack, tchBIPOC)
  \item
    Endogenous (predicted) variables (1 disturbance variance for each): we have 2 (Evals, Climate)\\
  \item
    Correlations between variables (1 covariance for each pairing): we have 0 (the potential covariance between stntBlack and tchBIPOC is not specified)\\
  \item
    Regression paths (arrows linking exogenous variables to endogenous variables): we have 5
  \end{itemize}
\end{itemize}

With 10 knowns and 9 unknowns, we have 1 degree of freedom in the structural portion of the model. This is an overidentified model and we are all set to evaluate it.

As a reminder, the \emph{measurement} model will always have better fit because it is a fully saturated (i.e., covariances between all latent variables), just-identified, \(df_M = 0\), structure will best replicate the sample covariance matrix. Our hope is that replacing covariances (double-headed arrows) with unidirectional paths and constraining some relations to be 0.0 will not result in a substantial deterioration of fit. That is to say, we hope that our more parsimonious model explains (or captures) the pattern of relations happening in the variance/covariance matrix.

\hypertarget{specifying-and-evaluating-the-structural-model}{%
\subsection{Specifying and Evaluating the Structural Model}\label{specifying-and-evaluating-the-structural-model}}

In the script below we retain the measurement definitions for the latent variables. Our structural paths, though, reflect our hypotheses. The topic of \href{https://lhbikos.github.io/ReC_MultivariateModeling/CompMed.html}{parallel mediation} is addressed in the context of path analysis in the \href{https://lhbikos.github.io/ReC_MultivariateModeling/}{Multivariate Modeling} volume. Describing it is beyond the scope of this chapter.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{struct1 }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{} }
\StringTok{  \#latent variable definitions for the factors with 3 or more indicators}
\StringTok{   Climate =\textasciitilde{} rBlst\_1 + Blst\_4 + Blst\_6 + Blst\_2 + Blst\_3 + Blst\_5}
\StringTok{   CourseEval =\textasciitilde{} cEval\_8 + cEval\_9 + cEval\_10 + cEval\_11 + cEval\_12 + cEval\_13 + cEval\_14 + cEval\_15 + cEval\_20 + cEval\_16 + cEval\_17}
\StringTok{   }
\StringTok{  \#latent variable definitions for the factors with 1 indicator; we set variance of the observed variable to be 0.00; this says that the LV will account for all of the variance in the observed variable}
\StringTok{   tBIPOC =\textasciitilde{} iBIPOC\_pr \#for the factor "t" is teacher; for variable "i" is instructor}
\StringTok{   sBlack =\textasciitilde{} cmBlack \#for factor "s" is student; for variable "cm" is classmates}
\StringTok{  }
\StringTok{   iBIPOC\_pr \textasciitilde{}\textasciitilde{} 0*iBIPOC\_pr \#this specifies the error variance of the single observed variable to be 0.00}
\StringTok{   cmBlack \textasciitilde{}\textasciitilde{} 0*cmBlack}
\StringTok{   }
\StringTok{   \#structural paths}
\StringTok{   Climate \textasciitilde{} b*CourseEval + c\_p1*tBIPOC + c\_p2*sBlack}
\StringTok{   CourseEval \textasciitilde{} a1*tBIPOC + a2*sBlack}
\StringTok{   }
\StringTok{   \#script that produces information about indirect, direct, and total effects}
\StringTok{   indirect1 := a1 * b}
\StringTok{   indirect2 := a2 * b}
\StringTok{   contrast := indirect1 {-} indirect2}
\StringTok{   total\_indirects := indirect1 + indirect2}
\StringTok{   total\_c := c\_p1 + c\_p2 + (indirect1) + (indirect2)}
\StringTok{   direct1 := c\_p1}
\StringTok{   direct2 := c\_p2}
\StringTok{   \textquotesingle{}}
\end{Highlighting}
\end{Shaded}

Next we use the \emph{lavaan::sem()} function to run the script.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# note change in script from cfa to sem}
\NormalTok{struct1\_fit }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{sem}\NormalTok{(struct1, }\AttributeTok{data =}\NormalTok{ Model\_df, }\AttributeTok{missing =} \StringTok{"fiml"}\NormalTok{,}
    \AttributeTok{orthogonal =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{s1fitsum }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(struct1\_fit, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{rsquare =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{s1fitsum}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 67 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        60

  Number of observations                            47
  Number of missing patterns                         5

Model Test User Model:
                                                      
  Test statistic                               239.897
  Degrees of freedom                               149
  P-value (Chi-square)                           0.000

Model Test Baseline Model:

  Test statistic                               590.701
  Degrees of freedom                               171
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.783
  Tucker-Lewis Index (TLI)                       0.751
                                                      
  Robust Comparative Fit Index (CFI)             0.776
  Robust Tucker-Lewis Index (TLI)                0.742

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -1282.512
  Loglikelihood unrestricted model (H1)      -1162.563
                                                      
  Akaike (AIC)                                2685.024
  Bayesian (BIC)                              2796.032
  Sample-size adjusted Bayesian (SABIC)       2607.850

Root Mean Square Error of Approximation:

  RMSEA                                          0.114
  90 Percent confidence interval - lower         0.087
  90 Percent confidence interval - upper         0.140
  P-value H_0: RMSEA <= 0.050                    0.000
  P-value H_0: RMSEA >= 0.080                    0.977
                                                      
  Robust RMSEA                                   0.117
  90 Percent confidence interval - lower         0.089
  90 Percent confidence interval - upper         0.143
  P-value H_0: Robust RMSEA <= 0.050             0.000
  P-value H_0: Robust RMSEA >= 0.080             0.983

Standardized Root Mean Square Residual:

  SRMR                                           0.100

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Observed
  Observed information based on                Hessian

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Climate =~                                                            
    rBlst_1           1.000                               0.854    0.540
    Blst_4            1.242    0.322    3.856    0.000    1.061    0.861
    Blst_6            1.343    0.347    3.865    0.000    1.147    0.868
    Blst_2            1.267    0.368    3.441    0.001    1.082    0.689
    Blst_3            0.935    0.289    3.232    0.001    0.799    0.625
    Blst_5            1.247    0.319    3.909    0.000    1.065    0.896
  CourseEval =~                                                         
    cEval_8           1.000                               0.509    0.618
    cEval_9           1.067    0.339    3.153    0.002    0.543    0.519
    cEval_10          1.352    0.331    4.086    0.000    0.688    0.710
    cEval_11          1.503    0.404    3.719    0.000    0.764    0.669
    cEval_12          1.184    0.409    2.895    0.004    0.602    0.480
    cEval_13          1.476    0.361    4.089    0.000    0.751    0.722
    cEval_14          1.870    0.505    3.701    0.000    0.951    0.705
    cEval_15          1.824    0.461    3.961    0.000    0.928    0.789
    cEval_20          1.413    0.428    3.301    0.001    0.719    0.638
    cEval_16          1.549    0.457    3.391    0.001    0.788    0.614
    cEval_17          1.835    0.539    3.403    0.001    0.934    0.655
  tBIPOC =~                                                             
    iBIPOC_pr         1.000                               0.396    1.000
  sBlack =~                                                             
    cmBlack           1.000                               7.560    1.000

Regressions:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Climate ~                                                             
    CorsEvl    (b)   -0.421    0.295   -1.428    0.153   -0.251   -0.251
    tBIPOC  (c_p1)   -0.369    0.339   -1.087    0.277   -0.171   -0.171
    sBlack  (c_p2)   -0.025    0.017   -1.466    0.143   -0.222   -0.222
  CourseEval ~                                                          
    tBIPOC    (a1)    0.353    0.204    1.732    0.083    0.275    0.275
    sBlack    (a2)    0.008    0.010    0.773    0.439    0.117    0.117

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  tBIPOC ~~                                                             
    sBlack            0.000                               0.000    0.000

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .rBlst_1           3.402    0.235   14.491    0.000    3.402    2.149
   .Blst_4            2.346    0.181   12.992    0.000    2.346    1.903
   .Blst_6            2.259    0.193   11.677    0.000    2.259    1.710
   .Blst_2            2.672    0.231   11.583    0.000    2.672    1.701
   .Blst_3            2.106    0.186   11.304    0.000    2.106    1.649
   .Blst_5            2.000    0.173   11.534    0.000    2.000    1.682
   .cEval_8           4.298    0.120   35.775    0.000    4.298    5.218
   .cEval_9           3.809    0.152   24.983    0.000    3.809    3.644
   .cEval_10          4.000    0.141   28.312    0.000    4.000    4.130
   .cEval_11          3.404    0.167   20.413    0.000    3.404    2.978
   .cEval_12          3.851    0.183   21.037    0.000    3.851    3.068
   .cEval_13          3.830    0.152   25.267    0.000    3.830    3.686
   .cEval_14          3.723    0.197   18.916    0.000    3.723    2.759
   .cEval_15          3.255    0.172   18.962    0.000    3.255    2.766
   .cEval_20          3.553    0.164   21.610    0.000    3.553    3.152
   .cEval_16          3.128    0.187   16.713    0.000    3.128    2.438
   .cEval_17          2.704    0.209   12.911    0.000    2.704    1.897
   .iBIPOC_pr         0.330    0.059    5.590    0.000    0.330    0.832
   .cmBlack           6.660    1.103    6.039    0.000    6.660    0.881

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .iBIPOC_pr         0.000                               0.000    0.000
   .cmBlack           0.000                               0.000    0.000
   .rBlst_1           1.776    0.388    4.579    0.000    1.776    0.709
   .Blst_4            0.394    0.111    3.544    0.000    0.394    0.259
   .Blst_6            0.430    0.126    3.425    0.001    0.430    0.246
   .Blst_2            1.298    0.295    4.393    0.000    1.298    0.526
   .Blst_3            0.994    0.217    4.577    0.000    0.994    0.609
   .Blst_5            0.279    0.094    2.985    0.003    0.279    0.198
   .cEval_8           0.420    0.098    4.295    0.000    0.420    0.618
   .cEval_9           0.797    0.176    4.542    0.000    0.797    0.730
   .cEval_10          0.465    0.119    3.920    0.000    0.465    0.495
   .cEval_11          0.723    0.165    4.375    0.000    0.723    0.553
   .cEval_12          1.212    0.260    4.666    0.000    1.212    0.770
   .cEval_13          0.516    0.127    4.062    0.000    0.516    0.478
   .cEval_14          0.916    0.221    4.139    0.000    0.916    0.503
   .cEval_15          0.524    0.145    3.608    0.000    0.524    0.378
   .cEval_20          0.754    0.178    4.229    0.000    0.754    0.593
   .cEval_16          1.025    0.229    4.483    0.000    1.025    0.623
   .cEval_17          1.161    0.274    4.230    0.000    1.161    0.571
   .Climate           0.600    0.316    1.899    0.058    0.822    0.822
   .CourseEval        0.236    0.109    2.171    0.030    0.910    0.910
    tBIPOC            0.157    0.033    4.755    0.000    1.000    1.000
    sBlack           57.161   11.791    4.848    0.000    1.000    1.000

R-Square:
                   Estimate
    iBIPOC_pr         1.000
    cmBlack           1.000
    rBlst_1           0.291
    Blst_4            0.741
    Blst_6            0.754
    Blst_2            0.474
    Blst_3            0.391
    Blst_5            0.802
    cEval_8           0.382
    cEval_9           0.270
    cEval_10          0.505
    cEval_11          0.447
    cEval_12          0.230
    cEval_13          0.522
    cEval_14          0.497
    cEval_15          0.622
    cEval_20          0.407
    cEval_16          0.377
    cEval_17          0.429
    Climate           0.178
    CourseEval        0.090

Defined Parameters:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
    indirect1        -0.149    0.129   -1.153    0.249   -0.069   -0.069
    indirect2        -0.003    0.005   -0.690    0.490   -0.029   -0.029
    contrast         -0.145    0.127   -1.144    0.253   -0.040   -0.040
    total_indircts   -0.152    0.131   -1.160    0.246   -0.098   -0.098
    total_c          -0.546    0.348   -1.570    0.116   -0.491   -0.491
    direct1          -0.369    0.339   -1.087    0.277   -0.171   -0.171
    direct2          -0.025    0.017   -1.466    0.143   -0.222   -0.222
\end{verbatim}

Here's how I might write up the results of the overall fit.

\begin{quote}
\textbf{Hybrid model}. A test of the hypothesized structural model had less than acceptable fit to the data: \(\chi ^{2}(171) = 590.70, p < 0.001, CFI = 0.783, RMSEA = 0.0.114 (90%CI [0.087, 0.140]), SRMR = 1.000
\).
\end{quote}

Plotting what we did is helpful for a conceptual understanding and to check our work. Let's plan ahead to update the default result of \emph{semPaths::semPlot} to reflect what we did. If we've made an error, this will show up in the product. Producing the map starts with understanding the coded (row/column) location of our variables.

\begin{figure}
\centering
\includegraphics{images/Hybrid/structural_map.png}
\caption{Image of the grid used for the semPLot}
\end{figure}

Let's start with the \emph{semPlot::semPaths} default result.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OtherTok{\textless{}{-}}\NormalTok{ semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(struct1\_fit, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{, }\AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{,}
    \AttributeTok{sizeMan =} \DecValTok{5}\NormalTok{, }\AttributeTok{node.width =} \DecValTok{1}\NormalTok{, }\AttributeTok{edge.label.cex =} \FloatTok{0.75}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{,}
    \AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{13_HybridModels_files/figure-latex/unnamed-chunk-32-1.pdf}

The code below included steps in creating a custom plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OtherTok{\textless{}{-}}\NormalTok{ semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{ (struct1\_fit, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{, }\AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{, }\AttributeTok{sizeMan =} \DecValTok{5}\NormalTok{, }\AttributeTok{node.width =} \DecValTok{1}\NormalTok{, }\AttributeTok{edge.label.cex =}\NormalTok{ .}\DecValTok{75}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{, }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{13_HybridModels_files/figure-latex/unnamed-chunk-33-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#I used this code to get a plot without the results printed on the paths}
\CommentTok{\#p \textless{}{-} semPlot::semPaths (struct1\_fit, what = "mod", sizeMan = 5, node.width = 1, edge.label.cex = .75, style = "lisrel", mar = c(5,5,5,5))}

\CommentTok{\#IMPORTANT:  Must use the node names (take directly from the SemPlot) assigned by SemPlot}
\CommentTok{\#You can change them as the last thing}
\NormalTok{m\_sem }\OtherTok{\textless{}{-}}\NormalTok{ semptools}\SpecialCharTok{::}\FunctionTok{layout\_matrix}\NormalTok{(}\AttributeTok{sBl =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{tBI =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{CrE =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{),}
                                  \AttributeTok{Clm =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{))}

\CommentTok{\#m\_sem \#check to see if they are where you thougth they would be; NA will be used as placeholders}

\CommentTok{\#tell where you want the indicators to face}
\NormalTok{point\_to }\OtherTok{\textless{}{-}}\NormalTok{ semptools}\SpecialCharTok{::}\FunctionTok{layout\_matrix}\NormalTok{ (}\AttributeTok{left =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                      \AttributeTok{left =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                      \AttributeTok{down =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{),}
                                      \AttributeTok{right =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\CommentTok{\#the next two codes {-}{-} indicator\_order and indicator\_factor are paired together, they specify the order of observed variables for each factor}
\NormalTok{indicator\_order }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"cmB"}\NormalTok{,}
                     \StringTok{"iBI"}\NormalTok{,}
                     \StringTok{"cE\_8"}\NormalTok{,}\StringTok{"cE\_9"}\NormalTok{,}\StringTok{"cE\_10"}\NormalTok{,}\StringTok{"cE\_11"}\NormalTok{,}\StringTok{"cE\_12"}\NormalTok{,}\StringTok{"cE\_13"}\NormalTok{,}\StringTok{"cE\_14"}\NormalTok{,}\StringTok{"cE\_15"}\NormalTok{,}\StringTok{"cE\_2"}\NormalTok{,}\StringTok{"cE\_16"}\NormalTok{,}\StringTok{"cE\_17"}\NormalTok{,}
                    \StringTok{"rB\_"}\NormalTok{, }\StringTok{"B\_4"}\NormalTok{, }\StringTok{"B\_6"}\NormalTok{, }\StringTok{"B\_2"}\NormalTok{, }\StringTok{"B\_3"}\NormalTok{, }\StringTok{"B\_5"}\NormalTok{)}
\NormalTok{indicator\_factor }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"sBl"}\NormalTok{,}
                      \StringTok{"tBI"}\NormalTok{,}
                      \StringTok{"CrE"}\NormalTok{, }\StringTok{"CrE"}\NormalTok{, }\StringTok{"CrE"}\NormalTok{, }\StringTok{"CrE"}\NormalTok{, }\StringTok{"CrE"}\NormalTok{, }\StringTok{"CrE"}\NormalTok{, }\StringTok{"CrE"}\NormalTok{, }\StringTok{"CrE"}\NormalTok{, }\StringTok{"CrE"}\NormalTok{, }\StringTok{"CrE"}\NormalTok{, }\StringTok{"CrE"}\NormalTok{,}
                      \StringTok{"Clm"}\NormalTok{, }\StringTok{"Clm"}\NormalTok{, }\StringTok{"Clm"}\NormalTok{, }\StringTok{"Clm"}\NormalTok{, }\StringTok{"Clm"}\NormalTok{, }\StringTok{"Clm"}\NormalTok{)}
\CommentTok{\#next set of code pushes the indicator variables away from the factor}
\NormalTok{indicator\_push }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{sBl =} \FloatTok{1.5}\NormalTok{, }\CommentTok{\#pushing the 1{-}item indicators only a little way away}
                    \AttributeTok{tBI =} \FloatTok{1.5}\NormalTok{,}
                    \AttributeTok{CrE =} \DecValTok{2}\NormalTok{,}\DecValTok{5}\NormalTok{, }\CommentTok{\#pushing the multi{-}item indicators further away)}
                    \AttributeTok{Clm =} \FloatTok{2.5}\NormalTok{)}
\NormalTok{indicator\_spread }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{CrE =} \DecValTok{2}\NormalTok{, }\CommentTok{\#spreading the boxes away from each other}
                    \AttributeTok{Clm =} \DecValTok{2}\NormalTok{)}

\NormalTok{p2 }\OtherTok{\textless{}{-}}\NormalTok{ semptools}\SpecialCharTok{::}\FunctionTok{set\_sem\_layout}\NormalTok{(p,}
                                \AttributeTok{indicator\_order =}\NormalTok{ indicator\_order,}
                                \AttributeTok{indicator\_factor =}\NormalTok{ indicator\_factor,}
                                \AttributeTok{factor\_layout =}\NormalTok{ m\_sem,}
                                \AttributeTok{factor\_point\_to =}\NormalTok{ point\_to,}
                                \AttributeTok{indicator\_push =}\NormalTok{ indicator\_push,}
                                \AttributeTok{indicator\_spread =}\NormalTok{ indicator\_spread)}
\FunctionTok{plot}\NormalTok{(p2)}
\end{Highlighting}
\end{Shaded}

\includegraphics{13_HybridModels_files/figure-latex/unnamed-chunk-33-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#changing node labels}
\NormalTok{p3 }\OtherTok{\textless{}{-}}\NormalTok{ semptools}\SpecialCharTok{::}\FunctionTok{change\_node\_label}\NormalTok{(p2,}
                                   \FunctionTok{c}\NormalTok{(}\AttributeTok{sBl =} \StringTok{"stntBlack"}\NormalTok{,}
                                     \AttributeTok{tBI =} \StringTok{"tchBIPOC"}\NormalTok{,}
                                     \AttributeTok{CrE =} \StringTok{"Evals"}\NormalTok{,}
                                     \AttributeTok{Clm =} \StringTok{"Climate"}\NormalTok{),}
                                   \AttributeTok{label.cex =} \FloatTok{1.1}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(p3)}
\end{Highlighting}
\end{Shaded}

\includegraphics{13_HybridModels_files/figure-latex/unnamed-chunk-33-3.pdf}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 0\tabcolsep) * \real{1.0000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Model Coefficients Assessing M1 and M2 as Parallel Mediators Between X and Y
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\end{longtable}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0805}}
  >{\centering\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0345}}
  >{\centering\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0460}}
  >{\centering\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0345}}
  >{\centering\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0460}}
  >{\centering\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.2989}}
  >{\centering\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0345}}
  >{\centering\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1379}}
  >{\centering\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1609}}
  >{\centering\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1264}}@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
IV & & M & & DV & \(B\) for \emph{a} and \emph{b} paths & & \(B\) & \(SE\) & \(p\) \\
tBIPOC & --\textgreater{} & Evals & --\textgreater{} & Climate & (0.353) X (-0.421) & = & -0.149 & 0.129 & 0.249 \\
cmBlack & --\textgreater{} & Evals & --\textgreater{} & Climate & (0.008) X (-0.421) & = & -0.003 & 0.005 & 0.490 \\
\end{longtable}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.6022}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1290}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1505}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1183}}@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
& \(B\) & \(SE\) & \(p\) \\
Total indirect effect & -0.152 & 0.131 & 0.246 \\
Direct effect of tBIPOC on Climate (c'1 path) & -0.369 & 0.339 & 0.277 \\
Direct effect of cmBlack on Climate (c'2 path) & -0.025 & 0.017 & 0.143 \\
\end{longtable}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 0\tabcolsep) * \real{1.0000}}@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\end{longtable}

\emph{Note}. X =definition; M1 = definition; M2 = definition; Y = definition. The significance of the indirect effects was calculated with bias-corrected confidence intervals (.95) bootstrap analysis.

\hypertarget{apa-style-write-up-of-the-results-1}{%
\subsection{APA Style Write-up of the Results}\label{apa-style-write-up-of-the-results-1}}

\hypertarget{preliminary-analyses-1}{%
\subsubsection{Preliminary Analyses}\label{preliminary-analyses-1}}

\begin{quote}
We began by creating a dataset that included only the variables of interest. Our initial inspection of the data indicated that 70 attempted the survey. The proportion of missingness in the responses ranged from 0 to 90. Across the dataframe there was 26\% of missingness across the cells. Approximately 60\% of the cases had nonmissing data. The predominant pattern of missingness included individuals opening the survey without completing any of the items. Beyond that, our inspection of a missingness map indicated a haphazard pattern of missingness \citep{enders_applied_2010}.
\end{quote}

\begin{quote}
Using Parent's \emph{available item analysis} {[}AIA; -\citet{parent_handling_2013}{]} as a guide, we deleted all cases where there was greater than 20\% of data missing. We reinspected the missingness of the dataset with 47 cases. Across the dataframe there was less than 1\% of missingness across the cells. Approximately 89\% of the cases had nonmissing data.
\end{quote}

\begin{quote}
Regarding the distributional characteristics of the data, skew and kurtosis values of the variables fell below the values of 3 (skew) and 8 to 20 (kurtosis) that Kline suggests are concerning \citeyearpar{kline_principles_2016}. We evaluated multivariate normality with the Mahalanobis distance test. Specifically, we used the \emph{outlier()} function in the \emph{psych} package and included all continuous variables in the calculation. Our visual inspection of the Q-Q plot suggested that the plotted line strayed from the straight line as the quantiles increased. Additionally, we appended the Mahalanobis distance scores as a variable to the data. Analyzing this variable, we found that no exceed three standard deviations beyond the median. Thus, with no outliers, we assumed multivariate normality and proceeded with the data analysis. Means, standard deviations, and a correlation matrix are found in Table 1.
\end{quote}

\hypertarget{primary-analyses}{%
\subsubsection{Primary Analyses}\label{primary-analyses}}

\begin{quote}
A measurement model that allowed all latent variables to correlate had clearly unacceptable fit to the data: \(\chi ^{2}(148) = 239.689, *p* < 0.001, CFI = 0.782, RMSEA = 0.115, 90%CI(0.087, 0.141), SRMR = 0.100
\). A test of the hypothesized structural model also had less than acceptable fit to the data: \(\chi ^{2}(171) = 590.70, p < 0.001, CFI = 0.783, RMSEA = 0.0.114 (90%CI [0.087, 0.140]), SRMR = 0.100
\). Similarly, there were no significant direct or indirect effects. Results are found in Table 2 and represented in Figure 1.
\end{quote}

\hypertarget{practice-problems-11}{%
\section{Practice Problems}\label{practice-problems-11}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. With each of these options I encourage you to:

\hypertarget{problem-1-download-a-fresh-sample}{%
\subsection{Problem \#1: Download a fresh sample}\label{problem-1-download-a-fresh-sample}}

As an open survey the \href{https://spupsych.az1.qualtrics.com/jfe/form/SV_b2cClqAlLGQ6nLU}{Rate-a-Recent-Course: A ReCentering Psych Stats Exercise} has the possibility of always being updated. There has been more data added to the survey since this lesson was rendered and/or I lectured it. If not, consider taking the survey and rating another course. Rerun the analyses with the updated data. Has it changed since the lesson was last lectured/updated?

\hypertarget{problem-2-swap-one-or-more-of-the-variables}{%
\subsection{Problem \#2: Swap one or more of the variables}\label{problem-2-swap-one-or-more-of-the-variables}}

The Rate-a-Recent-Course survey is composed of a number of variables. Select a different constellation of variables for the hybrid analysis.

\hypertarget{problem-3-try-something-entirely-new.-8}{%
\subsection{Problem \#3: Try something entirely new.}\label{problem-3-try-something-entirely-new.-8}}

Conduct a hybrid analysis using data for which you have permission and access (e.g., IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from the simulations found at the end of this OER).

Regardless of your choic(es) complete all the elements listed in the grading rubric.

\hypertarget{grading-rubric-8}{%
\subsection{Grading Rubric}\label{grading-rubric-8}}

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5493}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1972}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Possible
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Earned
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1. Describe and draw the research model you will evaluate. & 3 & \\
2. Import the data and format the variables in the model. & 5 & \_\_\_\_\_ \\
3. Analyze and manage missing data. & 5 & \_\_\_\_\_ \\
4. Assess the distributional characteristics of the data. & 5 & \_\_\_\_\_ \\
5. Conduct appropriate preliminary analyses (\emph{M}s, \emph{SD}s, \emph{r}-matrix). & 5 & \_\_\_\_\_ \\
6. Specify and evaluate a measurement model. & 5 & \_\_\_\_\_ \\
7. Specify and evaluate a structural model; tweak as necessary. & 5 & \_\_\_\_\_ \\
8. APA style results with table(s) and figure. & 5 & \_\_\_\_\_ \\
9. Explanation to grader. & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 43 & \_\_\_\_\_ \\
\end{longtable}

\hypertarget{homeworked-example-8}{%
\section{Homeworked Example}\label{homeworked-example-8}}

\href{https://youtube.com/playlist?list=PLtz5cFLQl4KONpFF3j9LLwT5UWrTjuFgv\&si=cE9TyjSUiXvAMluw}{Screencast Link}

For more information about the data used in this homeworked example, please refer to the description and codebook located at the end of the \href{https://lhbikos.github.io/ReCenterPsychStats/ReCintro.html\#introduction-to-the-data-set-used-for-homeworked-examples}{introductory lesson} in \href{https://lhbikos.github.io/ReCenterPsychStats/}{ReCentering Psych Stats}. An .rds file which holds the data is located in the \href{https://github.com/lhbikos/ReC_MultivModel/tree/main/Worked_Examples}{Worked Examples} folder at the GitHub site the hosts the OER. The file name is \emph{ReC.rds}.

The suggested practice problem for this chapter is to evaluate the measurement model that would precede the evaluation of a structural model. Next we respecify that model as the structural model. This model will (in all likelihood) be more parsimonious (i.e., have fewer paths) and have worse fit. The goal, though, is that our more parsimonious, structural model, is just as explanatory as the measurement model, where all factors were allowed to covary.

As we end the set of lessons on psychometric development and evaluation, and important part of this lesson is to start with very raw data and work through the scrubbing, scoring, and data diagnostics as we prepare it for the structural equation model that is the primary analysis

\hypertarget{describe-and-draw-the-research-model-you-will-evaluate}{%
\subsection*{Describe and draw the research model you will evaluate}\label{describe-and-draw-the-research-model-you-will-evaluate}}


It should have a minimum of three variables and could be one of the prior path-level models you already examined.

With a simple linear regression, I want to predict students' valuation (Valued, Y) of the statistics course from

\begin{itemize}
\tightlist
\item
  Traditional pedagogy (TradPed, X1)
\item
  Socially responsive pedagogy (SRPed (X2))
\item
  Centering status: 0 = precentered; 1 = recentered (Centering, X3)
\end{itemize}

X1 = Centering: explicit recentering (0 = precentered; 1 = recentered) X2 = TradPed: traditional pedagogy (continuously scaled with higher scores being more favorable) X3 = SRPed: socially responsive pedagogy (continuously scaled with higher scores being more favorable) Y = Valued: valued by me (continuously scaled with higher scores being more favorable)

I am hypothesizing that all three predictors will have a statistically significant, positive, effect on the outcome.

It helps me to make a quick sketch:

\begin{figure}
\centering
\includegraphics{Worked_Examples/Images/HypothesizedModel.png}
\caption{An image of the linear regression model for the homeworked example}
\end{figure}

\hypertarget{import-the-data-and-format-the-variables-in-the-model.}{%
\subsection*{Import the data and format the variables in the model.}\label{import-the-data-and-format-the-variables-in-the-model.}}


\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{raw }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"ReC.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The multiple regression approach we are using does not allow dependency in the data. Therefore, we will include only those who took the psychometrics class (i.e., excluding responses for the ANOVA and multivariate courses).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{raw }\OtherTok{\textless{}{-}}\NormalTok{ (dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(raw, Course }\SpecialCharTok{==} \StringTok{"Psychometrics"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Although this dataset is overall small, I will go ahead and make a babydf with the item-level variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babydf }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(raw, Centering, ClearResponsibilities, EffectiveAnswers,}
\NormalTok{    Feedback, ClearOrganization, ClearPresentation, ValObjectives, IncrUnderstanding,}
\NormalTok{    IncrInterest, InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration)}
\end{Highlighting}
\end{Shaded}

Let's check the structure of the variables:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(babydf)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Classes 'data.table' and 'data.frame':  112 obs. of  13 variables:
 $ Centering            : Factor w/ 2 levels "Pre","Re": 2 2 2 2 2 2 2 2 2 2 ...
 $ ClearResponsibilities: int  3 5 5 4 4 3 4 4 5 5 ...
 $ EffectiveAnswers     : int  3 5 5 3 3 2 4 4 4 4 ...
 $ Feedback             : int  2 5 5 4 4 4 4 4 4 4 ...
 $ ClearOrganization    : int  2 5 5 3 3 2 4 4 4 4 ...
 $ ClearPresentation    : int  2 4 4 2 4 1 4 3 4 4 ...
 $ ValObjectives        : int  3 4 4 4 4 5 4 4 5 4 ...
 $ IncrUnderstanding    : int  4 4 4 2 4 2 5 4 3 4 ...
 $ IncrInterest         : int  1 4 4 2 4 1 5 3 4 2 ...
 $ InclusvClassrm       : int  3 5 5 4 5 3 5 5 5 4 ...
 $ EquitableEval        : int  3 5 5 4 4 2 4 5 4 5 ...
 $ MultPerspectives     : int  1 5 5 5 4 3 5 3 5 NA ...
 $ DEIintegration       : int  2 5 5 5 5 5 5 5 5 4 ...
 - attr(*, ".internal.selfref")=<externalptr> 
\end{verbatim}

All of the item-level variables are integers (i.e., numerical). This is fine.

The centering variable will need to be dummy coded as 0/1:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babydf}\SpecialCharTok{$}\NormalTok{CEN }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(babydf}\SpecialCharTok{$}\NormalTok{Centering)}
\NormalTok{babydf}\SpecialCharTok{$}\NormalTok{CEN }\OtherTok{\textless{}{-}}\NormalTok{ (babydf}\SpecialCharTok{$}\NormalTok{CEN }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
\FunctionTok{str}\NormalTok{(babydf)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Classes 'data.table' and 'data.frame':  112 obs. of  14 variables:
 $ Centering            : Factor w/ 2 levels "Pre","Re": 2 2 2 2 2 2 2 2 2 2 ...
 $ ClearResponsibilities: int  3 5 5 4 4 3 4 4 5 5 ...
 $ EffectiveAnswers     : int  3 5 5 3 3 2 4 4 4 4 ...
 $ Feedback             : int  2 5 5 4 4 4 4 4 4 4 ...
 $ ClearOrganization    : int  2 5 5 3 3 2 4 4 4 4 ...
 $ ClearPresentation    : int  2 4 4 2 4 1 4 3 4 4 ...
 $ ValObjectives        : int  3 4 4 4 4 5 4 4 5 4 ...
 $ IncrUnderstanding    : int  4 4 4 2 4 2 5 4 3 4 ...
 $ IncrInterest         : int  1 4 4 2 4 1 5 3 4 2 ...
 $ InclusvClassrm       : int  3 5 5 4 5 3 5 5 5 4 ...
 $ EquitableEval        : int  3 5 5 4 4 2 4 5 4 5 ...
 $ MultPerspectives     : int  1 5 5 5 4 3 5 3 5 NA ...
 $ DEIintegration       : int  2 5 5 5 5 5 5 5 5 4 ...
 $ CEN                  : num  1 1 1 1 1 1 1 1 1 1 ...
 - attr(*, ".internal.selfref")=<externalptr> 
\end{verbatim}

\hypertarget{analyze-and-manage-missing-data.}{%
\subsection*{Analyze and manage missing data.}\label{analyze-and-manage-missing-data.}}


Structural equation models lend themselves to managing missing data with Parent's \citeyearpar{parent_handling_2013} \emph{available information analysis} (AIA) approach. My approach is to:

\begin{itemize}
\tightlist
\item
  Create a dataframe that includes only the variables that will be used in the analysis.
\item
  Delete all cases with greater than 20\% missingness.
\item
  If scale scores (or parcels) are used, calculate them if \textasciitilde80\% of the data for the calculation is present.
\item
  Use the \emph{full information maximum likelihood} (FIML) estimation procedure in \emph{lavaan}; this allows item-level missingness.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cases1 }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(babydf)  }\CommentTok{\#I produced this object for the sole purpose of feeding the number of cases into the inline text, below}
\NormalTok{cases1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 112
\end{verbatim}

112 students completed at least some of the course evaluation.

The next code creates a variable that counts the number of cells with missing data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\CommentTok{\# Create a variable (n\_miss) that counts the number missing}
\NormalTok{babydf}\SpecialCharTok{$}\NormalTok{n\_miss }\OtherTok{\textless{}{-}}\NormalTok{ babydf }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Centering}\SpecialCharTok{:}\NormalTok{DEIintegration) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    is.na }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rowSums}
\end{Highlighting}
\end{Shaded}

The next code creates a variable that calculates the proportion of the data that is missing. Additionally, it sorts the data from highest to lowest proportion of missingness.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a proportion missing by dividing n\_miss by the total number}
\CommentTok{\# of variables (21) Sort in order of descending frequency to get a}
\CommentTok{\# sense of the missingness}
\NormalTok{babydf }\OtherTok{\textless{}{-}}\NormalTok{ babydf }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop\_miss =}\NormalTok{ (n\_miss}\SpecialCharTok{/}\DecValTok{13}\NormalTok{) }\SpecialCharTok{*} \DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n\_miss))}
\end{Highlighting}
\end{Shaded}

From the code below we can the average missingness as well as the range.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PrMiss1 }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(babydf}\SpecialCharTok{$}\NormalTok{prop\_miss)}
\NormalTok{PrMiss1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   vars   n mean   sd median trimmed mad min   max range skew kurtosis   se
X1    1 112 1.24 3.66      0    0.17   0   0 15.38 15.38 2.94     7.69 0.35
\end{verbatim}

We see that row (or case) level missingness ranged from 0 to 15\%. This is great! Most students are completing the entire evaluation.

Finally, we canwrite code to provide the proportion of missingness across the entire data frame, and then the percent of cases/rows with nonmissing data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CellsMissing1 }\OtherTok{\textless{}{-}}\NormalTok{ formattable}\SpecialCharTok{::}\FunctionTok{percent}\NormalTok{(}\FunctionTok{mean}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(babydf)))  }\CommentTok{\#percent missing across df}
\NormalTok{RowsMissing1 }\OtherTok{\textless{}{-}}\NormalTok{ formattable}\SpecialCharTok{::}\FunctionTok{percent}\NormalTok{(}\FunctionTok{mean}\NormalTok{(}\FunctionTok{complete.cases}\NormalTok{(babydf)))  }\CommentTok{\#percent of rows with non{-}missing data}
\NormalTok{CellsMissing1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.00%
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RowsMissing1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 88.39%
\end{verbatim}

Across the data frame, 1\% of the data is missingness. Further, 88\% of students have non-missing data.

Let's conduct an analysis of missingness with the \emph{mice::md.pattern()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{missingness }\OtherTok{\textless{}{-}}\NormalTok{ mice}\SpecialCharTok{::}\FunctionTok{md.pattern}\NormalTok{(babydf, }\AttributeTok{plot =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rotate.names =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{13_HybridModels_files/figure-latex/unnamed-chunk-81-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{missingness}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Centering ClearResponsibilities Feedback ClearOrganization ClearPresentation
99         1                     1        1                 1                 1
7          1                     1        1                 1                 1
4          1                     1        1                 1                 1
1          1                     1        1                 1                 1
1          1                     1        1                 1                 1
           0                     0        0                 0                 0
   ValObjectives IncrUnderstanding IncrInterest EquitableEval CEN n_miss
99             1                 1            1             1   1      1
7              1                 1            1             1   1      1
4              1                 1            1             1   1      1
1              1                 1            1             1   1      1
1              1                 1            1             1   1      1
               0                 0            0             0   0      0
   prop_miss EffectiveAnswers MultPerspectives InclusvClassrm DEIintegration   
99         1                1                1              1              1  0
7          1                1                1              1              0  1
4          1                1                1              0              0  2
1          1                1                0              1              1  1
1          1                0                1              1              0  2
           0                1                1              4             12 18
\end{verbatim}

Tentative write-up:

\begin{quote}
Our initial inspection of the data indicated that 112 attempted the course evaluation. Across cases, the proportion of missingness in the responses ranged from 0\% to 15\%. Across the dataframe there was 1\% of missingness across the cells. Approximately 88\% of the cases had nonmissing data. Our inspection of a missingness map indicated missingness on several of the items assessing socially responsive pedagogy (e.g., multiple perspectives, inclusive classroom, DEI integration). This could be because these items were added to the course evaluations after the study began. Because missingess was so low, we retained all cases and did not inspect further.
\end{quote}

\hypertarget{assess-the-distributional-characteristics-of-the-data.}{%
\subsection*{Assess the distributional characteristics of the data.}\label{assess-the-distributional-characteristics-of-the-data.}}


\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(babydf)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                      vars   n mean   sd median trimmed  mad min   max range
Centering*               1 112 1.42 0.50    1.0    1.40 0.00   1  2.00  1.00
ClearResponsibilities    2 112 4.62 0.60    5.0    4.72 0.00   3  5.00  2.00
EffectiveAnswers         3 111 4.52 0.69    5.0    4.64 0.00   2  5.00  3.00
Feedback                 4 112 4.38 0.75    5.0    4.51 0.00   2  5.00  3.00
ClearOrganization        5 112 4.22 0.92    4.0    4.38 1.48   1  5.00  4.00
ClearPresentation        6 112 4.35 0.85    5.0    4.51 0.00   1  5.00  4.00
ValObjectives            7 112 4.53 0.57    5.0    4.58 0.00   3  5.00  2.00
IncrUnderstanding        8 112 4.38 0.76    4.5    4.50 0.74   1  5.00  4.00
IncrInterest             9 112 3.93 1.03    4.0    4.08 1.48   1  5.00  4.00
InclusvClassrm          10 108 4.63 0.65    5.0    4.76 0.00   2  5.00  3.00
EquitableEval           11 112 4.66 0.56    5.0    4.74 0.00   2  5.00  3.00
MultPerspectives        12 111 4.47 0.80    5.0    4.62 0.00   1  5.00  4.00
DEIintegration          13 100 4.56 0.73    5.0    4.72 0.00   2  5.00  3.00
CEN                     14 112 0.42 0.50    0.0    0.40 0.00   0  1.00  1.00
n_miss                  15 112 0.16 0.48    0.0    0.02 0.00   0  2.00  2.00
prop_miss               16 112 1.24 3.66    0.0    0.17 0.00   0 15.38 15.38
                       skew kurtosis   se
Centering*             0.32    -1.91 0.05
ClearResponsibilities -1.30     0.59 0.06
EffectiveAnswers      -1.42     1.87 0.07
Feedback              -1.25     1.52 0.07
ClearOrganization     -1.35     1.78 0.09
ClearPresentation     -1.69     3.51 0.08
ValObjectives         -0.69    -0.58 0.05
IncrUnderstanding     -1.61     3.66 0.07
IncrInterest          -1.04     0.69 0.10
InclusvClassrm        -1.70     2.31 0.06
EquitableEval         -1.70     3.45 0.05
MultPerspectives      -1.56     2.50 0.08
DEIintegration        -1.60     1.88 0.07
CEN                    0.32    -1.91 0.05
n_miss                 2.94     7.69 0.04
prop_miss              2.94     7.69 0.35
\end{verbatim}

Regarding the distributional characteristics of the data, skew and kurtosis values of the variables fell below the values of 3 (skew) and 8 to 20 (kurtosis) that Kline suggests are concerning \citeyearpar{kline_principles_2016}.

Next I will conduct an outlier analysis with the Mahalanobis test. The dataset for this can only have continuously scaled variables. First I will create a Mahal\_df that excludes the Centering variable (and those other variables we created to assess missingness).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Mahal\_df }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(babydf, ClearResponsibilities, EffectiveAnswers,}
\NormalTok{    Feedback, ClearOrganization, ClearPresentation, ValObjectives, IncrUnderstanding,}
\NormalTok{    IncrInterest, InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration)}
\end{Highlighting}
\end{Shaded}

We'll append a variable to the Mahal\_df that calculates the distance from the \emph{centroid} of the data. To the degree that the data strays from the diagonal line (and particularly if there are numbered variables), we have outliers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Mahal\_df}\SpecialCharTok{$}\NormalTok{Mahal }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{outlier}\NormalTok{(Mahal\_df)}
\end{Highlighting}
\end{Shaded}

\includegraphics{13_HybridModels_files/figure-latex/unnamed-chunk-84-1.pdf}

The code below appends a TRUE/FALSE variable to the data. Cases are TRUE if the Mahalanobis distance test is more than three standard deviations from the centroid.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\CommentTok{\# str(item\_scores\_df$Mahal)}
\NormalTok{Mahal\_df}\SpecialCharTok{$}\NormalTok{MOutlier }\OtherTok{\textless{}{-}} \FunctionTok{if\_else}\NormalTok{(Mahal\_df}\SpecialCharTok{$}\NormalTok{Mahal }\SpecialCharTok{\textgreater{}}\NormalTok{ (}\FunctionTok{median}\NormalTok{(Mahal\_df}\SpecialCharTok{$}\NormalTok{Mahal) }\SpecialCharTok{+}
\NormalTok{    (}\DecValTok{3} \SpecialCharTok{*} \FunctionTok{sd}\NormalTok{(Mahal\_df}\SpecialCharTok{$}\NormalTok{Mahal))), }\ConstantTok{TRUE}\NormalTok{, }\ConstantTok{FALSE}\NormalTok{)}

\FunctionTok{head}\NormalTok{(Mahal\_df)  }\CommentTok{\#shows us the first 6 rows of the data so we can see the new variables (Mahal, MOutlier)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   ClearResponsibilities EffectiveAnswers Feedback ClearOrganization
1:                     4                4        4                 3
2:                     5                5        4                 5
3:                     5               NA        4                 5
4:                     5                5        4                 4
5:                     5                5        5                 5
6:                     5                4        4                 4
   ClearPresentation ValObjectives IncrUnderstanding IncrInterest
1:                 4             4                 4            3
2:                 5             5                 4            3
3:                 5             5                 5            4
4:                 5             4                 5            4
5:                 5             4                 5            5
6:                 4             4                 4            2
   InclusvClassrm EquitableEval MultPerspectives DEIintegration     Mahal
1:             NA             4                3             NA 12.529051
2:             NA             5                4             NA 10.386495
3:              5             5                5             NA  6.607345
4:             NA             5                5             NA  7.815809
5:             NA             5                5             NA  7.250698
6:              4             5               NA              4 16.461642
   MOutlier
1:    FALSE
2:    FALSE
3:    FALSE
4:    FALSE
5:    FALSE
6:    FALSE
\end{verbatim}

Below, the code will count the number of outliers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{OutlierCount }\OtherTok{\textless{}{-}}\NormalTok{ Mahal\_df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{count}\NormalTok{(MOutlier)}
\NormalTok{OutlierCount}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   MOutlier   n
1:    FALSE 108
2:     TRUE   4
\end{verbatim}

We have only 4 outliers. Should we delete any? One common practice is to sort the data by the ``Mahal'' variable and look for ``jumps.'' If there is a consistent increase, then many researchers leave the data in.

\begin{quote}
Next we evaluated the distributional characteristics of the data. From a univariate perspective, skew and kurtosis values of the variables fell below the values of 3 (skew) and 8 to 20 (kurtosis) that Kline suggests are concerning \citeyearpar{kline_principles_2016}. We evaluated multivariate normality with the Mahalanobis distance test. Specifically, we used the \emph{outlier()} function in the \emph{psych} package and included all continuous variables in the calculation. Our visual inspection of the Q-Q plot suggested that the plotted line strayed from the straight line as the quantiles increased. Additionally, we appended the Mahalanobis distance scores as a variable to the data. Analyzing this variable, we found that four cases exceeded three standard deviations beyond the median. We sorted the Mahalanobis values. Because there was a continuous increase with no ``jumps'' we retained all cases.
\end{quote}

\hypertarget{conduct-appropriate-preliminary-analyses-ms-sds-r-matrix.}{%
\subsection*{\texorpdfstring{Conduct appropriate preliminary analyses (\emph{M}s, \emph{SD}s, \emph{r}-matrix).}{Conduct appropriate preliminary analyses (Ms, SDs, r-matrix).}}\label{conduct-appropriate-preliminary-analyses-ms-sds-r-matrix.}}


\hypertarget{internal-consistency-alpha-coefficients}{%
\subsubsection*{Internal consistency (alpha) coefficients}\label{internal-consistency-alpha-coefficients}}


Although these are typically reported in the Method section in the description of the measures, we should calculate internal consistency coefficients for our three scales.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ValuedVars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"ValObjectives"}\NormalTok{, }\StringTok{"IncrUnderstanding"}\NormalTok{, }\StringTok{"IncrInterest"}\NormalTok{)}
\NormalTok{TradPedVars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"ClearResponsibilities"}\NormalTok{, }\StringTok{"EffectiveAnswers"}\NormalTok{, }\StringTok{"Feedback"}\NormalTok{,}
    \StringTok{"ClearOrganization"}\NormalTok{, }\StringTok{"ClearPresentation"}\NormalTok{)}
\NormalTok{SRPedVars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"InclusvClassrm"}\NormalTok{, }\StringTok{"EquitableEval"}\NormalTok{, }\StringTok{"MultPerspectives"}\NormalTok{, }\StringTok{"DEIintegration"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(babydf[, ..ValuedVars])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = babydf[, ..ValuedVars])

  raw_alpha std.alpha G6(smc) average_r S/N  ase mean   sd median_r
      0.78       0.8    0.74      0.57 3.9 0.03  4.3 0.67     0.54

    95% confidence boundaries 
         lower alpha upper
Feldt     0.70  0.78  0.84
Duhachek  0.72  0.78  0.84

 Reliability if an item is dropped:
                  raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r
ValObjectives          0.80      0.82    0.69      0.69 4.5    0.036    NA
IncrUnderstanding      0.63      0.70    0.54      0.54 2.4    0.056    NA
IncrInterest           0.62      0.63    0.46      0.46 1.7    0.069    NA
                  med.r
ValObjectives      0.69
IncrUnderstanding  0.54
IncrInterest       0.46

 Item statistics 
                    n raw.r std.r r.cor r.drop mean   sd
ValObjectives     112  0.73  0.79  0.61   0.55  4.5 0.57
IncrUnderstanding 112  0.86  0.85  0.76   0.69  4.4 0.76
IncrInterest      112  0.92  0.88  0.81   0.73  3.9 1.03

Non missing response frequency for each item
                     1    2    3    4    5 miss
ValObjectives     0.00 0.00 0.04 0.40 0.56    0
IncrUnderstanding 0.01 0.03 0.04 0.43 0.50    0
IncrInterest      0.04 0.07 0.13 0.45 0.31    0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# if this code throws an error for you rewrite deleting the two dots}
\CommentTok{\# in front of the ValuedVars object}
\end{Highlighting}
\end{Shaded}

Alpha for the Valued-by-Me dimension is 0.80

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(babydf[, ..TradPedVars])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = babydf[, ..TradPedVars])

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
      0.88      0.89    0.88      0.62 8.1 0.017  4.4 0.63     0.64

    95% confidence boundaries 
         lower alpha upper
Feldt     0.84  0.88  0.91
Duhachek  0.85  0.88  0.92

 Reliability if an item is dropped:
                      raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r
ClearResponsibilities      0.85      0.86    0.83      0.60 5.9    0.023 0.0095
EffectiveAnswers           0.83      0.84    0.81      0.57 5.3    0.025 0.0065
Feedback                   0.88      0.90    0.88      0.68 8.7    0.017 0.0041
ClearOrganization          0.86      0.87    0.85      0.63 6.7    0.021 0.0137
ClearPresentation          0.85      0.86    0.84      0.61 6.3    0.023 0.0130
                      med.r
ClearResponsibilities  0.61
EffectiveAnswers       0.58
Feedback               0.65
ClearOrganization      0.60
ClearPresentation      0.61

 Item statistics 
                        n raw.r std.r r.cor r.drop mean   sd
ClearResponsibilities 112  0.85  0.86  0.84   0.77  4.6 0.60
EffectiveAnswers      111  0.89  0.90  0.89   0.83  4.5 0.69
Feedback              112  0.73  0.74  0.62   0.59  4.4 0.75
ClearOrganization     112  0.84  0.82  0.76   0.72  4.2 0.92
ClearPresentation     112  0.85  0.84  0.79   0.75  4.3 0.85

Non missing response frequency for each item
                         1    2    3    4    5 miss
ClearResponsibilities 0.00 0.00 0.06 0.26 0.68 0.00
EffectiveAnswers      0.00 0.02 0.05 0.32 0.61 0.01
Feedback              0.00 0.04 0.05 0.40 0.51 0.00
ClearOrganization     0.02 0.04 0.09 0.39 0.46 0.00
ClearPresentation     0.02 0.03 0.05 0.39 0.51 0.00
\end{verbatim}

Alpha for Traditional Pedagogy dimension is 0.89

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{(babydf[, ..SRPedVars])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = babydf[, ..SRPedVars])

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
      0.86      0.87    0.84      0.62 6.4 0.021  4.6 0.58     0.64

    95% confidence boundaries 
         lower alpha upper
Feldt     0.81  0.86   0.9
Duhachek  0.82  0.86   0.9

 Reliability if an item is dropped:
                 raw_alpha std.alpha G6(smc) average_r S/N alpha se   var.r
InclusvClassrm        0.80      0.80    0.74      0.58 4.1    0.031 0.00522
EquitableEval         0.84      0.85    0.79      0.65 5.6    0.026 0.00039
MultPerspectives      0.82      0.82    0.77      0.60 4.6    0.030 0.00892
DEIintegration        0.83      0.84    0.78      0.63 5.2    0.027 0.00064
                 med.r
InclusvClassrm    0.60
EquitableEval     0.65
MultPerspectives  0.64
DEIintegration    0.64

 Item statistics 
                   n raw.r std.r r.cor r.drop mean   sd
InclusvClassrm   108  0.87  0.88  0.83   0.77  4.6 0.65
EquitableEval    112  0.79  0.81  0.72   0.67  4.7 0.56
MultPerspectives 111  0.88  0.86  0.79   0.73  4.5 0.80
DEIintegration   100  0.84  0.83  0.75   0.69  4.6 0.73

Non missing response frequency for each item
                    1    2    3    4    5 miss
InclusvClassrm   0.00 0.01 0.06 0.21 0.71 0.04
EquitableEval    0.00 0.01 0.02 0.28 0.70 0.00
MultPerspectives 0.01 0.01 0.11 0.25 0.62 0.01
DEIintegration   0.00 0.02 0.08 0.22 0.68 0.11
\end{verbatim}

Alpha for the SCR Pedagogy dimension is 0.87

\hypertarget{means-standard-deviations-and-a-correlation-matrix}{%
\subsubsection*{Means, standard deviations, and a correlation matrix}\label{means-standard-deviations-and-a-correlation-matrix}}


Means, standard deviations, and a correlation matrix are also commonly reported. Because three of our constructs are scales, we will need to calculate their means for cases that have met the minimum thresshold for nonmissingness.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# calculate means for when a specified proportion of items are}
\CommentTok{\# non{-}missing}
\NormalTok{babydf}\SpecialCharTok{$}\NormalTok{Valued }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(babydf[, ..ValuedVars], }\FloatTok{0.66}\NormalTok{)  }\CommentTok{\#will create the mean for each individual if 66\% of variables are non{-}missing}
\NormalTok{babydf}\SpecialCharTok{$}\NormalTok{TradPed }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(babydf[, ..TradPedVars], }\FloatTok{0.66}\NormalTok{)  }\CommentTok{\#will create the mean for each individual if 66\% of variables are non{-}missing}
\NormalTok{babydf}\SpecialCharTok{$}\NormalTok{SRPed }\OtherTok{\textless{}{-}}\NormalTok{ sjstats}\SpecialCharTok{::}\FunctionTok{mean\_n}\NormalTok{(babydf[, ..SRPedVars], }\FloatTok{0.66}\NormalTok{)  }\CommentTok{\#will create the mean for each individual if 66\% of variables are non{-}missing}
\end{Highlighting}
\end{Shaded}

The \emph{apaTables::cor.table} function creates the standard table that will include the means, standard deviations, and correlation matrix. For some reason my concatonated list function isn't working so I'm just creating a corr\_df and will conduct the analyses with those.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corr\_df }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(babydf, Valued, TradPed, SRPed, CEN)}
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(corr\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Means, standard deviations, and correlations with confidence intervals
 

  Variable   M    SD   1            2           3          
  1. Valued  4.28 0.67                                     
                                                           
  2. TradPed 4.42 0.63 .76**                               
                       [.66, .83]                          
                                                           
  3. SRPed   4.58 0.58 .56**        .76**                  
                       [.42, .68]   [.67, .83]             
                                                           
  4. CEN     0.42 0.50 -.19*        -.18        -.09       
                       [-.37, -.01] [-.35, .00] [-.27, .10]
                                                           

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

\hypertarget{write-up-of-preliminary-analyses}{%
\subsubsection{Write up of preliminary analyses}\label{write-up-of-preliminary-analyses}}

\begin{quote}
Our initial inspection of the data indicated that 112 attempted the course evaluation. Across cases, the proportion of missingness in the responses ranged from 0\% to 15\%. Across the dataframe there was 1\% of missingness across the cells. Approximately 88\% of the cases had nonmissing data. The predominant pattern of missingness included individuals opening the survey without completing any of the items. Beyond that, our inspection of a missingness map indicated a missingness on several of the items assessing socially responsive pedagogy (e.g., multiple perspectives, inclusive classroom, DEI integration). This could be because these items were added to the course evaluations after the study began. Because missingess was so low, we retained all cases and did not inspect further.
\end{quote}

\begin{quote}
Next we evaluated the distributional characteristics of the data. From a univariate perspective, skew and kurtosis values of the variables fell below the values of 3 (skew) and 8 to 20 (kurtosis) that Kline suggests are concerning \citeyearpar{kline_principles_2016}. We evaluated multivariate normality with the Mahalanobis distance test. Specifically, we used the \emph{outlier()} function in the \emph{psych} package and included all continuous variables in the calculation. Our visual inspection of the Q-Q plot suggested that the plotted line strayed from the straight line as the quantiles increased. Additionally, we appended the Mahalanobis distance scores as a variable to the data. Analyzing this variable, we found that four cases exceeded three standard deviations beyond the median. We sorted the Mahalanobis values. Because there was a continuous increase with no ``jumps'' we retained all cases. Means, standard deviations, and a correlation matrix are found in Table 1.
\end{quote}

\hypertarget{specify-and-evaluate-a-measurement-model.}{%
\subsection*{\texorpdfstring{Specify and evaluate a \emph{measurement} model.}{Specify and evaluate a measurement model.}}\label{specify-and-evaluate-a-measurement-model.}}


A measurement model includes all of the variables to be included in the model in a structure that allows all the latent variables to freely correlate with each other. This model WILL HAVE the best fit of all models where there are fewer paths.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{msmt\_mod }\OtherTok{\textless{}{-}} \StringTok{"}
\StringTok{        \#\#measurement model}
\StringTok{         CTR =\textasciitilde{} CEN }
\StringTok{         TrP =\textasciitilde{} ClearResponsibilities + EffectiveAnswers + Feedback + ClearOrganization + ClearPresentation}
\StringTok{         SRP =\textasciitilde{} InclusvClassrm + EquitableEval + MultPerspectives + DEIintegration}
\StringTok{         Val =\textasciitilde{} ValObjectives + IncrUnderstanding + IncrInterest}
\StringTok{    }
\StringTok{         }
\StringTok{        \# Variance of the single item indicator}
\StringTok{         CTR \textasciitilde{}\textasciitilde{} 0*CEN}
\StringTok{        }
\StringTok{        \# Covariances}
\StringTok{         CTR \textasciitilde{}\textasciitilde{} TrP}
\StringTok{         CTR \textasciitilde{}\textasciitilde{} SRP}
\StringTok{         CTR \textasciitilde{}\textasciitilde{} Val}
\StringTok{         TrP \textasciitilde{}\textasciitilde{} SRP}
\StringTok{         TrP \textasciitilde{}\textasciitilde{} Val}
\StringTok{         SRP \textasciitilde{}\textasciitilde{} Val}
\StringTok{         }
\StringTok{        "}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{230916}\NormalTok{)}
\NormalTok{msmt\_fit }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{cfa}\NormalTok{(msmt\_mod, }\AttributeTok{data =}\NormalTok{ babydf, }\AttributeTok{missing =} \StringTok{"fiml"}\NormalTok{)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(msmt\_fit, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 91 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        44

  Number of observations                           112
  Number of missing patterns                         5

Model Test User Model:
                                                      
  Test statistic                               212.694
  Degrees of freedom                                60
  P-value (Chi-square)                           0.000

Model Test Baseline Model:

  Test statistic                              1040.629
  Degrees of freedom                                78
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.841
  Tucker-Lewis Index (TLI)                       0.794
                                                      
  Robust Comparative Fit Index (CFI)             0.831
  Robust Tucker-Lewis Index (TLI)                0.780

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -1123.976
  Loglikelihood unrestricted model (H1)      -1017.629
                                                      
  Akaike (AIC)                                2335.952
  Bayesian (BIC)                              2455.565
  Sample-size adjusted Bayesian (SABIC)       2316.510

Root Mean Square Error of Approximation:

  RMSEA                                          0.151
  90 Percent confidence interval - lower         0.129
  90 Percent confidence interval - upper         0.173
  P-value H_0: RMSEA <= 0.050                    0.000
  P-value H_0: RMSEA >= 0.080                    1.000
                                                      
  Robust RMSEA                                   0.158
  90 Percent confidence interval - lower         0.136
  90 Percent confidence interval - upper         0.181
  P-value H_0: Robust RMSEA <= 0.050             0.000
  P-value H_0: Robust RMSEA >= 0.080             1.000

Standardized Root Mean Square Residual:

  SRMR                                           0.071

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Observed
  Observed information based on                Hessian

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  CTR =~                                                                
    CEN               1.000                               0.494    1.000
  TrP =~                                                                
    ClearRspnsblts    1.000                               0.499    0.830
    EffectivAnswrs    1.222    0.101   12.109    0.000    0.610    0.895
    Feedback          0.981    0.130    7.558    0.000    0.490    0.655
    ClearOrganiztn    1.375    0.151    9.126    0.000    0.686    0.751
    ClearPresenttn    1.389    0.136   10.205    0.000    0.693    0.823
  SRP =~                                                                
    InclusvClassrm    1.000                               0.532    0.824
    EquitableEval     0.839    0.094    8.929    0.000    0.446    0.797
    MultPerspectvs    1.186    0.128    9.283    0.000    0.631    0.798
    DEIintegration    0.975    0.125    7.811    0.000    0.519    0.711
  Val =~                                                                
    ValObjectives     1.000                               0.379    0.670
    IncrUndrstndng    1.571    0.232    6.781    0.000    0.596    0.785
    IncrInterest      2.246    0.317    7.078    0.000    0.852    0.832

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  CTR ~~                                                                
   .CEN               0.000                               0.000      NaN
    TrP              -0.052    0.025   -2.080    0.038   -0.212   -0.212
    SRP              -0.030    0.027   -1.105    0.269   -0.114   -0.114
    Val              -0.040    0.020   -1.968    0.049   -0.214   -0.214
  TrP ~~                                                                
    SRP               0.231    0.041    5.684    0.000    0.871    0.871
    Val               0.164    0.033    4.948    0.000    0.868    0.868
  SRP ~~                                                                
    Val               0.139    0.031    4.411    0.000    0.687    0.687

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .CEN               0.420    0.047    8.999    0.000    0.420    0.850
   .ClearRspnsblts    4.616    0.057   81.248    0.000    4.616    7.677
   .EffectivAnswrs    4.526    0.064   70.214    0.000    4.526    6.643
   .Feedback          4.384    0.071   62.112    0.000    4.384    5.869
   .ClearOrganiztn    4.223    0.086   48.938    0.000    4.223    4.624
   .ClearPresenttn    4.348    0.080   54.647    0.000    4.348    5.164
   .InclusvClassrm    4.631    0.062   75.290    0.000    4.631    7.170
   .EquitableEval     4.661    0.053   88.099    0.000    4.661    8.325
   .MultPerspectvs    4.466    0.075   59.682    0.000    4.466    5.651
   .DEIintegration    4.544    0.071   63.735    0.000    4.544    6.228
   .ValObjectives     4.527    0.054   84.595    0.000    4.527    7.993
   .IncrUndrstndng    4.384    0.072   61.141    0.000    4.384    5.777
   .IncrInterest      3.929    0.097   40.604    0.000    3.929    3.837

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .CEN               0.000                               0.000    0.000
   .ClearRspnsblts    0.113    0.019    6.057    0.000    0.113    0.311
   .EffectivAnswrs    0.093    0.018    5.090    0.000    0.093    0.200
   .Feedback          0.318    0.045    7.057    0.000    0.318    0.571
   .ClearOrganiztn    0.363    0.054    6.770    0.000    0.363    0.435
   .ClearPresenttn    0.228    0.037    6.233    0.000    0.228    0.322
   .InclusvClassrm    0.134    0.025    5.265    0.000    0.134    0.322
   .EquitableEval     0.115    0.020    5.612    0.000    0.115    0.365
   .MultPerspectvs    0.227    0.039    5.783    0.000    0.227    0.363
   .DEIintegration    0.263    0.044    5.956    0.000    0.263    0.494
   .ValObjectives     0.177    0.027    6.461    0.000    0.177    0.551
   .IncrUndrstndng    0.221    0.040    5.487    0.000    0.221    0.383
   .IncrInterest      0.322    0.071    4.549    0.000    0.322    0.307
    CTR               0.244    0.033    7.483    0.000    1.000    1.000
    TrP               0.249    0.047    5.276    0.000    1.000    1.000
    SRP               0.283    0.056    5.033    0.000    1.000    1.000
    Val               0.144    0.038    3.755    0.000    1.000    1.000
\end{verbatim}

Below is script that will export the global fit indices (via \emph{tidySEM::table\_fit}) and the parameter estimates (e.g., factor loadings, structural regression weights, and parameters we requested such as the indirect effect) to .csv files that you can manipulate outside of R.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# global fit indices}
\NormalTok{msmt\_globalfit }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(msmt\_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Registered S3 method overwritten by 'tidySEM':
  method          from  
  predict.MxModel OpenMx
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{msmt\_fit\_pEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(msmt\_fit, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{, }\AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(msmt\_globalfit, }\AttributeTok{file =} \StringTok{"msmt\_globalfit.csv"}\NormalTok{)}
\CommentTok{\# the code below writes the parameter estimates into a .csv file}
\FunctionTok{write.csv}\NormalTok{(msmt\_fit\_pEsts, }\AttributeTok{file =} \StringTok{"msmt\_fit\_pEsts.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's plot what we did\textgreater{}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{msmt\_m1 }\OtherTok{\textless{}{-}}\NormalTok{ semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(msmt\_fit, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{, }\AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{,}
    \AttributeTok{sizeMan =} \DecValTok{5}\NormalTok{, }\AttributeTok{node.width =} \DecValTok{1}\NormalTok{, }\AttributeTok{edge.label.cex =} \FloatTok{0.75}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{,}
    \AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{13_HybridModels_files/figure-latex/unnamed-chunk-95-1.pdf}

\begin{quote}
With the exception of the SRMR, the fit of the measurement model was poor: \(\chi ^{2}(60) = 212.69, *p* < 0.001, CFI = .841, RMSEA = 0.151, CI90(0.129, 0.173), SRMR = 0.071\), SRMR = 0.071.
\end{quote}

If this project were for actual publication, I would take this a step further and respecify the model with parcels. Why? Because a parcelled model (which approaches a \emph{just-identified model} will usually have better global fit). I address this topic more completely in \href{https://lhbikos.github.io/ReC_MultivModel/MeasMod.html}{ReCentering Psych Stats: Multivariate Modeling}.

\hypertarget{specify-and-evaluate-a-structural-model}{%
\subsection*{\texorpdfstring{Specify and evaluate a \emph{structural} model}{Specify and evaluate a structural model}}\label{specify-and-evaluate-a-structural-model}}


As a reminder, I am hypothesizing a multiple regression predicting value to the student from traditional pedagogy, socially responsive pedagogy, and centering.

X1 = Centering: explicit recentering (0 = precentered; 1 = recentered) X2 = TradPed: traditional pedagogy (continuously scaled with higher scores being more favorable) X3 = SRPed: socially responsive pedagogy (continuously scaled with higher scores being more favorable) Y = Valued: valued by me (continuously scaled with higher scores being more favorable)

This structural model will have fewer paths than its predecessor, the measurement model. The \emph{lavaan::sem} specification looks quite similar to the measurement model. Differences are:

\begin{itemize}
\tightlist
\item
  The map of equations predicts Valued by centering, traditional pedagogy, and socially responsive pedagogy.
\item
  The \emph{lavaan::sem} code includes the statement ``auto.cov.lv.x=FALSE''; this prevents lavaan from allowing the latent variables to correlate with each other.
\end{itemize}

\begin{figure}
\centering
\includegraphics{Worked_Examples/images/HypothesizedModel.PNG}
\caption{An image of the parallel mediation model for the homeworked example}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ReC\_struct\_mod1 }\OtherTok{\textless{}{-}} \StringTok{"}
\StringTok{        \#measurement model}
\StringTok{         CTR =\textasciitilde{} CEN \#this is a single item indicator, I had to add code below to set the variance}
\StringTok{         TrP =\textasciitilde{} ClearResponsibilities + EffectiveAnswers + Feedback + ClearOrganization + ClearPresentation}
\StringTok{         SRP =\textasciitilde{} InclusvClassrm + EquitableEval + MultPerspectives + DEIintegration}
\StringTok{         Val =\textasciitilde{} ValObjectives + IncrUnderstanding + IncrInterest}
\StringTok{    }
\StringTok{        \# Variance of the single item indicator}
\StringTok{         CTR \textasciitilde{}\textasciitilde{} 0*CEN}
\StringTok{        }
\StringTok{        \#structural model}
\StringTok{          Val \textasciitilde{} CTR + TrP + SRP}
\StringTok{         }
\StringTok{          "}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{230916}\NormalTok{)  }\CommentTok{\#needed for reproducibility }
\NormalTok{ReC\_struct\_fit1 }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{sem}\NormalTok{(ReC\_struct\_mod1, }\AttributeTok{data =}\NormalTok{ babydf, }\AttributeTok{missing =} \StringTok{"fiml"}\NormalTok{,}
    \AttributeTok{fixed.x =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{auto.cov.lv.x =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{lavaan}\SpecialCharTok{::}\FunctionTok{summary}\NormalTok{(ReC\_struct\_fit1, }\AttributeTok{fit.measures =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{standardized =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{rsq =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
lavaan 0.6.17 ended normally after 87 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        41

  Number of observations                           112
  Number of missing patterns                         5

Model Test User Model:
                                                      
  Test statistic                               314.627
  Degrees of freedom                                63
  P-value (Chi-square)                           0.000

Model Test Baseline Model:

  Test statistic                              1040.629
  Degrees of freedom                                78
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.739
  Tucker-Lewis Index (TLI)                       0.676
                                                      
  Robust Comparative Fit Index (CFI)             0.734
  Robust Tucker-Lewis Index (TLI)                0.671

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -1174.942
  Loglikelihood unrestricted model (H1)      -1017.629
                                                      
  Akaike (AIC)                                2431.885
  Bayesian (BIC)                              2543.343
  Sample-size adjusted Bayesian (SABIC)       2413.769

Root Mean Square Error of Approximation:

  RMSEA                                          0.189
  90 Percent confidence interval - lower         0.168
  90 Percent confidence interval - upper         0.210
  P-value H_0: RMSEA <= 0.050                    0.000
  P-value H_0: RMSEA >= 0.080                    1.000
                                                      
  Robust RMSEA                                   0.194
  90 Percent confidence interval - lower         0.173
  90 Percent confidence interval - upper         0.215
  P-value H_0: Robust RMSEA <= 0.050             0.000
  P-value H_0: Robust RMSEA >= 0.080             1.000

Standardized Root Mean Square Residual:

  SRMR                                           0.289

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Observed
  Observed information based on                Hessian

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  CTR =~                                                                
    CEN               1.000                               0.494    1.000
  TrP =~                                                                
    ClearRspnsblts    1.000                               0.496    0.824
    EffectivAnswrs    1.259    0.102   12.299    0.000    0.624    0.916
    Feedback          0.939    0.133    7.048    0.000    0.465    0.623
    ClearOrganiztn    1.402    0.155    9.041    0.000    0.695    0.761
    ClearPresenttn    1.377    0.141    9.760    0.000    0.682    0.810
  SRP =~                                                                
    InclusvClassrm    1.000                               0.554    0.857
    EquitableEval     0.732    0.090    8.166    0.000    0.406    0.725
    MultPerspectvs    1.132    0.131    8.623    0.000    0.628    0.794
    DEIintegration    1.012    0.119    8.504    0.000    0.561    0.771
  Val =~                                                                
    ValObjectives     1.000                               0.369    0.659
    IncrUndrstndng    1.582    0.234    6.772    0.000    0.584    0.781
    IncrInterest      2.261    0.319    7.078    0.000    0.835    0.828

Regressions:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  Val ~                                                                 
    CTR              -0.039    0.054   -0.716    0.474   -0.052   -0.052
    TrP               0.638    0.125    5.102    0.000    0.856    0.856
    SRP               0.016    0.080    0.196    0.844    0.024    0.024

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  CTR ~~                                                                
   .CEN               0.000                               0.000      NaN

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .CEN               0.420    0.047    8.999    0.000    0.420    0.850
   .ClearRspnsblts    4.616    0.057   81.248    0.000    4.616    7.677
   .EffectivAnswrs    4.526    0.064   70.232    0.000    4.526    6.644
   .Feedback          4.384    0.071   62.112    0.000    4.384    5.869
   .ClearOrganiztn    4.223    0.086   48.938    0.000    4.223    4.624
   .ClearPresenttn    4.348    0.080   54.647    0.000    4.348    5.164
   .InclusvClassrm    4.628    0.062   75.037    0.000    4.628    7.151
   .EquitableEval     4.661    0.053   88.099    0.000    4.661    8.325
   .MultPerspectvs    4.465    0.075   59.618    0.000    4.465    5.645
   .DEIintegration    4.540    0.071   63.975    0.000    4.540    6.235
   .ValObjectives     4.527    0.053   85.399    0.000    4.527    8.069
   .IncrUndrstndng    4.384    0.071   61.956    0.000    4.384    5.854
   .IncrInterest      3.929    0.095   41.213    0.000    3.929    3.894

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .CEN               0.000                               0.000    0.000
   .ClearRspnsblts    0.116    0.019    6.047    0.000    0.116    0.321
   .EffectivAnswrs    0.075    0.018    4.054    0.000    0.075    0.161
   .Feedback          0.341    0.048    7.094    0.000    0.341    0.612
   .ClearOrganiztn    0.352    0.054    6.481    0.000    0.352    0.422
   .ClearPresenttn    0.244    0.040    6.071    0.000    0.244    0.343
   .InclusvClassrm    0.111    0.027    4.060    0.000    0.111    0.266
   .EquitableEval     0.149    0.024    6.157    0.000    0.149    0.475
   .MultPerspectvs    0.232    0.045    5.176    0.000    0.232    0.370
   .DEIintegration    0.215    0.039    5.498    0.000    0.215    0.406
   .ValObjectives     0.178    0.027    6.494    0.000    0.178    0.566
   .IncrUndrstndng    0.219    0.040    5.473    0.000    0.219    0.391
   .IncrInterest      0.320    0.070    4.552    0.000    0.320    0.315
    CTR               0.244    0.033    7.483    0.000    1.000    1.000
    TrP               0.246    0.047    5.210    0.000    1.000    1.000
    SRP               0.307    0.059    5.187    0.000    1.000    1.000
   .Val               0.036    0.013    2.802    0.005    0.264    0.264

R-Square:
                   Estimate
    CEN               1.000
    ClearRspnsblts    0.679
    EffectivAnswrs    0.839
    Feedback          0.388
    ClearOrganiztn    0.578
    ClearPresenttn    0.657
    InclusvClassrm    0.734
    EquitableEval     0.525
    MultPerspectvs    0.630
    DEIintegration    0.594
    ValObjectives     0.434
    IncrUndrstndng    0.609
    IncrInterest      0.685
    Val               0.736
\end{verbatim}

Below is script that will export the global fit indices (via \emph{tidySEM::table\_fit}) and the parameter estimates (e.g., factor loadings, structural regression weights, and parameters we requested such as the indirect effect) to .csv files that you can manipulate outside of R.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# global fit indices}
\NormalTok{ReC\_globalfit1 }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_fit}\NormalTok{(ReC\_struct\_fit1)}
\NormalTok{ReC\_struct\_pEsts }\OtherTok{\textless{}{-}}\NormalTok{ tidySEM}\SpecialCharTok{::}\FunctionTok{table\_results}\NormalTok{(ReC\_struct\_fit1, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{columns =} \ConstantTok{NULL}\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(ReC\_globalfit1, }\AttributeTok{file =} \StringTok{"ReC\_globalfit1.csv"}\NormalTok{)}
\CommentTok{\# the code below writes the parameter estimates into a .csv file}
\FunctionTok{write.csv}\NormalTok{(ReC\_struct\_pEsts, }\AttributeTok{file =} \StringTok{"ReC\_struct\_pEsts1.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's work up a figure. Luckily, the \emph{layout} and \emph{rotate} commands within within \emph{semPlot:semPlot} make it easy to produce a reasonable figure.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_ReC\_struct1 }\OtherTok{\textless{}{-}}\NormalTok{ semPlot}\SpecialCharTok{::}\FunctionTok{semPaths}\NormalTok{(ReC\_struct\_fit1, }\AttributeTok{what =} \StringTok{"col"}\NormalTok{, }\AttributeTok{whatLabels =} \StringTok{"stand"}\NormalTok{,}
    \AttributeTok{sizeMan =} \DecValTok{3}\NormalTok{, }\AttributeTok{node.width =} \DecValTok{1}\NormalTok{, }\AttributeTok{edge.label.cex =} \FloatTok{0.75}\NormalTok{, }\AttributeTok{style =} \StringTok{"lisrel"}\NormalTok{,}
    \AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{structural =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{curve =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{intercepts =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{layout =} \StringTok{"tree"}\NormalTok{, }\AttributeTok{rotation =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{13_HybridModels_files/figure-latex/unnamed-chunk-98-1.pdf}

If we table the results, here's what we have:

\textbf{Table 2 }

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 0\tabcolsep) * \real{1.0000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Model Coefficients Assessing the Effect of Perceived Value from Recentering, Traditional Pedagogy, and Socially Responsive Pedagogy
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\end{longtable}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.5075}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1194}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1194}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1343}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1194}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Predictor
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(B\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(SE_{B}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(p\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\beta\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Centering & -0.039 & 0.054 & 0.474 & -0.052 \\
Traditional Pedagogy & 0.638 & 0.125 & \textless{} 0.001 & 1.218 \\
Socially Responsive Pedagogy & -0.016 & 0.080 & 0.844 & 0.024 \\
\end{longtable}

\begin{quote}
Our structural model was a multiple regression, predicting perceived value to the student directly from centering, traditional pedagogy, and socially responsive pedagogy. Results of the global fit indices all fell below the thresholds of acceptability \((\chi^2(63) = 314.63, p < 0.001, CFI = 0.739, RMSEA = 0.189, 90CI[0.168, 0.210, SRMR = 0.289)\). As shown in Table 2, only traditional pedagogy was a significant predictor of value to the student.
\end{quote}

\hypertarget{apa-style-results-with-tables-and-figure.}{%
\subsection{APA style results with table(s) and figure.}\label{apa-style-results-with-tables-and-figure.}}

\begin{quote}
\textbf{Preliminary Analyses} \textgreater Our initial inspection of the data indicated that 112 attempted the course evaluation. Across cases, the proportion of missingness in the responses ranged from 0\% to 15\%. Across the dataframe there was 1\% of missingness across the cells. Approximately 88\% of the cases had nonmissing data. The predominant pattern of missingness included individuals opening the survey without completing any of the items. Beyond that, our inspection of a missingness map indicated a missingness on several of the items assessing socially responsive pedagogy (e.g., multiple perspectives, inclusive classroom, DEI integration). This could be because these items were added to the course evaluations after the study began. Because missingess was so low, we retained all cases and did not inspect further.
\end{quote}

\begin{quote}
Next we evaluated the distributional characteristics of the data. From a univariate perspective, skew and kurtosis values of the variables fell below the values of 3 (skew) and 8 to 20 (kurtosis) that Kline suggests are concerning \citeyearpar{kline_principles_2016}. We evaluated multivariate normality with the Mahalanobis distance test. Specifically, we used the \emph{outlier()} function in the \emph{psych} package and included all continuous variables in the calculation. Our visual inspection of the Q-Q plot suggested that the plotted line strayed from the straight line as the quantiles increased. Additionally, we appended the Mahalanobis distance scores as a variable to the data. Analyzing this variable, we found that four cases exceeded three standard deviations beyond the median. We sorted the Mahalanobis values. Because there was a continuous increase with no ``jumps'' we retained all cases.
\end{quote}

\begin{quote}
\textbf{Primary Analyses}
\end{quote}

\begin{quote}
We used structural equation modeling to predict course value to the student (Value) from traditional pedagogy (TradPed), socially responsive pedagogy (SRPed), and centering status (Centering, 0 = precentered, 1 = recentered).
\end{quote}

\begin{quote}
We began by specifying a measurement model that permitted all latent variables to covary. Fit of this model was poor: \(\chi ^{2}(60) = 212.69, *p* < 0.001, CFI = .841, RMSEA = 0.151, CI90(0.129, 0.173), SRMR = 0.071\). Our structural model was a multiple regression, predicting perceived value to the student directly from centering, traditional pedagogy, and socially responsive pedagogy. Results of the global fit indices for the structural model were worse than the measurement model: \((\chi^2(63) = 314.63, p < 0.001, CFI = 0.739, RMSEA = 0.189, 90CI[0.168, 0.210, SRMR = 0.289)\). As shown in Table 2, only traditional pedagogy was a significant predictor of the course's value to the student.
\end{quote}

\hypertarget{sims}{%
\chapter{Additional Simulations}\label{sims}}

\href{https://www.youtube.com/playlist?list=PLtz5cFLQl4KOOKKTNWJqTSUjx2Nm0D2ON}{Screencast Link}

In each of the suggestions for practice (i.e., ``homework''), I suggest that an option for a middle-level degree of difficulty is to use data from another set of simulated data. One option is to use simulations from other lessons. Especially in the early chapters, it is difficult to know what data from later chapters might be appropriate for such analyses. Because of this challenge, I am providing several simulations that are not used in any of the lessons that could be used as practice for \emph{all} of the lessons that involve psychometric evaluation. In this way, you would follow the construction of a scale through the standard process.

There are a few caveats to this process:

\begin{itemize}
\tightlist
\item
  In each of the simulations, I have added the code, ``eval = FALSE'' in the top of the chunk. This prevents the chunk from executing when the OER is \emph{building}. If you copy the script you should have no problems (but if you try to knit an .rmd file and wonder why no data is being produced, this is why).
\item
  Each of the psychometric evaluations included a number of additional measures used to assess convergent, discriminant, concurrent, predictive, and incremental validity. While I would have liked to have included them all,

  \begin{itemize}
  \tightlist
  \item
    I could not always locate source articles, and
  \item
    Adding too many scales caused my simulation code to fail.
  \item
    Thus, each simulation includes at least three additional measures that can be used for evaluating validity as it relates to measures within the nomological net (e.g., convergent, discriminant, concurrent, predictive, incremental).
  \end{itemize}
\item
  There are two simulations with each scale.

  \begin{itemize}
  \tightlist
  \item
    The first simulation provides the focal scale and the additional instruments.
  \item
    The second simulation creates ``A'' and ``B'' data for the focal scale. When you are practicing test-retest reliability, you can \emph{pretend} that A and B are from the same respondents. Later, when you are practicing invariance testing you can \emph{pretend} that the data are from two different groups.
  \end{itemize}
\item
  At least one of these scales as five subscales. In certain analyses (e.g., item analysis) this may become unwieldy. In such cases you may wish to select three of those subscales and run the analyses with just those. After all, ``it's just practice.'' I do recommend that you complete the majority of practice assignments with a minimum of three subscales.
\item
  Each of the simulations includes a random seed. If you and homework partners would like to work on the same simulation, you could gently increase your challenge by setting different random seeds and comparing the subtle differences in the solutions.
\item
  I recommend that you retrieve the original psychometric article so that you can compare your analytic choices and results with those completed in the article. At least two of the articles are available as preprints or in open-source journals.
\end{itemize}

\hypertarget{ibelong-scale}{%
\section{iBelong Scale}\label{ibelong-scale}}

The iBelong Scale \citep{lee_ibelong_2024} is a measure of racial-ethnic-cultural (REC) belonging for BIPOC people. The scale includes 25 items with responses rated on a 6-point Likert scale ranging from 1 (\emph{strongly disagree}) to 6 (\emph{strongly agree}). At the start of the survey, participants are asked to self-identify their REC identity (i.e., ``In terms of my racial-ethnic-cultural {[}REC{]} identity I consider myself to be:\_\_\_\_''). Higher scores indicate a stronger sense of REC belonging. Lee and Neville \citeyearpar{lee_ibelong_2024} reported that a five-factor solution resulted in the best fit.

Beneath each of the five factors are a list of the items; variable names follow in parentheses.

\begin{itemize}
\tightlist
\item
  Authenticity:

  \begin{itemize}
  \tightlist
  \item
    In general, I do not have to change my behaviors when I am with REC group members (Auth1)
  \item
    I am free to be myself with members of my REC group (Auth2)
  \item
    In general, I do not have to hide parts of who I am with members of my REC group (Auth3)
  \item
    Members of my REC group see me for who I am (Auth4)
  \item
    I feel accepted by my REC group for who I am (Auth5)
  \end{itemize}
\item
  Connection:

  \begin{itemize}
  \tightlist
  \item
    I have similar experiences to others in my REC group (Conn1)
  \item
    Others in my REC group see the world in a similar way as me (Conn2)
  \item
    My success is connected to the success of members of my REC group (Conn3)
  \item
    I have many things in common with people from my REC group (Conn4)
  \item
    I feel connected to my REC group because of our shared experiences (Conn5)
  \end{itemize}
\item
  Home

  \begin{itemize}
  \tightlist
  \item
    I feel close to other members of my REC group (Home1)
  \item
    I have shared interests with others in my REC group (Home2)
  \item
    I have shared values with others in my REC group (Home3)
  \item
    In general, I am at ease when I am with people from my REC group (Home4)
  \item
    In general, I feel ``at home'' when I am with members of my REC group (Home5)
  \end{itemize}
\item
  REC Thriving

  \begin{itemize}
  \tightlist
  \item
    I am happy that I am a member of my REC group (Thriv1)
  \item
    I am proud to be a member of my REC group (Thriv2)
  \item
    I like learning about my REC group's history (Thriv3)
  \item
    I value my REC group (Thriv4)
  \item
    I consider it an honor to be a member of my REC group (Thriv5) *Self-Definition
  \item
    I define what belonging to my REC group means to me (Self1)
  \item
    I am a member of my REC group on my own terms (Self2)
  \item
    I decide what my REC group membership is (Self3)
  \item
    I do not have to prove that I belong to my REC group to others in my REC group (Self4)
  \item
    Others do not define my belonging to my REC group (Self5)
  \end{itemize}
\end{itemize}

A \href{https://www.researchgate.net/publication/377700068_The_iBelong_Scale_Construction_and_validation_of_a_measure_of_racial-ethnic-cultural_belonging}{preprint} of the article is available at ResearchGate. Below is the script for simulating item-level data from the factor loadings, means, and sample size presented in the published article.

Six additional scales were reported in the Lee and Neville \citeyearpar{lee_ibelong_2024} article. Optimizing the ability for the simulation to converge and also to provide an array of scales within the nomological net, I chose the three scales below. Unfortunately, I could not locate factor loadings for all of them; in these cases, I uniformly assigned the factor loading of 0.8. The scales, their original citation, and information about how I simulated data for each are listed below.

\begin{itemize}
\tightlist
\item
  \textbf{General Belongingness Scale} {[}GBS; \citet{malone_general_2012}{]} is a 12-item item scale with Likert scaling ranging from 1 (\emph{strongly disagree}) to 7 (\emph{strongly agree}). There are two six-item subscales: Acceptance/Inclusion (``When I am with other people, I feel included'') and Rejection/Exclusion (``I feel like an outsider''). The items on the Rejection/Exclusion subscale must be rescored such that higher scores represent a greater sense of belonging in general. In the iBELONG study \citep{lee_ibelong_2024}, only a total GBS scale was used. My simulation used factor loadings from Malone et al. \citeyearpar{malone_general_2012} but treated them as a single scale. This means that there is need to reverse-score the Rejection/Exclusion items in this simulated data.
\item
  \textbf{The Collective Self-Esteem Scale--Race Specific Version} {[}CSESR; \citet{crocker_collective_1994}{]} is a 16-item scale with Likert scaling ranging from 1 (\emph{strongly disagree}) to 7 (\emph{strongly agree}). Higher scores represent a stronger sense of membership with one's own racial/ethnic group. There are four, four-item subscales: Membership CSE, Public CSE, Private CSE, and Importance to Identity, however the IBelong study \citep{lee_ibelong_2024} used a total scale score. An example item is, ``The racial-ethnic group I belong to is an important reflection of who I am.'' Because I was not able to locate factor loadings from a psychometric evaluation, I simulated the data by specifying a 0.8 as a standardized factor loading for each of the items on the general factor.
\item
  \textbf{The Multigroup Ethnic Identity Measure -- Revised} {[}MEIMR; \citet{brown_multigroup_2014}{]} is a 7-item scale. The first item is an open-ended question for identification of respondent ethnic group. The remaining six items are assessed with Likert scaling ranging from 1 (\emph{strongly disagree}) to 5 (\emph{strongly agree}). There are two, three-item subscales which assess exploration (e.g., ``I have spent time trying to find out more about my ethnic group, such as its history, tradition, and customs'') and commitment (``I have a strong sense of belonging to my own ethnic group.'') Higher scores indicate more positive ethnic identity. In the iBELONG study \citep{lee_ibelong_2024}, only a total MEIMR scale was used. My simulation used factor loadings from Brown et al. \citeyearpar{brown_multigroup_2014} but treated them as a single scale.
\end{itemize}

Because data is collected at the item level (and I want this resource to be as practical as possible, I have simulated the data for each of the scales at the item level. Simulating the data involved using factor loadings, means, and correlations between the scales. Because the simulation will produce ``out-of-bounds'' values, the code below re-scales the scores into the range of the Likert-type scaling and rounds them to whole values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Entering the intercorrelations, means, and standard deviations from the journal article}

\NormalTok{iBelong\_generating\_model }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}}
\StringTok{        \#measurement model}
\StringTok{        Authenticity  =\textasciitilde{} .66*Auth1 + .64*Auth2 + .63*Auth3 + .58*Auth4 + .55*Auth5 }
\StringTok{        Connection =\textasciitilde{} .72*Conn1 + .60*Conn2 + .59*Conn3 + .58*Conn4 + .58*Conn5}
\StringTok{        Home =\textasciitilde{} .64*Home1 + .63*Home2 + .57*Home3 + .57*Home4 + .53*Home5}
\StringTok{        RECthriving =\textasciitilde{} .68*Thriv1 + .66*Thriv2 + .59*Thriv3 + .54*Thriv4 + .49*Thriv5}
\StringTok{        SelfDefinition =\textasciitilde{} .82*Self1 + .65*Self2 + .65*Self3 + .54*Self4 + .49*Self5}
\StringTok{        GBS =\textasciitilde{} .70*GBS1 + .67*GBS2 + .65*GBS5 + .67*GBS8 + .70*GBS10 + .78*GBS11 + .78*GBS3 + .66*GBS4 + .77*GBS6 + .82*GBS7 + .79*GBS9 + .66*GBS12}
\StringTok{        CSESR =\textasciitilde{} .8*CSESR1 + .8*CSESR2 + .8*CSESR3 + .8*CSESR4 + .8*CSESR5 + .8*CSESR6 + .8*CSESR7 + .8*CSESR8 + .8*CSESR9 + .8*CSESR10 + .8*CSESR11 + .8*CSESR12 + .8*CSESR13 + .8*CSESR14 + .8*CSESR15 + .8*CSESR16 }
\StringTok{        MEIMR =\textasciitilde{} .60*MEIMR1 + .93*MEIMR4 + .81*MEIMR5 + .86*MEIMR2 + .84*MEIMR3 + .89*MEIMR6}
\StringTok{        }
\StringTok{        \#Means}
\StringTok{         Authenticity \textasciitilde{} 4.30*1}
\StringTok{         Connection \textasciitilde{} 4.16*1}
\StringTok{         Home \textasciitilde{} 4.51*1}
\StringTok{         RECthriving \textasciitilde{} 4.81*1}
\StringTok{         SelfDefinition \textasciitilde{} 4.53*1}
\StringTok{         GBS \textasciitilde{} 4.84*1}
\StringTok{         CSESR \textasciitilde{} 5.15*1}
\StringTok{         MEIMR \textasciitilde{} 3.81*1}

\StringTok{         \#Correlations}
\StringTok{         Authenticity \textasciitilde{}\textasciitilde{} .73*Connection}
\StringTok{         Authenticity \textasciitilde{}\textasciitilde{} .78*Home}
\StringTok{         Authenticity \textasciitilde{}\textasciitilde{} .64*RECthriving}
\StringTok{         Authenticity \textasciitilde{}\textasciitilde{} .52*SelfDefinition}
\StringTok{         Authenticity \textasciitilde{}\textasciitilde{} .58*GBS}
\StringTok{         Authenticity \textasciitilde{}\textasciitilde{} .58*CSESR}
\StringTok{         Authenticity \textasciitilde{}\textasciitilde{} .48*MEIMR}
\StringTok{         }
\StringTok{         Connection \textasciitilde{}\textasciitilde{} .81*Home}
\StringTok{         Connection \textasciitilde{}\textasciitilde{} .70*RECthriving}
\StringTok{         Connection \textasciitilde{}\textasciitilde{} .43*SelfDefinition}
\StringTok{         Connection \textasciitilde{}\textasciitilde{} .36*GBS}
\StringTok{         Connection \textasciitilde{}\textasciitilde{} .63*CSESR}
\StringTok{         Connection \textasciitilde{}\textasciitilde{} .62*MEIMR}
\StringTok{         }
\StringTok{         Home \textasciitilde{}\textasciitilde{} .74*RECthriving}
\StringTok{         Home \textasciitilde{}\textasciitilde{} .48*SelfDefinition}
\StringTok{         Home \textasciitilde{}\textasciitilde{} .51*GBS}
\StringTok{         Home \textasciitilde{}\textasciitilde{} .65*CSESR}
\StringTok{         Home \textasciitilde{}\textasciitilde{} .63*MEIMR}
\StringTok{         }
\StringTok{         RECthriving \textasciitilde{}\textasciitilde{} .46*SelfDefinition}
\StringTok{         RECthriving \textasciitilde{}\textasciitilde{} .42*GBS}
\StringTok{         RECthriving \textasciitilde{}\textasciitilde{} .70*CSESR}
\StringTok{         RECthriving \textasciitilde{}\textasciitilde{} .70*MEIMR}
\StringTok{         }
\StringTok{         SelfDefinition \textasciitilde{}\textasciitilde{} .29*GBS}
\StringTok{         SelfDefinition \textasciitilde{}\textasciitilde{} .38*CSESR}
\StringTok{         SelfDefinition \textasciitilde{}\textasciitilde{} .35*MEIMR}
\StringTok{         }
\StringTok{         \textquotesingle{}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240326}\NormalTok{)}
\NormalTok{iBel }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{simulateData}\NormalTok{(}\AttributeTok{model =}\NormalTok{ iBelong\_generating\_model,}
                              \AttributeTok{model.type =} \StringTok{"sem"}\NormalTok{,}
                              \AttributeTok{meanstructure =}\NormalTok{ T,}
                              \AttributeTok{sample.nobs=}\DecValTok{500}\NormalTok{,}
                              \AttributeTok{standardized=}\ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\#used to retrieve column indices used in the rescaling script below}
\NormalTok{col\_index }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(iBel))}

\CommentTok{\#The code below loops through each column of the dataframe and assigns the scaling accordingly}
\CommentTok{\#Rows 1 thru 25 are the iBelong scale}
\CommentTok{\#Rows 26 thru 37 are the General Belonging Scale}
\CommentTok{\#Rows 38 thru 53 are the Collective Self Esteem Scale Race Specific Version}
\CommentTok{\#Rows 54 thru 59 are the Multigroup Etnic Identity measure Revised}


\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(iBel))\{  }
  \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{25}\NormalTok{)\{   }
\NormalTok{    iBel[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(iBel[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{  \}}
    \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{26} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{37}\NormalTok{)\{   }
\NormalTok{    iBel[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(iBel[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{7}\NormalTok{))}
\NormalTok{    \}}
      \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{38} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{53}\NormalTok{)\{  }
\NormalTok{    iBel[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(iBel[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{7}\NormalTok{))}
\NormalTok{      \}}
        \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{54} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{59}\NormalTok{)\{  }
\NormalTok{    iBel[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(iBel[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{        \}}
\NormalTok{\}}

\CommentTok{\#rounding to integers so that the data resembles that which was collected}
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{iBel }\OtherTok{\textless{}{-}}\NormalTok{ iBel }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{round}\NormalTok{(}\DecValTok{0}\NormalTok{) }

\CommentTok{\#quick check of my work}
\CommentTok{\#psych::describe(iBel) }
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either an .rds object (preserves any formatting you might do) or a.csv file (think ``Excel lite'').

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(iBel, \textquotesingle{}iBel.rds\textquotesingle{}) bring back the simulated dat from an}
\CommentTok{\# .rds file iBel \textless{}{-} readRDS(\textquotesingle{}iBel.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(iBel,}
\CommentTok{\# file=\textquotesingle{}iBel.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE) bring}
\CommentTok{\# back the simulated dat from a .csv file iBel \textless{}{-}}
\CommentTok{\# read.csv(\textquotesingle{}iBel.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

Lessons in this volume teach test-retest reliability and invariance testing. In both of these circumstances, data from two different administrations of the focal test are required. For test-retest, these would be completed by the same person; for invariance-testing, these would be completed by different people. Below I have simulated two sets of data for the iBelong Scale \citep{lee_ibelong_2024}. I have named them ``A'' and ``B''. For your homework purposes, you can determine if they will represent testing with the same individual (appropriate for test-retest reliability) or different groups (appropriate for invariance testing).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iBelong\_generating\_modelAB }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}}
\StringTok{        \#measurement model}
\StringTok{        AuthenticityA  =\textasciitilde{} .66*Auth1a + .64*Auth2a + .63*Auth3a + .58*Auth4a + .55*Auth5a}
\StringTok{        ConnectionA =\textasciitilde{} .72*Conn1a + .60*Conn2a + .59*Conn3a + .58*Conn4a + .58*Conn5a}
\StringTok{        HomeA =\textasciitilde{} .64*Home1a + .63*Home2a + .57*Home3a + .57*Home4a + .53*Home5a}
\StringTok{        RECthrivingA =\textasciitilde{} .68*Thriv1a + .66*Thriv2a + .59*Thriv3a + .54*Thriv4a + .49*Thriv5a}
\StringTok{        SelfDefinitionA =\textasciitilde{} .82*Self1a + .65*Self2a + .65*Self3a + .54*Self4a + .49*Self5a}
\StringTok{        AuthenticityB  =\textasciitilde{} .63*Auth1b + .68*Auth2b + .69*Auth3b + .52*Auth4b + .54*Auth5b}
\StringTok{        ConnectionB =\textasciitilde{} .70*Conn1b + .63*Conn2b + .55*Conn3b + .54*Conn4b + .54*Conn5b}
\StringTok{        HomeB =\textasciitilde{} .60*Home1b + .64*Home2b + .51*Home3b + .52*Home4b + .59*Home5b}
\StringTok{        RECthrivingB =\textasciitilde{} .66*Thriv1b + .64*Thriv2b + .52*Thriv3b + .59*Thriv4b + .53*Thriv5b}
\StringTok{        SelfDefinitionB =\textasciitilde{} .75*Self1b + .69*Self2b + .68*Self3b + .57*Self4b + .53*Self5b}

\StringTok{   }
\StringTok{        \#Means}
\StringTok{         AuthenticityA \textasciitilde{} 4.30*1}
\StringTok{         ConnectionA \textasciitilde{} 4.16*1}
\StringTok{         HomeA \textasciitilde{} 4.51*1}
\StringTok{         RECthrivingA \textasciitilde{} 4.81*1}
\StringTok{         SelfDefinitionA \textasciitilde{} 4.53*1}
\StringTok{        }
\StringTok{         AuthenticityB \textasciitilde{} 4.28*1}
\StringTok{         ConnectionB \textasciitilde{} 4.20*1}
\StringTok{         HomeB \textasciitilde{} 4.44*1}
\StringTok{         RECthrivingB \textasciitilde{} 4.9*1}
\StringTok{         SelfDefinitionB \textasciitilde{} 4.49*1}
\StringTok{         }
\StringTok{         \#Correlations}
\StringTok{         AuthenticityA \textasciitilde{}\textasciitilde{} .81*AuthenticityB}
\StringTok{         ConnectionA \textasciitilde{}\textasciitilde{} .85*ConnectionB}
\StringTok{         HomeA \textasciitilde{}\textasciitilde{} .77*HomeB}
\StringTok{         RECthrivingA \textasciitilde{}\textasciitilde{} .85*RECthrivingB}
\StringTok{         SelfDefinitionA \textasciitilde{}\textasciitilde{} .84*SelfDefinitionB}
\StringTok{        }
\StringTok{         \textquotesingle{}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240326}\NormalTok{)}
\NormalTok{iBelAB }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{simulateData}\NormalTok{(}\AttributeTok{model =}\NormalTok{ iBelong\_generating\_modelAB,}
                              \AttributeTok{model.type =} \StringTok{"sem"}\NormalTok{,}
                              \AttributeTok{meanstructure =}\NormalTok{ T,}
                              \AttributeTok{sample.nobs=}\DecValTok{500}\NormalTok{,}
                              \AttributeTok{standardized=}\ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\#used to retrieve column indices used in the rescaling script below}
\NormalTok{col\_index }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(iBelAB))}

\CommentTok{\#The code below loops through each column of the dataframe and assigns the scaling accordingly}
\CommentTok{\#All rows are the iBel scales, administrations A and B}


\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(iBelAB))\{  }
  \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{50}\NormalTok{)\{   }
\NormalTok{    iBelAB[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(iBelAB[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{  \}}
\NormalTok{\}}

\CommentTok{\#rounding to integers so that the data resembles that which was collected}
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{iBelAB }\OtherTok{\textless{}{-}}\NormalTok{ iBelAB }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{round}\NormalTok{(}\DecValTok{0}\NormalTok{) }

\CommentTok{\#quick check of my work}
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(iBelAB) }
\end{Highlighting}
\end{Shaded}

If you want to use the data for invariance testing, the data will need to be in \emph{long} form. Here's code to get it there.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First creating a tiny df with just the GroupA observations Add an}
\CommentTok{\# ID variable to each row}
\NormalTok{iBelA }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(iBelAB, Auth1a, Auth2a, Auth3a, Auth4a, Auth5a,}
\NormalTok{    Conn1a, Conn2a, Conn3a, Conn4a, Conn5a, Home1a, Home2a, Home3a, Home4a,}
\NormalTok{    Home5a, Thriv1a, Thriv2a, Thriv3a, Thriv4a, Thriv5a)}
\CommentTok{\# adding a variable to indicate these are from Group A}
\NormalTok{iBelA}\SpecialCharTok{$}\NormalTok{Group }\OtherTok{\textless{}{-}} \StringTok{"GroupA"}
\CommentTok{\# renaming so that when I bind the A and B dfs they have the same}
\CommentTok{\# variable names}
\NormalTok{iBelA }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{rename}\NormalTok{(iBelA, }\AttributeTok{Auth1 =} \StringTok{"Auth1a"}\NormalTok{, }\AttributeTok{Auth2 =} \StringTok{"Auth2a"}\NormalTok{, }\AttributeTok{Auth3 =} \StringTok{"Auth3a"}\NormalTok{,}
    \AttributeTok{Auth4 =} \StringTok{"Auth4a"}\NormalTok{, }\AttributeTok{Auth5 =} \StringTok{"Auth5a"}\NormalTok{, }\AttributeTok{Conn1 =} \StringTok{"Conn1a"}\NormalTok{, }\AttributeTok{Conn2 =} \StringTok{"Conn2a"}\NormalTok{,}
    \AttributeTok{Conn3 =} \StringTok{"Conn3a"}\NormalTok{, }\AttributeTok{Conn4 =} \StringTok{"Conn4a"}\NormalTok{, }\AttributeTok{Conn5 =} \StringTok{"Conn5a"}\NormalTok{, }\AttributeTok{Home1 =} \StringTok{"Home1a"}\NormalTok{,}
    \AttributeTok{Home2 =} \StringTok{"Home2a"}\NormalTok{, }\AttributeTok{Home3 =} \StringTok{"Home3a"}\NormalTok{, }\AttributeTok{Home4 =} \StringTok{"Home4a"}\NormalTok{, }\AttributeTok{Home5 =} \StringTok{"Home5a"}\NormalTok{,}
    \AttributeTok{Thriv1 =} \StringTok{"Thriv1a"}\NormalTok{, }\AttributeTok{Thriv2 =} \StringTok{"Thriv2a"}\NormalTok{, }\AttributeTok{Thriv3 =} \StringTok{"Thriv3a"}\NormalTok{, }\AttributeTok{Thriv4 =} \StringTok{"Thriv4a"}\NormalTok{,}
    \AttributeTok{Thriv5 =} \StringTok{"Thriv5a"}\NormalTok{)}

\CommentTok{\# Second creating a tiny df with just the GroupB observations}
\NormalTok{iBelB }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(iBelAB, Auth1b, Auth2b, Auth3b, Auth4b, Auth5b,}
\NormalTok{    Conn1b, Conn2b, Conn3b, Conn4b, Conn5b, Home1b, Home2b, Home3b, Home4b,}
\NormalTok{    Home5b, Thriv1b, Thriv2b, Thriv3b, Thriv4b, Thriv5b)}
\CommentTok{\# adding a variable to indicate these are from Group B}
\NormalTok{iBelB}\SpecialCharTok{$}\NormalTok{Group }\OtherTok{\textless{}{-}} \StringTok{"GroupB"}
\CommentTok{\# renaming so that when I bind the A and B dfs they have the same}
\CommentTok{\# variable names}
\NormalTok{iBelB }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{rename}\NormalTok{(iBelB, }\AttributeTok{Auth1 =} \StringTok{"Auth1b"}\NormalTok{, }\AttributeTok{Auth2 =} \StringTok{"Auth2b"}\NormalTok{, }\AttributeTok{Auth3 =} \StringTok{"Auth3b"}\NormalTok{,}
    \AttributeTok{Auth4 =} \StringTok{"Auth4b"}\NormalTok{, }\AttributeTok{Auth5 =} \StringTok{"Auth5b"}\NormalTok{, }\AttributeTok{Conn1 =} \StringTok{"Conn1b"}\NormalTok{, }\AttributeTok{Conn2 =} \StringTok{"Conn2b"}\NormalTok{,}
    \AttributeTok{Conn3 =} \StringTok{"Conn3b"}\NormalTok{, }\AttributeTok{Conn4 =} \StringTok{"Conn4b"}\NormalTok{, }\AttributeTok{Conn5 =} \StringTok{"Conn5b"}\NormalTok{, }\AttributeTok{Home1 =} \StringTok{"Home1b"}\NormalTok{,}
    \AttributeTok{Home2 =} \StringTok{"Home2b"}\NormalTok{, }\AttributeTok{Home3 =} \StringTok{"Home3b"}\NormalTok{, }\AttributeTok{Home4 =} \StringTok{"Home4b"}\NormalTok{, }\AttributeTok{Home5 =} \StringTok{"Home5b"}\NormalTok{,}
    \AttributeTok{Thriv1 =} \StringTok{"Thriv1b"}\NormalTok{, }\AttributeTok{Thriv2 =} \StringTok{"Thriv2b"}\NormalTok{, }\AttributeTok{Thriv3 =} \StringTok{"Thriv3b"}\NormalTok{, }\AttributeTok{Thriv4 =} \StringTok{"Thriv4b"}\NormalTok{,}
    \AttributeTok{Thriv5 =} \StringTok{"Thriv5b"}\NormalTok{)}

\CommentTok{\# Binding the A and B sets of data use this df for the invariance}
\CommentTok{\# homework}
\NormalTok{iBelLONG }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(iBelA, iBelB)}
\end{Highlighting}
\end{Shaded}

\hypertarget{identity-threat}{%
\section{Identity Threat}\label{identity-threat}}

The Identity Threat Scale \citep{george_when_2023}is a measure of threat to identity value, meanings, and enactment. The measure is designed to adapt to different identities. The scale includes 19 items with responses rated on a 6-point Likert scale ranging from 1 (\emph{strongly disagree}) to 5 (\emph{strongly agree}). Items are written such that researchers can provide the focal identity (e.g., ``a teacher,'' ``LGBTQ''). Across all three scales, higher scores reflect higher threat. It appears that the authors landed on a single-order, correlated factors, structure.

Beneath each of the three factors are a list of the items. Variable names follow in parentheses.

\begin{itemize}
\tightlist
\item
  Identity Value:

  \begin{itemize}
  \tightlist
  \item
    I feel that there is a negative value attached to my identity as {[}LGBTQ{]}. (Val1)
  \item
    Being {[}LGBTQ{]} is worth less in the eyes of others than before. (Val2)
  \item
    I feel that others attach a negative value to my identity as {[}LGBTQ{]}. (Val3)
  \item
    I feel that my identity as {[}LGBTQ{]} is devalued by others. (Val4)
  \item
    I feel that others see little value in my identity as {[}LGBTQ{]}.(Val5)
  \end{itemize}
\item
  Identity Meaning:

  \begin{itemize}
  \tightlist
  \item
    I am no longer sure what it means to be {[}LGBTQ{]}.(Mng1)
  \item
    I am questioning what it means to be {[}LGBTQ{]}. (Mng2)
  \item
    I find myself questioning what it means to be {[}LGBTQ{]}. (Mng3)
  \item
    The core of what it means to be {[}LGBTQ{]} is changing in a way I do not like. (Mng4)
  \item
    What it means to be {[}LGBTQ{]} is changing in a way I do not like. (Mng5)
  \item
    What it means to be {[}LGBTQ{]} is being called into question. (Mng6)
  \item
    Being {[}LGBTQ{]} used to mean something different. (Mng7) -I feel that being {[}LGBTQ{]} does not mean the same thing anymore. (Mng8)
  \end{itemize}
\item
  Identity Enactment

  \begin{itemize}
  \tightlist
  \item
    I am limited in my ability to express my identity as {[}LGBTQ{]}. (Enact1)
  \item
    I may no longer be able to engage in activities that express my identity as {[}LGBTQ{]}. (Enact2)
  \item
    I may no longer be able to show that I am {[}LGBTQ{]}. (Enact3)
  \item
    I worry about no longer being able to express my identity as {[}LGBTQ{]}. (Enacte4)
  \item
    I worry that I cannot behave in the way {[}an LGBTQ person{]} behaves. (Enact5)
  \item
    I worry that I cannot show people that I am {[}LGBTQ{]}. (Enact6)
  \end{itemize}
\end{itemize}

Although I have simulated data, the authors posted their data and codebooks from the different phases at the \href{https://osf.io/b5hrt/?view_only=c1f17b92103c47c8abc9fb795056cbb9}{OSF repository}. Additional materials are available as a \href{https://supp.apa.org/psycarticles/supplemental/apl0001114/APL-2022-3613_Supplemental_Materials.pdf}{supplement} to the article.

There were a number of scales utilized in this study. Because including them all caused problems of convergence, beyond the subscales in the focal measure, I have simulated data for only a few more. This will provide practice in evaluating convergent, discriminant, and incremental validity.

\begin{itemize}
\tightlist
\item
  \textbf{Self-esteem} was assessed with \href{https://www.apa.org/obesity-guideline/rosenberg-self-esteem.pdf}{Rosenberg's 10-item scale}. Responses are measured on a 4-point Likert scale ranging from 1 (\emph{strongly agree}) to 4 (\emph{strongly disagree}). Some items are reverse-coded. However, using the data from this simulation presumes that reverse-scoring has already been completed. Higher scores indicate higher self-esteem. An example item is ``On the whole, I am satisfied with myself.'' Factor loadings were obtained from Mullen et al. \citeyearpar{mullen_evaluation_2013}.
\item
  \textbf{Identity suppression} was assessed with 4 items from Madera et al.'s \citeyearpar{madera_bringing_2012} 10 item scale. It is a bit unclear, but it appears that items were rated on a 1 (\emph{strongly disagree}) to 7 (\emph{strongly agree}) scale with higher scores reflecting higher identity suppression. A sample item is, ``I refrain from talking about my identity as LGBTQ at work.''
\item
  \textbf{Workplace microaggressions} was captured with 11 items from Resnick and Galupo's \citep{resnick_assessing_2019} scale. Responses are measured on a Likert type scale ranging from 1 (\emph{never}) to 5 (\emph{a great deal}). Higher scores indicate higher frequency of microaggressions. An example item is ``Having my behaviors mimicked in a joking way due to my LGBTQ identity.'' It is unclear if they used one or more scales. I simulated the data from Resnick and Galupo, selecting 3 and 4 items from three subscales.
\item
  \textbf{Green behaviors} were captured with three items from Norton et al.'s \citep{norton_bridging_2017} scale. Responses are measured on a Likert scale ranging from 1 (\emph{strongly disagree}) to 5 (\emph{strongly agree}). The response stem was, ``Tomorrow, I intend to\ldots{}'' and items included, ``\ldots act in environmentally friendly ways,'' ``\ldots carry out environmentally friendly behaviors at work,'' and ``\ldots perform pro-environmental behaviors while at work. In the absence of factor loadings, I made them up.
\end{itemize}

Because data is collected at the item level (and I want this resource to be as practical as possible, I have simulated the data for each of the scales at the item level. I guided the simulation using factor loadings from the exploratory factor analysis results and means and correlations from the Stage 5 analysis where 516 individuals who identified as LGBTQ completed the survey. Because the simulation will produce ``out-of-bounds'' values, the code below rescales the scores into the range of the Likert-type scaling and rounds them to whole values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Entering the intercorrelations, means, and standard deviations from the journal article}

\NormalTok{IdentityThreat\_generating\_model }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}}
\StringTok{        \#measurement model}
\StringTok{        Value  =\textasciitilde{} .83*Val1 + .78*Val2 + .88*Val3 + .80*Val4 + .73*Val5}
\StringTok{        Meaning =\textasciitilde{} .66*Mng1 + .81*Mng2 + .85*Mng3 + .78*Mng4 + .74*Mng5 + .64*Mng6 + .65*Mng7 + .73*Mng8}
\StringTok{        Enactment =\textasciitilde{} .66*Enact1 + .64*Enact2 + .82*Enact3 + .70*Enact4 + .66*Enact5 + .94*Enact6}
\StringTok{        SelfEsteem =\textasciitilde{} .95*SE1 + .98*SE2 + .66*SE3 + .81*SE4 + .55*SE5 + .80*SE6 + .75*SE7 + .66*SE8 + .78*SE9 + .76*SE10}
\StringTok{        IDsuppression =\textasciitilde{} .78*IDs1 + .67*IDs2 + .82*IDs3 + .90*IDs4}
\StringTok{        WkplcMicroAgg =\textasciitilde{} .84*Micro1 + .87*Micro2 + .72*Micro3 + .78*Micro4 + .67*Micro5 + .81*Micro6 + .63*Micro7 + .50*Micro8 + .58*Micro9 + .83*Micro10 + .63*Micro11}
\StringTok{        GreenBx =\textasciitilde{} .77*G1 + .81*G2 + .83*G3}
\StringTok{       }
\StringTok{        \#Means}
\StringTok{         Value \textasciitilde{} 2.15*1}
\StringTok{         Meaning \textasciitilde{} 2.10*1}
\StringTok{         Enactment \textasciitilde{} 2.17*1}
\StringTok{         SelfEsteem \textasciitilde{} 3.44*1}
\StringTok{         IDsuppression \textasciitilde{} 3.13*1}
\StringTok{         WkplcMicroAgg \textasciitilde{} 1.24*1}
\StringTok{         GreenBx \textasciitilde{} 3.49*1}
\StringTok{      }
\StringTok{        \#Correlations}
\StringTok{         Value \textasciitilde{}\textasciitilde{} .58*Meaning}
\StringTok{         Value \textasciitilde{}\textasciitilde{} .73*Enactment}
\StringTok{         Value \textasciitilde{}\textasciitilde{} {-}.21*SelfEsteem}
\StringTok{         Value \textasciitilde{}\textasciitilde{} .35*IDsuppression}
\StringTok{         Value \textasciitilde{}\textasciitilde{} .41*WkplcMicroAgg}
\StringTok{         Value \textasciitilde{}\textasciitilde{} {-}.09*GreenBx}
\StringTok{         }
\StringTok{         Meaning \textasciitilde{}\textasciitilde{} .60*Enactment}
\StringTok{         Meaning \textasciitilde{}\textasciitilde{} .63*SelfEsteem}
\StringTok{         Meaning \textasciitilde{}\textasciitilde{} .26*IDsuppression}
\StringTok{         Meaning \textasciitilde{}\textasciitilde{} .24*WkplcMicroAgg}
\StringTok{         Meaning \textasciitilde{}\textasciitilde{} {-}.08*GreenBx}
\StringTok{         }
\StringTok{         Enactment \textasciitilde{}\textasciitilde{} {-}.31*SelfEsteem}
\StringTok{         Enactment \textasciitilde{}\textasciitilde{} .54*IDsuppression}
\StringTok{         Enactment \textasciitilde{}\textasciitilde{} .24*WkplcMicroAgg}
\StringTok{         Enactment \textasciitilde{}\textasciitilde{} {-}.06*GreenBx}
\StringTok{         }
\StringTok{         SelfEsteem \textasciitilde{}\textasciitilde{} {-}.24*IDsuppression}
\StringTok{         SelfEsteem \textasciitilde{}\textasciitilde{} {-}.06*WkplcMicroAgg}
\StringTok{         SelfEsteem \textasciitilde{}\textasciitilde{} .15*GreenBx}
\StringTok{         }
\StringTok{         IDsuppression \textasciitilde{}\textasciitilde{} .00*WkplcMicroAgg}
\StringTok{         IDsuppression \textasciitilde{}\textasciitilde{} {-}.02*GreenBx}

\StringTok{        \textquotesingle{}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240330}\NormalTok{)}
\NormalTok{IdThreat }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{simulateData}\NormalTok{(}\AttributeTok{model =}\NormalTok{ IdentityThreat\_generating\_model,}
                              \AttributeTok{model.type =} \StringTok{"sem"}\NormalTok{,}
                              \AttributeTok{meanstructure =}\NormalTok{ T,}
                              \AttributeTok{sample.nobs=}\DecValTok{516}\NormalTok{,}
                              \AttributeTok{standardized=}\ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\#used to retrieve column indices used in the rescaling script below}
\NormalTok{col\_index }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(IdThreat))}

\CommentTok{\#The code below loops through each column of the dataframe and assigns the scaling accordingly}
\CommentTok{\#Rows 1 thru 19 are the Identity Threat}
\CommentTok{\#Rows 20 thru 29 are Self Esteem}
\CommentTok{\#Rows 30 thru 33 are Identity Suppression}
\CommentTok{\#Rows 34 thru 44 are Workplace Microaggessions}
\CommentTok{\#Rows 45 thru 47 are Green Behaviors}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(IdThreat))\{  }
  \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{19}\NormalTok{)\{   }
\NormalTok{   IdThreat[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(IdThreat[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{7}\NormalTok{))}
\NormalTok{  \}}
    \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{20} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{29}\NormalTok{)\{   }
\NormalTok{    IdThreat[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(IdThreat[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\NormalTok{    \}}
    \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{30} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{33}\NormalTok{)\{   }
\NormalTok{    IdThreat[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(IdThreat[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{7}\NormalTok{))}
\NormalTok{      \}}
    \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{34} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{44}\NormalTok{)\{   }
\NormalTok{    IdThreat[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(IdThreat[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{    \}}
    \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{45} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{47}\NormalTok{)\{   }
\NormalTok{    IdThreat[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(IdThreat[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{    \}}
\NormalTok{\}}

\CommentTok{\#rounding to integers so that the data resembles that which was collected}
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{IdThreat }\OtherTok{\textless{}{-}}\NormalTok{ IdThreat }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{round}\NormalTok{(}\DecValTok{0}\NormalTok{) }

\CommentTok{\#quick check of my work}
\CommentTok{\#psych::describe(IdThreat) }
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either an .rds object (preserves any formatting you might do) or a.csv file (think ``Excel lite'').

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(IdThreat, \textquotesingle{}IdThreat.rds\textquotesingle{}) bring back the simulated dat}
\CommentTok{\# from an .rds file IdThreat \textless{}{-} readRDS(\textquotesingle{}IdThreat.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(IdThreat,}
\CommentTok{\# file=\textquotesingle{}IdThreat.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE)}
\CommentTok{\# bring back the simulated dat from a .csv file IdThreat \textless{}{-}}
\CommentTok{\# read.csv(\textquotesingle{}IdThreat.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

Lessons in this volume teach test-retest reliability and invariance testing. In both of these circumstances, data from two different administrations of the focal test are required. For test-retest, these would be completed by the same person; for invariance-testing, these would be completed by different people. Below I have simulated two sets of data for The Identity Threat Scale \citep{george_when_2023}. I have named them ``A'' and ``B''. For your homework purposes, you can determine if they will represent testing with the same individual (appropriate for test-retest reliability) or different groups (appropriate for invariance testing).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{idThreat\_generating\_modelAB }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}}
\StringTok{        \#measurement model}
\StringTok{        ValueA  =\textasciitilde{} .83*Val1a + .78*Val2a + .88*Val3a + .80*Val4a + .73*Val5a}
\StringTok{        MeaningA =\textasciitilde{} .66*Mng1a + .81*Mng2a + .85*Mng3a + .78*Mng4a + .74*Mng5a + .64*Mng6a + .65*Mng7a + .73*Mng8a}
\StringTok{        EnactmentA =\textasciitilde{} .66*Enact1a + .64*Enact2a + .82*Enact3a + .70*Enact4a + .66*Enact5a + .94*Enact6a}
\StringTok{        ValueB  =\textasciitilde{} .87*Val1b + .72*Val2b + .85*Val3b + .76*Val4b + .70*Val5b}
\StringTok{        MeaningB =\textasciitilde{} .69*Mng1b + .89*Mng2b + .82*Mng3b + .72*Mng4b + .78*Mng5b + .60*Mng6b + .69*Mng7b + .71*Mng8b}
\StringTok{        EnactmentB =\textasciitilde{} .61*Enact1b + .62*Enact2b + .75*Enact3b + .75*Enact4b + .69*Enact5b + .82*Enact6b}

\StringTok{        \#Means}
\StringTok{         ValueA \textasciitilde{} 1.97*1}
\StringTok{         MeaningA \textasciitilde{} 2.16*1}
\StringTok{         EnactmentA \textasciitilde{} 2.14*1}
\StringTok{         ValueB \textasciitilde{} 2.10*1}
\StringTok{         MeaningB \textasciitilde{} 2.28*1}
\StringTok{         EnactmentB \textasciitilde{} 2.24*1}
\StringTok{         }
\StringTok{         }
\StringTok{         \#Correlations}
\StringTok{         ValueA \textasciitilde{}\textasciitilde{} .80*MeaningA}
\StringTok{         ValueA \textasciitilde{}\textasciitilde{} .80*EnactmentA}
\StringTok{         ValueA \textasciitilde{}\textasciitilde{} .63*ValueB}
\StringTok{         ValueA \textasciitilde{}\textasciitilde{} .57*MeaningB}
\StringTok{         ValueA \textasciitilde{}\textasciitilde{} .56*EnactmentB}
\StringTok{         }
\StringTok{         MeaningA \textasciitilde{}\textasciitilde{} .77*EnactmentA}
\StringTok{         MeaningA \textasciitilde{}\textasciitilde{} .59*ValueB}
\StringTok{         MeaningA \textasciitilde{}\textasciitilde{} .70*MeaningB}
\StringTok{         MeaningA \textasciitilde{}\textasciitilde{} .58*EnactmentB}
\StringTok{         }
\StringTok{         EnactmentA \textasciitilde{}\textasciitilde{} .54*ValueB}
\StringTok{         EnactmentA \textasciitilde{}\textasciitilde{} .79*MeaningB}
\StringTok{         EnactmentA \textasciitilde{}\textasciitilde{} .66*EnactmentB}
\StringTok{         }
\StringTok{         ValueB \textasciitilde{}\textasciitilde{} .81*MeaningB}
\StringTok{         ValueB \textasciitilde{}\textasciitilde{} .82*EnactmentB}
\StringTok{         }
\StringTok{         MeaningB \textasciitilde{}\textasciitilde{} .81*EnactmentB}
\StringTok{        }
\StringTok{         \textquotesingle{}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240326}\NormalTok{)}
\NormalTok{idThreatAB }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{simulateData}\NormalTok{(}\AttributeTok{model =}\NormalTok{ idThreat\_generating\_modelAB,}
                              \AttributeTok{model.type =} \StringTok{"sem"}\NormalTok{,}
                              \AttributeTok{meanstructure =}\NormalTok{ T,}
                              \AttributeTok{sample.nobs=}\DecValTok{500}\NormalTok{,}
                              \AttributeTok{standardized=}\ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\#used to retrieve column indices used in the rescaling script below}
\NormalTok{col\_index }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(idThreatAB))}

\CommentTok{\#The code below loops through each column of the dataframe and assigns the scaling accordingly}
\CommentTok{\#All rows are the iBel scales, administrations A and B}


\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(idThreatAB))\{  }
  \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{38}\NormalTok{)\{   }
\NormalTok{    idThreatAB[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(idThreatAB[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{  \}}
\NormalTok{\}}

\CommentTok{\#rounding to integers so that the data resembles that which was collected}
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{idThreatAB }\OtherTok{\textless{}{-}}\NormalTok{ idThreatAB }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{round}\NormalTok{(}\DecValTok{0}\NormalTok{) }

\CommentTok{\#quick check of my work}
\CommentTok{\#psych::describe(iBelAB) }
\end{Highlighting}
\end{Shaded}

If you want to use the data for invariance testing, the data will need to be in \emph{long} form. Here's code to get it there.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First creating a tiny df with just the GroupA observations Add an}
\CommentTok{\# ID variable to each row}
\NormalTok{IdThreatA }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(idThreatAB, Val1a, Val2a, Val3a, Val4a, Val5a,}
\NormalTok{    Mng1a, Mng2a, Mng3a, Mng4a, Mng5a, Mng6a, Mng7a, Mng8a, Enact1a, Enact2a,}
\NormalTok{    Enact3a, Enact4a, Enact5a, Enact6a)}
\CommentTok{\# adding a variable to indicate these are from Group A}
\NormalTok{IdThreatA}\SpecialCharTok{$}\NormalTok{Group }\OtherTok{\textless{}{-}} \StringTok{"GroupA"}
\CommentTok{\# renaming so that when I bind the A and B dfs they have the same}
\CommentTok{\# variable names}
\NormalTok{IdThreatA }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{rename}\NormalTok{(IdThreatA, }\AttributeTok{Val1 =} \StringTok{"Val1a"}\NormalTok{, }\AttributeTok{Val2 =} \StringTok{"Val2a"}\NormalTok{, }\AttributeTok{Val3 =} \StringTok{"Val3a"}\NormalTok{,}
    \AttributeTok{Val4 =} \StringTok{"Val4a"}\NormalTok{, }\AttributeTok{Val5 =} \StringTok{"Val5a"}\NormalTok{, }\AttributeTok{Mng1 =} \StringTok{"Mng1a"}\NormalTok{, }\AttributeTok{Mng2 =} \StringTok{"Mng2a"}\NormalTok{, }\AttributeTok{Mng3 =} \StringTok{"Mng3a"}\NormalTok{,}
    \AttributeTok{Mng4 =} \StringTok{"Mng4a"}\NormalTok{, }\AttributeTok{Mng5 =} \StringTok{"Mng5a"}\NormalTok{, }\AttributeTok{Mng6 =} \StringTok{"Mng6a"}\NormalTok{, }\AttributeTok{Mng7 =} \StringTok{"Mng7a"}\NormalTok{, }\AttributeTok{Mng8 =} \StringTok{"Mng8a"}\NormalTok{,}
    \AttributeTok{Enact1 =} \StringTok{"Enact1a"}\NormalTok{, }\AttributeTok{Enact2 =} \StringTok{"Enact2a"}\NormalTok{, }\AttributeTok{Enact3 =} \StringTok{"Enact3a"}\NormalTok{, }\AttributeTok{Enact4 =} \StringTok{"Enact4a"}\NormalTok{,}
    \AttributeTok{Enact5 =} \StringTok{"Enact5a"}\NormalTok{, }\AttributeTok{Enact6 =} \StringTok{"Enact6a"}\NormalTok{)}

\CommentTok{\# Second creating a tiny df with just the GroupB observations Add an}
\CommentTok{\# ID variable to each row}
\NormalTok{IdThreatB }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(idThreatAB, Val1b, Val2b, Val3b, Val4b, Val5b,}
\NormalTok{    Mng1b, Mng2b, Mng3b, Mng4b, Mng5b, Mng6b, Mng7b, Mng8b, Enact1b, Enact2b,}
\NormalTok{    Enact3b, Enact4b, Enact5b, Enact6b)}
\CommentTok{\# adding a variable to indicate these are from Group A}
\NormalTok{IdThreatB}\SpecialCharTok{$}\NormalTok{Group }\OtherTok{\textless{}{-}} \StringTok{"GroupB"}
\CommentTok{\# renaming so that when I bind the A and B dfs they have the same}
\CommentTok{\# variable names}
\NormalTok{IdThreatB }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{rename}\NormalTok{(IdThreatB, }\AttributeTok{Val1 =} \StringTok{"Val1b"}\NormalTok{, }\AttributeTok{Val2 =} \StringTok{"Val2b"}\NormalTok{, }\AttributeTok{Val3 =} \StringTok{"Val3b"}\NormalTok{,}
    \AttributeTok{Val4 =} \StringTok{"Val4b"}\NormalTok{, }\AttributeTok{Val5 =} \StringTok{"Val5b"}\NormalTok{, }\AttributeTok{Mng1 =} \StringTok{"Mng1b"}\NormalTok{, }\AttributeTok{Mng2 =} \StringTok{"Mng2b"}\NormalTok{, }\AttributeTok{Mng3 =} \StringTok{"Mng3b"}\NormalTok{,}
    \AttributeTok{Mng4 =} \StringTok{"Mng4b"}\NormalTok{, }\AttributeTok{Mng5 =} \StringTok{"Mng5b"}\NormalTok{, }\AttributeTok{Mng6 =} \StringTok{"Mng6b"}\NormalTok{, }\AttributeTok{Mng7 =} \StringTok{"Mng7b"}\NormalTok{, }\AttributeTok{Mng8 =} \StringTok{"Mng8b"}\NormalTok{,}
    \AttributeTok{Enact1 =} \StringTok{"Enact1b"}\NormalTok{, }\AttributeTok{Enact2 =} \StringTok{"Enact2b"}\NormalTok{, }\AttributeTok{Enact3 =} \StringTok{"Enact3b"}\NormalTok{, }\AttributeTok{Enact4 =} \StringTok{"Enact4b"}\NormalTok{,}
    \AttributeTok{Enact5 =} \StringTok{"Enact5b"}\NormalTok{, }\AttributeTok{Enact6 =} \StringTok{"Enact6b"}\NormalTok{)}

\CommentTok{\# Binding the A and B sets of data use this df for the invariance}
\CommentTok{\# homework}
\NormalTok{IdThreatLONG }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(IdThreatA, IdThreatB)}
\end{Highlighting}
\end{Shaded}

\hypertarget{anti-racism-behavioral-inventory}{%
\section{Anti-Racism Behavioral Inventory}\label{anti-racism-behavioral-inventory}}

The Anti-Racism Behavioral Inventory \citep{pieterse_anti-racism_2022} is a measure that assesses anti-racism awareness and behavior among white Americans. The scale includes 21 items with responses rated on a 5-point Likert scale ranging from 1 (\emph{strongly disagree}) to 5 (\emph{strongly agree}). Higher scores indicate a greater anti-racist awareness and behavior.

Beneath each of the five factors are a list of the items. Variable names follow in parentheses.

\begin{itemize}
\tightlist
\item
  Individual Advocacy:

  \begin{itemize}
  \tightlist
  \item
    When I hear people telling racist jokes and using negative racial stereotypes, I usually confront them (Advoc1)
  \item
    I actively seek to understand how I participate in both intentional and unintentional racism (Advoc2)
  \item
    I actively seek to educate myself about the experience of racism (Advoc3)
  \item
    I interrupt racist conversations and jokes when I heard my friends talking that way (Advoc4)
  \item
    I have challenged acts of racism that I have witnessed in my workplace or at school (Advoc5)
  \item
    I make it a point to educate myself about the experience of historically oppressed groups in the US (e.g.~slavery, internment of Japanese, American Indians, and the trail of tears etc.) (Advoc6)
  \item
    I often speak to my friends about the problem of racism in the US, and what we can do about it (Advoc7)
  \item
    I do not like to talk about racism in public (Advoc8) NEEDS REVERSING
  \item
    I interrupt racist conversations and jokes when I hear them in my family (Advoc9)
  \end{itemize}
\item
  Awareness of Racism:

  \begin{itemize}
  \tightlist
  \item
    I feel guilty and ashamed when I think of the history of racism and slavery in the US (Aware1)
  \item
    It bothers me that my country has yet to acknowledge the impact of slavery (Aware2)
  \item
    The US should offer some type of payment to the descendants of slaves (Aware3)
  \item
    The US has not acknowledged the impact of slavery (Aware4)
  \item
    Because of racism in the US, Blacks do not have the same educational opportunities as compared to Whites (Aware5)
  \item
    Within the US, racism is largely perpetuated by the White racial majority (Aware6)
  \item
    The police unfairly target Black men and Latinos (Aware7)
  \end{itemize}
\item
  Institutional Advocacy

  \begin{itemize}
  \tightlist
  \item
    I give money to organizations working against racism and discrimination (Inst1)
  \item
    When I read articles in newspapers or magazines that are perpetuating racist ideas, I generally write a letter to the editor (Inst2)
  \item
    I am actively involved in exposing companies that uphold exclusionary and racist practices (Inst3)
  \item
    I write letters to local and state politicians to voice my concerns about racism (Inst4)
  \item
    I volunteer with anti-racist or racial justice organizations (Inst5)
  \end{itemize}
\end{itemize}

This article was published in the open access Journal for Social Action in Counseling and Psychology and is available at its \href{https://openjournals.bsu.edu/jsacp/article/view/3679}{website}. Below is the script for simulating item-level data from the factor loadings, means, and sample size presented in the published article.

Four additional scales were reported in the Pieterse et al. \citeyearpar{pieterse_anti-racism_2022} article. Unfortunately, I could not locate factor loadings for all of them; in these cases, I uniformly assigned the factor loading of 0.8. The scales, their original citation, and information about how I simulated data for each are listed below.

\begin{itemize}
\tightlist
\item
  \textbf{White Privilege} was assessed using a 13-item subscale from The Privilege and Oppression Inventory** \citep{hays_initial_2007}. Items are rated on a Likert scale ranging from 1 (\emph{strongly disagree}) to 6 (\emph{strongly agree}). An example item is, ``Whites have the power to exclude other groups.'' Higher scores indicate greater levels of White privilege.
\item
  \textbf{Color Blind Racial Attitudes} \citep{neville_construction_2000} is a 21-item scale that assesses implicit racism-related attitudes. The 21 items are assessed on a Likert scale ranging from 1 to 5 with higher scores reflecting greater levels of color-blind racial attitudes. I was able to simulate two of the three subscales of the instrument: Racial Privilege (RP) and Blatant Racial Issues (BR).
\end{itemize}

Because data is collected at the item level (and I want this resource to be as practical as possible, I have simulated the data for each of the scales at the item level. Simulating the data involved using factor loadings, means, and correlations between the scales. Where possible, factor loadings are retrieved from original (or replication) articles that report on their psychometric evaluation. The means and correlations are from Pietierse et al. \citep{pieterse_anti-racism_2022} Because the simulation will produce ``out-of-bounds'' values, the code below rescales the scores into the range of the Likert-type scaling and rounds them to whole values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Entering the intercorrelations, means, and standard deviations from the journal article}

\NormalTok{Antiracism\_generating\_model }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}}
\StringTok{        \#measurement model}
\StringTok{        ARBI =\textasciitilde{} .274*Advoc1 + .699*Advoc2 + .752*Advoc3 + .465*Advoc4 + .479*Advoc5 + .617*Advoc6 + .672*Advoc7 + .455*Advoc8 + .420*Advoc9 + .135*Aware1 + .568*Aware2 + .627*Aware3 + .566*Aware4 + .660*Aware5 + .513*Aware6 + .565*Aware7 + .536*Inst1 + .116*Inst2 + .408*Inst3 + .295*Inst4 + .638*Inst5}
\StringTok{        Advocacy  =\textasciitilde{} .618*Advoc1 + .055*Advoc2 + .035*Advoc3 + .712*Advoc4 + .275*Advoc5 + .124*Advoc6 + .090*Advoc7 + {-}.250*Advoc8 + .623*Advoc9}
\StringTok{        Awareness =\textasciitilde{} .211*Aware1 + .669*Aware2 + .428*Aware3 + .692*Aware4 + .445*Aware5 + .505*Aware6 + .528*Aware7}
\StringTok{        Institutional =\textasciitilde{} .381*Inst1 + .547*Inst2 + .648*Inst3 + .618*Inst4 + .392*Inst5}
\StringTok{        WhitePriv =\textasciitilde{} .80*WP1 + .59*WP2 + .62*WP3 + .65*WP4 + .80*WP5 + .50*WP6 + .78*WP7 + .57*WP8 + .64*WP9 + .49*WP10 + .64*WP11 + .61*WP12 + .40*WP13}
\StringTok{        CoBRAS\_RP =\textasciitilde{} .68*RP1 + .66*RP2 + .64*RP3 + .62*RP4 + .62*RP5 + .58*RP6 + .54*RP7}
\StringTok{        CoBRAS\_BR =\textasciitilde{} .77*BR1 + .67*BR2 + .65*BR3 + .65*BR4 + .62*BR5 + .55*BR6 + .48*BR7}
\StringTok{        }
\StringTok{        \#Means}
\StringTok{         ARBI \textasciitilde{} 3.91*1}
\StringTok{         Advocacy \textasciitilde{}5.33*1}
\StringTok{         Awareness \textasciitilde{} 4.26*1}
\StringTok{         Institutional \textasciitilde{} 1.66*1}
\StringTok{         WhitePriv \textasciitilde{} 5.20*1}
\StringTok{         CoBRAS\_RP \textasciitilde{} 2.54*1}
\StringTok{         CoBRAS\_BR \textasciitilde{} 1.65*1}
\StringTok{         }
\StringTok{        \#Correlations}
\StringTok{         ARBI \textasciitilde{}\textasciitilde{} .78*Awareness}
\StringTok{         ARBI \textasciitilde{}\textasciitilde{} .87*Advocacy}
\StringTok{         ARBI \textasciitilde{}\textasciitilde{} .75*Institutional}
\StringTok{         ARBI \textasciitilde{}\textasciitilde{} .64*WhitePriv}
\StringTok{         ARBI \textasciitilde{}\textasciitilde{} {-}.65*CoBRAS\_RP}
\StringTok{         ARBI \textasciitilde{}\textasciitilde{} {-}.60*CoBRAS\_BR}
\StringTok{         }
\StringTok{         Awareness \textasciitilde{}\textasciitilde{} .50*Advocacy}
\StringTok{         Awareness \textasciitilde{}\textasciitilde{} .34*Institutional}
\StringTok{         Awareness \textasciitilde{}\textasciitilde{} .81*WhitePriv}
\StringTok{         Awareness \textasciitilde{}\textasciitilde{} {-}.78*CoBRAS\_RP}
\StringTok{         Awareness \textasciitilde{}\textasciitilde{} {-}.62*CoBRAS\_BR}
\StringTok{         }
\StringTok{         Advocacy \textasciitilde{}\textasciitilde{} .55*Institutional}
\StringTok{         Advocacy \textasciitilde{}\textasciitilde{} .43*WhitePriv}
\StringTok{         Advocacy \textasciitilde{}\textasciitilde{} {-}.45*CoBRAS\_RP}
\StringTok{         Advocacy \textasciitilde{}\textasciitilde{} {-}.46*CoBRAS\_BR}

\StringTok{         Institutional \textasciitilde{}\textasciitilde{} .27*WhitePriv}
\StringTok{         Institutional \textasciitilde{}\textasciitilde{} {-}.31*CoBRAS\_RP}
\StringTok{         Institutional \textasciitilde{}\textasciitilde{} {-}.34*CoBRAS\_BR}
\StringTok{         }
\StringTok{         WhitePriv \textasciitilde{}\textasciitilde{} {-}.85*CoBRAS\_RP}
\StringTok{         WhitePriv \textasciitilde{}\textasciitilde{} {-}.6*CoBRAS\_BR}
\StringTok{         }
\StringTok{         CoBRAS\_RP \textasciitilde{}\textasciitilde{} .60*CoBRAS\_BR}
\StringTok{         \textquotesingle{}}
         
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240218}\NormalTok{)}
\NormalTok{AntiR }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{simulateData}\NormalTok{(}\AttributeTok{model =}\NormalTok{ Antiracism\_generating\_model,}
                              \AttributeTok{model.type =} \StringTok{"sem"}\NormalTok{,}
                              \AttributeTok{meanstructure =}\NormalTok{ T,}
                              \AttributeTok{sample.nobs=}\DecValTok{153}\NormalTok{,}
                              \AttributeTok{standardized=}\ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\#used to retrieve column indices used in the rescaling script below}
\NormalTok{col\_index }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(AntiR))}

\CommentTok{\#The code below loops through each column of the dataframe and assigns the scaling accordingly}
\CommentTok{\#Rows 1 thru 21 are the ARBI }
\CommentTok{\#Rows 22 thru 34 are the White Privilege Scale}
\CommentTok{\#Rows 35 thru 48 are the CoBRAS}


\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(AntiR))\{  }
  \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{21}\NormalTok{)\{   }
\NormalTok{    AntiR[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(AntiR[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{  \}}
    \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{22} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{34}\NormalTok{)\{   }
\NormalTok{    AntiR[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(AntiR[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{    \}}
      \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{35} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{48}\NormalTok{)\{  }
\NormalTok{    AntiR[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(AntiR[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{      \}}
\NormalTok{\}}

\CommentTok{\#rounding to integers so that the data resembles that which was collected}
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{AntiR }\OtherTok{\textless{}{-}}\NormalTok{ AntiR }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{round}\NormalTok{(}\DecValTok{0}\NormalTok{) }

\CommentTok{\#quick check of my work}
\CommentTok{\#psych::describe(AntiR) }

\CommentTok{\#Reversing the supportive item on the Perceptions of LGBTQ Campus Climate Scale so that the exercises will be consistent with the format in which the data was collected}
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either an .rds object (preserves any formatting you might do) or a.csv file (think ``Excel lite'').

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(AntiR, \textquotesingle{}AntiR.rds\textquotesingle{}) bring back the simulated dat from}
\CommentTok{\# an .rds file AntiR \textless{}{-} readRDS(\textquotesingle{}AntiR.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(AntiR,}
\CommentTok{\# file=\textquotesingle{}AntiR.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE) bring}
\CommentTok{\# back the simulated dat from a .csv file AntiR \textless{}{-}}
\CommentTok{\# read.csv(\textquotesingle{}AntiR.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

Lessons in this volume teach test-retest reliability and invariance testing. In both of these circumstances, data from two different administrations of the focal test are required. For test-retest, these would be completed by the same person; for invariance-testing, these would be completed by different people. Below I have simulated two sets of data for the Anti-Racism Behavioral Inventory \citep{pieterse_anti-racism_2022}. I have named them ``A'' and ``B''. For your homework purposes, you can determine if they will represent testing with the same individual (appropriate for test-retest reliability) or different groups (appropriate for invariance testing).

The Pieterse et al.\citeyearpar{pieterse_anti-racism_2022} article did not report invariance testing nor test-retest reliabilities. Therefore, I simulated this data following the general patterns of such testing.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ARBI\_generating\_modelAB }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}}
\StringTok{        \#measurement model}
\StringTok{        AdvocacyA  =\textasciitilde{} .450*Advoc1a + .671*Advoc2a + .715*Advoc3a + .650*Advoc4a + .548*Advoc5a + .634*Advoc6a + .593*Advoc7a + .532*Advoc8a + .592*Advoc9a}
\StringTok{        AwarenessA =\textasciitilde{} .239*Aware1a + .836*Aware2a + .754*Aware3a + .877*Aware4a + .791*Aware5a + .727*Aware6a + .782*Aware7a}
\StringTok{        InstitutionalA =\textasciitilde{} .698*Inst1a + .397*Inst2a + .680*Inst3a + .589*Inst4a + .798*Inst5a2293*Advoc7b + 532*Advoc8b + .564*Advoc9b}
\StringTok{        AdvocacyB =\textasciitilde{} .378*Advoc1b + .666*Advoc2b + .683*Advoc3b + .555*Advoc4b + .672*Advoc5b + .822*Advoc6b + .511*Advoc7b + .488*Advoc8b + .513*Advoc9b}
\StringTok{        AwarenessB =\textasciitilde{} .253*Aware1b + .800*Aware2b + .721*Aware3b + .842*Aware4b + .701*Aware5b + .797*Aware6b + .742*Aware7b}
\StringTok{        InstitutionalB =\textasciitilde{} .623*Inst1b + .301*Inst2b + .666*Inst3b + .533*Inst4b + .721*Inst5b}

\StringTok{        \#Means}
\StringTok{         AdvocacyA \textasciitilde{}5.33*1}
\StringTok{         AwarenessA \textasciitilde{} 4.26*1}
\StringTok{         InstitutionalA \textasciitilde{} 1.66*1}
\StringTok{         AdvocacyB \textasciitilde{}5.25*1}
\StringTok{         AwarenessB \textasciitilde{} 4.30*1}
\StringTok{         InstitutionalB \textasciitilde{} 1.71*1}
\StringTok{         }
\StringTok{         }
\StringTok{         \#Correlations}
\StringTok{         AdvocacyA \textasciitilde{}\textasciitilde{} .80*AwarenessA}
\StringTok{         AdvocacyA \textasciitilde{}\textasciitilde{} .80*InstitutionalA}
\StringTok{         AdvocacyA \textasciitilde{}\textasciitilde{} .63*AdvocacyB}
\StringTok{         AdvocacyA \textasciitilde{}\textasciitilde{} .57*AwarenessB}
\StringTok{         AdvocacyA \textasciitilde{}\textasciitilde{} .56*InstitutionalB}
\StringTok{         }
\StringTok{         AwarenessA \textasciitilde{}\textasciitilde{} .77*InstitutionalA}
\StringTok{         AwarenessA \textasciitilde{}\textasciitilde{} .59*AdvocacyB}
\StringTok{         AwarenessA \textasciitilde{}\textasciitilde{} .70*AwarenessB}
\StringTok{         AwarenessA \textasciitilde{}\textasciitilde{} .58*InstitutionalB}
\StringTok{         }
\StringTok{         InstitutionalA \textasciitilde{}\textasciitilde{} .54*AdvocacyB}
\StringTok{         InstitutionalA \textasciitilde{}\textasciitilde{} .79*AwarenessB}
\StringTok{         InstitutionalA \textasciitilde{}\textasciitilde{} .66*InstitutionalB}
\StringTok{         }
\StringTok{         AdvocacyB \textasciitilde{}\textasciitilde{} .81*AwarenessB}
\StringTok{         AdvocacyB \textasciitilde{}\textasciitilde{} .82*InstitutionalB}
\StringTok{         }
\StringTok{         AwarenessB \textasciitilde{}\textasciitilde{} .81*InstitutionalB}
\StringTok{        }
\StringTok{         \textquotesingle{}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{240326}\NormalTok{)}
\NormalTok{ARBI\_AB }\OtherTok{\textless{}{-}}\NormalTok{ lavaan}\SpecialCharTok{::}\FunctionTok{simulateData}\NormalTok{(}\AttributeTok{model =}\NormalTok{ ARBI\_generating\_modelAB,}
                              \AttributeTok{model.type =} \StringTok{"sem"}\NormalTok{,}
                              \AttributeTok{meanstructure =}\NormalTok{ T,}
                              \AttributeTok{sample.nobs=}\DecValTok{300}\NormalTok{,}
                              \AttributeTok{standardized=}\ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\#used to retrieve column indices used in the rescaling script below}
\NormalTok{col\_index }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(ARBI\_AB))}

\CommentTok{\#The code below loops through each column of the dataframe and assigns the scaling accordingly}
\CommentTok{\#All rows are the iBel scales, administrations A and B}


\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(ARBI\_AB))\{  }
  \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{\textgreater{}=} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ i }\SpecialCharTok{\textless{}=} \DecValTok{42}\NormalTok{)\{   }
\NormalTok{    ARBI\_AB[,i] }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(ARBI\_AB[,i], }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{  \}}
\NormalTok{\}}

\CommentTok{\#rounding to integers so that the data resembles that which was collected}
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{ARBI\_AB }\OtherTok{\textless{}{-}}\NormalTok{ ARBI\_AB }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{round}\NormalTok{(}\DecValTok{0}\NormalTok{) }

\CommentTok{\#quick check of my work}
\CommentTok{\#psych::describe(iBelAB) }
\end{Highlighting}
\end{Shaded}

If you want to use the data for invariance testing, the data will need to be in \emph{long} form. Here's code to get it there.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First creating a tiny df with just the GroupA observations Add an}
\CommentTok{\# ID variable to each row}
\NormalTok{ARBI\_A }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(ARBI\_AB, Advoc1a, Advoc2a, Advoc3a, Advoc4a, Advoc5a,}
\NormalTok{    Advoc6a, Advoc7a, Advoc8a, Advoc9a, Aware1a, Aware2a, Aware3a, Aware4a,}
\NormalTok{    Aware5a, Aware6a, Aware7a, Inst1a, Inst2a, Inst3a, Inst4a)}
\CommentTok{\# adding a variable to indicate these are from Group A}
\NormalTok{ARBI\_A}\SpecialCharTok{$}\NormalTok{Group }\OtherTok{\textless{}{-}} \StringTok{"GroupA"}
\CommentTok{\# renaming so that when I bind the A and B dfs they have the same}
\CommentTok{\# variable names}
\NormalTok{ARBI\_A }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{rename}\NormalTok{(ARBI\_A, }\AttributeTok{Advoc1 =} \StringTok{"Advoc1a"}\NormalTok{, }\AttributeTok{Advoc2 =} \StringTok{"Advoc2a"}\NormalTok{,}
    \AttributeTok{Advoc3 =} \StringTok{"Advoc3a"}\NormalTok{, }\AttributeTok{Advoc4 =} \StringTok{"Advoc4a"}\NormalTok{, }\AttributeTok{Advoc5 =} \StringTok{"Advoc5a"}\NormalTok{, }\AttributeTok{Advoc6 =} \StringTok{"Advoc6a"}\NormalTok{,}
    \AttributeTok{Advoc7 =} \StringTok{"Advoc7a"}\NormalTok{, }\AttributeTok{Advoc8 =} \StringTok{"Advoc8a"}\NormalTok{, }\AttributeTok{Advoc9 =} \StringTok{"Advoc9a"}\NormalTok{, }\AttributeTok{Aware1 =} \StringTok{"Aware1a"}\NormalTok{,}
    \AttributeTok{Aware2 =} \StringTok{"Aware2a"}\NormalTok{, }\AttributeTok{Aware3 =} \StringTok{"Aware3a"}\NormalTok{, }\AttributeTok{Aware4 =} \StringTok{"Aware4a"}\NormalTok{, }\AttributeTok{Aware5 =} \StringTok{"Aware5a"}\NormalTok{,}
    \AttributeTok{Aware6 =} \StringTok{"Aware6a"}\NormalTok{, }\AttributeTok{Aware7 =} \StringTok{"Aware7a"}\NormalTok{, }\AttributeTok{Inst1 =} \StringTok{"Inst1a"}\NormalTok{, }\AttributeTok{Inst2 =} \StringTok{"Inst2a"}\NormalTok{,}
    \AttributeTok{Inst3 =} \StringTok{"Inst3a"}\NormalTok{, }\AttributeTok{Inst4 =} \StringTok{"Inst4a"}\NormalTok{)}

\CommentTok{\# Second creating a tiny df with just the GroupB observations Add an}
\CommentTok{\# ID variable to each row}
\NormalTok{ARBI\_B }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(ARBI\_AB, Advoc1b, Advoc2b, Advoc3b, Advoc4b, Advoc5b,}
\NormalTok{    Advoc6b, Advoc7b, Advoc8b, Advoc9b, Aware1b, Aware2b, Aware3b, Aware4b,}
\NormalTok{    Aware5b, Aware6b, Aware7b, Inst1b, Inst2b, Inst3b, Inst4b)}
\CommentTok{\# adding a variable to indicate these are from Group A}
\NormalTok{ARBI\_B}\SpecialCharTok{$}\NormalTok{Group }\OtherTok{\textless{}{-}} \StringTok{"GroupB"}
\CommentTok{\# renaming so that when I bind the A and B dfs they have the same}
\CommentTok{\# variable names}
\NormalTok{ARBI\_B }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{rename}\NormalTok{(ARBI\_B, }\AttributeTok{Advoc1 =} \StringTok{"Advoc1b"}\NormalTok{, }\AttributeTok{Advoc2 =} \StringTok{"Advoc2b"}\NormalTok{,}
    \AttributeTok{Advoc3 =} \StringTok{"Advoc3b"}\NormalTok{, }\AttributeTok{Advoc4 =} \StringTok{"Advoc4b"}\NormalTok{, }\AttributeTok{Advoc5 =} \StringTok{"Advoc5b"}\NormalTok{, }\AttributeTok{Advoc6 =} \StringTok{"Advoc6b"}\NormalTok{,}
    \AttributeTok{Advoc7 =} \StringTok{"Advoc7b"}\NormalTok{, }\AttributeTok{Advoc8 =} \StringTok{"Advoc8b"}\NormalTok{, }\AttributeTok{Advoc9 =} \StringTok{"Advoc9b"}\NormalTok{, }\AttributeTok{Aware1 =} \StringTok{"Aware1b"}\NormalTok{,}
    \AttributeTok{Aware2 =} \StringTok{"Aware2b"}\NormalTok{, }\AttributeTok{Aware3 =} \StringTok{"Aware3b"}\NormalTok{, }\AttributeTok{Aware4 =} \StringTok{"Aware4b"}\NormalTok{, }\AttributeTok{Aware5 =} \StringTok{"Aware5b"}\NormalTok{,}
    \AttributeTok{Aware6 =} \StringTok{"Aware6b"}\NormalTok{, }\AttributeTok{Aware7 =} \StringTok{"Aware7b"}\NormalTok{, }\AttributeTok{Inst1 =} \StringTok{"Inst1b"}\NormalTok{, }\AttributeTok{Inst2 =} \StringTok{"Inst2b"}\NormalTok{,}
    \AttributeTok{Inst3 =} \StringTok{"Inst3b"}\NormalTok{, }\AttributeTok{Inst4 =} \StringTok{"Inst4b"}\NormalTok{)}

\CommentTok{\# Binding the A and B sets of data use this df for the invariance}
\CommentTok{\# homework}
\NormalTok{ARBI\_long }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(ARBI\_A, ARBI\_B)}
\end{Highlighting}
\end{Shaded}

\hypertarget{REFS}{%
\chapter*{REFERENCES}\label{REFS}}


  \bibliography{STATSnMETH.bib}

\end{document}
