# Principal Axis Factoring {#PAF}

[Screencasted Lecture Link](lihttps://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=04c108ff-257e-4893-b6c3-adad0038666bnk) 
 
```{r  include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA) #keeps out the hashtags in the knits
options(scipen=999)#eliminates scientific notation
```

This is the second week of *exploratory* principal components analysis (PCA) and factor analysis (EFA).  This time the focus is on actual *factor analysis*. There are numerous approaches. I will be demonstrating principal axis factoring (PAF).

## Navigating this Lesson

There is more than 1.5 hours of lecture.  If you work through the materials with me it would be plan for an additional 1.5 hours.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail.  All original materials are provided at the [Github site](https://github.com/lhbikos/ReC_Psychometrics) that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's [introduction](#ReCintro)

### Learning Objectives

Focusing on this week's materials, make sure you can:

* Distinguish between PCA and EFA on several levels:  
  + recognize PCA and EFA from a path diagram
  + define keywords associated with each:  factor loadings, linear components, describe v. explain.  
* Recognize/define an identity matrix -- what test would you use to diagnose it?
* Recognize/define multicollinearity and singularity -- what test would you use to diagnose it?
* Describe the desired pattern of "loadings" (i.e., the relative weights of an item on its own scale compared to other scales)
* Compare the results from item analysis, PCA, PAF, and omega.

### Planning for Practice

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. Whichever you choose, it would be terrific if you used the same dataframe across as many psychometrics lessons as possible so you can compare the results.

The least complex is to change the random seed and rework the problem demonstrated in the lesson. The results *should* map onto the ones obtained in the lecture. 

The second option involves utilizing one of the simulated datasets available in this OER. Szymanski and Bissonette's [-@szymanski_perceptions_2020]Perceptions of the LGBTQ College Campus Climate Scale: Development and psychometric evaluation was used as the research vignette for the validity, reliability, and item analysis lessons. Although I switched vignettes, the Szymanski and Bissonette example is ready for PAF

As a third option, you are welcome to use data to which you have access and is suitable for PAF These could include other vignettes from this OER, other simualated data, or your own data (presuming you have permissoin to use it). In either case, please plan to:

* Properly format and prepare the data.
* Conduct diagnostic tests to determine the suitability of the data for PAF.
* Conducting tests to guide the decisions about number of factors to extract.
* Conducting orthogonal and oblique extractions (at least two each with different numbers of factors).
* Selecting one solution and preparing an APA style results section (with table and figure).
* Compare your results in light of any other psychometrics lessons where you have used this data (especially the [item analysis](#ItemAnalSurvey) and [PCA](#PCA) lessons).

### Readings & Resources

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

* Revelle, William. (n.d.). Chapter 6: Constructs, components, and factor models. In *An introduction to psychometric theory with applications in R*. Retrieved from https://personality-project.org/r/book/#chapter6
  - pp. 150 to 167.  Stop at "Non-Simple Structure Solutions:  The Simplex and Circumplex."
  - A simultaneously theoretical review of psychometric theory while working with R and data to understand the concepts.
* Revelle, W. (2019). *How To: Use the psych package for Factor Analysis and data reduction*.
  - Treat as reference.  Pages 13 through 24 provide technical information about what we are doing.
* Lewis, J. A., & Neville, H. A. (2015). Construction and initial validation of the Gendered Racial Microaggressions Scale for Black women. *Journal of Counseling Psychology, 62*(2), 289â€“302. https://doi.org/10.1037/cou0000062
  - Our research vignette for this lesson.
### Packages

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.
```{r }
#will install the package if not already installed
#if(!require(psych)){install.packages("psych")}
#if(!require(tidyverse)){install.packages("tidyverse")}
#if(!require(MASS)){install.packages("MASS")}
#if(!require(sjstats)){install.packages("sjstats")}
#if(!require(apaTables)){install.packages("apaTables")}
#if(!require(qualtRics)){install.packages("qualtRics")}
```

## Exploratory Factor Analysis (with a quick contrast to PCA)

Whereas principal components analysis (PCA) is a regression analysis technique, principal factor analysis is "...a latent variable model" [@revelle_william_personality_nodate].

Exploratory factor analysis has a rich history.  In 1904, Spearman used it for a single factor.  In 1947, Thurstone generalized it to multiple factors.  Factor analysis is frequently used and controversial.

Factor analysis and principal components are commonly confused:

**Principal components** 

* linear sums of variables, 
* solved with an eigenvalue or singular decomposition
* represents a $n*n$ matrix in terms of the first *k* components and attempts to reproduce all of the $R$ matrix.
* paths point from the items to a total scale score -- all represented as observed/manifest (square) variables

**Factor analysis** 

* linear sums of unknown factors 
* estimated as best fitting solutions, normally through iterative procedures.
* Controversial because
  + at the *structural* level (i.e., covariance or correlation matrix), there are normally more observed variables than parameters to estimate them and the procedure seeks to find the best fitting solution using ordinary least squares, weighted least squares, or maximum likelihood
  + at the *data* level, the model is indeterminate, although scores can be extimated
  + this leads some to argue for using principal components; but fans of factor analysis suggest that it is useful for theory construction and evaluation
* attempts to model only the *common* part of the matrix, which means all of the off-diagonal elements and the common part of the diagonal (the *communalities*); the *uniquenesses* are the non-common (leftover) part
* Stated another way, the factor model partitions the correlation or covariance matrix into
  +  *common factors*, $FF'$, and
  + that which is *unique*, $U^2$ (the diagonal matrix of *uniquenesses*)
* paths point from latent variable representing the factor (oval) to the items (squares) illustrating that the factor/LV "causes" the item's score

![Comparison of path models for PCA and EFA](images/PAF/PCAvPAF.png)

Our focus today is on the PAF approach to scale construction. By utilizing the same research vignette as in the [PCA lesson](#PCA), we can identify similarities in differences in the approach, results, and interpretation.  Let's first take a look at the workflow for PAF.

## PAF Workflow

Below is a screenshot of the workflow. The original document is located in the [Github site](https://github.com/lhbikos/ReC_Psychometrics) that hosts the ReCentering Psych Stats:  Psychometrics OER. You may find it refreshing that, with the exception of the change from "components" to "factors," the workflow for PCA and PAF are quite similar.

![Image of the workflow for PAF](images/PAF/PAFworkflow.png)

Steps in the process include:

* Creating an items only dataframe where any items are scaled in the same direction (e.g., negatively worded items are reverse-scored).
* Conducting tests that assess the statistical assumptions of PAF to ensure that the data is appropriate for PAF.
* Determining the number of factors (think "subscales") to extract. 
* Conducting the factor extraction -- this process will likely occur iteratively,
  - exploring orthogonal (uncorrelated/independent) and oblique (correlated)factors, and
  - changing the number of factors to extract

Because the intended audience for the ReCentering Psych Stats OER is the scientist-practitioner-advocate, this lesson focuses on the workflow and decisions. As you might guess, the details of PAF can be quite complex. Some important notions to consider that may not be obvious from lesson, are these:

* The values of factor loadings are directly related to the correlation matrix.
  - Although I do not explain this in detail, nearly every analytic step attempts to convey this notion by presenting equivalent analytic options using the raw data and correlation matrix.
* PAF (like PCA and related EFA procecures) is about *dimension reduction*; our goal is fewer factors (think subscales) than there are items.
  - In this lesson's vignette there are 25 items on the scale and we will have 4 subscales.
* As latent variable procedure,PAF is both *exploratory* and *factor analysis.*  This is in contrast to our prior [PCA lesson](#PCA). Recall that PCA is a regression-based model and therefore not "factor analysis."
* Matrix algebra (e.g., using the transpose of a matrix, multiplying matrices together) plays a critical role in the analytic solution.

## Research Vignette

This lesson's research vignette emerges from Lewis and Neville's, Gendered Racial Microaggressions Scale for Black Women [-@lewis_construction_2015]. The article reports on two separate studies that comprised the development, refinement, and psychometric evaluation of two, parallel, versions (stress appraisal, frequency) of scale. We simulate data from the final construction of the stress appraisal version as the basis of the lecture.

Lewis and Neville [-@lewis_construction_2015] reported support for a total scale score (25 items) and four subscales. Below, I list the four subscales, their number of items, and a single example item. At the outset, let me provide a content warning. For those who hold this particular identity (or related identities) the content in the items may be upsetting. In other lessons, I often provide a variable name that gives an indication of the primary content of the item. In the case of the GRMS, I will simply provide an abbreviation of the subscale name and its respective item number. This will allow us to easily inspect the alignment of the item with its intended factor/subscale, and hopefully minimize discomfort. If you are not a member of this particular identity, I encourage you to learn about these microaggressions by reading the article in its entirety.  Please do not ask members of this group to explain why these microaggressions are harmful or ask if they have encountered them.  The four factors, number of items, and sample item are as follows:

* Assumptions of Beauty and Sexual Objectification
  - 10 items
  - "Objectified me based on physical features."
  - Abbreviated in the simulated data as "Obj#"
* Silenced and Marginalized
  - 7 items
  - "Someone has tried to 'put me in my place.'"
  - Abbreviated in the simulated data as "Marg#"
* Strong Black Woman Stereotype
  - 5 items
  - "I have been told that I am too assertive."
  - Abbreviated in the simulated data as "Strong#"
* Angry Black Woman Stereotype
  - 3 items
  - "Someone accused me of being angry when speaking calm."
  - Abbreviated in the simulated data as "Angry#"

Below I walk through the data simulation. This is not an essential portion of the lesson, but I will lecture it in case you are interested. None of the items are negatively worded (relative to the other items), so there is no need to reverse-score any items. This is the same data as used in the [PCA lesson](#PCA). If you have already simulated and exported it, you only need to import it.

```{r }
set.seed(210921)
GRMSmat <- matrix(c(.69, .69, .60, .59, .55, .55, .54, .50, .41, .41, .04, -.15, .06, .12, .20, -.01, -.22, -.02, .02, .12, -.09, .06, .19, -.03, -.13,
                  .07, -.07, .00, .07, -.18, .22, .23, -.01, .03, .02, .93, .81, .69, .67, .61, .58, .54, -.04, -.07, -.04, .00, .19, .00, .04, .08,
                  -.08, -.08, 00, .06, .16, -.06, .08, .16, .22, .23, -.04, .01, -.05, -.11, -.16, .25, .16, .59, .55, .54, .54, .51, -.12, .08, .03,
                  -.06, .03, .16, .01, .05, .09, -.08, -.06, .07, -.03, -.08, .18, .03, .06, .06, -.21, .21, .21, .03, -.06, .26, -.14, .70, .69, .68), ncol=4) #primary factor loadings for the four factors
rownames(GRMSmat) <- c("Obj1", "Obj2", "Obj3", "Obj4", "Obj5", "Obj6", "Obj7", "Obj8", "Obj9", "Obj10", "Marg1", "Marg2", "Marg3", "Marg4", "Marg5", "Marg6", "Marg7", "Strong1", "Strong2", "Strong3", "Strong4", "Strong5", "Angry1", "Angry2", "Angry3") #variable names for the six items
#rownames(Szyf2) <- paste("V", seq(1:6), sep=" ") #prior code I replaced with above
colnames(GRMSmat) <- c("Objectified", "Marginalized", "Strong", "Angry")
GRMSCorMat <- GRMSmat %*% t(GRMSmat) #create the correlation matrix
diag(GRMSCorMat) <- 1
#SzyCorMat #prints the correlation matrix
GRMS_M <- c(1.78,	1.85,	1.97,	1.93,	2.01,	1.76,	1.91,	2.22,	1.83,	1.88, 2,	3.5,	2.43,	3.44,	2.39,	2.89,	2.7, 1.28,	2.25,	1.45,	1.57,	1.4, 2.02,	2.53,	2.39) #item means; I made these up based on the M and SDs for the factors
GRMS_SD <- c(1.11,	1.23,	0.97,	0.85,	1.19,	1.32,	1.04,	0.98,	1.01,	1.03, 1.01,	0.97,	1.32,	1.24,	1.31,	1.42,	1.2, 0.85,	0.94,	0.78,	1.11,	0.84, 1.14,	1.2,	1.21) #item standard deviations; I made these up based on the M and SDs for the factors
GRMSCovMat <- GRMS_SD %*% t(GRMS_SD) * GRMSCorMat #creates a covariance matrix from the correlation matrix
#SzyCovMat #displays the covariance matrix

dfGRMS <- as.data.frame(round(MASS::mvrnorm(n=259, mu = GRMS_M, Sigma = GRMSCovMat, empirical = TRUE),0)) #creates the item level data from the sample size, mean, and covariance matrix
dfGRMS[dfGRMS>5]<-5 #restricts the upperbound of all variables to be 5 or less
dfGRMS[dfGRMS<0]<-0 #resticts the lowerbound of all variable to be 0 or greater
#colMeans(GRMS) #displays column means

#Below is code if you would like and ID number. For this lesson's purposes and ID number would just need to be removed, so I will not include it in the original simulation.
#library(tidyverse)
#dfGRMS <- dfGRMS %>% dplyr::mutate(ID = row_number()) #add ID to each row
#dfGRMS <- dfGRMS%>%dplyr::select(ID, everything())#moving the ID number to the first column; requires
```

Let's take a quick peek at the data to see if everthing looks right.
```{r}
psych::describe(dfGRMS)
```

The optional script below will let you save the simulated data to your computing environment as either a .csv file (think "Excel lite") or .rds object (preserves any formatting you might do).
```{r}
#write the simulated data  as a .csv
#write.table(dfGRMS, file="dfGRMS.csv", sep=",", col.names=TRUE, row.names=FALSE)
#bring back the simulated dat from a .csv file
#dfGRMS <- read.csv ("dfGRMS.csv", header = TRUE)
```

```{r}
#to save the df as an .rds (think "R object") file on your computer; it should save in the same file as the .rmd file you are working with
#saveRDS(dfGRMS, "dfGRMS.rds")
#bring back the simulated dat from an .rds file
#dfGRMS <- readRDS("dfGRMS.rds")
```

## Working the Vignette

It may be useful to recall how we might understand factors in the psychometric sense:  

* clusters of correlated items in an $R$-matrix
* statistical entities that can be plotted as classification axes where coordinates of variables along each axis represent the strength of the relationship between that variable to each factor.
* mathematical equations, resembling regression equations, where each variable is represented according to its relative weight

### Data Prep      

![Image of an excerpt from the workflow](images/PAF/dataprep.png)

Since the first step is data preparation we start each analysis let's start by:

* reverse coding any items that are phrased in the opposite direction 
* creating a *df* (as an object) that only contains the items in their properly scored direction (i.e., you might need to replace the original item with the reverse-coded item); there shoud be no other variables (e.g., ID, demographic variables, other scales) in this df
  - because the GRMS has no items like this we can skip these two steps

Our example today requries no reverse coding and the dataset I simulated has only item-level data (with no ID and no other variables). This means we are ready to start the PAF process.

```{r }
psych::describe(dfGRMS) 
```

Let's take a look at (and make an object of) the correlation matrix.  

```{r }
GRMSr <- cor(dfGRMS) #correlation matrix (with the negatively scored item already reversed) created and saved as object
round(GRMSr, 2)
```

In case you want to examine it in sections (easier to view):

```{r }
#round(GRMSr[,1:8], 2)
#round(GRMSr[,9:16], 2)
#round(GRMSr[,17:25], 2)
```

As with PCA, we can analyze PAF data with either raw data or correlation matrix.  I will do both to demonstrate (a) that it's possible and to (b) continue emphasizing that this is a *structural* analysis.  That is, we are trying to see if our more parsimonious extraction *reproduces* this original correlation matrix.

#### Three Diagnostic Tests to Evaluate the Appropriateness of the Data for Component-or-Factor Analysis      
![Image of an excerpt from the workflow](images/PAF/diagnostix.png)

#### Is my sample adequate for PAF?      

We return to the **KMO** (Kaiser-Meyer-Olkin), an index of *sampling adequacy* that can  be used with the actual sample to let us know if the sample size is sufficient (or if we should collect more data).

Kaiser's 1974 recommendations were:

* bare minimum of .5
* values between .5 and .7 as mediocre
* values between .7 and .8 as good
* values above .9 are superb

We use the *KMO()* function from the *psych* package with either raw or matrix dat.

```{r }
psych::KMO(dfGRMS)
#psych::KMO(GRMSr) #for the KMO function, do not specify sample size if using the matrix form of the data
```

We examine the KMO values for both the overall matrix and the individual items.

At the matrix level, our $KMO = .86$, which falls into Kaiser's definition of *superb*.  

At the item level, the KMO should be > .50.  Variables with values below .5 should be evaluated for exclusion from the analysis (or run the analysis with and without the variable and compare the difference).  Because removing/adding variables impacts the KMO, be sure to re-evaluate.

At the item level, our KMO values range between .72 (Angry1) and .92 (Obj6, Obj7). 

Considering both item- and matrix- levels, we conclude that the sample size and the data are adequate for component-or-factor analysis.

#### Are the correlations among the variables large enough to be analyzed?      

**Bartlett's** lets us know if a matrix is an *identity matrix.* In an identity matrix, then all correlation coefficients (everything on the off-diagonal) would be 0.0 (and everything on the diagonal would be 1.0).  

A significant Barlett's (i.e., $p < .05$) tells that the $R$-matrix is not an identity matrix.  That is, there are some relationships between variables that can be analyzed.

The *cortest.bartlett()* function in the *psych* package and can be run either from the raw data or R matrix formats.

```{r }
psych::cortest.bartlett(dfGRMS) #from the raw data
#raw data produces the warning "R was not square, finding R from data." This means nothing other than we fed it raw data and the function is creating a matrix from which to do the analysis.

#psych::cortest.bartlett(GRMSr, n = 259) #if using the matrix, must specify sample size
```

Our Bartlett's test is significant:  $\chi ^{1}(300)=1683.76, p < .001$. This supports a component-or-factor analytic approach for investigating the data.

#### Is there multicollinearity or singularity in my data?      

The **determinant of the correlation matrix** should be greater than 0.00001 (that would be 4 zeros, then the 1).  If it is smaller than 0.00001 then we may have an issue with *multicollinearity* (i.e., variables that are too highly correlated) or *singularity* (variables that are perfectly correlated).

The determinant function comes from base R.  It is easiest to compute when the correlation matrix is the object.  However, it is also possible to specify the command to work with the raw data.

```{r }
#det(GRMSr) 
det(cor(dfGRMS))#if using the raw data
```

With a value of 0.00115, our determinant is greater than the 0.00001 requirement.  If it were not, then we could identify problematic variables (i.e., those correlating too highly with others; those not correlating sufficiently with others) and re-run the diagnostic statitics.

**Summary:**  Data screening were conducted to determine the suitability of the data for this analyses. The Kaiser-Meyer-Olkin measure of sampling adequacy (KMO; Kaiser, 1970) represents the ratio of the squared correlation between variables to the squared partial correlation between variables. KMO ranges from 0.00 to 1.00; values closer to 1.00 indicate that the patterns of correlations are relatively compact and that factor analysis should yield distinct and reliable factors (Field, 2012). In our dataset, the KMO value was .86, indicating acceptable sampling adequacy. The Barlettâ€™s Test of Sphericity examines whether the population correlation matrix resembles an identity matrix (Field, 2012). When the *p* value for the Bartlettâ€™s test is < .05, we are fairly certain we have clusters of correlated variables. In our dataset, $\chi ^{1}(300)=1683.76, p < .001$, indicating the correlations between items are sufficiently large enough for principal axis factoring.  The determinant of the correlation matrix alerts us to any issues of multicollinearity or singularity and should be larger than 0.00001. Our determinant was 0.00115 and, again, indicated that our data was suitable for the analysis.

*Note*:  If this looks familiar.  It is! The same diagnostics are used in PAF and [PCA](#PCA).

### Principal Axis Factoring (PAF)      

![Image of an excerpt from the workflow](images/PAF/NumFactors.png)

We can use the *fa()* function, specifying *fm = "pa"* from the *psych* package with raw or matrix data.

One difference from PCA is that factor analysis will not (cannot) calculate as many factors as there are items.  This means that we should select a reasonable number like 20 (since there are 25 items).  However, I received a number of errors/warnings and 10 is the first number that would run. I also received the warning, "maximum iteration exceeded."  Therefore I increased "max.iter" to 100.

Our goal is to begin to get an idea of the cumulative variance explained and number of factors to extract, so we really only need to identify more than factors we expect to extract.

```{r }
#grmsPAF1 <- psych::fa(GRMSr, nfactors=10, fm = "pa", max.iter = 100, rotate="none")# using the matrix data and specifying the # of factors.

grmsPAF1 <- psych::fa(dfGRMS, nfactors = 10, fm = "pa", max.iter = 100, rotate = "none")# using raw data and specifying the max number of factors

#I received the warning "maximum iteration exceeded". It gave output, but it's best if we don't get that warning, so I increased it to 100. 

grmsPAF1 #this object holds a great deal of information 
```
*The total variance for a particular variable will have two factors:  some of it will be share with other variables (common variance) and some of it will be specific to that measure (unique variance).  Random variance is also specific to one item, but not reliably so.*

We can examine this most easily by examining the matrix (second screen). 

The columns PA1 thru PA10 are the (uninteresting at this point) unrotated loadings.  These are the loading from each factor to each variable. PA stands for "principal axis."

Scrolling to the far right we are interested in:

**Communalities** are represented as $h^2$. These are the proportions of common variance present in the variables.  A variable that has no specific (or random) variance would have a communality of 1.0.  If a variable shares none of its variance with any other variable its communality would be 0.0. As a point of comparison, in PCA these started as 1.0 because we extracted the same number of factors as items. In PAF, because we must extract fewer factors than items, these will have some value.

**Uniquenessess* are represented as $u2$.  These are the amount of unique variance for each variable.  They are calculated as $1 - h^2$ (or 1 minus the communality).  

The final column, *com* represents *item complexity.*  This is an indication of how well an item reflects a single construct. If it is 1.0 then the item loads only on one factor  If it is 2.0, it loads evenly on two factors, and so forth. For now, we can ignore this. *I mostly wanted to reassure you that "com" is not "communality"; h2 is communality*.

Let's switch to the first screen of output.

**Eigenvalues** are displayed in the row called, *SS loadings* (i.e., the sum of squared loadings).  They represent the variance explained by the particular factor. PA1 explains 4.04 units of variance (out of a possible 25; the # of factors).  As a proportion, this is 4.04/25 = 0.16 (reported in the *Proportion Var* row).

PA1 explains 3.51 units of variance (out of a possible 25; the # of potential factors).  As a proportion, this is 3.51/25 = 0.1404 (reported in the *Proportion Var* row).

```{r}
3.51/25
```
Note.

We look at the eigenvalues to see how many are > 1.0 (Kaiser's eigenvalue > 1 criteria criteria).  We see there are 4 that meet Kaiser's critera and 4 that meet Joliffe's criteria (eigenvalues > .77). 

**Cumulative Var** is helpful to determine how many factors we'd like to retain to balance parsimony (few as possible) with the amount of variance we want to explain.  The eigenvalues are in descending order.  Using both Kaiser's (eigenvalue > 1.0) and Joiliffe's (eigenvalue > 0.7) criteria, we landed on a four-factor solution.  Extracting four factors (like we did with PCA will) will explain 40% of the variance. Eigenvalues are only one criteria, let's look at the scree plot.

**Scree plot**:  
Eigenvalues are stored in the *grmsPAF1* object's variable, "values".  We can see all the values captured by this object with the *names()* function:

```{r}
names(grmsPAF1)
```

Plotting the eigen*values* produces a scree plot. We can use this to further guage the number of factors we should extract.
```{r }
plot(grmsPAF1$values, type = "b") #type = "b" gives us "both" lines and points;  type = "l" gives lines and is relatively worthless
```

We look for the point of *inflexion*.  That is, where the baseline levels out into a plateau.  As I noted in the lesson PCA, this is one of the most clear scree plots (suggesting 4 factors) I've seen. We are benefitting by having created the simulated data from the factor results.

#### Specifying the number of factors      

![Image of an excerpt from the workflow](images/PAF/Specify.png)

Having determined the number of factors, we rerun the analysis with this specification. Especially when researchers may not have a clear theoretical structure that guides the process, researchers may do this iteratively with varying numbers of factors. Lewis and Neville [@lewis_construction_2015] examined solutions with 2, 3, 4, and 5 factors (they did a parallel *factor* analysis; this lesson demonstrates principal axis factoring).

```{r }
#grmsPAF2 <- psych::fa(GRMSr, nfactors=4, fm = "pa", rotate="none")
grmsPAF2 <- psych::fa(dfGRMS, nfactors = 4, fm = "pa", rotate = "none") #can copy prior script, but change nfactors and object name
grmsPAF2
```

Our eigenvalues/SS loadings wiggle around a bit from the initial run. With four factors, we now, cumulatively, explain 39% of the variance.  

*Communality* is the proportion of common variance within a variable.  Changing from 10 to 4 factors will change this value ($h2$) as well as its associated *uniqueness* ($u2$), which is calculated as "1.0 minus the communality." 

Now we see that 43% of the variance associate with Obj1 is common/shared (the $h2$ value).

As a reminder of what we are doing, recall that we are looking for a more *parsimonious* explanation than 25 items on the GRMS  By respecifying a smaller number of factors, we lose some information.  That is, the retained factors (now 4) cannot explain all of the variance present in the data (as we saw, it explains about 39%, cumulatively). The amount of variance explained in each variable is represented by the communalities after extraction.

We can also inspect the communalities through the lens of Kaiser's criterion (the eigenvalue > 1 criteria) to see if we think that "four" was a good number of factors to extract.

Kaiser's criterion is believed to be accurate if:

* when there are fewer than 30 variables (we had 25) and, after extraction, the communalities are greater than .70
  + looking at our data, only 1 communality (Marg1) is > .70, so, this does not support extracting four factors
* when the sample size is greater than 250 (ours was 259) and the average communality is > .60
  + we can extract the communalities from our object and calculate the mean the average communality

Using the *names()* function again, we see that "communality" is available for manipulation. 
```{r}
#names(grmsPAF2)
```

We can use this value to calculate their mean.

```{r}
mean(grmsPAF2$communality)
#sum(grmsPAF2$communality) #checking my work by calculating the sum and dividing by 25
#9.836131/25
```

We see that our average communality is 0.39. These two criteria suggest that we may not have the best solution. That said (in our defense):

*  We used the scree plot as a guide and it was very clear.
*  We have an adequate sample size and that was supported with the KMO.
*  Are the number of factors consistent with theory?  We have not yet inspected the factor loadings. This will provide us with more information.

We could do several things:

* rerun with a different number of factors (recall Lewis and Neville [-@lewis_construction_2015] ran models with 2, 3, 4, and 5 factors)
* conduct more diagnostics tests
  + reproduced correlation matrix
  + the difference between the reproduced correlation matrix and the correlation matrix in the data
  
The *factor.model()* function in *psych* produces the *reproduced correlation matrix* by using the *loadings* in our extracted object.  Conceptually, this matrix is the correlations that should be produced if we did not have the raw data but we only had the factor loadings.  We could do fancy matrix algebra and produce these.

The questions, though, is:  How close did we get?  How different is the *reproduced correlation matrix* from *GRMSmatrix* -- the $R$-matrix produced from our raw data.

```{r }
round(psych::factor.model(grmsPAF2$loadings), 3)#produces the reproduced correlation matrix
```

We're not really interested in this matrix.  We just need it to compare it to the *GRMSmatrix* to produce the residuals.  We do that next.

**Residuals** are the difference between the reproduced (i.e., those created from our factor loadings) and $R$-matrix produced by the raw data.  

If we look at the $r_{_{Obj1Obj2}}$ in our original correlation matrix (theoretically from the raw data [although we simulated data]), the value is 0.41.  The reproduced correlation for this pair is 0.439.  The diffference is -0.029.  Our table shows -0.031 because of rounding error.

```{r}
.41 - .439
```

By using the *factor.residuals()* function we can calculate the residuals.  Here we will see this difference calculated for us, for all the elements in the matrix.
```{r }
round(psych::factor.residuals(GRMSr, grmsPAF2$loadings), 3)
```

There are several strategies to evaluate this matrix:

* see how large the residuals are, compared to the original correlations
  + the worst possible model would occur if we extracted no factors and would be the size of the original correlations
  + if the correlations were small to start with, we expect small residuals
  + if the correlations were large to start with, the residuals will be relatively larger (this is not terribly problematic)
* comparing residuals requires squaring them first (because residuals can be both positive and negative)
  + the sum of the squared residuals divided by the sum of the squared correlations is an estimate of model fit.  Subtracting this from 1.0 means that it ranges from 0 to 1.  Values > .95 are an indication of good fit.

Analyzing the residuals means we need to extract only the upper right of the triangle them into an object. We can do this in steps.

```{r }
grmsPAF2_resids <- psych::factor.residuals(GRMSr, grmsPAF2$loadings)#first extract the resids
grmsPAF2_resids <- as.matrix(grmsPAF2_resids[upper.tri(grmsPAF2_resids)])#the object has the residuals in a single column
head(grmsPAF2_resids)
```

One criteria of residual analysis is to see how many residuals there are that are greater than an absolute value of 0.05. The result will be a single column with TRUE if it is > |0.05| and false if it is smaller. The sum function will tell us how many TRUE responses are in the matrix.  Further, we can write script to obtain the proportion of total number of residuals. 

```{r }
large.resid <- abs(grmsPAF2_resids) > 0.05
#large.resid
sum(large.resid)
round(sum(large.resid) / nrow(grmsPAF2_resids),3)
```

We learn that there are 2 residuals greater than the absolute value of 0.05.  This represents less than 1% of the total number of residuals.

There are no hard rules about what proportion of residuals can be greater than 0.05. Field recommends that it stay below 50% [@field_discovering_2012].

Another approach to analyzing residuals is to look at their mean.  Because of the +/- valences, we need to square them (to eliminate the negative), take the average, then take the square root.

```{r }
round(sqrt(mean(grmsPAF2_resids^2)), 3)
```

While there are no clear guidelines to interpret these, one recommendation is to consider extracting more factors if the value is higher than 0.08 [@field_discovering_2012]. 

Finally, we expect our residuals to be normally distributed.  A histogram can help us inspect the distribution.

```{r }
hist(grmsPAF2_resids)
```

Not bad!  It looks reasonably normal.  No outliers.

####  Quick recap of how to evaluate the # of factors we extracted      

* If fewer than 30 variables, the eigenvalue > 1 (Kaiser's) critera is fine, so long as communalities are all > .70.
* If sample size > 250 and the average communalitie are .6 or greater, this is fine.
* When *N* > 200, the scree plot can be used.
* Regarding residuals
  + fewer than 50% should have absolute values > 0.05
  + model fit should be > 0.90
  
### Factor Rotation        

![Image of an excerpt from the workflow](images/PAF/rotate.png)

The original solution of a principal components or principal axes factor analysis is a set of vectors that best account for the observed covariance or correlation matrix.  Each additional component or factor accounts for progressively less and less variance.  The solution is efficient (yay) but difficult to interpret (boo).

Thanks to Thurstone's five rules toward a simple structure (circa 1947), interpretation of a matrix is facilitaed by *rotation* (multiplying a matrix by a matrix of orthogonal vectors that preserve the communalities of each variable).  Both the original matrix and the solution will be orthogonal. 

*Parsimony* becomes a statistical consideration (an equation, in fact) and goal and is maximized when each variable has a 1.0 loading on one factor and the rest are zero.

Different rotation strategies emphasize different goals related to parsimony:

*Quartimax* seeks to maximize the notion of variable parsimony (each variable is associated with one factor) and permits the rotation toward a general factor (ignoring smaller factors).

*Varimax* maximizes the variance of squared loadings taken over items instead of over factors and *avoids* a general factor.

Rotation improves the interpretation of the factor by maximizing the loading on each variable on one of the extracted factors while minimizing the loading on all other factors.  Rotation works by changing the absolute values of the variables while keeping their differential values constant.

There are two big choices (to be made on theoretical grounds):

* Orthogonal rotation if you think that the factors are independent/unrelated.
  + most common orthogonal rotation is varimax
* Oblique rotation if you think that the factors are related correlated.
  + oblimin and promax are common oblique rotations

#### Orthogonal rotation

![Image of an excerpt from the workflow](images/PAF/orthogonal.png)

```{r }
#grmsPAF2ORTH <- psych::fa(GRMSr, nfactors = 4, fm = "pa", rotate = "varimax")
grmsPAF2ORTH <- psych::fa(dfGRMS, nfactors = 4, fm = "pa", rotate = "varimax")
grmsPAF2ORTH
```

Essentially we have the same information as before, except that loadings are calculated after rotation (which adjusts the absolute values of the factor loadings while keeping their differential values constant).  Our communality and uniqueness values remain the same. The eigenvalues (SS loadings) should even out, but the proportion of variance explained and cumulative variance (39%) will remain the same.
  
The *print.psych()* function facilitates interpretation and prioritizes the information about which we care most:

* "cut" will display loadings above .3, this allows us to see
  - if some items load on no factors
  - if some items have cross-loadings (and their relative weights)
* "sort" will reorder the loadings to make it clearer (to the best of its ability...in the case of ties) to which factor/scale it belongs

```{r }
grmsPAF2_table <- psych::print.psych(grmsPAF2ORTH, cut = 0.3, sort = TRUE)
```

In the unrotated solution, most variables loaded on the first factor.  After rotation, there are four clear factors/scales.  Further, there is clear (or at least reasonable) factor/scale membership for each item and no cross-loadings. This is a bit different than the PCA orthogonal rotation where the Marg7 item had a cross-loading.  

If this were a new scale and we had not yet established ideas for subscales, the next step is to look back at the items, themselves, and try to name the scales/factors. If our scale construction included a priori/planned subscales, here's where we hope the items fall where they were hypothesized to do so. Our simulated data worked perfectly and replicated the four scales that Lewis and Neville [@lewis_construction_2015] reported in the article.

* Assumptions of Beauty and Sexual Objectification
* Silenced and Marginalized
* Strong Woman Stereotype
* Angry Woman Stereotype

We can also create a figure of the result. Note the direction of the arrows from the factor (latent variable) to the items in PAF; in PCA the arrows went from item to factor.

```{r }
psych::fa.diagram(grmsPAF2ORTH)
```

We can extract the factor loadings and write them to a table. This can be useful in preparing an APA style table for a manuscript or presentation.

```{r}
#names(grmsPAF2ORTH)
pafORTH_table <- round(grmsPAF2ORTH$loadings,3)
write.table(pafORTH_table, file="pafORTH_table.csv", sep=",", col.names=TRUE, row.names=FALSE)
pafORTH_table
```

#### Oblique rotation

![Image of an excerpt from the workflow](images/PAF/oblique.png)

Whereas the orthogonal rotation sought to maximize the independence/unrelatedness of the coponents, an oblique rotation will allow them to be correlated. Researchers often explore both solutions, but then report only one.

```{r }
#grmsPAF2obl <- psych::fa(GRMSr, nfactors = 4, fm = "pa", rotate = "oblimin")
grmsPAF2obl <- psych::fa(dfGRMS, nfactors = 4, fm = "pa", rotate = "oblimin")
grmsPAF2obl
```

We can make it a little easier to interpret by removing all factor loadings below .30.

```{r }
psych::print.psych(grmsPAF2obl, cut = 0.3, sort=TRUE)
```
All of the items stayed in their respective factors Note, though, that because our specification included "sort=TRUE" that the relative weights wiggled around and so the items are listed in a different order than in the orthogonal rotation.

The oblique rotation allows us to see the correlation between the factors/scales.  This was not available in the orthogonal rotation because the assumption of the orthogonal/varimax rotation is that the scales/factors are uncorrelated; hence in the analysis they were fixed to 0.0.

We can see that all the scales have almost no relation with each other. That is the the correlations range between -0.04 to 0.05.  This is unusual and likely a biproduct of simulating data. It does, though, support the orthogonal rotation as the preferred one.

Of course there is always a little complexity.  In oblique rotations, there is a distinction between the *pattern* matrix (which reports factor loadings and is comparable to the matrix we interpreted for the orthogonal rotation) and the *structure* matrix (takes into account the relationship between the factors/scales -- it is a product of the pattern matrix and the matrix containing the correlation coefficients between the factors/scales).  Most interpret the pattern matrix because it is simpler; however it could be that values in the pattern matrix are suppressed because of relations between the factors.  Therefore, the structure matrix can be a useful check and some editors will request it.

Obtaining the structure matrix requires two steps. First, multiply the factor loadings with the phi matrix.

```{r }
grmsPAF2obl$loadings %*% grmsPAF2obl$Phi
```

Then use Field's [-@field_discovering_2012] function to produce the matrix.
```{r }
#Field's function to produce the structure matrix
factor.structure <- function(fa, cut = 0.2, decimals = 2){
	structure.matrix <- psych::fa.sort(fa$loadings %*% fa$Phi)
	structure.matrix <- data.frame(ifelse(abs(structure.matrix) < cut, "", round(structure.matrix, decimals)))
	return(structure.matrix)
	}
	
factor.structure(grmsPAF2obl, cut = 0.3)
```
Although some of the relative values changed, our items were stable regarding their factor membership.

### Factor Scores

Factor *scores* (PA scores) can be created for each case (row) on each factor (column). These can be used to assess the relative standing of one person on the construct/variable to another.  We can also use them in regression (in place of means or sums) when groups of predictors correlate so highly that there is multicolliearity.

Computation involves multiplying an individual's item-level responses by the factor loadings we obtained through the PAF process.  The results will be one score per factor for each row/case.
 
```{r }
#in all of this, don't forget to be specifiying the datset that has the reverse-coded item replaced 
grmsPAF2obl <- psych::fa(dfGRMS, nfactors = 4, fm = "pa", rotate = "oblimin", scores = TRUE)
head(grmsPAF2obl$scores, 10) #shows us only the first 10 (of N = 2571)
dfGRMS <- cbind(dfGRMS, grmsPAF2obl$scores) #adds them to our raw dataset
```

To bring this full circle, we can see the correlation of the factor scores; the pattern maps onto what we saw previously in the correlations between factors in the oblique rotation.

```{r }
psych::corr.test(dfGRMS [c("PA1", "PA2", "PA3", "PA4")])
```

We can extract the factor loadings and write them to a table. This can be useful in preparing an APA style table for a manuscript or presentation.

```{r}
#names(grmsPAF2obl)
pafOBL_table <- round(grmsPAF2obl$loadings,3)
write.table(pafOBL_table, file="pafOBL_table.csv", sep=",", col.names=TRUE, row.names=FALSE)
pafOBL_table
```

We can also obtain a figure of this PAF with oblique rotation.
```{r }
psych::fa.diagram(grmsPAF2obl)
```

We were lucky in that the we worked the final solution from the research vignette.  The authors told us that they tried 2, 3, 4, and 5 factor solutions, using parallel analysis (instead of PAF). Most researchers will explore varying structures and orthogonal and oblique rotations.

![Image of an excerpt from the workflow](images/PAF/rinse_repeat.png)

## APA Style Results

**Results**

The dimensionality of the 25 items from the Gendered Racial Microagressions Scale for Black Women was analyzed using principal axis factoring. First, data were screened to determine the suitability of the data for this analyses. The Kaiser-Meyer- Olkin measure of sampling adequacy (KMO; Kaiser, 1970) represents the ratio of the squared correlation between variables to the squared partial correlation between variables. KMO ranges from 0.00 to 1.00; values closer to 1.00 indicate that the patterns of correlations are relatively compact and that factor analysis should yield distinct and reliable factors (Field, 2012). In our dataset, the KMO value was .86, indicating acceptable sampling adequacy. The Barlettâ€™s Test of Sphericity examines whether the population correlation matrix resembles an identity matrix (Field, 2012). When the p value for the Bartlettâ€™s test is < .05, we are fairly certain we have clusters of correlated variables. In our dataset, $\chi ^{1}(300)=1683.76, p < .001$, indicating the correlations between items are sufficiently large enough for principal factors analysis.  The determinant of the correlation matrix alerts us to any issues of multicollinearity or singularity and should be larger than 0.00001. Our determinant was 0.00115 and, again, indicated that our data was suitable for the analysis.

Four criteria were used to determine the number of factors to rotate: a priori theory, the scree test, the Eigenvalue-greater-than-one criteria, and the interpretability of the solution. Kaiser's eigenvalue-greater-than-one criteria suggested four factors, and, in combination explained 49% of the variance. The scree plot showed an inflexion that would justified retaining four factors.  Based on the convergence of these decisions, four factors were extracted.  We investigated each with orthogonal (Varimax) and oblique (oblimin) procedures.  Given the non-significant correlations (ranging from -0.04 to 0.05) and the clear factor loadings in the orthogonal rotation, we determined that an orthogonal solution was most appropriate.  

The rotated solution, as shown in Table 1 and Figure 1, yielded four interpretable factors, each listed with the proportion of variance accounted for: assumptions of beauty and sexual objectification (13%), silenced and marginalized (13%), strong woman stereotype (7%), and angry woman stereotype (7%). 

Regarding the Table 1, I would include a table with ALL the values, bolding those with factor membership. This will be easy because we exported all those values to a .csv file. 
  
### Comparing FA and PCA

* FA derives a mathematical solution from which factors are estimated
  + Only FA can estimate underlying factors, but it relies on the various assumptions to be met
* PCA decomposes the original data into a set of linear variates
  + This limits its concern to establishing which factors exist within the data and how a particular variable might contribute to that factor
  
* Generally FA and PCA result in similar solutions
  + When there are 30 or more variables and communalities are > .7 for all variables, different solutions are unlikely (Stevens, 2002)
  + When there are < 20 variables and low communalities (< .4) different solutions are likely to emerge
  + Both are inferential statistics

* Critics of PCA suggest
  + "at best it is a common factor analysis with some error added and at worst an unrecognizable hodgepodge of things from which nothing can be determined" (Cliff, 1987, p. 349)
  + PCA should never be described as FA and the resulting components should not be treated as reverently as true, latent variable, *factors*
  + To most of us (i.e., scientist-practitioner-advocates), the difference is largely from the algorithm used to  drive the solutions.  This is true for Field [@field_discovering_2012] also, who uses the terms interchangeably.  My take:  use whichever you like, just be precise in the language describing what you did.
  
## Going Back to the Future:  What, then, is Omega?

Now that we've had an introduction to factor analysis, let's revisit the $\omega$ grouping of reliability estimates.  In the context of *psychometrics* it may be useful to think of factors as scales/subscales where *g* refers to the amount of variance in the *general* factor (or total scale score) and subcales to be items that have something in common that is separate from what is *g*.

Model based estimates examine the correlations or covariances of the items and decompose the test variance into that which is 

* common to all items (**g**, a general factor), 
* specific to some items (**f**, orthogonal group factors), and 
* unique to each item (confounding **s** specific, and **e** error variance)

In the *psych* package

* $\omega_{t}$ represents the total reliability of the test ($\omega_{t}$)
  + In the *psych* package, this is calculated from a bifactor model where there is one general *g* factor (i.e., each item loads on the single general factor), one or more group factors (*f*), and an item-specific factor (*s*).
* $\omega_{h}$ extracts a higher order factor from the correlation matrix of lower level factors, then applies the Schmid and Leiman (1957) transformation to find the general loadings on the original items. Stated another way, it is a measure o f the general factor saturation (*g*; the amount of variance attributable to one comon factor). The subscript "h" acknowledges the hierarchical nature of the approach.
  +  the $\omega_{h}$ approach is exploratory and defined if there are three or more group factors (with only two group factors, the default is to assume they are equally important, hence the factor loadings of those subscales will be equal)
  + Najera Catalan [@najera_catalan_reliability_2019] suggests that $\omega_{h}$ is the best measure of reliability when dealing with multiple dimensions.
* $\omega_{g}$ is an estimate that uses a bifactor solution via the SEM package *lavaan* and tends to be a larger (because it forces all the cross loadings of lower level factors to be 0)
  +  the $\omega_{g}$ is confirmatory, requiring the specification of which variables load on each group factor
* *psych::omegaSem()* reports both EFA and CFA solutions
  - We will use the *psych::omegaSem()* function

Note that in our specification, we indicate there are two factors. We do not tell it (anywhere!) what items belong to what factors (think, *subscales*). One test will be to see if the items align with their respective factors.

```{r}
#Because we added the factor scores to our df (and now it has more variables than just our items), I will estimate omegaSem with the correlation matrix; I will need to tell it the n.obs

psych::omegaSem(GRMSr, nfactors = 4, n.obs=259)
```


There's a ton of output!  How do we make sense of it?

First, our items aligned perfectly with their respective factors (subscales). That is, it would be problematic if the items switched factors.

Second, we can interpret our results. Like alpha, the omegas range from 0 to 1, where values closer to 1 represent good reliability [@najera_catalan_reliability_2019]. For unidimensional measures, $\omega_{t}$ values above 0.80 seem to be an indicator of good reliability.  For multidimensional measures with well-defined dimensions we strive for $\omega_{h}$ values above 0.65 (and $\omega_{t}$ > 0.8). These recommendations are based on a Monte Carlo study that examined a host of reliability indicators and how their values corresponded with accurate predictions of poverty status. With this in mind, let's examine the output related to our simulated research vignette.

Let's examine the output in the lower portion where the values are "from a confirmatory model using sem."

Omega is a reliability estimate for factor analysis that represents the proportion of variance in the GRMS scale attributable to common variance, rather than error. The omega for the total reliability of the test ($\omega_{t}$; which included the general factors and the subscale factors) was .73, meaning that 73% of the variance in the total scale is due to the factors and 27% (100% - 73%) is attributable to error. 

Omega hierarchical ($\omega_{h}$) estimates are the proportion of variance in the GRMS score attributable to the general factor, which in effect treats the subscales as error.  $\omega_{h}$ for the the GRMS total scale was .40 A quick calculation with $\omega_{h}$ (.40) and $\omega_{t}$ (.73; .40/.73 = .55) lets us know that that 55% of the reliable variance in the GRMS total scale is attributable to the general factor. 

```{r}
.4/.73
```

Amongst the output is the Cronbach's alpha coefficient (.60). Lewis and Neville [-@lewis_construction_2015] did not report omega results. They reported an alpha of .92 for the version of the GRMS that assessed stress appraisal. 

## Comparing PAF to Item Analysis and PCA

In the lesson on PCA we began a table that compared our item analysis (item corrected-total correlations with item-other scale correlations) and PCA results (both orthogonal and oblique). Let's now add our PAF results (both orthogonal and oblique).

In the prior lecture I saved the file as both .rds and .csv objects. I will bring back in the .rds object and add to it.

```{r}
GRMScomps <- readRDS("GRMS_Comparisons.rds")
grmsPAF2ORTH
#names(grmsPAF2ORTH)
pafORTH_loadings <- data.frame(unclass(grmsPAF2ORTH$loadings))#I had to add "unclass" to the loadings to render them into a df
pafORTH_loadings$Items <- c("Obj1", "Obj2", "Obj3", "Obj4", "Obj5", "Obj6", "Obj7", "Obj8", "Obj9", "Obj10","Marg1", "Marg2", "Marg3", "Marg4", "Marg5", "Marg6", "Marg7", "Strong1", "Strong2", "Strong3", "Strong4", "Strong5", "Angry1", "Angry2", "Angry3") #Item names for joining (and to make sure we know which variable is which)
pafORTH_loadings <- dplyr::rename (pafORTH_loadings, PAF_OR_Mar = PA1, PAF_OR_Obj = PA2, PAF_OR_Ang = PA3, PAF_OR_Str = PA4)
GRMScomps <- dplyr::full_join(GRMScomps, pafORTH_loadings, by = "Items")#I had to add "unclass" to the loadings to render them into a df

#Now adding the PAF oblique loadings
pafOBLQ_loadings <- data.frame(unclass(grmsPAF2obl$loadings))#I had to add "unclass" to the loadings to render them into a df
pafOBLQ_loadings$Items <- c("Obj1", "Obj2", "Obj3", "Obj4", "Obj5", "Obj6", "Obj7", "Obj8", "Obj9", "Obj10","Marg1", "Marg2", "Marg3", "Marg4", "Marg5", "Marg6", "Marg7", "Strong1", "Strong2", "Strong3", "Strong4", "Strong5", "Angry1", "Angry2", "Angry3") #Item names for joining (and to make sure we know which variable is which)
pafOBLQ_loadings <- dplyr::rename (pafOBLQ_loadings, PAF_OB_Mar = PA1, PAF_OB_Obj = PA2, PAF_OB_Ang = PA3, PAF_OB_Str = PA4)
GRMScomps <- dplyr::full_join(GRMScomps, pafOBLQ_loadings, by = "Items")#I had to add "unclass" to the loadings to render them into a df

write.csv(GRMScomps, file = "GRMS_Comps.csv", sep = ",", row.names=FALSE, col.names=TRUE)#Writes the table to a .csv file where you can open it with Excel and format )
```

As a research vignette, this has worked extremely well, modeling consistency across the item analysis, principal components analysis (PCA), and principal axis factoring (PAF). That is, items load highest on their own scale (whether it is a component or factor), have no cross-loadings, and do not switch scale memberships from analysis to analysis.

![Comparison of path models for PCA and EFA](images/PAF/GRMScomps.png)

## Practice Problems
   
In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. In the *ReCentering Psych Stats:  Psychometrics* OER, it would be ideal if you have selected a dataset you can utilize across the lessons. The least complex is to change the random seed in the research and rework the problem demonstrated in the lesson. The most complex is to use data of your own. In either case, please plan to:

* Properly format and prepare the data.
* Conduct diagnostic tests to determine the suitability of the data for PAF.
* Conducting tests to guide the decisions about number of factors to extract.
* Conducting orthogonal and oblique extractions (at least two each with different numbers of factors).
* Selecting one solution and preparing an APA style results section (with table and figure).
* Compare your results in light of any other psychometrics lessons where you have used this data (especially the [item analysis](#ItemAnalSurvey) and [PCA](#PCA) lessons).

### Problem #1:  Play around with this simulation.

Copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results.  If PAF is new to you, perhaps you just change the number in "set.seed(210921)" from 210921 to something else. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go.

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

|Assignment Component                    | Points Possible   | Points Earned|
|:-------------------------------------- |:----------------: |:------------:|
|1. Check and, if needed, format data    |      5            |_____  |           
|2. Conduct and interpret the three diagnostic tests to determine if PAF is appropriate as an analysis (KMO, Bartlett's, determinant).                |      5            |_____  |  
|3. Determine how many factors to extract (e.g., scree plot, eigenvalues, theory). |5  |_____  | 
|4. Conduct an orthogonal extraction and rotation. |5        |_____  |
|5. Conduct an oblique extraction and rotation. |5           |_____  |  
|6. Repeat the orthogonal and oblique extractions/rotations with a different number of specified factors. | 5 |_____  |               
|7. APA style results section with table and figure of one of the solutions.|    5        |_____  |   
|8. Explanation to grader                 |    5        |_____  |       
|**Totals**                               |    40       |_____  |           


### Problem #2:  Conduct a PAF with the Szymanski and Bissonette [-@szymanski_perceptions_2020] research vignette that was used in prior lessons.

The second option involves utilizing one of the simulated datasets available in this OER. Szymanski and Bissonette's [-@szymanski_perceptions_2020]Perceptions of the LGBTQ College Campus Climate Scale: Development and psychometric evaluation was used as the research vignette for the validity, reliability, and item analysis lessons. Although I switched vignettes, the Szymanski and Bissonette example is ready for PAF

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

|Assignment Component                    | Points Possible   | Points Earned|
|:-------------------------------------- |:----------------: |:------------:|
|1. Check and, if needed, format data    |      5            |_____  |           
|2. Conduct and interpret the three diagnostic tests to determine if PAF is appropriate as an analysis (KMO, Bartlett's, determinant).                |      5            |_____  |  
|3. Determine how many factors to extract (e.g., scree plot, eigenvalues, theory). |5  |_____  | 
|4. Conduct an orthogonal extraction and rotation. |5        |_____  |
|5. Conduct an oblique extraction and rotation. |5           |_____  |  
|6. Repeat the orthogonal and oblique extractions/rotations with a different number of specified factors. | 5 |_____  |               
|7. APA style results section with table and figure of one of the solutions.|    5        |_____  |   
|8. Explanation to grader                 |    5        |_____  |       
|**Totals**                               |    40       |_____  |  

### Problem #3:  Try something entirely new.

Using data for which you have permission and access (e.g.,  IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), complete PAF. The data should allow for at least two (ideally three) factors/subscales.

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

|Assignment Component                    | Points Possible   | Points Earned|
|:-------------------------------------- |:----------------: |:------------:|
|1. Check and, if needed, format data    |      5            |_____  |           
|2. Conduct and interpret the three diagnostic tests to determine if PAF is appropriate as an analysis (KMO, Bartlett's, determinant).                |      5            |_____  |  
|3. Determine how many factors to extract (e.g., scree plot, eigenvalues, theory). |5  |_____  | 
|4. Conduct an orthogonal extraction and rotation. |5        |_____  |
|5. Conduct an oblique extraction and rotation. |5           |_____  |  
|6. Repeat the orthogonal and oblique extractions/rotations with a different number of specified factors. | 5 |_____  |               
|7. APA style results section with table and figure of one of the solutions.|    5        |_____  |   
|8. Explanation to grader                 |    5        |_____  |       
|**Totals**                               |    40       |_____  |  
      

```{r include=FALSE}
sessionInfo()
```
