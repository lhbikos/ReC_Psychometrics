# Psychometric Validity:  Basic Concepts {#rxy}

 [Screencasted Lecture Link](link) 
 
```{r  include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA) #keeps out the hashtags in the knits
options(scipen=999)#eliminates scientific notation
```

The focus of this lecture is to provide an introduction to validity.  This includes understanding some of the concerns of validity, different aspects of validity, and factors as they affect validity coefficients. 


## Navigating this Lesson

There is just over one hour of lecture.  If you work through the materials with me it would be plan for an additional hour.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail.  All original materials are provided at the [Github site](https://github.com/lhbikos/ReC_Psychometrics) that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's [introduction](#ReCintro)

### Learning Objectives

Focusing on this week's materials, make sure you can:

* Distinguish between different types of validity based on short descriptions.
* Compute and interpret validity coefficients.
* Evaluate the incremental validity of an instrument-of-interest.
* Define and interpret the standard error of estimate.
* Develop a rationale that defends importance of establishing the validity of a measuring instrument.

### Planning for Practice

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. With each of these options I encourage you to interpret examine aspects of the construct validity through the creation and interpretation of validity coefficients. Ideally, you will examine both convergent/discriminant as well as incremental validity.

### Readings & Resources

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

* Jhangiani, R. S., Chiang, I.-C. A., Cuttler, C., & Leighton, D. C. (2019). Reliability and Validity. In *Research Methods in Psychology*. https://doi.org/10.17605/OSF.IO/HF7DQ
* Clark, L. A. & Watson, D. (1995). Constructing validity: Basic issues in objective scale development. Psychological Assessment, 7, 309-319.
  - In this manuscript, Clark and Watson (1995) create a beautiful blend of theoretical issues and practical suggestions for creating measures that evidence construct validity. From the practical perspective, the authors first guide potential scale constructors through the literature review and creating an item pool (including tips on writing items). The authors address structural validity by first beginning with strategies for constructing the test. In this section, the authors revisit the issue of dimensionality (i.e., alpha vs. factor analysis). Finally, the authors look at initial data collection (addressing sample size) and psychometric evaluation. 

### Packages

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.
```{r }
#will install the package if not already installed
#if(!require(tidyverse)){install.packages("tidyverse")}
#if(!require(MASS)){install.packages("MASS")}
#if(!require(psych)){install.packages("psych")}

```

## Research Vignette

Explorations of validity are frequently correlational in nature. This lesson provides descriptions of numerous pathways for establishing an instrument's validity.  Best practices involving numerous demonstrations of validity. Across several lessons, we will rework several several of the analyses reported in the research vignette. For this lesson in particular, the research vignette allows both convergent/discriminant validity as well as incremental validity.

The research vignette for this lesson is the development and psychometric evaluation of the Perceptions of the LGBTQ College Campus Climate Scale [@szymanski_perceptions_2020]. The scale is six items with responses rated on a 7-point Likert scale ranging from 1 (*strongly disagree*) to 7 (*strongly agree*). Higher scores indicate more negative perceptions of the LGBTQ campus climate Szymanski and Bissonette have suggested that the psychometric evaluation supports using the scale in its entirety or as subscales composed of the following items:

* College response to LGBTQ students:  
  - My university/college is cold and uncaring toward LGBTQ students. 
  - My university/college is unresponsive to the needs of LGBTQ students. 
  - My university/colleg provides a supportive environment for LGBTQ students. 
* LGBTQ Stigma:  
  - Negative attitudes toward LGBTQ persons are openly expressed on my university/college campus. 
  - Heterosexism, homophobia, biphobia, transphobia, and cissexism are visible on my university/college campus. 
  - LGBTQ students are harassed on my university/college campus. 

A [preprint](https://www.researchgate.net/publication/332062781_Perceptions_of_the_LGBTQ_College_Campus_Climate_Scale_Development_and_Psychometric_Evaluation/link/5ca0bef945851506d7377da7/download) of the article is available at ResearchGate. 

In the [lesson](#rxx) on reliability, I simulate item-level data. However, this lesson we will be interested in the correlations between the total and subscale scores with five additional scales:

* LGBTQ victimization 
* Satisfaction with college
* Intention to persist in college
* Generalized anxiety disorder symptoms
* Symptoms of depression

```{r}
Szy_mu <- c(3.16, 2.71, 3.61, .11, 5.61, 4.41, 1.45, 1.29)
Szy_sd <- c(1.26, 1.33, 1.51, .23, 1.15, .53, .80, .78)
Szy_r_mat <- matrix(c(1,   .88, .90, .35, -.56, -.27, .25, .24,
                  .88,  1,  .58, .25, -.59, -.29, .17, .18,
                  .90, .58,  1,  .37, -.41, -.19, .27, .24,
                  .35, .25, .37,  1,  -.22, -.04, .23, .21,
                 -.56,-.59, -.41, -.22, 1,   .53, -.29, -.32,
                 -.27, -.29, -.19, -.04, .53, 1, -.22, -.26,
                  .25, .17,  .27,  .23,  -.29, -.22, 1, .76,
                  .24,  .18, .24, .21, -.32,  -.26, .76,  1), ncol = 8)
Szy_cov_mat <- Szy_sd %*% t(Szy_sd) * Szy_r_mat

set.seed(210907)
SzyDF <- round(as.data.frame(MASS::mvrnorm(n = 646, mu=Szy_mu, Sigma=Szy_cov_mat, tol=1e-3, empirical=TRUE)),2) #adding "tol=1e-3" fixed the not positive matrix error
SzyDF <- round(dplyr::rename(SzyDF, CClimate = V1, CResponse = V2, Stigma = V3, Victimization = V4, CollSat = V5, Persistence = V6, Anxiety = V7, Depression = V8),2)
#round(cor(SzyDF),2)
```

## Fundamentals of Validity

**Validity** (the classic definition) is the ability of a test to measure what it purports to measure. Supporting that definition are these notions:

* Validity is extent of matching, congruence, or “goodness of fit” between the operational definition and concept it is supposed to measure.
* An instrument is said to be valid if it taps the concept it claims to measure.
* Validity is the appropriateness of the interpretation of the results of an assessment procedure for a given group of individuals, not to the procedure itself.
* Validity is a matter of degree; it does not exist on an all-or-none basis.
* Validity is always specific to some particular use or interpretation.
* Validity is a unitary concept.
* Validity involves an overall evaluative judgment.

Over the years (and, perhaps within each construct), validity has somewhat of an *evolutionary* path from a focus on content, to prediction, to theory and hypothesis testing.

When the focus is on **content**, we are concerned with the:

* assessment of what individuals had learned in specific content areas 
* relevance of its content (i.e., we compare the content to the content domain)

When the focus is on **prediction**, we are concerned with:

* how different persons respond in a given situation (now or later). 
* the correlation coefficient between test scores (predictor) and the assessment of a criterion (performance in a situation)

A focus on **theory and hypothesis testing** adds:

* a strengthened theoretical orientation
* a close linkage between psychological theory and verification through empirical and experimental hypothesis testing
* an emphasis on constructs in describing and understanding human behavior.

**Constructs** are broad categories, derived from the common features shared by directly observable behavioral variables. They are theoretical entities and not directly observable. **Construct validity** is at the heart of psychometric evaluation. We define **construct validity** as the fundamental and all-inclusive validity concept, insofar as it specifies what the test measures. Content and predictive validation procedures are among the many sources of information that contribute to the understanding of the constructs assessed by a test.

## Validity Criteria

We have just stated that validity is an overall, evaluative judgment. Within that umbrella are different criteria by which we judge the validity of a measure. We casually refer to them as *types*, but each speaks to that unitary concept.

### Content Validity

Content validity is concerned with the representativeness of the domain being assessed. Content validation procedures may differ depending on whether the test is in the educational/achievement context or if it is more of an attitude/behavioral survey.

In the educational/achievement context, content validation seeks to ensure the items on an exam are appropriate for the content domain being assessed.

A **table of specifications** is a two-way chart which indicates the instructionally relevant learning tasks to be measured. Percentages in the table indicate the relative degree of emphasis that each content area

Let's imagine that I was creating a table of specifications for items on a quiz for this very chapter. The columns represent the types of outcomes we might expect. The American Psychological Association often talks about *KSAs* (knowledge, skills, attitudes), so I will utilize those as a framework. You'll notice that the number of items and percentages do not align mathematically. Because, in the exam, I would likely weight application items (e.g., "work the problem") more highly than knowledge items (e.g., multiple choice), the relative weighting may differ.

**Table of Specifications**

|Learning Objectives                |Knowledge  |Skills     |Attitudes  |% of test |
|:----------------------------------|:---------:|:---------:|:---------:|:---------:|
|Distinguish between different types of validity based on short descriptions.| 6 items| | |30% |
|Compute and interpret validity coefficients.|  | 2 items   |           |       15%|
|Evaluate the incremental validity of an instrument-of-interest.| |1 item  |  |  20%|
|Define and interpret the standard error of estimate.|1 item |    |           |  15%|
|Develop a rationale that defends importance of establishing the validity of a measuring instrument.| | |1 item |20%  |   
|TOTALS                             |7 items    |3 items    |1 item     |100%  |  



**Subject matter experts** (SMEs)s are individuals chosen to evaluate items based on their degree of knowledge of the subject being assessed.  If used:

*  report how many and professional qualifications; 
*  to classify items, report the directions they were given and the extent of agreement among judges. 
  
Empirical procedures for enhancing content validity of educational assessments may include

*  comparing item-level and total scores with grades; lower grades should get lower scores;
*  analyzing individual errors; 
*  observing student work methods (have the students “think aloud” in front of an examiner);
*  evaluating the role of speed, noting how many do not complete the test in the time allowed;
*  correlating the scores with a reading comprehension test (if the exam is highly correlated, then it may be a test of reading and not another subject).  Alternatively, if it is a reading comprehension test; give the student the questions (without the passage) to see how well they answered the questions on the basis of prior knowledge.

For surveys and tests outside of educational settings, content validation procedures ask, "Does the test cover a representative sample of the specified skills and knowledge?" and "Is test performance reasonably free from the influence of irrelevant variables? Naturally, SMEs might be used.

An example of content validation from Industrial-Organizational Psychology is the job analysis which precedes the development of test for employee selection and classification. 
Not all tests require content analysis. In aptitude and personality tests we are probably more interested in other types of validity evaluation.

### Face Validity:  The "un"validity

Face validity is concerned with the question, “How does an assessment look on the ‘face of it’?” Let's imagine that on a qualification exam for electricians, a math items asks the electrician to estimate the amount of yarn needed to complete a project. The item may be more *face valid* if the calculation was with wire. Thus, face validity can often be improved by reformulating test items in terms that appear relevant and plausible for the context. 

Face validity should never be regarded as a substitute for objectively determined validity. In contrast, it should not be assumed that when a (valid and reliable) test has been modified to increase its face validity, that its objective validity and reliability is unaltered. That is, it must be reevaluated. 

### Criterion Related Validity

Criterion related validity has to do with the test's ability to *predict* an outcome (the criterion). If the criterion is something that occurs simultaneously, it is an assessment of **concurrent validity**; if it is in the future then it is an assessment of **predictive validity.**

A **criterion** is "the thing" that the test should, theoretically, be able to *predict*. That prediction could be occurring at the same time (*concurrent validity*) or at a future time (*predictive validity*). Regardless, the estimate of the criteria must be independent of the survey/assessment being evaluated. The table below provides examples of types of tests and concurrent and predictive validity criteria.	

|Type of Test                           |Concurrent Criteria Example         |Predictive Criteria Example        |
|:--------------------------------------|:-----------------------------------|:-----------------------------------
|A shorter (or cheaper) standardized achievement test|school grades, existing standardized tests|subsequent graduation/college admissions, cumulative GPA|         
|Employee selection tests|decision made by a search committee |subsequent retention or promotion of the selected employee| 
|Assessment of depression severity (shorter or cheaper)|diagnosis from a mental health professional; correlation with an established measure|inpatient hospitalization or act of self-harm|              
      
**Contrasted groups** is a specific type of criterion-related validity. Clearly differentiated groups (e.g., sales clerks versus excutives; engineers versus musicians) are chosen to see if exam performance or profiles differ in predictable ways.

**Criterion contamination** occurs when test scores, themselves, are used to make decisions about the criteria. To prevent this,

*	No person who participates in the assignment of criterion ratings can have any knowledge of the examinee’s test scores.  
*	the test scores must be kept strictly confidentially. 

There are a number of issues related to criterion-related validity. 

* Is the criterion choice appropriate?
  - Criterion validity is only as good as the validity of the criterion to which one is making a comparison.
  –	In the 1980s and 1990s there was more attention in this area. That is critics questioned the quality of the criterion being used.
* To what degree can the results of criterion-related validity be generalized?
  –	Most tests are developed (intentionally) for a local context, setting, or audience. Consequently, in the local context, the criterion-prediction sample is usually too small (i.e., 50 cases or less).
  –	Those who want to generalize the test to a broader population should evaluate the test in relationship to the new purpose.
* Is there a role for meta-analysis?
  –	Repeated validation studies of our tests, on different samples, results in a number of small-scale studies, each with their own validity coefficients.
  –	We can use meta-analytic procedures in reporting the results of validity coefficients when they are used for establishing criterion validity. 

### Construct Validity

**Construct validity** was introduced in 1954 in the first edition of APA's testing standards and is defined as the extent to which the test may be said to measure a theoretical construct or trait. The overarching focus is on the role of *psychological theory* in test construction and the ability to formulate hypotheses that can supported (or not) in the evaluation process. Construct validity is established by the accumulation of information from a variety of sources. 

There are a number of sources that can be used to support construct validity.  

### Internal Consistency

In the next [chapter](#rxx), you will learn that **internal consistency** is generally considered to be an index of reliability. In the context of criterion-related validity, a goal is to ensure that the criterion is the total score on the test, itself. To that end, some of the following could also support this aspect of validity: 

* Comparing high and low scorers. Items that fail to show a significantly greater proportion of “passes” in the upper than the lower group are considered invalid, and are modified or eliminated.
* Computing a biserial correlation between the item and total score.  
* Correlating the subtest score with the total score. Any subtest whose correlation with the total score is too low, is eliminated. 

Although some take issue with this notion, the degree of *homogeneity* (the degree to which items assess the same thing) has some bearing on construct validity. There is a tradeoff between items that measure a narrow slice of the construct definition (internal consistency estimates are likely to be higher) and those that sample the construct definition more broadly (internal consistency estimates are likely to be lower).

Admittedly, the contribution of internal consistency data is limited. In absence of external data, it tells us little about WHAT the test measures.

### Structural Validity

#### Exploratory Factor Analysis 

**Exploratory factor analysis** (EFA) is used to simplify the description of  behavior by reducing the number of categories (factors or dimensions) to as many as the numbers of the items to fewer. In instrument development, techniques like *principal components analysis* or *principal axis factoring* are used to identify clusters (latent factors) among items. We frequently treat these as scales and subscales.

Imagine the use of 20 tests to 300 people. There would be 190 correlations. 

* Irrespective of content, we can probably summarize the intercorrelations of tests with 5-6 factors.
* When the clustering of tests includes vocabulary, analogies, opposites, and sentence completions, we might suggest a "verbal comprehension factor."
* Factorial validity is the correlation of the test with whatever is common to a group of tests or other indices of behavior. If our single test has a correlation of .66 with the factor on which it loads, then the “factorial validity of the new test as a measure of the common trait is .66.”

When EFA is utilized the items are "fed" into an iterative process that analyzes the relations and "reveals" (or suggests -- we are the ones who interpret the data) how many factors (think scales/subscales) andwhich items comprise them.

#### Confirmatory Factor Analysis

**Confirmatory factor analysis** (CFA) involves specifying, a priori, a proposed relationship of items, scales, and subscales and then testing its *goodness of fit.* In CFA (a form of structural equation modeling [SEM]), the latent variables (usually the higher order scales and total scale score) are positioned to cause the responses on the indicators/items.

Subsequent lessons provide examples of both EFA and CFA approaches to psychometrics.

### Experimental Interventions 

Construct validity is also supported by hypothesis testing and experimentation. If we expect that the construct assessed by the instrument is malleable (e.g., depression) and that an intervention could change it, then a random clinical trial that evaluated the effectiveness of an intervention (and it worked -- depression scores declined) would simultaneously provide support for the intervention as well as the instrument. 

### Convergent and Discriminant Validity 

In a psychometric evaluation, we will often administer our instrument-of-interest along with a battery of instruments that are more-and-less related. **Convergent validity** is supported when there are *moderately high* correlations between our tests and the instruments with which we expect moderately high correlations.  In contrast, **discriminant validity** is established by low and/or non-significant correlations between our instrument-of-interest and instruments that should be unrelated. For example, we want a low and non-significant correlation between a quantitative reasoning test and scores on a reading comprehension test.  Why?  Because if the correlation is too high, the test cannot discriminate between reading comprehension and math. 

There are no strict cut-offs to establish convergence or discrimination. We can even ask, "Could a correlation intended to support convergence be too high?"  It is possible!  Unless the instrument-of-interest offers advantages such as brevity or cost, then correlations that fall into the ranges of multicollinearity or singularity can indicate unnecessary duplication or redundancy.

In our research vignette, Szymanski and Bissonette [-@szymanski_perceptions_2020] conduct a correlation matrix that reports the bivariate relations between the LGBTQ Campus Climate full-scale as well as the College Response and Stigma subscales with measures that assess (a) LGBTQ victimization, (b) satisfaction with college, (c) persistence attitudes, and (d) anxiety, and (e) depression. 
```{r}
apaTables::apa.cor.table(SzyDF, filename = "SzyCor.doc", table.number = 1, show.sig.stars=TRUE, landscape=TRUE)
```
Examination of these values (which align well with the results in the table) follow the expected pattern.  That is, higher scores on the overall Campus Climate, College Response, and Stigma scales result in higher levels of victimization, anxiety and depression. Conversely, they are associated with lower college satisfaction and persistence.

The College Response and Stigma subscales' relations with satisfaction with college (-.59, -.41, respectively) and persistence attitudes (-.29*, -.19, respectively) are examples of the convergent and discriminant patterns (College Response has higher relations with these than Stigma) that support construct validity.

The **multitrait-multimethod matrix** is a systematic experimental design for the dual approach of convergent and discriminant validation, which requires the assessment of two or more traits (classically math, English, and reading scores) by two more methods (self, parent, and teacher). Conducting a web-based image search on this term will show a matrix of alpha coefficients and correlation coefficients that are interpreted in relationship to each other.  Roughly:

* alpha (internal consistency) coefficients should be the highest,
* validity coefficients (correlations of the same trait assessed by different methods) should be higher than correlations between different traits measured by different methods,
* validity coefficients (correlations of the same trait assessed by different methods) should be higher than different traits measured by the same method.

### Incremental Validity

**Incremental validity** is the increase in predictive validity attributable to the test. It indicates the contribution the test makes to the selection of individuals who will meet the minimum standards in criterion performance. There are different ways to assess this, one of the most common is to first enter known predictors and then see if the instrument-of-interest continues to account variance over-and-above those that are entered.

In the Szymanski and Bissonette [-@szymanski_perceptions_2020] psychometric evaluation, the negative relations with satisfaction with college and intention to persist in college as well as positive relations with both anxiety and depression persisted even after controlling for LGBTQ victimization experiences.

I will demonstrate this procedure, predicting the contribution that the LGBTQ Campus Climate total scale scrore has on predicting intention to persist in college, over and above LGBTQ victimization. 

The process is to use hierarchical linear regression.  Two models are built.  In the first mode ("PfV" stands [in my mind] for "Persistence from Victimization"), persistence is predicted from victimization.  The second model adds the LGBTQ Campus Climate Scale.  I asked for summaries of each model.  Then the *anova()* function compares the model.

```{r}
PfV <- lm(Persistence ~ Victimization, data = SzyDF)
PfVC <- lm(Persistence ~ Victimization + CClimate, data = SzyDF)
summary(PfV)

```
From the PfVCmodel we learn that victimation has a non-significant effect on intentions to persist in college  Further, the $R^2$ is quite small (0.002).

```{r}
summary(PfVC)
```
In the PfVC model, we see that the LGBTQ Campus Climate full scale score has a significant impact on intentions to persist.  Specifically, foreach additional point higher on the Campus Climate Score, intentions to persist decrease by .13 points. Together, the model accounts for 7% of the variance (this is also an $R^2$ change of 7%).

```{r}
anova(PfV, PfVC)
```
We see that there is a statistically significant difference between the models.


Let's try another model.

```{r}
AfV <- lm(Anxiety ~ Victimization, data = SzyDF)
AfVC <- lm(Anxiety ~ Victimization + CClimate, data = SzyDF)
summary(AfV)
summary(AfVC)
anova(AfV, AfVC)
```
This model is a little more exciting in that our first model (AfV) is statistically significant. That is, victimization has a statistically significant effect on anxiety, accounting for 5% of the variance.  Even so, when added, the LGBTQ Campus Climate total scale score is also significant, and accounts for an additional 4% of variance ($\Delta{R^2}$),$R^2$ = 0.85. There is a statistically significant difference between models (*F*[1, 643] = 22.98, *p* < .001).

### Considering the Individual and Social Consequences of Testing 

Messick [@messick_consequences_2000] and others recommend that the “consequences of testing” be included in the concept of test validity. Messick's point was to consider the the unintended consequences of specific uses. That is, their use may be detrimental to individuals or to members of certain ethnic or other populations with diverse experiential backgrounds.  Examples of inappropriate use have included:
  
* The California Psychological Inventory (CPI) being used as a screening tool for employment as a security job. Two of its items inquired about same-sex activities and the employer was using this to screen out gay men.  Applicants were able to demonstrate, in court, a consistent rejection of gay applicants.
* While this is not a psychological test, urine samples are often collected as drug screening tools. In reality, urine can reveal a number of things -- such as pregnancy.

The issue begs “conflicting goals.” In this case, the problem was not caused by the test. Rather, by its misuse. Studying the “consequences” of testing is one that is not necessarily answerable by empirical data/statistical analysis. It requires critical observation, human judgment, and systematic debate.

## Factors Affecting Validity Coefficients

Keeping in mind that a *validity coefficient* is merely the correlation between the test and some criteria, the same elements that impact the magnitude and significance of a correlation coefficient will similarly effect a validity coefficient.

**Nature of the group**  A test that has high validity in predicting a particular criterion in one population,  may have little or no validity in predicting the same criterion in another population. If a test is designed for use in diverse populations, information about the population generalizability should be reported in the technical manuals. 

**Sample heterogeneity** Other things being equal, if there is a linear relationship between X and Y, it will have a greater magnitude when the sample is heterogeneous. 

**Pre-selection** Just like internal and external validity in a research design can be threatened by selection issues, pre-selection can also impact the validity coefficients of a measure. For example, if we are evaluating a new test for job selection, we may select a group of newly hired employees. We plan to collect some measure of job performance at a later date. Our results may be limited by the criteria used to select the employees.  Were they volunteers? Were they only those hired? Were they ALL of the applicants. 

**Validity coefficients may change over time**. Consider the relationship between the college boards and grade point average at Yale University. Fifty years ago $r_{xy} = .72$; today $r_{xy} = .52$.  Why?  The nature of the student body has become more diverse (50 years ago, the student body was predominantly White, high SES, and male).

The **form of the relationship** matters. The Pearson R assumes the relationship between the predictor and criterion variables is linear, uniform, and homoschedastistic (equal variability throughout the range of a bivariate distribution). When the variability is unequal throughout the range of the distribution the relationship is heteroscedastic.

```{r fig.cap="Illustration of heteroschedasticity", echo=FALSE}
n=rep(1:100,2)
a=0
b = 1
sigma2 = n^1.3
eps = rnorm(n,mean=0,sd=sqrt(sigma2))
y=a+b*n + eps
mod <- lm(y ~ n)
plot(n,y)
```

There could also be other factors involved in the relationship between the instrument and the criterion:

* curvilinearity
* an undetected mechanism such as a moderator

Finally, what is our thresshold for acceptability?

* Consider statistical signifance -- but also its limitations (e.g., power, Type I error, Type II error)
* Consider the magnitude of the correlation; and also $R^2$ (the proportion of variance accounted for)
* Consider error:
  - The standard error of the estimate shows the margin of error to be expected in the individuals predicted criterion score as the result of the imperfect validity of the instrument.

$$SE_{est} = SD_{y}\sqrt{1 - r_{xy}^{2}}$$
Where
$r_{xy}^{2}$ is the square of the validity coefficient
$SD_{y}$ is the standard deviation of the criterion scores 

If the validity were perfect ($r_{xy}^{2}$ = 1.00), the error of estimate would be 0.00. 
If the validity were zero, the error of estimate would equal $SD_{y}$.

Interpreting $SE_{est}$
 
If $r_{xy}$ = .80, then $\sqrt{1 - r_{xy}^{2}} = .60$            
Error is 60% as large as it would be by chance.Stated another way, predicting an individual’s criterion performance has a margin of error that is 40% smaller than it would be by chance.

To obtain the $SE_{est}$, we merely multiply by the $SD_{y}$. This puts error in the metric of the criterion variable.


Your Turn
If  $r_{xy}$ = .25, then   $\sqrt{1 - r_{xy}^{2}} =$ ??            

Make a statement about chance.
Make a statement about margin of error.

## Practice Problems
   
In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. With each of these options I encourage you to interpret examine aspects of the construct validity through the creation and interpretation of validity coefficients. Ideally, you will examine both convergent/discriminant validity as well as incremental validity.

### Problem #1:  Play around with this simulation.

Copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results.  

If calculating  is new to you, perhaps you just change the number in "set.seed(210907)" from 210907 to something else. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go.

|Assignment Component                    | Points Possible   | Points Earned|
|:---------------------------------------|:-----------------:|:------------:|
|1. Check and, if needed, format data    |      5            |_____  |           
|2. Create a correlation matrix that includes the instrument-of-interest and the variables that will have varying degrees of relation|     5            |_____  |
|3. Interpret the validity coefficients  |     5             |_____  |  
|4. With at least three variables, evaluate the degree to which the instrument demonstrates incremental validity (this should involve two regression equations and their statistical comparison)                              |        5           |_____  |               
|5. Explanation to grader               |    5               |_____  |   
|**Totals**                             |      25            |_____  |          


### Problem #2:  Conduct the reliability analysis selecting different variables.

The Szymanski and Bissonette [-@szymanski_perceptions_2020] article conducted a handful of incremental validity assessments. Select different outcome variables (e.g., depression) and/or use the subscales as the instrument-of-interest.

|Assignment Component                    | Points Possible   | Points Earned|
|:---------------------------------------|:-----------------:|:------------:|
|1. Check and, if needed, format data    |      5            |_____  |           
|2. Create a correlation matrix that includes the instrument-of-interest and the variables that will have varying degrees of relation|     5            |_____  |
|3. Interpret the validity coefficients  |     5             |_____  |  
|4. With at least three variables, evaluate the degree to which the instrument demonstrates incremental validity (this should involve two regression equations and their statistical comparison)                              |        5           |_____  |               
|5. Explanation to grader               |    5               |_____  |   
|**Totals**                             |      25            |_____  |                 |        5           |_____  |               
|5. Explanation to grader               |    5               |_____  |   
|**Totals**                             |      25            |_____  |  

### Problem #3:  Try something entirely new.

Using data for which you have permission and access (e.g.,  IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), create validity coefficients and use three variables to estimate the incremental validity of the instrument-of-interest.

|Assignment Component                    | Points Possible   | Points Earned|
|:---------------------------------------|:-----------------:|:------------:|
|1. Check and, if needed, format data    |      5            |_____  |           
|2. Create a correlation matrix that includes the instrument-of-interest and the variables that will have varying degrees of relation|     5            |_____  |
|3. Interpret the validity coefficients  |     5             |_____  |  
|4. With at least three variables, evaluate the degree to which the instrument demonstrates incremental validity (this should involve two regression equations and their statistical comparison)                              |        5           |_____  |               
|5. Explanation to grader               |    5               |_____  |   
|**Totals**                             |      25            |_____  |       

```{r include=FALSE}
sessionInfo()
```




