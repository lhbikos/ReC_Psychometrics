
```{r include = FALSE}
options(scipen=999)
```

## Homeworked Example
[Screencast Link](https://youtube.com/playlist?list=PLtz5cFLQl4KONpFF3j9LLwT5UWrTjuFgv&si=cE9TyjSUiXvAMluw)

For more information about the data used in this homeworked example, please refer to the description and codebook located at the end of the [introductory lesson](https://lhbikos.github.io/ReCenterPsychStats/ReCintro.html#introduction-to-the-data-set-used-for-homeworked-examples) in [ReCentering Psych Stats](https://lhbikos.github.io/ReCenterPsychStats/). An .rds file which holds the data is located in the [Worked Examples](https://github.com/lhbikos/ReC_MultivModel/tree/main/Worked_Examples) folder at the GitHub site the hosts the OER. The file name is *ReC.rds*.

The suggested practice problem for this chapter is to evaluate the measurement model that would precede the evaluation of a structural model. Next we respecify that model as the structural model. This model will (in all likelihood) be more parsimonious (i.e., have fewer paths) and have worse fit. The goal, though, is that our more parsimonious, structural model, is just as explanatory as the measurement model, where all factors were allowed to covary.

As we end the set of lessons on psychometric development and evaluation, and important part of this lesson is to start with very raw data and work through the scrubbing, scoring, and data diagnostics as we prepare it for the structural equation model that is the primary analysis

###  Describe and draw the research model you will evaluate {-} 

It should have a minimum of three variables and could be one of the prior path-level models you already examined.  

With a simple linear regression, I want to predict students' valuation (Valued, Y) of the statistics course from

* Traditional pedagogy (TradPed, X1)
* Socially responsive pedagogy (SRPed (X2))
* Centering status:  0 = precentered; 1 = recentered (Centering, X3)
 
X1 = Centering: explicit recentering (0 = precentered; 1 = recentered)
X2 = TradPed: traditional pedagogy (continuously scaled with higher scores being more favorable)
X3 = SRPed:  socially responsive pedagogy (continuously scaled with higher scores being more favorable)
Y = Valued: valued by me (continuously scaled with higher scores being more favorable)

I am hypothesizing that all three predictors will have a statistically significant, positive, effect on the outcome.

It helps me to make a quick sketch:

![An image of the linear regression model for the homeworked example](Worked_Examples/Images/HypothesizedModel.png)


### Import the data and format the variables in the model. {-}

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
raw <- readRDS("ReC.rds")
```

The multiple regression approach we are using does not allow dependency in the data. Therefore, we will include only those who took the psychometrics class (i.e., excluding responses for the ANOVA and multivariate courses).
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
raw <-(dplyr::filter(raw, Course == "Psychometrics"))
```

Although this dataset is overall small, I will go ahead and make a babydf with the item-level variables.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
babydf <- dplyr::select(raw, Centering, ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation, ValObjectives, IncrUnderstanding, IncrInterest, InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration)
```

Let's check the structure of the variables:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
str(babydf)
```
All of the item-level variables are integers (i.e., numerical). This is fine. 

The centering variable will need to be dummy coded as 0/1:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
babydf$CEN <- as.numeric(babydf$Centering)
babydf$CEN <- (babydf$CEN - 1)
str(babydf)
```
### Analyze and manage missing data. {-}

Structural equation models lend themselves to managing missing data with Parent's [-@parent_handling_2013] *available information analysis* (AIA) approach. My approach is to:

* Create a dataframe that includes only the variables that will be used in the analysis.
* Delete all cases with greater than 20% missingness.
* If scale scores (or parcels) are used, calculate them if ~80% of the data for the calculation is present.
* Use the *full information maximum likelihood* (FIML) estimation procedure in *lavaan*; this allows item-level missingness. 

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
cases1 <- nrow(babydf) #I produced this object for the sole purpose of feeding the number of cases into the inline text, below
cases1
```
112 students completed at least some of the course evaluation.

The next code creates a variable that counts the number of cells with missing data.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70), message = FALSE, warning = FALSE}
library(tidyverse)
#Create a variable (n_miss) that counts the number missing
babydf$n_miss <- babydf%>%
dplyr::select(Centering:DEIintegration) %>% 
is.na %>% 
rowSums

```

The next code creates a variable that calculates the proportion of the data that is missing. Additionally, it sorts the data from highest to lowest proportion of missingness.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70), message = FALSE, warning = FALSE}
#Create a proportion missing by dividing n_miss by the total number of variables (21)
#Sort in order of descending frequency to get a sense of the missingness
babydf<- babydf%>%
dplyr::mutate(prop_miss = (n_miss/13)*100)%>%
  arrange(desc(n_miss))
```

From the code below we can the average missingness as well as the range.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
PrMiss1 <- psych::describe(babydf$prop_miss)
PrMiss1
```
We see that row (or case) level missingness ranged from 0 to 15%. This is great! Most students are completing the entire evaluation.

Finally, we canwrite code to provide the proportion of missingness across the entire data frame, and then the percent of cases/rows with nonmissing data.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
CellsMissing1 <-formattable::percent(mean(is.na(babydf))) #percent missing across df
RowsMissing1 <- formattable::percent(mean(complete.cases(babydf))) #percent of rows with non-missing data
CellsMissing1
RowsMissing1
```
Across the data frame, 1% of the data is missingness.  Further, 88% of students have non-missing data. 

Let's conduct an analysis of missingness with the *mice::md.pattern()* function.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
missingness <- mice::md.pattern(babydf, plot = TRUE, rotate.names=TRUE)
missingness
```

Tentative write-up:

>Our initial inspection of the data indicated that 112 attempted the course evaluation. Across cases, the proportion of missingness in the responses ranged from 0% to 15%. Across the dataframe there was 1% of missingness across the cells. Approximately 88% of the cases had nonmissing data. Our inspection of a missingness map indicated missingness on several of the items assessing socially responsive pedagogy (e.g., multiple perspectives, inclusive classroom, DEI integration). This could be because these items were added to the course evaluations after the study began. Because missingess was so low, we retained all cases and did not inspect further.

### Assess the distributional characteristics of the data. {-}

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
psych::describe(babydf)
```
Regarding the distributional characteristics of the data, skew and kurtosis values of the variables fell below the values of 3 (skew) and 8 to 20 (kurtosis) that Kline suggests are concerning [-@kline_principles_2016].

Next I will conduct an outlier analysis with the Mahalanobis test. The dataset for this can only have continuously scaled variables. First I will create a Mahal_df that excludes the Centering variable (and those other variables we created to assess missingness).

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
Mahal_df <- dplyr::select(babydf, ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation, ValObjectives, IncrUnderstanding, IncrInterest, InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration)
```

We'll append a variable to the Mahal_df that calculates the distance from the *centroid* of the data. To the degree that the data strays from the diagonal line (and particularly if there are numbered variables), we have outliers.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
Mahal_df$Mahal <- psych::outlier(Mahal_df)
```

The code below appends a TRUE/FALSE variable to the data. Cases are TRUE if the Mahalanobis distance test is more than three standard deviations from the centroid.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
library(dplyr)
#str(item_scores_df$Mahal)
Mahal_df$MOutlier <- if_else(Mahal_df$Mahal > (median(Mahal_df$Mahal) + (3*sd(Mahal_df$Mahal))), TRUE, FALSE)

head(Mahal_df) #shows us the first 6 rows of the data so we can see the new variables (Mahal, MOutlier)
```

Below, the code will count the number of outliers.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
OutlierCount <- Mahal_df%>%
  count(MOutlier)
OutlierCount
```
We have only 4 outliers. Should we delete any? One common practice is to sort the data by the "Mahal" variable and look for "jumps." If there is a consistent increase, then many researchers leave the data in.

>Next we evaluated the distributional characteristics of the data. From a univariate perspective, skew and kurtosis values of the variables fell below the values of 3 (skew) and 8 to 20 (kurtosis) that Kline suggests are concerning [-@kline_principles_2016]. We evaluated multivariate normality with the Mahalanobis distance test. Specifically, we used the *outlier()* function in the *psych* package and included all continuous variables in the calculation. Our visual inspection of the Q-Q plot suggested that the plotted line strayed from the straight line as the quantiles increased.  Additionally, we appended the Mahalanobis distance scores as a variable to the data. Analyzing this variable, we found that four cases exceeded three standard deviations beyond the median. We sorted the Mahalanobis values. Because there was a continuous increase with no "jumps" we retained all cases.  

### Conduct appropriate preliminary analyses (*M*s, *SD*s, *r*-matrix). {-}

#### Internal consistency (alpha) coefficients {-}

Although these are typically reported in the Method section in the description of the measures, we should calculate internal consistency coefficients for our three scales.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
ValuedVars <- c("ValObjectives", "IncrUnderstanding", "IncrInterest")
TradPedVars <- c("ClearResponsibilities", "EffectiveAnswers", "Feedback", "ClearOrganization", "ClearPresentation")
SRPedVars <- c("InclusvClassrm", "EquitableEval", "MultPerspectives", "DEIintegration")
```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
psych::alpha(babydf[, ..ValuedVars])

#if this code throws an error for you rewrite deleting the two dots in front of the ValuedVars object
```

Alpha for the Valued-by-Me dimension is 0.80

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
psych::alpha(babydf[, ..TradPedVars])
```

Alpha for Traditional Pedagogy dimension is 0.89


```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
psych::alpha(babydf[, ..SRPedVars])
```
Alpha for the SCR Pedagogy dimension is 0.87

#### Means, standard deviations, and a correlation matrix {-}

Means, standard deviations, and a correlation matrix are also commonly reported. Because three of our constructs are scales, we will need to calculate their means for cases that have met the minimum thresshold for nonmissingness.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}

#calculate means for when a specified proportion of items are non-missing
babydf$Valued <- sjstats::mean_n(babydf[,..ValuedVars], .66)#will create the mean for each individual if 66% of variables are non-missing
babydf$TradPed <- sjstats::mean_n(babydf[, ..TradPedVars], .66)#will create the mean for each individual if 66% of variables are non-missing
babydf$SRPed <- sjstats::mean_n(babydf[, ..SRPedVars], .66)#will create the mean for each individual if 66% of variables are non-missing
```

The *apaTables::cor.table* function creates the standard table that will include the means, standard deviations, and correlation matrix.  For some reason my concatonated list function isn't working so I'm just creating a corr_df and will conduct the analyses with those.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
corr_df <- dplyr::select(babydf, Valued, TradPed, SRPed, CEN)
apaTables::apa.cor.table(corr_df)
```
#### Write up of preliminary analyses

>Our initial inspection of the data indicated that 112 attempted the course evaluation. Across cases, the proportion of missingness in the responses ranged from 0% to 15%. Across the dataframe there was 1% of missingness across the cells. Approximately 88% of the cases had nonmissing data. The predominant pattern of missingness included individuals opening the survey without completing any of the items. Beyond that, our inspection of a missingness map indicated a missingness on several of the items assessing socially responsive pedagogy (e.g., multiple perspectives, inclusive classroom, DEI integration). This could be because these items were added to the course evaluations after the study began. Because missingess was so low, we retained all cases and did not inspect further.

>Next we evaluated the distributional characteristics of the data. From a univariate perspective, skew and kurtosis values of the variables fell below the values of 3 (skew) and 8 to 20 (kurtosis) that Kline suggests are concerning [-@kline_principles_2016]. We evaluated multivariate normality with the Mahalanobis distance test. Specifically, we used the *outlier()* function in the *psych* package and included all continuous variables in the calculation. Our visual inspection of the Q-Q plot suggested that the plotted line strayed from the straight line as the quantiles increased.  Additionally, we appended the Mahalanobis distance scores as a variable to the data. Analyzing this variable, we found that four cases exceeded three standard deviations beyond the median. We sorted the Mahalanobis values. Because there was a continuous increase with no "jumps" we retained all cases.  Means, standard deviations, and a correlation matrix are found in Table 1.


### Specify and evaluate a *measurement* model. {-}

A measurement model includes all of the variables to be included in the model in a structure that allows all the latent variables to freely correlate with each other. This model WILL HAVE the best fit of all models where there are fewer paths.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
msmt_mod <- "
        ##measurement model
         CTR =~ CEN 
         TrP =~ ClearResponsibilities + EffectiveAnswers + Feedback + ClearOrganization + ClearPresentation
         SRP =~ InclusvClassrm + EquitableEval + MultPerspectives + DEIintegration
         Val =~ ValObjectives + IncrUnderstanding + IncrInterest
    
         
        # Variance of the single item indicator
         CTR ~~ 0*CEN
        
        # Covariances
         CTR ~~ TrP
         CTR ~~ SRP
         CTR ~~ Val
         TrP ~~ SRP
         TrP ~~ Val
         SRP ~~ Val
         
        "

set.seed(230916)
msmt_fit <- lavaan::cfa(msmt_mod, data = babydf, missing = "fiml")
lavaan::summary(msmt_fit, fit.measures = TRUE, standardized = TRUE)

```

Below is script that will export the global fit indices (via *tidySEM::table_fit*) and the parameter estimates (e.g., factor loadings, structural regression weights, and parameters we requested such as the indirect effect) to .csv files that you can manipulate outside of R.  

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#global fit indices
msmt_globalfit <- tidySEM::table_fit(msmt_fit)
msmt_fit_pEsts <- tidySEM::table_results(msmt_fit, digits=3, columns = NULL)
write.csv(msmt_globalfit, file = "msmt_globalfit.csv")
#the code below writes the parameter estimates into a .csv file
write.csv(msmt_fit_pEsts, file = "msmt_fit_pEsts.csv")
```

Let's plot what we did>
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
msmt_m1 <- semPlot::semPaths(msmt_fit, what = "col", whatLabels = "stand", sizeMan = 5, node.width = 1, edge.label.cex = .75, style = "lisrel", mar = c(5,5,5,5))
```

>With the exception of the SRMR, the fit of the measurement model was poor: $\chi ^{2}(60) = 212.69, *p* < 0.001, CFI = .841, RMSEA = 0.151, CI90(0.129, 0.173), SRMR = 0.071$, SRMR = 0.071.

If this project were for actual publication, I would take this a step further and respecify the model with parcels. Why? Because a parcelled model (which approaches a *just-identified model* will usually have better global fit). I address this topic more completely in [ReCentering Psych Stats: Multivariate Modeling](https://lhbikos.github.io/ReC_MultivModel/MeasMod.html).

### Specify and evaluate a *structural* model {-}

As a reminder, I am hypothesizing a multiple regression predicting value to the student from traditional pedagogy, socially responsive pedagogy, and centering. 

X1 = Centering: explicit recentering (0 = precentered; 1 = recentered)
X2 = TradPed: traditional pedagogy (continuously scaled with higher scores being more favorable)
X3 = SRPed:  socially responsive pedagogy (continuously scaled with higher scores being more favorable)
Y = Valued: valued by me (continuously scaled with higher scores being more favorable)

This structural model will have fewer paths than its predecessor, the measurement model. The *lavaan::sem* specification looks quite similar to the measurement model. Differences are:

* The map of equations predicts Valued by centering, traditional pedagogy, and socially responsive pedagogy.
* The *lavaan::sem* code includes the statement "auto.cov.lv.x=FALSE"; this prevents lavaan from allowing the latent variables to correlate with each other.


![An image of the parallel mediation model for the homeworked example](Worked_Examples/images/HypothesizedModel.PNG)


```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
ReC_struct_mod1 <- "
        #measurement model
         CTR =~ CEN #this is a single item indicator, I had to add code below to set the variance
         TrP =~ ClearResponsibilities + EffectiveAnswers + Feedback + ClearOrganization + ClearPresentation
         SRP =~ InclusvClassrm + EquitableEval + MultPerspectives + DEIintegration
         Val =~ ValObjectives + IncrUnderstanding + IncrInterest
    
        # Variance of the single item indicator
         CTR ~~ 0*CEN
        
        #structural model
          Val ~ CTR + TrP + SRP
         
          "
set.seed(230916) #needed for reproducibility 
ReC_struct_fit1 <- lavaan::sem(ReC_struct_mod1, data = babydf, missing= 'fiml', fixed.x=FALSE, auto.cov.lv.x=FALSE) 
lavaan::summary(ReC_struct_fit1, fit.measures=TRUE, standardized=TRUE, rsq = TRUE)


```

Below is script that will export the global fit indices (via *tidySEM::table_fit*) and the parameter estimates (e.g., factor loadings, structural regression weights, and parameters we requested such as the indirect effect) to .csv files that you can manipulate outside of R.  
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#global fit indices
ReC_globalfit1 <- tidySEM::table_fit(ReC_struct_fit1)
ReC_struct_pEsts <- tidySEM::table_results(ReC_struct_fit1, digits=3, columns = NULL)
write.csv(ReC_globalfit1, file = "ReC_globalfit1.csv")
#the code below writes the parameter estimates into a .csv file
write.csv(ReC_struct_pEsts, file = "ReC_struct_pEsts1.csv")
```

Let's work up a figure. Luckily, the *layout* and *rotate* commands within within *semPlot:semPlot* make it easy to produce a reasonable figure.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
plot_ReC_struct1 <- semPlot::semPaths(ReC_struct_fit1, what = "col", whatLabels = "stand", sizeMan = 3, node.width = 1, edge.label.cex = .75, style = "lisrel", mar = c(2,2,2,2), structural = FALSE, curve=FALSE, intercepts=FALSE, layout = "tree", rotation = 2)
```


If we table the results, here's what we have:

**Table 2 ** 

|Model Coefficients Assessing the Effect of Perceived Value from Recentering, Traditional Pedagogy, and Socially Responsive Pedagogy
|:----------------------------------------------------------------------|

| Predictor                        |$B$     |$SE_{B}$|$p$      |$\beta$ |                  
|:---------------------------------|:------:|:------:|:-------:|:-------|
|Centering                         |-0.039  |0.054	 |0.474	   |-0.052  | 
|Traditional Pedagogy              |0.638	  |0.125	 |< 0.001  |1.218   |
|Socially Responsive Pedagogy      |-0.016  |0.080	 |0.844	   |0.024   |
 


>Our structural model was a multiple regression, predicting perceived value to the student directly from centering, traditional pedagogy, and socially responsive pedagogy. Results of the global fit indices all fell below the thresholds of acceptability $(\chi^2(63) = 314.63, p < 0.001, CFI = 0.739, RMSEA = 0.189, 90CI[0.168, 0.210, SRMR =  0.289)$. As shown in Table 2, only traditional pedagogy was a significant predictor of value to the student.


### APA style results with table(s) and figure.

>**Preliminary Analyses**
>>Our initial inspection of the data indicated that 112 attempted the course evaluation. Across cases, the proportion of missingness in the responses ranged from 0% to 15%. Across the dataframe there was 1% of missingness across the cells. Approximately 88% of the cases had nonmissing data. The predominant pattern of missingness included individuals opening the survey without completing any of the items. Beyond that, our inspection of a missingness map indicated a missingness on several of the items assessing socially responsive pedagogy (e.g., multiple perspectives, inclusive classroom, DEI integration). This could be because these items were added to the course evaluations after the study began. Because missingess was so low, we retained all cases and did not inspect further.

>Next we evaluated the distributional characteristics of the data. From a univariate perspective, skew and kurtosis values of the variables fell below the values of 3 (skew) and 8 to 20 (kurtosis) that Kline suggests are concerning [-@kline_principles_2016]. We evaluated multivariate normality with the Mahalanobis distance test. Specifically, we used the *outlier()* function in the *psych* package and included all continuous variables in the calculation. Our visual inspection of the Q-Q plot suggested that the plotted line strayed from the straight line as the quantiles increased.  Additionally, we appended the Mahalanobis distance scores as a variable to the data. Analyzing this variable, we found that four cases exceeded three standard deviations beyond the median. We sorted the Mahalanobis values. Because there was a continuous increase with no "jumps" we retained all cases.  

>**Primary Analyses**

>We used structural equation modeling to predict course value to the student (Value) from traditional pedagogy (TradPed), socially responsive pedagogy (SRPed), and centering status (Centering, 0 = precentered, 1 = recentered).

>We began by specifying a measurement model that permitted all latent variables to covary. Fit of this model was poor: $\chi ^{2}(60) = 212.69, *p* < 0.001, CFI = .841, RMSEA = 0.151, CI90(0.129, 0.173), SRMR = 0.071$. Our structural model was a multiple regression, predicting perceived value to the student directly from centering, traditional pedagogy, and socially responsive pedagogy. Results of the global fit indices for the structural model were worse than the measurement model: $(\chi^2(63) = 314.63, p < 0.001, CFI = 0.739, RMSEA = 0.189, 90CI[0.168, 0.210, SRMR =  0.289)$. As shown in Table 2, only traditional pedagogy was a significant predictor of the course's value to the student.
