
# Invariance Testing {#Invariance}

 [Screencasted Lecture Link](https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=e7e805b0-0cda-4259-af17-adcd001178da) 
 
```{r  include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA) #keeps out the hashtags in the knits
options(scipen=999)#eliminates scientific notation
```

The focus of this lecture is invariance testing -- that is, evaluating if a scale operates equivalently across two samples.

## Navigating this Lesson

There is a little more than 1.5 hours of lecture.  If you work through the materials with me it would be plan for an additional 1.5 hours.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail.  All original materials are provided at the [GitHub site](https://github.com/lhbikos/ReC_Psychometrics) that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's [introduction](#ReCintro).

### Learning Objectives

Focusing on this week's materials, make sure you can:

* Specify a series of models that will test for multigroup invariance.
* Interpret model adequacy and fit.
* Compare models on the basis of statistical criteria.
* Recall which parameters (e.g., structures, loadings, intercepts, residuals) are constrained in configural, weak, strong, and strict models
    + and know, without evaluation, which of these models will (necessarily) have the best fit
* interpret $\chi_{D}^{2}$ and $\Delta CFI$ tests to determine if there are statisticaly significant differences in model fit.

### Planning for Practice

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. The least complex is to change the random seed and rework the problem demonstrated in the lesson. The results *should* map onto the ones obtained in the lecture. 

The second option involves utilizing one of the simulated datasets available in this OER. The [last lesson](#sims) in the OER contains three simulations that could be used for all of the statistics-based practice suggestions. Especially if you started with one of these examples in an earlier lesson, I highly recommend you continue with that.

As a third option, you are welcome to use data to which you have access and is suitable for CFA. This could include other simulated data, data found on an open access repository, data from the ReCentering Psych Stats survey described in the [Qualtrics lesson](#qualTRIX), or your own data (presuming you have permission to use it). 
 
In any case, please plan to:

* Specify, interpret, and write up preliminary results for CFA models that examine
  - entire sample (making no distinction between groups)
  - configural invariance
  - weak invariance
  - strong invariance
  - strict invariance
* Create an APA style results section with appropriate table(s) and figure(s)
* Talk about it with someone

### Readings & Resources

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

Byrne, B. M. (2016). Adaptation of assessment scales in cross-national research: Issues, guidelines, and caveats. *International Perspectives in Psychology: Research, Practice, Consultation, 5*(1), 51–65. https://doi.org/10.1037/ipp0000042

Conover, K. J., Israel, T., & Nylund-Gibson, K. (n.d.). Development and Validation of the Ableist Microaggressions Scale. The Counseling Psychologist, 30.

* Our research vignette for this lesson

Hirschfeld, G., & von Brachel, R. (2014). Multiple-Group confirmatory factor analysis in R – A tutorial in measurement invariance with continuous and ordinal indicators. 19(7), 12.

Kline, R. (2016). Principles and practice of structural equation modeling (Fourth ed., Methodology in the social sciences). New York: The Guilford Press.

* Chapter 16:  Multiple-Samples Analysis and Measurement Invariance

Rosseel, Y. (2019). The *lavaan* tutorial. Belgium: Department of Data Analysis, Ghent University. http://lavaan.ugent.be/tutorial/tutorial.pdf 

* Section 8/Multiple groups

### Packages

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#will install the package if not already installed
#if(!require(MASS)){install.packages("MASS")}
#if(!require(sjstats)){install.packages("sjstats")}
#if(!require(psych)){install.packages("psych")}
#if(!require(tidyverse)){install.packages("tidyverse")}
#if(!require(lavaan)){install.packages("lavaan")}
#if(!require(semPlot)){install.packages("semPlot")}
#if(!require(semTable)){install.packages("semTable")}
```


## Invariance Testing (aka Multiple-Samples SEM or Multiple-Group CFA [MG-CFA])

### Introducing the Topic and the Terminology 

As we enter this topic of *invariance* it will be useful to take a few moments to look at the definitions. It is common for the use of the terms "invariance" and "noninvariance" to be confusing. As we look at each of these definitions I have often wondered why we don't start with "variant" or "variation."

* **Variance**: is not a term we are using today. However, recalling the notions of *variable* (which contrasts with *constant*), *variation*, *variability* may help you with *invariance* (which means, it doesn't vary).

*	**Invariance** is synonymous with equivalence. That is, there are not statistically significant differences between the two versions/models being compared.

*	**Noninvariance** is synonymous with nonequivalence. That is, there are statistically significant differences between the two versions/models being compared

* **Equality constraints** are imposed by the researcher when we specify (require) two or more parameters to be equal.  The particular constraints could be placed on between factor loadings, covariances between factors, intercepts, error variances, error covariances, and so forth.  Such constraints simplify the analysis because only one coefficient is needed rather than two.  In a multiple-samples/groups, like invariance testing, a cross-group equality constraint forces the computer to derive equal estimates of the same parameter across all groups.  This specification corresponds to the null hypothesis that the parameter is equal in all populations from which the samples are drawn.  We then conduct formal difference tests to see if, in fact, the model fit is worse when the two groups are constrained to be equal on that parameter (or more likely, set of parameters).

**Measurement invariance** is a property when a set of indicators measures the same constructs with equal precision over different samples. 

A scale is said to have measurement invariance (or, measurement equivalence) across groups if respondents with identical levels of the latent construct have the same expected raw scores on the measure [@hirschfeld_multiple-group_2014].

We can think of this in several ways:

* whether values of model parameters of substantive interest vary in meaningful ways across different samples,

* as an interaction -- whether sample membership *moderates* the relations specified in a model; if there is evidence for a *group x model* interaction, then the program must be allowed to derive separate estimates of some parameters in each sample in order for the model to have acceptable fit over all samples involved,

* whether scores from the operationalization of a construct have the same meaning under differing conditions
    + these conditions could involve time of measurement, test administration methods, or populations (national samples, clinical/community samples, children/adults, and so forth)
    + absence of invariance says that findings of differences between persons cannot be unambiguously isolated from differences owing to time, methods, group membership (thus, there would be no clear basis for drawing inferences from the scores)
  
**Longitudinal measurement invariance** evaluates the stability in measurement parameters over time for the same population.

**Method invariance** is concerned with whether different methods of administration (online survey versus paper/pencil) are invariant.

My experience with invariance testing is the multiple language/cross-cultural/international context. Here we often ask, "What makes a test culturally transferable?" Byrne's [-@byrne_adaptation_2016] article provides a current, excellent, thorough review. Highlights include:

* In the past we could claim that a test was culturally adaptive if it involved
    + Translation/backtranslation
    + Replication of factor structure within the culture
    + Replication of validity and reliability estimates
    
*	Today, there is a movement toward testing adaptation 
    + Including all the past steps, PLUS
    + Invariance testing to explore the factor structure across cultures
    + Investigation of item bias and construct relations
    
In this lesson we will focus rather narrowly on Byrne's [-@byrne_adaptation_2016] strategy for the statistical/psychometric evaluation of invariance. You might also be interested in Gerstein's,  Systematic Test of Equivalence Procedure (STEP; [-@gerstein_theory_2021]), which walks the researcher, item-by-item, through six step analysis of the cultural appropriateness of each item. The researcher is prompted to consider why items are and are not appropriate/relevant and how they might be modified. 

### Evaluation Strategies

There are two primary options for establishing multigroup invariance.

#### Free baseline approach

In the free baseline approach, testing for measurement invariance is a hierarchical, *model trimming*, strategy. Specifically, the *configural model* (the initial, unconstrained model) is gradually restricted by adding cross-group equality constraints in a sequence that corresponds to *weak*, *strong*, and *strict* invariance. At the point that the invariance hypothesis cannot be retained, testing stops (i.e., more restricted models are not considered).

In the free baseline approach:

* Respecification moves from nesting to nested models.
* Fit generally worsens in each subsequent model.
* The goal is non-significant differences in fit with each additional set of cross-group equality constraints.

#### Constrained baseline approach

In the constrained baseline approach, testing for measurement invariance is  a *model building* approach where the most restricted model (*strict*; with equal pattern coefficients, intercepts, and residuals) is the baseline. If necessary, these are sequentially released and compared backwards through the hierarchy (*strong*, *weak*, *configural*) but some researchers will switch around the order in which constraints are released.

In the constrained baseline approach:

* Respecification moves from nested to nesting models.
* Fit generally improves in each subsequent model
* The goal is to have satisfactory fit in the most restricted model (but this often is not the case).

Ideally-and-theoretically, model trimming and building approaches will end up in the same place, but this is not guaranteed.

### Invariance Testing Workflow

Today we will will use the *free baseline* approach in testing the measurement equivalence of a scale across two groups. 

![Image of a flowchart and decision-tree for multi-group invariance testing](images/Invariance/Workflow_Invariance.png)

Multigroup invariance testing involves:

* Structuring up the item-level dataset (i.e., reverse-coding any variables)
* For the groups-of-interest, identifying a common *baseline model* that meets acceptable standards for model fit. These standards include:
  - theoretical and statistical identification
  - appropriate magnitude and direction of factor loadings. 
* Specifying and comparing a series of increasingly restrictive models. These models include:
  - Configural invariance (the same CFA structure)
  - Weak invariance (configural + pattern/factor loadings)
  - Strong invariance (weak + item intercepts)
  - Strict (strong + error variances and covariances)
* At the point (i.e., the model) that fit is unacceptable, consider investigating and reporting the source of partial measurement invariance.

### Successive Gradations of Measurement Invariance

There are four levels of invariance. Invariance testing is a sequential and incremental process. Thus, each successive level of variance is prerequisite on meeting the criteria of the prior. Different authors use different names for these same notions. 

#### Configural invariance

Configural invariance is the least restrictive level. It implies that the number of latent variables and the pattern of loadings on the latent variables on indicators are similar across the groups.

Configural invariance is tested by specifying the same CFA model in each group.  Both the number of factors and the correspondence between factors and indicators are the same, but all parameters are freely estimated in each group.  

* If this model is not consistent with the data, then measurement invariance does not hold at any subsequent level.

* If the model is retained, it says that the same factors are manifested (in potentially different ways) in each group.  Differences could include: 
    + unequal pattern coefficients
    + unequal intercepts
    + unequal error variances
    
* If there is only configural variance, a different weighting scheme would be needed for each group.

#### Weak invariance

Weak invariance is sometimes termed *pattern invariance* and *metric invariance*.  Weak invariance requires *configural variance* plus equality of unstandardized pattern coefficients.  That is, the magnitude of the loadings is similar across the groups.

The hypothesis of weak invariance is tested by:

* Imposing an equality constraint over groups on the unstandardized coefficient of each indicator. Then
* Comparing with the chi-square difference ($\chi_{D}^{2}$) test the configural invariance model and the weak invariance model
    + In comparing models, if the fit of more restrictive invariance model tested *is not appreciably worse* than the immediately prior restrictive model, then the more restrictive weak invariance hypothesis is retained.
    + Thus, the weak invariance model would be compared to the configural invariance model.  IF we use the $\chi_{D}^{2}$ > .05 (and we'll learn later that there are better/more options), then we can claim weak invariance.
    + Think back to what we learned about comparing nested/hierarchical models. As we continue through this invariance *hierarchy* (configural, weak, strong, strict), each of the more restrictive measures will have worse fit (the prior, lesser restrictive model will be the nesting model with "more sticks" and fewer degrees of freedom).  Therefore, we would really like there to be NO DIFFERENCE in model fit when we add between-group equality constraints.
* If weak invariance is supported, then we can claim that constructs are manifested the same way in each group. This means that:
    + slopes from regressing the indicators on their respective factors are equal across all groups, and
    + factor scores can be calculated using the same weighting scheme in all groups tested
* If weak invariance is rejected, then...
    + the factors (or at least a subset of items corresponding to those factors) have different meanings in different groups
    + *extreme response styles (ERS)* may affect response variability, for example, low ERS is the tendency to avoid endorsing the most extreme options (e.g., never, always); high ERS is the tendency to endorse the most extreme options
* If we can support weak invariance, we are justified in formally comparing estimated factor variances or covariances across different groups, but because indicators are affected by both factors and sources of unique (residual) variation, we need MORE in order to statistically compare observed variances or covariances over groups. This comes from the next level.

#### Strong invariance**

Strong invariance is also termed *scalar invariance*; it is predicated on weak invariance. Strong invariance implies that the item loadings plus the item intercepts are similar across the groups.  It also implies that there are no systematic response biases.  It is required in order to meaningfully compare the means of latent variables across different groups.  

* Item intercepts are considered to be the origin or starting value of the scale that your factor is based on. Thus, participants who have the same value on the latent construct should have equal values on the items on which the construct is based. These are related to the mean structure, hence you'll see some refer to this as means.
* The intercept estimates the score on an indicator given a true score of zero on the corresponding factor
* *Equality of intercepts* says that different groups use the response scale of that indicator in the same way; that is, a person from one group and a person from a different group with the same level on the factor should obtain the same score on the indicator.

The hypothesis of strong invariance is tested by:

* Imposing equality constraints on unstandardized pattern coefficients and intercepts. Then,
* Comparing this model with the model of the equality-constrained pattern coefficients (i.e., the weak invariance model) with the $\chi_{D}^{2}$  
   + If the fit of the more restrictive invariance model tested *is not appreciably worse* than the immediately prior restrictive model, then the more restrictive weak invariance hypothesis is retained.  
    + Thus, the strong invariance model would be compared to the weak invariance model.  If $\chi_{D}^{2}$ > .05, then we can claim strong invariance.  

* If strong invariance is rejected, then we may be concerned about a *differential additive (acquiescence) response style*: systematic influences unrelated to the factors that decrease or increase the overall level of responding on an indicator in a particular population  
    + Example:  if patients are weighed in street clothes in the clinic and in a gown at the hospital, an additive constant is added to true body weight, dependent upon where patients are tested; this contaminates the estimates of mean weight differences over the two clinics.  
    + If a response style affects all indicators, then invariance testing will not detect this pattern; instead the estimates of the construct will be influenced by response styles that are uniform over all indicators.  
  
* **Differential item functioning** is the pattern that an indicator has appreciably unequal pattern coefficients or intercepts over groups; DIF violates measurement invariance.
    + A goal in multiple-samples CFA is to locate the indicator(s) responsible for rejecting the hypothesis of weak or strong invariance
    + In test development, we flag items as candidates for revision or deletion
* If strong invariance is supported
    + group differences in estimated factor means will be unbiased  
    + group differences in indicators means or estimated factor scores will be directly related to the factor means and will not be distorted by differential additive response bias  
    + the factors have a common meaning over groups and any constant effects on the indicators are cancelled out when observed means are compared over groups  
    + strong invariance is the minimal level required for meaningful interpretation of group mean contrasts  
  
#### Strict invariance

Strict invariance requires strong invariance plus the equality in error variances and covariances across groups. This means that the indicators measure the same factors in each group with the same degree of precision. There are some rifts about what exactly constitutes strict invariance:  

* residual invariance is required in order to claim that factors are measured identically across group (Deshon, 2004; Wu et al., 2007)  
* Because unique (residual) error variance reflects random measurement error and systematic variance, the sum of these two components must be equal across groups (Little, 2013). Kline says that it may be too strict and somewhat unreasonable/unattainable.  Little (2013) also cautioned against enforcing this requirement because if the sum of random and systematic parts of unique variance is not exactly equal, the amount of misfit due to equality-constrained residual variances must contaminate estimates elsewhere in the model.

### Tests for Model Comparison

It is not sufficient to declare any level of invariance (i.e., constrained (configural, weak, string, or strong) to be adequate on the basis of the traditional evaluation of fit (i.e., strength and significance of factor loadings, fit indices). Rather the whole models must be compared through formal statistical comparison.  There are several options and they all have caveats.

A non-significant chi-square difference test ($\chi_{D}^{2} > .05$) that compares less-and-more restrictive models indicates that the stricter invariance hypothesis should *not* be rejected.  That is, it supports invariance for the more restricted model.

* In large samples, this could be statistically significant, even though the absolute differences in parameter estimates are trivial.
* Thus, the $\chi_{D}^{2}$ could indicate lack of measurement invariance when the imposition of cross-group equality constraints makes relatively little difference in fit.  Options for verifying:
  + compare unstandardized solutions across groups
  + inspect changes in approximate fit indices
  + BUT...there are few guidelines for how to do this

When the  CFA change statistic is smaller than .01 ($\Delta CFI < .01$) there is evidence that the stricter invariance hypothesis should *not* be rejected.  That is, it supports invariance for the more restricted model.

* Simulation studies suggested that stability for different model characteristics such as number of indicators per factor.  Here are some findings (guidelines?) for different testing scenarios:
    + In super large samples (~6,000) use $\Delta CFI < .002$.
    + When group sizes are small ($n < 300$) and unequal, use $\Delta CFI < .005$ and $\Delta RMSEA < .010$.
    + When group sizes are larger ($n > 300$), equal, and the pattern of invariance was mixed (i.e., there are at least two invariant parameters, each of which is from a different category [pattern coefficient, intercept, residual variance]), use $\Delta CFI < .010$ and $\Delta RMSEA < .015$.

$\Delta NCI$ was also stable, but Kline did not provide a threshold (and I don't see the NCI reported much in psychometrics papers.

The general practice seems to favor reporting both the $\chi_{D}^{2}$ and $\Delta CFI$. Even if  $\chi_{D}^{2} > .05$, a $\Delta CFI < .01$ supports invariance between models.

### Partial measurement invariance

The notion of partial measurement invariance was introduced by Byrne, Shavelson, and Muthen (1989) and is often used to describe an *intermediate* state of invariance.  For example, weak invariance assumes cross-group equality of each unstandardized pattern coefficient. If some, but not all pattern coefficients are invariant, then only *partial weak invariance* can be claimed.

The researcher may investigate which pattern coefficients are noninvariant and relax (or free) them to differ across groups. Once freed, the research might choose to compare the models with the  $\chi_{D}^{2} > .05$, a $\Delta CFI < .01$. Once enough pattern coefficients have been freed and the fit across models is equivalent, the researcher might continue the process of determining the degree of invariance in the more restricted evaluations (e.g., strong, strict). 

Even if the researcher does not continue with testing for invariance in the increasingly restricting models, they have learned which pattern coefficients vary across groups.

## Research Vignette

This lesson's research vignette emerges from Conover et al's Ableist Microaggressions Scale (AMS [-@conover_development_2017]). The article reports on a series of three studies comprised the development, refinement, and psychometric evaluation of the AMS. I simulated data from the results of the exploratory factor analysis in the second study. 

Conover et al. [-@conover_development_2017] reported support for using a total scale score (22 items) or four, correlated, subscales. Below, I list the four subscales, their number of items, and a single example item. At the outset, let me provide a content warning. For those who hold this particular identity (or related identities) the content in the items may be upsetting. In other lessons, I often provide a variable name that gives an indication of the primary content of the item. In the case of the AMS, I will simply provide an abbreviation of the subscale name and its respective item number. This will allow us to easily inspect the alignment of the item with its intended factor, and hopefully minimize discomfort. If you are not a member of this particular identity, I encourage you to learn about these microaggressions by reading the article in its entirety.  Please do not ask members of this group to explain why these microaggressions are harmful or ask if they have encountered them.  

There are 20 items on the AMS scale. The frequency scaling ranged from 0(*never*) to  5(*very frequently*). Higher scores indicate higher frequency of microaggressions.

The four factors, number of items, and sample item are as follows:

* Helplessness (5 items)
  - "People feel they need to do something to help me because I have a disability."
  - Abbreviated in the simulated data as "Help#"
* Minimization (3 items)
  - "People minimize my disability or suggest that it could be worse."
  - Abbreviated in the simulated data as "Min#"
* Denial of Personhood (5 items)
  - "People don't see me as a whole person because I have a disability."
  - Abbreviated in the simulated data as "Pers#"
* Otherization (7 items)
  - "People indicate that they would not date a person with a disability."
  - Abbreviated in the simulated data as "Oth#"
  
In the simulation below, I use the same factor loadings from the EFA and correlations between factors for both the mild and severe groupings. I use the reported means for the mild group, and arbitrarily make them higher for the severe group. 

Below I walk through the data simulation. This is not an essential portion of the lesson, but I will lecture it in case you are interested. None of the items are negatively worded (relative to the other items), so there is no need to reverse-score any items.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70), warning=FALSE, message=FALSE }
#Simulating the data for the respondents with mild disability
AMSmild_generating_model <- '
        #measurement model
        Help  =~ .74*Help1 + .75*Help2 + .65*Help3 + .58*Help4 + .62*Help5
        Minim =~ .71*Min1 + .52*Min2 + .47*Min3
        Person =~ .71*Per1 + .84*Per2 + .74*Per3 + .56*Per4 + .42*Per5 
        Other =~ .89*Oth1 + .73*Oth2 + .70*Oth3 + .46*Oth4 + .41*Oth5 + .40*Oth6 + .32*Oth7

        #Means
         Help ~1.96*1
         Minim ~ 2.76*1
         Person ~ 1.51*1
         Other ~1.17*1
        
         #Correlations
         Help ~~ .27*Minim
         Help ~~ .66*Person
         Help ~~ .68* Other
         Minim ~~ .36*Person
         Minim ~~ .30*Other
         Person ~~ .76*Other
       
         '

set.seed(240504)
AMSmild <- lavaan::simulateData(model = AMSmild_generating_model,
                              model.type = "sem",
                              meanstructure = T,
                              sample.nobs=548,
                              standardized=FALSE)

#Adding a variable that denotes mild condition

AMSmild$Group <- "Mild"


AMSsev_generating_model <- '
        #measurement model
        Help  =~ .68*Help1 + .76*Help2 + .57*Help3 + .62*Help4 + .72*Help5
        Minim =~ .75*Min1 + .59*Min2 + .51*Min3
        Person =~ .69*Per1 + .79*Per2 + .79*Per3 + .58*Per4 + .51*Per5 
        Other =~ .72*Oth1 + .71*Oth2 + .75*Oth3 + .51*Oth4 + .51*Oth5 + .36*Oth6 + .42*Oth7

        #means
         Help ~ 3.5*1
         Minim ~ 3.3*1
         Person ~ 3.01*1
         Other ~ 2.32*1

        #correlations
         Help ~~ .31*Minim
         Help ~~ .44*Person
         Help ~~ .55* Other
         Minim ~~ .40*Person
         Minim ~~ .35*Other
         Person ~~ .49*Other
'
set.seed(240504)
AMSsev <- lavaan::simulateData(model = AMSsev_generating_model,
                              model.type = "sem",
                              meanstructure = T,
                              sample.nobs=285,
                              standardized=FALSE)
#used to retrieve column indices used in the rescaling script below

#Adding a variable that denotes severe condition
AMSsev$Group <- "Severe"

#Binding the separate groups together in a single file

dfAMSi <- dplyr::bind_rows(AMSmild, AMSsev)

col_index <- as.data.frame(colnames(dfAMSi))

#The code below loops through each column of the dataframe and assigns the scaling accordingly
#All rows are the iBel scales, administrations A and B


for(i in 1:ncol(dfAMSi)){  
  if(i >= 1 & i <= 20){   
    dfAMSi[,i] <- scales::rescale(dfAMSi[,i], c(0, 5))
  }
}

#Now round to zero
library(tidyverse)
dfAMSi <- dfAMSi %>%
  mutate_if(is.numeric,round, digits = 0)

#quick check of my work
psych::describe(dfAMSi) 
```

The optional script below will let you save the simulated data to your computing environment as either a .csv file (think "Excel lite") or .rds object (preserves any formatting you might do). If you save the .csv file and bring it back in, you will lose any formatting (e.g., ordered factors will be interpreted as character variables).

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#write the simulated data  as a .csv
#write.table(dfAMSi, file="dfAMSi.csv", sep=",", col.names=TRUE, row.names=FALSE)
#bring back the simulated dat from a .csv file
#dfAMSi <- read.csv ("dfAMSi.csv", header = TRUE)
#str(dfAMSi)
```
In this lesson I made the Severity variable a factor during the simulation. Importing the exported .csv file will lose that formating. Therefore, unless you need to use a .csv file outside of R, I recommend using the .rds file.

An .rds file preserves all formatting to variables prior to the export and re-import. If you already exported/imported the .csv file, you will need to re-run the simulation.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#to save the df as an .rds (think "R object") file on your computer; it should save in the same file as the .rmd file you are working with
#saveRDS(dfAMSi, "dfAMSi.rds")
#bring back the simulated dat from an .rds file
#dfAMSi <- readRDS("dfAMSi.rds")

```
Let's check the structure of the data:

```{r}
str(dfAMSi)
```
We need "Group" to be a factor. 

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
dfAMSi[,'Group'] <- as.factor(dfAMSi[,'Group'])
```


## Whole-Group and Baseline Analyses

Conover et al.[-@conover_development_2017] conducted the invariance testing with the four-factor, correlated factors model. Let's start by simply by creating an overall measurement model from the dataset without regard to group membership.  

### Whole Group CFA

With the number of items per scale ranging from 3 to 7 on this multidimensional, first-order, factor structure we are sufficiently *identified*.  Remember, rule is at least 3 items/indicators per factor for unidimensional scales and 2 items/indicators per factor for a multidimensional scale.  

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
set.seed(240504)
AMS4CorrMod <-  '
            Helplessness =~ Help1 + Help2 + Help3 + Help4 + Help5
            Minimization =~ Min1 + Min2 + Min3
            DenialPersonhood =~ Per1 + Per2 + Per3 + Per4 + Per5 
            Otherization =~ Oth1 + Oth2 + Oth3 + Oth4 + Oth5 + Oth6 + Oth7
'
set.seed(240504)            
AMS4CorrFit <- lavaan::cfa(AMS4CorrMod, data = dfAMSi)
lavaan::summary(AMS4CorrFit, fit.measures = TRUE, standardized = TRUE)
```
Among my first steps are also to write the code to export the results. The *tidySEM* package has useful functions to export the fit statistics, parameter estimates, and correlations among the latent variables (i.e., factors).

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
AllFitStats <- tidySEM::table_fit(AMS4CorrFit)
All_paramEsts <- tidySEM::table_results(AMS4CorrFit, digits=3, columns = NULL)
AllCorrs <- tidySEM::table_cors(AMS4CorrFit, digits=3)
#to see each of the tables, remove the hashtab
#AllFitStats
#All_paramEsts
#AllCorrs
```

Next, I export them.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
write.csv(AllFitStats, file = "AllFitStats.csv")
write.csv(All_paramEsts, file = "All_paramEsts.csv")
write.csv(AllCorrs, file = "AllCorrs.csv")
```

### Interpreting the Output

|Criteria                       | Our Results                         | Criteria met?|
|:------------------------------|:-----------------------------------:|:------------:|
|Factor loadings significant, strong, proper valence |Help:  0.55 to 0.66; Min: 0.41 to 0.55; Pers: 0.47 to 0.68 Oth: 0.39 to 0.63 |Yes           | 
|Non-significant chi-square     |$\chi ^{2}(164) = 155.24, p = 0.676$  |Yes          |  
|$CFI\geq .95$                  |CFI = 1.000                           |Yes          |  
|$RMSEA\leq .05$ (but definitely < .10)|RMSEA = 0.000, 90%CI(0.000, 0.013)|Yes      |  
|$SRMR\leq .08$ (but definitely < .10) |SRMR = 0.023                 |Yes          | 
|Combination rule: $CFI \geq .95$ and $SRMR \leq .08$|CFI = 1.000, SRS = 0.023|Yes     |

 
### Partial Write-up

>**Correlated factors model for all in sample**. The model where factors were free to covary demonstrated the following fit to the data:$\chi ^{2}(164) = 155.24, p = 0.676$, CFI = 1.000, RMSEA = 0.000, 90%CI(0.000, 0.013), SRMR = 0.023.  Factor loadings ranged from 0.55 to 0.66 for the Helplessness scale, 0.41 to 0.55 for the Minimization scale, 0.47 to 0.68 for the Denial of Personhood scale, and 0.39 to 0.63 for the Otherization scale.

Producing a figure can be useful to represent what we did to others as well as checking our own work. That is, "Did we think we did what we intended?"  When the *what = "col", whatLabels = "stand") combination is shown, paths that are "fixed" are represented by dashed lines. Below, we expect to see each the four factors predicting only the items associated with their factor, one item for each factor (the first on the left) should be specified as the indicator variable (and represented with a dashed line), and the factors/latent variables should not be freed to covary (i.e., an uncorrelated traits or orthogonal model). Because they are "fixed" to be 0.00, they will be represented with dashed curves with double-headed arrows.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#displays standardized pattern coefficients
semPlot::semPaths(AMS4CorrFit, layout = "tree", style = "lisrel", what = "col", whatLabels = "stand") 
#displays estimates/unstandardized regression weights
#semPlot::semPaths(AMS4CorrFit, layout = "tree", style = "lisrel", what = "col", whatLabels = "est")
```

Our fit is fairly similar to what Conover et al. reported in their article. Specifically, their four-factor, correlated factors model, had a statistically significant chi-square.  Regarding fit:  CFI = .89, SRMR = .07, and RMSEA = .07 CI90% (.06, .07). As researchers, they were satisfied with the result and they asked the question, "Is measure invariant across disability severity."  A first (but not complete) step is to evaluate the model, separately for the groups of interest.  In their case it was mild (where they combined mild and moderate levels of severity) and severe (combining severe and very severe levels).  

### Baseline Model when Severity = Mild

Let's start by subsetting the data.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
mild_df <- subset(dfAMSi, Group == "Mild")
severe_df <- subset(dfAMSi, Group == "Severe")
```

Let's run the CFA model for those participants whose data were classified as "mild."
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
set.seed(240504)
MildFit <- lavaan::cfa(AMS4CorrMod, data = mild_df)
lavaan::summary(MildFit, fit.measures = TRUE, standardized = TRUE)
```
Not surprisingly, our results are similar to the total group. I notice that the pattern coefficients wiggle around a little more (one as low as .13) but that the fit indices seem a little stronger.

|Criteria                          | Mild                    | Severe       |
|:---------------------------------|:-----------------------:|:------------:|
|Factor loadings: Help             |0.451 to 0.581           |              |
|Factor loadings: Min              |0.339 to 0.639           |              |
|Factor loadings: Pers             |0.382 to 0.691           |              |
|Factor loadings: Oth              |0.315 to 0.645           |              |
|Non-significant chi-square        |$p = 0.458$              |              |  
|$CFI\geq .95$                     |CFI = 0.999              |              |  
|$SRMR\leq .08$ (but definitely < .10) |SRMR = 0.031         |              |
|$RMSEA\leq .05$ (but definitely < .10)|RMSEA =  0.004, 90%CI(0.000, 0.020)|  | 


### Baseline Model when Severity = Severe
 
Let's run the CFA model again for those participants whose data were classified as "severe."

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
set.seed(240504)
SevereFit <- lavaan::cfa(AMS4CorrMod, data = severe_df)
lavaan::summary(SevereFit, fit.measures = TRUE, standardized = TRUE)
```

Our visual inspection of the similarity of psychometric characteristics suggests that the measure is functioning similarly across the two levels of severity. 

|Criteria                          | Mild                | Severe       |
|:---------------------------------|:-------------------:|:------------:|
|Factor loadings: Help             |0.451 to 0.581       |0.408 to 0.642|
|Factor loadings: Min              |0.339 to 0.639       |0.407 to 0.527|
|Factor loadings: Pers             |0.382 to 0.691       |0.349 to 0.687|
|Factor loadings: Oth              |0.315 to 0.645       |0.393 to 0.621|
|Non-significant chi-square        |$p = 0.458$          |$p = 0.522$   |  
|$CFI\geq .95$                     |CFI = 0.999          |CFI = 1.000   |  
|$SRMR\leq .08$ (but definitely < .10) |SRMR = 0.044     |              |
|$RMSEA\leq .05$ (but definitely < .10)|RMSEA =  0.004, 90%CI(0.000, 0.026)|  |  

This, though, does not constitute a formal evaluation. Thus, we continue with testing for multigroup invariance.

## Configural Invariance

Configural invariance is our least restrictive level.  We are essentially specifying ONE STRUCTURE -- four correlated factors, each with 3 to 7 items/indicators.  Each model is allowed to have its own loadings, error variances, and so forth.  It's only the structure (the *configuration*) that is consistent.

The same model we had before works.  We create the configural model simply by specifying *group = "Severity"* in the *cfa()* function.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
set.seed(240504)
configural <- lavaan::cfa(AMS4CorrMod, data = dfAMSi, group = "Group")
lavaan::summary(configural, fit.measures = TRUE, standardized = TRUE)
```
Let's format these results into tables.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
ConfigFitStats <- tidySEM::table_fit(configural)
Config_paramEsts <- tidySEM::table_results(configural, digits=3, columns = NULL)
ConfigCorrs <- tidySEM::table_cors(configural, digits=3)
#to see each of the tables, remove the hashtab
#ConfigFitStats
#Config_paramEsts
#ConfigCorrs
```

Then, export them.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
write.csv(ConfigFitStats, file = "ConfigFitStats.csv")
write.csv(Config_paramEsts, file = "Config_paramEsts.csv")
write.csv(ConfigCorrs, file = "ConfigCorrs.csv")
```

Examining the plots can help us understand what we've just done. This will result in two tables, one for each of the models. Recall, we are requiring the structure to be the same, but allowing the values to vary.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#semPlot::semPaths(configural, layout = "tree", style = "lisrel", what = "col", whatLabels = "stand")
semPlot::semPaths(configural, layout = "tree", style = "lisrel", what = "col", whatLabels = "est")
#If R stalls, open the console. I received the instruction, "Hit <Return> to see next plot:"
#Then it ran!
```

*semPath()* automatically produced TWO figures.  Toggling between them, we see the configuration is the same, but some of the values change on the paths.  In the next models we'll tighten those down.

### Interpreting the Output

|Criteria                       | Our Results                         | Criteria met?|
|:------------------------------|:-----------------------------------:|:------------:|
|Mild:  factor loadings significant, strong, proper valence |Help:  0.45 to 0.58; Min: 0.34 to 0.64; Pers: 0.38 to 0.69; Oth: 0.32 to 0.65
 |Yes      | 
|Severe:  factor loadings significant, strong, proper valence |Help:  0.41 to 0.64; Min: 0.41 to 0.47; Pers: 0.35 to 0.56;  Oth: 0.39 to 0.62
 |Yes         | 
|Non-significant chi-square     |$\chi ^{2}(328) = 327.57, p = 0.496$  |Yes           |  
|$CFI\geq .95$ or $CFI\geq .90$ |CFI = 1.000                           |Yes        |  
|$RMSEA\leq .05$ (but definitely < .10)|RMSEA = 0.000, 90%CI(0.000, 0.018)|Yes         |
|$SRMR\leq .08$ (but definitely < .10) |SRMR = 0.032                   |Yes          | 
|Combination rule: $CFI \geq .95$ and $SRMR \leq .08$|CFI = 1.000  SRMR = 0.034|Yes     |

 
### Partial Write-up

>**Configural Model**. The configural model, which constrained only the relative configuration of variables in the model to be the same in both groups had adequate fit to the data: $\chi ^{2}(328) = 327.57, p = 0.496$, CFI = 1.000, SRMR = 0.034, RMSEA =  0.000, 90%CI(0.000, 0.018).

## Weak Invariance

Weak invariances is predicated on configural invariance and it adds cross-group equality constraints on the pattern (factor) loadings.

A priori, we know this will not (can not) be better than configural invariance. We are simply hoping that it is the same or not statistically, significantly different.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
set.seed(240504)
weak <- lavaan::cfa(AMS4CorrMod, data = dfAMSi, group = "Group", group.equal = "loadings")
lavaan::summary(weak, fit.measures = TRUE, standardized = TRUE)
lavaan::anova(configural, weak)
```
$\chi_{D}^{2}(16) = 25.42, p = 0.063$
Let's format these results into tables.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
WeakFitStats <- tidySEM::table_fit(weak)
Weak_paramEsts <- tidySEM::table_results(weak, digits=3, columns = NULL)
WeakCorrs <- tidySEM::table_cors(weak, digits=3)
#to see each of the tables, remove the hashtag
#WeakFitStats
#Weak_paramEsts
#WeakCorrs
```

Then, export them.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
write.csv(WeakFitStats, file = "WeakFitStats.csv")
write.csv(Weak_paramEsts, file = "Weak_paramEsts.csv")
write.csv(WeakCorrs, file = "WeakCorrs.csv")
```

### Interpreting the Output

Note that although the "Std.all" values differ from each other, the "Estimates" (factor loadings) are identical across Mild and Severe groups. Each also has a "label" (e.g., .p2., .p3.) which indicates that they have been constrained to be equal. The "Std.all" differ between degree of disability severity due to the difference in standard deviations of the indicators.

|Criteria                       | Our Results                         | Criteria met?|
|:------------------------------|:-----------------------------------:|:------------:|
|Mild:  factor loadings significant, strong, proper valence |Help:  0.47 to 0.57; Min: 0.40 to 0.53; Pers: 0.38 to 0.66; Oth: 0.35 to 0.64
 |Yes         | 
|Severe:  factor loadings significant, strong, proper valence |Help: 0.41 to 0.57; Min: 0.41 to 0.53; Pers: 0.35 to 0.63;  Oth: 0.35 to 0.64
 |Yes      | 
|Non-significant chi-square     |$\chi ^{2}(344) = 353.00, p = 0.357$  |Yes          |  
|$CFI\geq .95$ or $CFI\geq .90$       |CFI = 0.996                           |Yes           |  
|$RMSEA\leq .05$ (but definitely < .10)|RMSEA = 0.008, 90%CI(0.000, 0.020)|Yes         |
|$SRMR\leq .08$ (but definitely < .10) |SRMR = 0.035                   |Yes          | 
|Combination rule: $CFI \geq .95$ and $SRMR \leq .08$|CFI = 1.000, SRMR = 0.035 |Yes   |


### Partial Write-up

>**Weak invariance model**. The weak invariance model constrained the configuration of variables and all factor loadings to be constant across groups.  Fit indices were comparable to the configural model: $\chi ^{2}(344) = 353.00, p = 0.357$, CFI = 0.996, SRMR = 0.039,  RMSEA = 0.008, 90%CI(0.000, 0.020.  Invariance of the factor loadings was supported by the non-significant difference tests that assessed model similarity: $\chi_{D}^{2}(16) = 25.42, p = 0.063$; $\Delta CFI = 0.004$   

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#The CFI difference test is calculated by simple subtraction
1.000 - 0.996
```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#semPlot::semPaths(weak, layout = "tree", style = "lisrel", what = "col", whatLabels = "stand")
semPlot::semPaths(weak, layout = "tree", style = "lisrel", what = "col", whatLabels = "est")
#If R stalls, open the console. I received the instruction, "Hit <Return> to see next plot:"
#Then it ran!
```

## Strong Invariance

Strong invariance is predicated on configural and weak invariance, but also constrains the indicator means/intercepts.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
set.seed(240504)
strong <- lavaan::cfa(AMS4CorrMod, data = dfAMSi, group = "Group", group.equal=c("loadings", "intercepts"))
lavaan::summary(strong, fit.measures = TRUE, standardized = TRUE)
lavaan::anova(configural, weak, strong)
```

Let's format these results into tables.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
strongFitStats <- tidySEM::table_fit(strong)
strong_paramEsts <- tidySEM::table_results(strong, digits=3, columns = NULL)
strongCorrs <- tidySEM::table_cors(strong, digits=3)
#to see each of the tables, remove the hashtag
#strongFitStats
#strong_paramEsts
#strongCorrs
```

Then, export them.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
write.csv(strongFitStats, file = "strongFitStats.csv")
write.csv(strong_paramEsts, file = "strong_paramEsts.csv")
write.csv(strongCorrs, file = "strongCorrs.csv")
```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#semPlot::semPaths(strong, layout = "tree", style = "lisrel", what = "col", whatLabels = "stand")
semPlot::semPaths(strong, layout = "tree", style = "lisrel", what = "col", whatLabels = "est")
#If R stalls, open the console. I received the instruction, "Hit <Return> to see next plot:"
#Then it ran!
```
### Interpreting the Output

Note that although the "Std.all" values differ from each other, the "Estimates" (factor loadings) are identical across Mild and Severe groups. Each also has a "label" (e.g., .p2., .p3.) which indicates that they have been constrained to be equal. The "Std.all" differ between degree of disability severity due to the difference in standard deviations of the indicators.

|Criteria                       | Our Results                         | Criteria met?|
|:------------------------------|:-----------------------------------:|:------------:|
|Mild:  factor loadings significant, strong, proper valence |Help:  0.45 to 0.57; Min: 0.39 to 0.51; Pers: 0.42 to 0.63; Oth: 0.36 to 0.59
 | Yes|
|Severe:  factor loadings significant, strong, proper valence |Help:  0.44 to 0.60; Min: 0.39 to 0.52; Pers: 0.39 to 0.60;  Oth: 0.38 to 0.59
 |Yes| 
|Non-significant chi-square     |$\chi ^{2}(360) = 412.75 p = 0.029$  |No          |  
|$CFI\geq .95$ or $CFI\geq .90$ |CFI = 0.975                           |Yes           |  
|$RMSEA\leq .05$ (but definitely < .10)|RMSEA = 0.019, CI90%(0.007 to 0.027)|Yes   |
|$SRMR\leq .08$ (but definitely < .10) |SRMR = 0.045                  |Yes          | 
|Combination rule: $CFI \geq .95$ and $SRMR \leq .08$|CFI = .805, SRMR = 0.036 |Yes   |


### Partial Write-up

>**Strong invariance model**. In the strong invariance model, configuration, factor loadings, and indicator means/intercepts were constrained to be the same for each group.  Fit indices were less than ideal: $\chi ^{2}(360) = 412.75 p = 0.029$, CFI = 0.975, SRMR = 0.045  , RMSEA =0.019, CI90%(0.007 to 0.027). The difference tests that evaluated model similarity suggested there was factorial noninvariance: ($\chi_{D}^{2}(16) =  59.754, p = 0.01$; $\Delta CFI = 0.021$. Given that the $\chi_{D}^{2}$ test is statistically significant and the  $\Delta CFI 0.01$ we cannot claim strong invariance and we therefore do not test stricter models.    

```{r}
 0.996 - 0.975
```

**Should I be worried if measurement invariance stops here?**  
Byrne [-@byrne_structural_2016] wrote, "Historically, the Joreskog tradition of invariance testing held that the equality of error variances and their covariances should also be tested.  However, it is now widely accepted that to do so represents an overly restrictive test of the data" (p. 230).  

Further, in an awesome article examining the factorial invariance of the Calling & Vocation Questionnaire [@autin_career_2017] in a binational sample, strict invariance (the next level of restraint) was not even mentioned.  Further, after strong invariance was not achieved the authors wrote, "Therefore, poor fit in only this model does not necessarily indicate the factor structure operates differently for different groups" (p. 695).

SO...as a researcher, I would be happy if I had configural (just the shape) and weak (parameter loadings) invariance. 

Plus..a little later in the lecture we head into *partial measurement invariance.*

If we fail at this stage (or at any earlier stage), we would normally not continue. Because this lesson is for training, we will continue onto the last model.


## Strict Invariance

Strict invariance is predicated on configural, weak, and strong invariance.  To that, it adds cross-group equality constraints on the residuals.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
set.seed(240504)
strict <- lavaan::cfa(AMS4CorrMod, data = dfAMSi, group = "Group", group.equal = c("loadings", "intercepts", "residuals"))
lavaan::summary(strict, fit.measures = TRUE, standardized = TRUE,)
lavaan::anova(configural, weak, strong, strict)
```

Let's format these results into tables.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
strictFitStats <- tidySEM::table_fit(strict)
strict_paramEsts <- tidySEM::table_results(strict, digits=3, columns = NULL)
strictCorrs <- tidySEM::table_cors(strict, digits=3)
#to see each of the tables, remove the hashtag
#strictFitStats
#strict_paramEsts
#strictCorrs
```

Then, export them.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
write.csv(strictFitStats, file = "strictFitStats.csv")
write.csv(strict_paramEsts, file = "strict_paramEsts.csv")
write.csv(strictCorrs, file = "strictCorrs.csv")
```
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#semPlot::semPaths(strict, layout = "tree", style = "lisrel", what = "col", whatLabels = "stand")
semPlot::semPaths(strict, layout = "tree", style = "lisrel", what = "col", whatLabels = "est")
#If R stalls, open the console. I received the instruction, "Hit <Return> to see next plot:"
#Then it ran!
```
### Interpreting the Output

Note that although the "Std.all" values differ from each other, the "Estimates" (factor loadings) are identical across Mild and Severe groups. Each also has a "label" (e.g., .p2., .p3.) which indicates that they have been constrained to be equal. The "Std.all" differ between degree of disability severity due to the difference in standard deviations of the indicators.

|Criteria                       | Our Results                         | Criteria met?|
|:------------------------------|:-----------------------------------:|:------------:|
|Mild:  factor loadings significant, strong, proper valence |Help: .44 to .57; Min: .44 to .52; Pers: .41 to .60; Oth: .36 to .58|
|Severe:  factor loadings significant, strong, proper valence |Help: .46 to .59; Min: .38 to .51 Pers: .43 to .62;  Oth: .37 to .60
 |Yes| 
|Non-significant chi-square     |$\chi ^{2}(380) = 439.69 p < 0.019$  |No        |  
|$CFI\geq .95$ or $CFI\geq .90$   |CFI = 0.971                           |No           |  
|$RMSEA\leq .05$ (but definitely < .10)|RMSEA = 0.019, CI90%(0.009 to 0.027)|Yes(ish)   |
|$SRMR\leq .08$ (but definitely < .10) |SRMR = 0.046                 |Yes(ish)          | 
|Combination rule: $CFI \geq .95$ and $SRMR \leq .08$|CFI = 0.971, SRMR = 0.046 |No   |


### Partial Write-up

**Strict invariance model**. In the strict invariance model, configuration, factor loadings, indicator means/intercepts, and residuals were constrained to be the same for each group.  Fit indices were less than ideal: $\chi ^{2}(380) = 439.69 p < 0.019$ , CFI = 0.971, SRMR = 0.046, RMSEA = 0.019, (90%CI = 0.009 to 0.027).  Although the non-significant chi-square difference test and the change CFI test indicate that the strong and strict invariance models are not statistically significant from each other ($\chi_{D}^{2}(20) = 26.946, p = 0.137$; $\Delta CFI = 0.004$) our earlier data indicated that we cannot claim invariance beyond the weak model.      

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#CFI difference test, calculated by hand
 0.975 - 0.971 
```

### Partial Measurement Invariance

*Partial measurement invariance* can be seen as an intermediate state of invariance and whatever stage (beyond configural; weak, strong, strict) that the criteria for invariance is not met.  For example:

* if the model failed at weak invariance, tests of partial measurement invariance could determine which factor loadings are (and are not) invariant across groups;
* if the model failed at strong invariance, tests of partial measurement invariance could determine which intercepts/means are (and are not) invariant across groups;
* (this one is aspirational) if the model failed at strict invariance, tests of partial measurement invariance could determine which residuals are (and are not) invariant across groups.

Using some of the investigative tools in *lavaan* and the associated packages, researchers can identify which elements are noninvariant.  They can free the constraints until the fit statistics are acceptable and the the Chi-square difference and $\Delta CFI$ tests are no longer significant.

Conover et al. [-@conover_development_2017] reported that the AMS was invariant at configural and weak invariance (i.e., constraining factor loadings to be equal). At the level of strong invariance (i.e., adding constraints to the intercepts), results, the majority of fit indices remained acceptable (CFI = .90, RMSEA = .06, SRMR = .08). However the chi-square difference and CFI change tests were statistically significant:  $\chi_{D}^{2}(20) = 78.83, p < .01$; $\Delta CFI = -.010$. The Conover et al. article did not report further investigation regarding partial measurement invariance.


## APA Style Write-up of the Results

As in the Conover et al. [-@conover_development_2017] article, the write-up of invariance testing would likely be part of a multi-stage evaluation. Therefore, this section would be preceded by a variety of steps in a psychometric evaluation. Here is an example of how I might write this up.

### Measurement Invariance Across Disability Severity

>To test if the factor structures of the AMS were stable across disability severity, we used measurement invariance analyses.  First, we constructed CFA models with the *lavaan* (v. 0.6-17) package in R and created two groups representing mild and severe. Within *lavaan* we successivly constrained parameters representing the configural, weak (loadings), strong (intercepts), and strict (residuals) structures [@hirschfeld_multiple-group_2014; @kline_principles_2015].  A poor fit in any of these models suggests that the aspect being constrained does not operate consistently for the different groups. The degree of invariance was determined jointly when $\chi_{D}^2, p > .05$; and a $\Delta CFI < .01$.  

>The configural model constrains only the relative configuration of variables in the model to be the same in both groups.  In other words, no factor loadings or indicator means are constrained to be the same across groups, but the organization of indicators is the same for both groups.  Next, in weak factorial invariance, the configuration of variables and all factor loadings are constrained to be the same for each group.  Poor fit here suggests that the factor loadings vary in size between the two groups. In strong factorial invariance, both the configuration, factor loadings, and the indicator means are constrained to be the same for each group.  A reduction in fit here, but not in the previous steps, suggests that indicators have different means in both groups, which might be expected when comparing two groups of people, such as in a t-test.  Therefore, poor fit in only this model does not necessarily indicate the factor structure operates differently for different groups.  Finally, strict factorial invariance requires strong invariance and equality in error variances and covariances across groups.  This means that the indicators measure the same factors in each group with the same degree of precision.  

>We selected fit criteria for their capacity to assess different aspects of the statistical analysis.  As is common among SEM researchers, we reported the chi-square goodness of fit ($\chi^2$).  This evaluates the discrepancy between the unrestricted sample matrix and the restricted covariance matrix.  Although the associated $p$ value indicates adequate fit when the value is non-significant, it is widely recognized that large sample size can result in a statistically significant $p$ value [@byrne_structural_2016-1].  The comparative fit index (CFI) is an incremental index, comparing the hypothesized model with the baseline model, and should be at least .90 and perhaps higher than .95 [@hu_cutoff_1999].  The root mean square error of approximation (RMSEA) takes into account the error of approximation in the population and expresses it per degree of freedom.  As such, the fit indicator considers the complexity of the model.  Ideal values are equal to or less than .05, values less than .08 represent reasonable fit, and values between .08 and .10 represent mediocre fit.  The standardized root mean residual (SRMR) is a standardized measure of the mean absolute covariance residual -- the overall difference between the observed and predicted correlations.  Values greater than .10 may indicate poor fit and inspection of residuals is then advised.  Researchers have advised caution when using these criteria as strict cutoffs, and other factors such as sample size and model complexity should be considered [@kline_principles_2015].

>AMS items each were loaded on their respective correlated factors. The configural model, which constrained only the relative configuration of variables in the model to be the same in both groups had adequate fit to the data: $\chi ^{2}(328) = 327.57, p = 0.496$, CFI = 1.000, SRMR = 0.034, RMSEA =  0.000, 90%CI(0.000, 0.018). The weak invariance model constrained the configuration of variables and all factor loadings to be constant across groups.  Fit indices were comparable to the configural model: $\chi ^{2}(344) = 353.00, p = 0.357$, CFI = 0.996, SRMR = 0.039,  RMSEA = 0.008, 90%CI(0.000, 0.020.  Invariance of the factor loadings was supported by the non-significant difference tests that assessed model similarity: $\chi_{D}^{2}(16) = 25.42, p = 0.063$; $\Delta CFI = 0.004$. In the strong invariance model, configuration, factor loadings, and indicator means/intercepts were constrained to be the same for each group.  Fit indices were less than ideal: $\chi ^{2}(360) = 412.75 p = 0.029$, CFI = 0.975, SRMR = 0.045  , RMSEA =0.019, CI90%(0.007 to 0.027). The difference tests that evaluated model similarity suggested there was factorial noninvariance: ($\chi_{D}^{2}(16) =  59.754, p = 0.01$; $\Delta CFI = 0.021$. Given that the $\chi_{D}^{2}$ test is statistically significant and the  $\Delta CFI 0.01$ we cannot claim strong invariance and we therefore do not test stricter models.  Because we found noninvariance at the strong level, we did not attempt to model strict invariance. 

>Overall, this analysis suggests that the factor structure of the AMS was stable for mild/moderate and severe/very severe levels of disability. Figure 1 provides an illustration of the factor structure. Tables 1 and 2 provide fit indices for each of the factor structures and a summary of the measurement invariance tests.


## Practice Problems
   
In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty The least complex is to change the random seed and rework the problem demonstrated in the lesson. The results *should* map onto the ones obtained in the lecture. 

The second option would be to adapt one of the codes in the [simluations chapter](#sims) to create two groups for which invariance testing would be appropriate for that measure.

As a third option, you are welcome to use data to which you have access and is suitable for invariance testing. In either case, you will be expected to:

* Specify, interpret, and write up preliminary results for CFA models that examine
  - entire sample (making no distinction between groups)
  - configural invariance
  - weak invariance
  - strong invariance
  - strict invariance
* Create an APA style results section with appropriate table(s) and figure(s)
* Talk about it with someone

### Problem #1:  Play around with this simulation.

Copy the script for the simulation and then change the number in "set.seed(211023)" from 211023 to something else. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go.

### Problem #2: Adapt one of the simulated data sets.

The [Simulations](#sims) includes simulated data from many of the research vignettes used in this volume. Using guidance provided in this lesson, adapt one of those simulations to include at least two groups for which invariance testing would be appropriate.


### Problem #3:  Try something entirely new.

Using data for which you have permission and access (e.g., IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository) complete the multi-group invariance testing process.


### Grading Rubric

|Assignment Component                    | Points Possible   | Points Earned|
|:-------------------------------------- |:----------------: |:------------:|
|1. Check and, if needed, format data |      5            |_____  |         
|2. Specify, evaluate, and interpret the CFA for the entire sample (making no distinction between groups). Write up the preliminary results.     |      5            |_____  |
|3. Specify, evaluate, and interpret the CFA for configural invariance. Write up the preliminary results.     |      5            |_____  |
|4. Specify, evaluate, and interpret the CFA for weak invariance. Conduct the analysis to compare fit the weak and configural models. Write up the preliminary results.     |      5            |_____  |
|5. Specify, evaluate, and interpret the CFA for strong invariance. Conduct the analysis to compare fit the strong and weak models. Write up the preliminary results.     |      5            |_____  |
|6. Specify, evaluate, and interpret the CFA for strict invariance. Conduct the analysis to compare fit the strict and strong models. Write up the preliminary results.     |      5            |_____  |
|7. Create an APA style results section. Do not report any invariance tests past the one that failed. Include a table(s) and figure(s)    |      10            |_____  |
|8. Explanation to grader                 |      5        |_____  |
|**Totals**                               |      45       |_____  |          

```{r include=FALSE}
sessionInfo()
```




